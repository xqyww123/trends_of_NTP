{"doi": "10.48550/arXiv.2410.15700", "date": "2024-10-21", "title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert Iteration on Large-Scale LEAN Problems", "authors": "Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, Kai Chen", "abstract": "Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. The\nmajor learning paradigm is expert iteration, which necessitates a pre-defined\ndataset comprising numerous mathematical problems. In this process, LLMs\nattempt to prove problems within the dataset and iteratively refine their\ncapabilities through self-training on the proofs they discover. We propose to\nuse large scale LEAN problem datasets Lean-workbook for expert iteration with\nmore than 20,000 CPU days. During expert iteration, we found log-linear trends\nbetween solved problem amount with proof length and CPU usage. We train a\ncritic model to select relatively easy problems for policy models to make\ntrials and guide the model to search for deeper proofs. InternLM2.5-StepProver\nachieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet,\nand Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the\nMiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus\nwhich shows a significant improvement compared to only 9.5% of problems proved\nwhen Lean-Workbook-Plus was released. We open-source our models and searched\nproofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.", "journal": ""}
{"doi": "10.48550/arXiv.2404.12534", "date": "2024-04-18", "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean", "authors": "Peiyang Song, Kaiyu Yang, Anima Anandkumar", "abstract": "Neural theorem proving combines large language models (LLMs) with proof\nassistants such as Lean, where the correctness of formal proofs can be\nrigorously verified, leaving no room for hallucination. With existing neural\ntheorem provers pretrained on a fixed collection of data and offering valuable\nsuggestions at times, it is challenging for them to continually prove novel\ntheorems in a fully autonomous mode, where human insights may be critical. In\nthis paper, we explore LLMs as copilots that assist humans in proving theorems.\nWe introduce Lean Copilot, an general framework for running LLM inference\nnatively in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nLean users can use our pretrained models or bring their own ones that run\neither locally (with or without GPUs) or on the cloud. Using Lean Copilot, we\nbuild LLM-based tools that suggest proof steps, complete proof goals, and\nselect relevant premises. Experimental results on the Mathematics in Lean\ntextbook demonstrate the effectiveness of our method compared to existing\nrule-based proof automation in Lean (aesop). When assisting humans, Lean\nCopilot requires only 2.08 manually-entered proof steps on average (3.86\nrequired by aesop); when automating the theorem proving process, Lean Copilot\nautomates 74.2% proof steps on average, 85% better than aesop (40.1%). We open\nsource all code and artifacts under a permissive MIT license to facilitate\nfurther research.", "journal": ""}
{"doi": "10.48550/arXiv.2502.07640", "date": "2025-02-11", "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving", "authors": "Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin", "abstract": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.", "journal": ""}
{"doi": "10.48550/arXiv.2202.01629", "date": "2022-02-03", "title": "Use and abuse of instance parameters in the Lean mathematical library", "authors": "Anne Baanen", "abstract": "The Lean mathematical library mathlib features extensive use of the typeclass\npattern for organising mathematical structures, based on Lean's mechanism of\ninstance parameters. Related mechanisms for typeclasses are available in other\nprovers including Agda, Coq and Isabelle with varying degrees of adoption. This\npaper analyses representative examples of design patterns involving instance\nparameters in the current Lean 3 version of mathlib, focussing on complications\narising at scale and how the mathlib community deals with them.", "journal": ""}
{"doi": "10.48550/arXiv.2405.10188", "date": "2024-05-16", "title": "Bridging Syntax and Semantics of Lean Expressions in E-Graphs", "authors": "Marcus Rossel, Andr\u00e9s Goens", "abstract": "Interactive theorem provers, like Isabelle/HOL, Coq and Lean, have expressive\nlanguages that allow the formalization of general mathematical objects and\nproofs. In this context, an important goal is to reduce the time and effort\nneeded to prove theorems. A significant means of achieving this is by improving\nproof automation. We have implemented an early prototype of proof automation\nfor equational reasoning in Lean by using equality saturation. To achieve this,\nwe need to bridge the gap between Lean's expression semantics and the\nsyntactically driven e-graphs in equality saturation. This involves handling\nbound variables, implicit typing, as well as Lean's definitional equality,\nwhich is more general than syntactic equality and involves notions like\n$\\alpha$-equivalence, $\\beta$-reduction, and $\\eta$-reduction. In this extended\nabstract, we highlight how we attempt to bridge this gap, and which challenges\nremain to be solved. Notably, while our techniques are partially unsound, the\nresulting proof automation remains sound by virtue of Lean's proof checking.", "journal": ""}
{"doi": "10.48550/arXiv.2312.14188", "date": "2023-12-20", "title": "Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method", "authors": "Rahul Vishwakarma, Subhankar Mishra", "abstract": "Theorem proving is a fundamental task in mathematics. With the advent of\nlarge language models (LLMs) and interactive theorem provers (ITPs) like Lean,\nthere has been growing interest in integrating LLMs and ITPs to automate\ntheorem proving. In this approach, the LLM generates proof steps (tactics), and\nthe ITP checks the applicability of the tactics at the current goal. The two\nsystems work together to complete the proof. In this paper, we introduce\nDS-Prover, a novel dynamic sampling method for theorem proving. This method\ndynamically determines the number of tactics to apply to expand the current\ngoal, taking into account the remaining time compared to the total allocated\ntime for proving a theorem. This makes the proof search process more efficient\nby adjusting the balance between exploration and exploitation as time passes.\nWe also augment the training dataset by decomposing simplification and rewrite\ntactics with multiple premises into tactics with single premises. This gives\nthe model more examples to learn from and helps it to predict the tactics with\npremises more accurately. We perform our experiments using the Mathlib dataset\nof the Lean theorem prover and report the performance on two standard datasets,\nMiniF2F and ProofNet. Our methods achieve significant performance gains on both\ndatasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the\nProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the\nbest-reported Pass@1 of 29.6% using Lean.", "journal": ""}
{"doi": "10.48550/arXiv.2203.09835", "date": "2022-03-18", "title": "Reliably Reproducing Machine-Checked Proofs with the Coq Platform", "authors": "Karl Palmskog, Enrico Tassi, Th\u00e9o Zimmermann", "abstract": "The Coq Platform is a continuously developed distribution of the Coq proof\nassistant together with commonly used libraries, plugins, and external tools\nuseful in Coq-based formal verification projects. The Coq Platform enables\nreproducing and extending Coq artifacts in research, education, and industry,\ne.g., formalized mathematics and verified software systems. In this paper, we\ndescribe the background and motivation for the Platform, and outline its\norganization and development process. We also compare the Coq Platform to\nsimilar distributions and processes in the proof assistant community, such as\nfor Isabelle and Lean, and in the wider open source software community.", "journal": ""}
{"doi": "10.48550/arXiv.2405.14333", "date": "2024-05-23", "title": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data", "authors": "Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang", "abstract": "Proof assistants like Lean have revolutionized mathematical proof\nverification, ensuring high accuracy and reliability. Although large language\nmodels (LLMs) show promise in mathematical reasoning, their advancement in\nformal theorem proving is hindered by a lack of training data. To address this\nissue, we introduce an approach to generate extensive Lean 4 proof data derived\nfrom high-school and undergraduate-level mathematical competition problems.\nThis approach involves translating natural language problems into formal\nstatements, filtering out low-quality statements, and generating proofs to\ncreate synthetic data. After fine-tuning the DeepSeekMath 7B model on this\nsynthetic dataset, which comprises 8 million formal statements with proofs, our\nmodel achieved whole-proof generation accuracies of 46.3% with 64 samples and\n52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at\n23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.\nAdditionally, our model successfully proved 5 out of 148 problems in the Lean 4\nFormalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4\nfailed to prove any. These results demonstrate the potential of leveraging\nlarge-scale synthetic data to enhance theorem-proving capabilities in LLMs.\nBoth the synthetic dataset and the model will be made available to facilitate\nfurther research in this promising field.", "journal": ""}
{"doi": "10.48550/arXiv.2411.18872", "date": "2024-11-28", "title": "A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems", "authors": "Roozbeh Yousefzadeh, Xuenan Cao, Azim Ospanov", "abstract": "Using AI to write formal proofs for mathematical problems is a challenging\ntask that has seen some advancements in recent years. Automated systems such as\nLean can verify the correctness of proofs written in formal language, yet\nwriting the proofs in formal language can be challenging for humans and\nmachines. The miniF2F benchmark has 20 IMO problems in its test set, yet formal\nproofs are available only for 6 of these problems (3 of which are only written\nby mathematicians). The model with best accuracy can only prove 2 of these 20\nIMO problems, from 1950s and 60s, while its training set is a secret. In this\nwork, we write complete, original formal proofs for the remaining IMO problems\nin Lean along with 3 extra problems from IMO 2022 and 2023. This effort expands\nthe availability of proof currently in the public domain by creating 5,880\nlines of Lean proof. The goal of the paper is to pave the way for developing AI\nmodels that can automatically write the formal proofs for all the IMO problems\nin miniF2F and beyond by providing an evaluation benchmark. In this pursuit, we\ndevise a method to decompose the proofs of these problems into their building\nblocks, constructing a dataset of 1,329 lemmas with more than 40k lines of Lean\ncode. These lemmas are not trivial, yet they are approachable, providing the\nopportunity to evaluate and diagnose the failures and successes of AI models.\nWe evaluate the ability of the SOTA LLMs on our dataset and analyze their\nsuccess and failure modes from different perspectives. Our dataset and code is\navailable at: https://github.com/roozbeh-yz/IMO-Steps.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17925", "date": "2025-02-25", "title": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction", "authors": "Suozhi Huang, Peiyang Song, Robert Joseph George, Anima Anandkumar", "abstract": "Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies.", "journal": ""}
{"doi": "10.48550/arXiv.2502.04671", "date": "2025-02-07", "title": "ProofWala: Multilingual Proof Data Synthesis and Theorem-Proving", "authors": "Amitayush Thakur, George Tsoukalas, Greg Durrett, Swarat Chaudhuri", "abstract": "Neural networks have shown substantial promise at automatic theorem-proving\nin interactive proof assistants (ITPs) like Lean and Coq. However, most neural\ntheorem-proving models are restricted to specific ITPs, leaving out\nopportunities for cross-lingual $\\textit{transfer}$ between ITPs. We address\nthis weakness with a multilingual proof framework, ${\\rm P{\\small ROOF}W{\\small\nALA}}$, that allows a standardized form of interaction between neural\ntheorem-provers and two established ITPs (Coq and Lean). It enables the\ncollection of multilingual proof step data -- data recording the result of\nproof actions on ITP states -- for training neural provers. ${\\rm P{\\small\nROOF}W{\\small ALA}}$ allows the systematic evaluation of a model's performance\nacross different ITPs and problem domains via efficient parallel proof search\nalgorithms. We show that multilingual training enabled by ${\\rm P{\\small\nROOF}W{\\small ALA}}$ can lead to successful transfer across ITPs. Specifically,\na model trained on a mix of ${\\rm P{\\small ROOF}W{\\small ALA}}$-generated Coq\nand Lean data outperforms Lean-only and Coq-only models on the standard\nprove-at-$k$ metric. We open source all code including code for the ${\\rm\nP{\\small ROOF}W{\\small ALA}}$ Framework\n(https://github.com/trishullab/proof-wala), and the Multilingual ITP\ninteraction framework (https://github.com/trishullab/itp-interface).", "journal": ""}
{"doi": "10.48550/arXiv.2502.15507", "date": "2025-02-21", "title": "Activation Steering in Neural Theorem Provers", "authors": "Shashank Kirtania", "abstract": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.", "journal": ""}
{"doi": "10.48550/arXiv.2406.03847", "date": "2024-06-06", "title": "Lean Workbook: A large-scale Lean problem set formalized from natural language math problems", "authors": "Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen", "abstract": "Large language models have demonstrated impressive capabilities across\nvarious natural language processing tasks, especially in solving mathematical\nproblems. However, large language models are not good at math theorem proving\nusing formal languages like Lean. A significant challenge in this area is the\nscarcity of training data available in these formal languages. To address this\nissue, we propose a novel pipeline that iteratively generates and filters\nsynthetic data to translate natural language mathematical problems into Lean 4\nstatements, and vice versa. Our results indicate that the synthetic data\npipeline can provide useful training data and improve the performance of LLMs\nin translating and understanding complex mathematical problems and proofs. Our\nfinal dataset contains about 57K formal-informal question pairs along with\nsearched proof from the math contest forum and 21 new IMO questions. We\nopen-source our code at https://github.com/InternLM/InternLM-Math and our data\nat https://huggingface.co/datasets/InternLM/Lean-Workbook.", "journal": ""}
{"doi": "10.48550/arXiv.2502.00212", "date": "2025-01-31", "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving", "authors": "Kefan Dong, Tengyu Ma", "abstract": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens\ngenerated during the training in Lean, STP proves 26.3% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (61.7%, pass@3200),\nProofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@3200).", "journal": ""}
{"doi": "10.48550/arXiv.2410.10878", "date": "2024-10-09", "title": "Herald: A Natural Language Annotated Lean 4 Dataset", "authors": "Guoxiong Gao, Yutong Wang, Jiedong Jiang, Qi Gao, Zihan Qin, Tianyi Xu, Bin Dong", "abstract": "Verifiable formal languages like Lean have profoundly impacted mathematical\nreasoning, particularly through the use of large language models (LLMs) for\nautomated reasoning. A significant challenge in training LLMs for these formal\nlanguages is the lack of parallel datasets that align natural language with\nformal language proofs. To address this challenge, this paper introduces a\nnovel framework for translating the Mathlib4 corpus (a unified library of\nmathematics in formal language Lean 4) into natural language. Building upon\nthis, we employ a dual augmentation strategy that combines tactic-based and\ninformal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer.\nWe present the results of this pipeline on Mathlib4 as Herald (Hierarchy and\nRetrieval-based Translated Lean Dataset). We also propose the Herald\nTranslator, which is fine-tuned on Herald. Herald translator achieves a 93.2%\naccuracy (Pass@128) on formalizing statements in the miniF2F-test and a 22.5%\naccuracy on our internal graduate-level textbook dataset, outperforming\nInternLM2-Math-Plus-7B (74.0% and 7.5%) and TheoremLlama (50.1% and 4.0%).\nFurthermore, we propose a section-level translation framework for real-world\napplications. As a direct application of Herald translator, we have\nsuccessfully translated a template section in the Stack project, marking a\nnotable progress in the automatic formalization of graduate-level mathematical\nliterature. Our model, along with the datasets, are open-sourced to the public.", "journal": ""}
{"doi": "10.48550/arXiv.1905.09381", "date": "2019-05-21", "title": "Learning to Prove Theorems via Interacting with Proof Assistants", "authors": "Kaiyu Yang, Jia Deng", "abstract": "Humans prove theorems by relying on substantial high-level reasoning and\nproblem-specific insights. Proof assistants offer a formalism that resembles\nhuman mathematical reasoning, representing theorems in higher-order logic and\nproofs as high-level tactics. However, human experts have to construct proofs\nmanually by entering tactics into the proof assistant. In this paper, we study\nthe problem of using machine learning to automate the interaction with proof\nassistants. We construct CoqGym, a large-scale dataset and learning environment\ncontaining 71K human-written proofs from 123 projects developed with the Coq\nproof assistant. We develop ASTactic, a deep learning-based model that\ngenerates tactics as programs in the form of abstract syntax trees (ASTs).\nExperiments show that ASTactic trained on CoqGym can generate effective tactics\nand can be used to prove new theorems not previously provable by automated\nmethods. Code is available at https://github.com/princeton-vl/CoqGym.", "journal": ""}
{"doi": "10.48550/arXiv.2310.04353", "date": "2023-10-06", "title": "An In-Context Learning Agent for Formal Theorem-Proving", "authors": "Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, Swarat Chaudhuri", "abstract": "We present an in-context learning agent for formal theorem-proving in\nenvironments like Lean and Coq. Current state-of-the-art models for the problem\nare finetuned on environment-specific proof data. By contrast, our approach,\ncalled COPRA, repeatedly asks a high-capacity, general-purpose large language\nmodel (GPT-4) to propose tactic applications from within a stateful\nbacktracking search. Proposed tactics are executed in the underlying proof\nenvironment. Feedback from the execution is used to build the prompt for the\nnext model query, along with selected information from the search history and\nlemmas retrieved from an external database. We evaluate our implementation of\nCOPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the\nCompCert project. On these benchmarks, COPRA significantly outperforms few-shot\ninvocations of GPT-4. It also compares favorably against finetuning-based\napproaches, outperforming ReProver, a state-of-the-art finetuned approach for\nLean, in terms of the pass@1 metric. Our code and data are available at\nhttps://github.com/trishullab/copra.", "journal": ""}
{"doi": "10.48550/arXiv.2407.03203", "date": "2024-07-03", "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts", "authors": "Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang", "abstract": "Proving mathematical theorems using computer-verifiable formal languages like\nLean significantly impacts mathematical reasoning. One approach to formal\ntheorem proving involves generating complete proofs using Large Language Models\n(LLMs) based on Natural Language (NL) proofs. However, due to the scarcity of\naligned NL and Formal Language (FL) theorem-proving data most modern LLMs\nexhibit suboptimal performance.This scarcity results in a paucity of\nmethodologies for training LLMs and techniques to fully utilize their\ncapabilities in composing formal proofs. To address these challenges, this\npaper proposes TheoremLlama, an end-to-end framework that trains a\ngeneral-purpose LLM to be a Lean4 expert. TheoremLlama includes NL-FL dataset\ngeneration and bootstrapping method to obtain aligned dataset, curriculum\nlearning and block training techniques to train the model, and iterative proof\nwriting method to write Lean4 proofs that work together synergistically. Using\nthe dataset generation method in TheoremLlama, we provide Open Bootstrapped\nTheorems (OBT), an NL-FL aligned and bootstrapped dataset. Our novel NL-FL\nbootstrapping method, where NL proofs are integrated into Lean4 code for\ntraining datasets, leverages the NL reasoning ability of LLMs for formal\nreasoning. The TheoremLlama framework achieves cumulative accuracies of 36.48%\nand 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the\nGPT-4 baseline of 22.95% and 25.41%. Our code, model checkpoints, and the\ngenerated dataset is published in GitHub", "journal": ""}
{"doi": "10.48550/arXiv.2310.18457", "date": "2023-10-27", "title": "LLMSTEP: LLM proofstep suggestions in Lean", "authors": "Sean Welleck, Rahul Saha", "abstract": "We present LLMSTEP, a tool for integrating a language model into the Lean\nproof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to\na server hosting a language model. The language model generates suggestions,\nwhich are checked in Lean and displayed to a user in their development\nenvironment. We provide a baseline language model, along with code for\nfine-tuning and evaluation to support further development. We provide server\nimplementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a\nstep towards fast, effective language model suggestions for any user.", "journal": ""}
{"doi": "10.48550/arXiv.2408.11172", "date": "2024-08-20", "title": "SubgoalXL: Subgoal-based Expert Learning for Theorem Proving", "authors": "Xueliang Zhao, Lin Zheng, Haige Bo, Changran Hu, Urmish Thakker, Lingpeng Kong", "abstract": "Formal theorem proving, a field at the intersection of mathematics and\ncomputer science, has seen renewed interest with advancements in large language\nmodels (LLMs). This paper introduces SubgoalXL, a novel approach that\nsynergizes subgoal-based proofs with expert learning to enhance LLMs'\ncapabilities in formal theorem proving within the Isabelle environment.\nSubgoalXL addresses two critical challenges: the scarcity of specialized\nmathematics and theorem-proving data, and the need for improved multi-step\nreasoning abilities in LLMs. By optimizing data efficiency and employing\nsubgoal-level supervision, SubgoalXL extracts richer information from limited\nhuman-generated proofs. The framework integrates subgoal-oriented proof\nstrategies with an expert learning system, iteratively refining formal\nstatement, proof, and subgoal generators. Leveraging the Isabelle environment's\nadvantages in subgoal-based proofs, SubgoalXL achieves a new state-of-the-art\nperformance of 56.1\\% in Isabelle on the standard miniF2F dataset, marking an\nabsolute improvement of 4.9\\%. Notably, SubgoalXL successfully solves 41 AMC12,\n9 AIME, and 3 IMO problems from miniF2F. These results underscore the\neffectiveness of maximizing limited data utility and employing targeted\nguidance for complex reasoning in formal theorem proving, contributing to the\nongoing advancement of AI reasoning capabilities. The implementation is\navailable at \\url{https://github.com/zhaoxlpku/SubgoalXL}.", "journal": ""}
{"doi": "10.48550/arXiv.2503.03205", "date": "2025-03-05", "title": "MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving", "authors": "Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang", "abstract": "Solving mathematical problems using computer-verifiable languages like Lean\nhas significantly impacted mathematical and computer science communities.\nState-of-the-art methods utilize single Large Language Models (LLMs) as agents\nor provers to either generate complete proof or perform tree searches. However,\nsingle-agent methods inherently lack a structured way to combine high-level\nreasoning in Natural Language (NL) with Formal Language (FL) verification\nfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long\nChain-of-Thought framework, (to the best of our knowledge), the first\nmulti-agent framework for Lean4 theorem proving that balance high-level NL\nreasoning and FL verification in Long CoT. Using this structured interaction,\nour approach enables deeper insights and long-term coherence in proof\ngeneration, with which past methods struggle. We do this by leveraging emergent\nformal reasoning ability in Long CoT using our novel LoT-Transfer Learning\ntraining-inference pipeline. Extensive experiments show that our framework\nachieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,\nlargely outperforming GPT-4 (22.95%), single-agent tree search\n(InternLM-Step-Prover, 50.70%), and whole-proof generation\n(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight\nthe potential of combining Long CoT with formal verification for a more\ninsightful generation in a broader perspective.", "journal": ""}
{"doi": "10.48550/arXiv.2402.06332", "date": "2024-02-09", "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning", "authors": "Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin", "abstract": "The math abilities of large language models can represent their abstract\nreasoning ability. In this paper, we introduce and open-source our math\nreasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We\nunify chain-of-thought reasoning, reward modeling, formal reasoning, data\naugmentation, and code interpreter in a unified seq2seq format and supervise\nour model to be a versatile math reasoner, verifier, prover, and augmenter.\nThese abilities can be used to develop the next math LLMs or self-iteration.\nInternLM-Math obtains open-sourced state-of-the-art performance under the\nsetting of in-context learning, supervised fine-tuning, and code-assisted\nreasoning in various informal and formal benchmarks including GSM8K, MATH,\nHungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves\n30.3 on the MiniF2F test set without fine-tuning. We further explore how to use\nLEAN to solve math problems and study its performance under the setting of\nmulti-task learning which shows the possibility of using LEAN as a unified\nplatform for solving and proving in math. Our models, codes, and data are\nreleased at \\url{https://github.com/InternLM/InternLM-Math}.", "journal": ""}
{"doi": "10.48550/arXiv.2109.00110", "date": "2021-08-31", "title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics", "authors": "Kunhao Zheng, Jesse Michael Han, Stanislas Polu", "abstract": "We present miniF2F, a dataset of formal Olympiad-level mathematics problems\nstatements intended to provide a unified cross-system benchmark for neural\ntheorem proving. The miniF2F benchmark currently targets Metamath, Lean,\nIsabelle (partially) and HOL Light (partially) and consists of 488 problem\nstatements drawn from the AIME, AMC, and the International Mathematical\nOlympiad (IMO), as well as material from high-school and undergraduate\nmathematics courses. We report baseline results using GPT-f, a neural theorem\nprover based on GPT-3 and provide an analysis of its performance. We intend for\nminiF2F to be a community-driven effort and hope that our benchmark will help\nspur advances in neural theorem proving.", "journal": ""}
{"doi": "10.48550/arXiv.2403.12733", "date": "2024-03-19", "title": "Small Scale Reflection for the Working Lean User", "authors": "Vladimir Gladshtein, George P\u00eerlea, Ilya Sergey", "abstract": "We present the design and implementation of the Small Scale Reflection proof\nmethodology and tactic language (a.k.a. SSR) for the Lean 4 proof assistant.\nLike its Coq predecessor SSReflect, our Lean 4 implementation, dubbed LeanSSR,\nprovides powerful rewriting principles and means for effective management of\nhypotheses in the proof context. Unlike SSReflect for Coq, LeanSSR does not\nrequire explicit switching between the logical and symbolic representation of a\ngoal, allowing for even more concise proof scripts that seamlessly combine\ndeduction steps with proofs by computation.\n  In this paper, we first provide a gentle introduction to the principles of\nstructuring mechanised proofs using LeanSSR. Next, we show how the native\nsupport for metaprogramming in Lean 4 makes it possible to develop LeanSSR\nentirely within the proof assistant, greatly improving the overall experience\nof both tactic implementers and proof engineers. Finally, we demonstrate the\nutility of LeanSSR by conducting two case studies: (a) porting a collection of\nCoq lemmas about sequences from the widely used Mathematical Components library\nand (b) reimplementing proofs in the finite set library of Lean's mathlib4.\nBoth case studies show significant reduction in proof sizes.", "journal": ""}
{"doi": "10.48550/arXiv.2306.15626", "date": "2023-06-27", "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models", "authors": "Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar", "abstract": "Large language models (LLMs) have shown promise in proving formal theorems\nusing proof assistants such as Lean. However, existing methods are difficult to\nreproduce or build on, due to private code, data, and large compute\nrequirements. This has created substantial barriers to research on machine\nlearning methods for theorem proving. This paper removes these barriers by\nintroducing LeanDojo: an open-source Lean playground consisting of toolkits,\ndata, models, and benchmarks. LeanDojo extracts data from Lean and enables\ninteraction with the proof environment programmatically. It contains\nfine-grained annotations of premises in proofs, providing valuable data for\npremise selection: a key bottleneck in theorem proving. Using this data, we\ndevelop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented\nwith retrieval for selecting premises from a vast math library. It is\ninexpensive and needs only one GPU week of training. Our retriever leverages\nLeanDojo's program analysis capability to identify accessible premises and hard\nnegative examples, which makes retrieval much more effective. Furthermore, we\nconstruct a new benchmark consisting of 98,734 theorems and proofs extracted\nfrom Lean's math library. It features challenging data split requiring the\nprover to generalize to theorems relying on novel premises that are never used\nin training. We use this benchmark for training and evaluation, and\nexperimental results demonstrate the effectiveness of ReProver over\nnon-retrieval baselines and GPT-4. We thus provide the first set of open-source\nLLM-based theorem provers without any proprietary datasets and release it under\na permissive MIT license to facilitate further research.", "journal": ""}
{"doi": "10.48550/arXiv.2002.09282", "date": "2020-02-14", "title": "Homotopy Type Theory in Isabelle", "authors": "Joshua Chen", "abstract": "This paper introduces Isabelle/HoTT, the first development of homotopy type\ntheory in the Isabelle proof assistant. Building on earlier work by Paulson, I\nuse Isabelle's existing logical framework infrastructure to implement essential\nautomation, such as type checking and term elaboration, that is usually handled\non the source code level of dependently typed systems. I also integrate the\npropositions-as-types paradigm with the declarative Isar proof language,\nproviding an alternative to the tactic-based proofs of Coq and the proof terms\nof Agda. The infrastructure developed is then used to formalize foundational\nresults from the Homotopy Type Theory book.", "journal": ""}
{"doi": "10.48550/arXiv.2501.18310", "date": "2025-01-30", "title": "Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis", "authors": "Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao", "abstract": "The synergy between deep learning models and traditional automation tools\nplays a pivotal role in developing robust neural theorem provers (NTPs).\nHowever, for proof synthesis with LLMs, previous work applies automation tools\neither only when the model explicitly calls the method, or only at a single\ngranularity level, failing to fully exploit the power of built-in tactics and\noff-the-shelf automated theorem provers. In this work, we propose ProofAug, a\nnovel theorem proving method that enjoys superior sample efficiency through\nequipping proof-generation LLMs with automation methods in different\ngranularities via fine-grained structure analysis of model-generated proof\nproposals. Furthermore, ProofAug serves as a versatile plug-and-play module\nthat seamlessly integrates with any tree-search algorithm, enabling our\nconstruction of an efficient recursive proving (ERP) module to further enhance\nperformance. The superiority of our method is validated on the miniF2F-test\nbenchmark using the open-source deepseek-math-7b-base model and the Isabelle\nproof assistant. Notably, by additionally employing a mixed prompting strategy,\nwe achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%\nfor the original version), setting a new SOTA across all proof languages with a\ntotal sample budget of only 2100. Our code is available at\nhttps://github.com/haoxiongliu/ProofAug.", "journal": ""}
{"doi": "10.48550/arXiv.2407.11214", "date": "2024-07-15", "title": "PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition", "authors": "George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, Swarat Chaudhuri", "abstract": "We present PutnamBench, a new multi-language benchmark for evaluating the\nability of neural theorem-provers to solve competition mathematics problems.\nPutnamBench consists of 1692 hand-constructed formalizations of 640 theorems\nsourced from the William Lowell Putnam Mathematical Competition, the premier\nundergraduate-level mathematics competition in North America. All the problems\nhave formalizations in Lean 4 and Isabelle; a substantial subset also has Coq\nformalizations. PutnamBench requires significant problem-solving ability and\nproficiency in a broad range of topics taught in undergraduate mathematics\ncourses. We use PutnamBench to evaluate several established neural and symbolic\ntheorem-provers. These approaches can only solve a handful of the PutnamBench\nproblems, establishing the benchmark as a difficult open challenge for research\non neural theorem-proving. PutnamBench is available at\nhttps://github.com/trishullab/PutnamBench.", "journal": ""}
{"doi": "10.48550/arXiv.2410.16429", "date": "2024-10-21", "title": "Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4", "authors": "Leni Aniva, Chuyue Sun, Brando Miranda, Clark Barrett, Sanmi Koyejo", "abstract": "Machine-assisted theorem proving refers to the process of conducting\nstructured reasoning to automatically generate proofs for mathematical\ntheorems. Recently, there has been a surge of interest in using machine\nlearning models in conjunction with proof assistants to perform this task. In\nthis paper, we introduce Pantograph, a tool that provides a versatile interface\nto the Lean 4 proof assistant and enables efficient proof search via powerful\nsearch algorithms such as Monte Carlo Tree Search. In addition, Pantograph\nenables high-level reasoning by enabling a more robust handling of Lean 4's\ninference steps. We provide an overview of Pantograph's architecture and\nfeatures. We also report on an illustrative use case: using machine learning\nmodels and proof sketches to prove Lean 4 theorems. Pantograph's innovative\nfeatures pave the way for more advanced machine learning models to perform\ncomplex proof searches and high-level reasoning, equipping future researchers\nto design more versatile and powerful theorem provers.", "journal": ""}
{"doi": "10.48550/arXiv.2402.16878", "date": "2024-02-12", "title": "EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages", "authors": "Johnathan Mercer", "abstract": "Formal mathematics is the discipline of translating mathematics into a\nprogramming language in which any statement can be unequivocally checked by a\ncomputer. Mathematicians and computer scientists have spent decades of\npainstaking formalization efforts developing languages such as Coq, HOL, and\nLean. Machine learning research has converged on these formal math corpora and\ngiven rise to an assortment of methodologies to aid in interactive and\nautomated theorem proving. However, these papers have primarily focused on one\nmethod, for one proof task, in one language. This paper introduces EvoGPT-f: a\nnovel evolutionary framework for the first systematic quantitative analysis of\nthe differential machine learnability of five formal math corpora (Lean 3, Lean\n4, Coq, HOL 4, HOL Light) using four tokenization methods (character,\nword-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not\nput to rest the question of the \"best\" or \"easiest\" language to learn. Rather,\nthis framework and preliminary findings begin to illuminate the differential\nmachine learnability of these languages, offering a foundation to forge more\nsystematic quantitative and qualitative comparative research across\ncommunities.", "journal": ""}
{"doi": "10.48550/arXiv.2406.11915", "date": "2024-06-16", "title": "miniCodeProps: a Minimal Benchmark for Proving Code Properties", "authors": "Evan Lohn, Sean Welleck", "abstract": "AI agents have shown initial promise in automating mathematical theorem\nproving in proof assistants such as Lean. The same proof assistants can be used\nto verify the correctness of code by pairing code with specifications and\nproofs that the specifications hold. Automating the writing of code,\nspecifications, and proofs could lower the cost of verification, or,\nambitiously, enable an AI agent to output safe, provably correct code. However,\nit remains unclear whether current neural theorem provers can automatically\nverify even relatively simple programs. We present miniCodeProps, a benchmark\nof 201 program specifications in the Lean proof assistant, aimed at the\nsubproblem of automatically generating a proof for a provided program and\nspecification. miniCodeProps contains specifications about simple,\nself-contained programs (e.g., lists, natural numbers, binary trees) with\nvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient to\nbreak current LLM-based provers, with state-of-the-art methods showing promise\non the easy properties in miniCodeProps, yet failing to prove nearly all of the\nmedium and hard properties. We publicly release miniCodeProps as a benchmark\nfor furthering automated theorem proving in the context of formally verified\ncode.", "journal": ""}
{"doi": "10.48550/arXiv.1908.09479", "date": "2019-08-26", "title": "EKSTRAKTO A tool to reconstruct Dedukti proofs from TSTP files (extended abstract)", "authors": "Mohamed Yacine El Haddad, Guillaume Burel, Fr\u00e9d\u00e9ric Blanqui", "abstract": "Proof assistants often call automated theorem provers to prove subgoals.\nHowever, each prover has its own proof calculus and the proof traces that it\nproduces often lack many details to build a complete proof. Hence these traces\nare hard to check and reuse in proof assistants. Dedukti is a proof checker\nwhose proofs can be translated to various proof assistants: Coq, HOL, Lean,\nMatita, PVS. We implemented a tool that extracts TPTP subproblems from a TSTP\nfile and reconstructs complete proofs in Dedukti using automated provers able\nto generate Dedukti proofs like ZenonModulo or ArchSAT. This tool is generic:\nit assumes nothing about the proof calculus of the prover producing the trace,\nand it can use different provers to produce the Dedukti proof. We applied our\ntool on traces produced by automated theorem provers on the CNF problems of the\nTPTP library and we were able to reconstruct a proof for a large proportion of\nthem, significantly increasing the number of Dedukti proofs that could be\nobtained for those problems.", "journal": "EPTCS 301, 2019, pp. 27-35"}
{"doi": "10.48550/arXiv.2407.10040", "date": "2024-07-14", "title": "Lean-STaR: Learning to Interleave Thinking and Proving", "authors": "Haohan Lin, Zhiqing Sun, Sean Welleck, Yiming Yang", "abstract": "Traditional language model-based theorem proving assumes that by training on\na sufficient amount of formal proof data, a model will learn to prove theorems.\nOur key observation is that a wealth of informal information that is not\npresent in formal proofs can be useful for learning to prove theorems. For\ninstance, humans think through steps of a proof, but this thought process is\nnot visible in the resulting code. We present Lean-STaR, a framework for\ntraining language models to produce informal thoughts prior to each step of a\nproof, thereby boosting the model's theorem-proving capabilities. Lean-STaR\nuses retrospective ground-truth tactics to generate synthetic thoughts for\ntraining the language model. At inference time, the trained model directly\ngenerates the thoughts prior to the prediction of the tactics in each proof\nstep. Building on the self-taught reasoner framework, we then apply expert\niteration to further fine-tune the model on the correct proofs it samples and\nverifies using the Lean solver. Lean-STaR achieves state-of-the-art results on\nthe miniF2F-test benchmark within the Lean theorem proving environment,\nsignificantly outperforming base models ($\\boldsymbol{43.4\\% \\rightarrow\n46.3\\%,}$ Pass@64). We also analyze the impact of the augmented thoughts on\nvarious aspects of the theorem proving process, providing insights into their\neffectiveness.", "journal": ""}
{"doi": "10.48550/arXiv.2410.04753", "date": "2024-10-07", "title": "ImProver: Agent-Based Automated Proof Optimization", "authors": "Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, Sean Welleck", "abstract": "Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.", "journal": ""}
{"doi": "10.48550/arXiv.2403.12627", "date": "2024-03-19", "title": "Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code", "authors": "Andreas Florath", "abstract": "In the realm of formal theorem proving, the Coq proof assistant stands out\nfor its rigorous approach to verifying mathematical assertions and software\ncorrectness. Despite the advances in artificial intelligence and machine\nlearning, the specialized nature of Coq syntax and semantics poses unique\nchallenges for Large Language Models (LLMs). Addressing this gap, we present a\ncomprehensive dataset specifically designed to enhance LLMs' proficiency in\ninterpreting and generating Coq code. This dataset, derived from a collection\nof over 10,000 Coq source files, encompasses a wide array of propositions,\nproofs, and definitions, enriched with metadata including source references and\nlicensing information. Our primary aim is to facilitate the development of LLMs\ncapable of generating syntactically correct and semantically meaningful Coq\nconstructs, thereby advancing the frontier of automated theorem proving.\nInitial experiments with this dataset have showcased its significant potential;\nmodels trained on this data exhibited enhanced accuracy in Coq code generation.\nNotably, a particular experiment revealed that a fine-tuned LLM was capable of\ngenerating 141 valid proofs for a basic lemma, highlighting the dataset's\nutility in facilitating the discovery of diverse and valid proof strategies.\nThis paper discusses the dataset's composition, the methodology behind its\ncreation, and the implications of our findings for the future of machine\nlearning in formal verification. The dataset is accessible for further research\nand exploration:\nhttps://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1", "journal": ""}
{"doi": "10.48550/arXiv.2408.08152", "date": "2024-08-15", "title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search", "authors": "Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, Chong Ruan", "abstract": "We introduce DeepSeek-Prover-V1.5, an open-source language model designed for\ntheorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both\ntraining and inference processes. Pre-trained on DeepSeekMath-Base with\nspecialization in formal mathematical languages, the model undergoes supervised\nfine-tuning using an enhanced formal theorem proving dataset derived from\nDeepSeek-Prover-V1. Further refinement is achieved through reinforcement\nlearning from proof assistant feedback (RLPAF). Beyond the single-pass\nwhole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a\nvariant of Monte-Carlo tree search that employs an intrinsic-reward-driven\nexploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5\ndemonstrates significant improvements over DeepSeek-Prover-V1, achieving new\nstate-of-the-art results on the test set of the high school level miniF2F\nbenchmark ($63.5\\%$) and the undergraduate level ProofNet benchmark ($25.3\\%$).", "journal": ""}
{"doi": "10.48550/arXiv.2501.15797", "date": "2025-01-27", "title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models", "authors": "Tianbo Yang, Mingqi Yan, Hongyi Zhao, Tianshuo Yang", "abstract": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.", "journal": ""}
{"doi": "10.48550/arXiv.2409.05977", "date": "2024-09-09", "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different Fields in Lean 4", "authors": "Xichen Tang", "abstract": "Formalizing mathematical proofs using computerized verification languages\nlike Lean 4 has the potential to significantly impact the field of mathematics,\nit offers prominent capabilities for advancing mathematical reasoning. However,\nexisting efforts are largely limited to creating formalized versions of proofs\nfrom extensive online mathematical corpora, struggling to keep pace with the\nrapidly evolving nature of mathematics. To bridge the gap between traditional\nand computerized proof techniques, this paper explores the use of Large\nLanguage Models (LLMs) to generate formal proof steps and complete formalized\nproofs. By converting natural language (NL) mathematical proofs into formalized\nversions, this work introduces the basic structure and tactics of the Lean 4\nlanguage. The goal is to determine how AI can be leveraged to assist the\nmathematical formalization process and improve its performance. Several\nexamples are provided that demonstrate solving problems using both traditional\nand Lean 4-based approaches. Ultimately, this paper presents an explanation of\nthe foundations of Lean 4 and comparative analyses of the mathematical\nformalization process using traditional and AI-augmented techniques. The\nfindings indicate that AI- powered tools have significant potential to\naccelerate and enhance the formalization of mathematical proofs, paving the way\nfor more efficient and reliable theorem-proving for AI for Math in the future.", "journal": ""}
{"doi": "10.48550/arXiv.2410.06209", "date": "2024-10-08", "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving", "authors": "Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar", "abstract": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for formal theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully generates formal proofs for 155 theorems across 23 diverse Lean\nrepositories where formal proofs were previously missing, many from advanced\nmathematics. It performs significantly better than the static LLM baseline,\nproving challenging theorems in domains like abstract algebra and algebraic\ntopology while showcasing a clear progression of learning from basic concepts\nto advanced topics. In addition, we analyze LeanAgent's superior performance on\nkey lifelong learning metrics. LeanAgent achieves exceptional scores in\nstability and backward transfer, where learning new tasks improves performance\non previously learned tasks. This emphasizes LeanAgent's continuous\ngeneralizability and improvement, explaining its superior theorem-proving\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2201.00255", "date": "2022-01-01", "title": "The solutions to single-variable polynomials, implemented and verified in Lean", "authors": "Nicholas Dyson, Benedikt Ahrens, Jacopo Emmenegger", "abstract": "In this work, we describe our experience in learning the use of a computer\nproof assistant - specifically, Lean - from scratch, through proving formulae\nfor the solutions of polynomial equations. Specifically, in this work we\ncharacterize the solutions of quadratic, cubic, and quartic polynomials over\ncertain fields, specifically, fields with operations returning square and cubic\nroots of characteristic other than two or three. The purpose of this work is\nthus twofold. Firstly, it describes the learning experience of a starting Lean\nuser, including a detailed comparison between our work in Lean and very closely\nrelated work in Coq. Secondly, our results represent a modest improvement over\nthe aforementioned related work in Coq, which we hope will be of some\nscientific interest.", "journal": ""}
{"doi": "10.48550/arXiv.2104.09366", "date": "2021-04-19", "title": "Simple Type Theory is not too Simple: Grothendieck's Schemes without Dependent Types", "authors": "Anthony Bordg, Lawrence Paulson, Wenda Li", "abstract": "Church's simple type theory is often deemed too simple for elaborate\nmathematical constructions. In particular, doubts were raised whether schemes\ncould be formalized in this setting and a challenge was issued. Schemes are\nsophisticated mathematical objects in algebraic geometry introduced by\nAlexander Grothendieck in 1960. In this article we report on a successful\nformalization of schemes in the simple type theory of the proof assistant\nIsabelle/HOL, and we discuss the design choices which make this work possible.\nWe show in the particular case of schemes how the powerful dependent types of\nCoq or Lean can be traded for a minimalist apparatus called locales.", "journal": "Experimental Mathematics, 31:2 (2022), 364-382"}
{"doi": "10.48550/arXiv.2303.04488", "date": "2023-03-08", "title": "Magnushammer: A Transformer-Based Approach to Premise Selection", "authors": "Maciej Miku\u0142a, Szymon Tworkowski, Szymon Antoniak, Bartosz Piotrowski, Albert Qiaochu Jiang, Jin Peng Zhou, Christian Szegedy, \u0141ukasz Kuci\u0144ski, Piotr Mi\u0142o\u015b, Yuhuai Wu", "abstract": "This paper presents a novel approach to premise selection, a crucial\nreasoning task in automated theorem proving. Traditionally, symbolic methods\nthat rely on extensive domain knowledge and engineering effort are applied to\nthis task. In contrast, this work demonstrates that contrastive training with\nthe transformer architecture can achieve higher-quality retrieval of relevant\npremises, without the engineering overhead. Our method, Magnushammer,\noutperforms the most advanced and widely used automation tool in interactive\ntheorem proving called Sledgehammer. On the PISA and miniF2F benchmarks\nMagnushammer achieves $59.5\\%$ (against $38.3\\%$) and $34.0\\%$ (against\n$20.9\\%$) success rates, respectively. By combining \\method with a\nlanguage-model-based automated theorem prover, we further improve the\nstate-of-the-art proof success rate from $57.0\\%$ to $71.0\\%$ on the PISA\nbenchmark using $4$x fewer parameters. Moreover, we develop and open source a\nnovel dataset for premise selection, containing textual representations of\n(proof state, relevant premise) pairs. To the best of our knowledge, this is\nthe largest available premise selection dataset, and the first one for the\nIsabelle proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1709.02096", "date": "2017-09-07", "title": "An Existence Theorem of Nash Equilibrium in Coq and Isabelle", "authors": "St\u00e9phane Le Roux, \u00c9rik Martin-Dorel, Jan-Georg Smaus", "abstract": "Nash equilibrium (NE) is a central concept in game theory. Here we prove\nformally a published theorem on existence of an NE in two proof assistants, Coq\nand Isabelle: starting from a game with finitely many outcomes, one may derive\na game by rewriting each of these outcomes with either of two basic outcomes,\nnamely that Player 1 wins or that Player 2 wins. If all ways of deriving such a\nwin/lose game lead to a game where one player has a winning strategy, the\noriginal game also has a Nash equilibrium.\n  This article makes three other contributions: first, while the original proof\ninvoked linear extension of strict partial orders, here we avoid it by\ngeneralizing the relevant definition. Second, we notice that the theorem also\nimplies the existence of a secure equilibrium, a stronger version of NE that\nwas introduced for model checking. Third, we also notice that the constructive\nproof of the theorem computes secure equilibria for non-zero-sum priority games\n(generalizing parity games) in quasi-polynomial time.", "journal": "EPTCS 256, 2017, pp. 46-60"}
{"doi": "10.48550/arXiv.2405.17216", "date": "2024-05-27", "title": "Autoformalizing Euclidean Geometry", "authors": "Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, Xujie Si", "abstract": "Autoformalization involves automatically translating informal math into\nformal theorems and proofs that are machine-verifiable. Euclidean geometry\nprovides an interesting and controllable domain for studying autoformalization.\nIn this paper, we introduce a neuro-symbolic framework for autoformalizing\nEuclidean geometry, which combines domain knowledge, SMT solvers, and large\nlanguage models (LLMs). One challenge in Euclidean geometry is that informal\nproofs rely on diagrams, leaving gaps in texts that are hard to formalize. To\naddress this issue, we use theorem provers to fill in such diagrammatic\ninformation automatically, so that the LLM only needs to autoformalize the\nexplicit textual steps, making it easier for the model. We also provide\nautomatic semantic evaluation for autoformalized theorem statements. We\nconstruct LeanEuclid, an autoformalization benchmark consisting of problems\nfrom Euclid's Elements and the UniGeo dataset formalized in the Lean proof\nassistant. Experiments with GPT-4 and GPT-4V show the capability and\nlimitations of state-of-the-art LLMs on autoformalizing geometry problems. The\ndata and code are available at https://github.com/loganrjmurphy/LeanEuclid.", "journal": ""}
{"doi": "10.48550/arXiv.2408.03350", "date": "2024-08-05", "title": "miniCTX: Neural Theorem Proving with (Long-)Contexts", "authors": "Jiewen Hu, Thomas Zhu, Sean Welleck", "abstract": "Real-world formal theorem proving often depends on a wealth of context,\nincluding definitions, lemmas, comments, file structure, and other information.\nWe introduce miniCTX, which tests a model's ability to prove formal\nmathematical theorems that depend on new context that is not seen during\ntraining. miniCTX contains theorems sourced from real Lean projects and\ntextbooks, each associated with a context that can span tens of thousands of\ntokens. Models are tasked with proving a theorem given access to code from the\ntheorem's repository, which contains context that is needed for the proof. As a\nbaseline for miniCTX, we tested fine-tuning and prompting methods that\ncondition theorem proving on preceding context. Both approaches substantially\noutperform traditional methods that rely solely on state information. We found\nthat this ability to use context is not captured by previous benchmarks such as\nminiF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting\nand annotating theorem proving data, making it easy to add new projects into\nminiCTX to ensure that contexts are not seen during training. miniCTX offers a\nchallenging and realistic evaluation of neural theorem provers.", "journal": ""}
{"doi": "10.48550/arXiv.2205.12615", "date": "2022-05-25", "title": "Autoformalization with Large Language Models", "authors": "Yuhuai Wu, Albert Q. Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, Christian Szegedy", "abstract": "Autoformalization is the process of automatically translating from natural\nlanguage mathematics to formal specifications and proofs. A successful\nautoformalization system could advance the fields of formal verification,\nprogram synthesis, and artificial intelligence. While the long-term goal of\nautoformalization seemed elusive for a long time, we show large language models\nprovide new prospects towards this goal. We make the surprising observation\nthat LLMs can correctly translate a significant portion ($25.3\\%$) of\nmathematical competition problems perfectly to formal specifications in\nIsabelle/HOL. We demonstrate the usefulness of this process by improving a\npreviously introduced neural theorem prover via training on these\nautoformalized theorems. Our methodology results in a new state-of-the-art\nresult on the MiniF2F theorem proving benchmark, improving the proof rate from\n$29.6\\%$ to $35.2\\%$.", "journal": ""}
{"doi": "10.48550/arXiv.2304.00994", "date": "2023-03-17", "title": "Machine-Learned Premise Selection for Lean", "authors": "Bartosz Piotrowski, Ramon Fern\u00e1ndez Mir, Edward Ayers", "abstract": "We introduce a machine-learning-based tool for the Lean proof assistant that\nsuggests relevant premises for theorems being proved by a user. The design\nprinciples for the tool are (1) tight integration with the proof assistant, (2)\nease of use and installation, (3) a lightweight and fast approach. For this\npurpose, we designed a custom version of the random forest model, trained in an\nonline fashion. It is implemented directly in Lean, which was possible thanks\nto the rich and efficient metaprogramming features of Lean 4. The random forest\nis trained on data extracted from mathlib -- Lean's mathematics library. We\nexperiment with various options for producing training features and labels. The\nadvice from a trained model is accessible to the user via the suggest_premises\ntactic which can be called in an editor while constructing a proof\ninteractively.", "journal": ""}
{"doi": "10.48550/arXiv.2102.06203", "date": "2021-02-11", "title": "Proof Artifact Co-training for Theorem Proving with Language Models", "authors": "Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W. Ayers, Stanislas Polu", "abstract": "Labeled data for imitation learning of theorem proving in large libraries of\nformalized mathematics is scarce as such libraries require years of\nconcentrated effort by human specialists to be built. This is particularly\nchallenging when applying large Transformer language models to tactic\nprediction, because the scaling of performance with respect to model size is\nquickly disrupted in the data-scarce, easily-overfitted regime. We propose PACT\n({\\bf P}roof {\\bf A}rtifact {\\bf C}o-{\\bf T}raining), a general methodology for\nextracting abundant self-supervised data from kernel-level proof terms for\nco-training alongside the usual tactic prediction objective. We apply this\nmethodology to Lean, an interactive proof assistant which hosts some of the\nmost sophisticated formalized mathematics to date. We instrument Lean with a\nneural theorem prover driven by a Transformer language model and show that PACT\nimproves theorem proving success rate on a held-out suite of test theorems from\n32\\% to 48\\%.", "journal": ""}
{"doi": "10.48550/arXiv.2402.08957", "date": "2024-02-14", "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data", "authors": "Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang", "abstract": "Recent large language models (LLMs) have witnessed significant advancement in\nvarious tasks, including mathematical reasoning and theorem proving. As these\ntwo tasks require strict and formal multi-step inference, they are appealing\ndomains for exploring the reasoning ability of LLMs but still face important\nchallenges. Previous studies such as Chain-of-Thought (CoT) have revealed the\neffectiveness of intermediate steps guidance. However, such step-wise\nannotation requires heavy labor, leading to insufficient training steps for\ncurrent benchmarks. To fill this gap, this work introduces MUSTARD, a data\ngeneration framework that masters uniform synthesis of theorem and proof data\nof high quality and diversity. MUSTARD synthesizes data in three stages: (1) It\nsamples a few mathematical concept seeds as the problem category. (2) Then, it\nprompts a generative language model with the sampled concepts to obtain both\nthe problems and their step-wise formal solutions. (3) Lastly, the framework\nutilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With\nthe proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE\nwith 5,866 valid data points. Each data point contains an informal statement,\nan informal proof, and a translated formal proof that passes the prover\nvalidation. We perform extensive analysis and demonstrate that MUSTARD\ngenerates validated high-quality step-by-step data. We further apply the\nMUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B\nachieves a 15.41% average relative performance gain in automated theorem\nproving, and 8.18% in math word problems. Codes and data are available at\nhttps://github.com/Eleanor-H/MUSTARD.", "journal": "ICLR 2024 spotlight"}
{"doi": "10.48550/arXiv.1808.06413", "date": "2018-08-20", "title": "Concrete Semantics with Coq and CoqHammer", "authors": "\u0141ukasz Czajka, Burak Ekici, Cezary Kaliszyk", "abstract": "The \"Concrete Semantics\" book gives an introduction to imperative programming\nlanguages accompanied by an Isabelle/HOL formalization. In this paper we\ndiscuss a re-formalization of the book using the Coq proof assistant. In order\nto achieve a similar brevity of the formal text we extensively use CoqHammer,\nas well as Coq Ltac-level automation. We compare the formalization efficiency,\ncompactness, and the readability of the proof scripts originating from a Coq\nre-formalization of two chapters from the book.", "journal": ""}
{"doi": "10.48550/arXiv.1304.6626", "date": "2013-04-24", "title": "PIDE as front-end technology for Coq", "authors": "Makarius Wenzel", "abstract": "Isabelle/PIDE is the current Prover IDE technology for Isabelle. It has been\ndeveloped in ML and Scala in the past 4-5 years for this particular proof\nassistant, but with an open mind towards other systems. PIDE is based on an\nasynchronous document model, where the prover receives edits continuously and\nupdates its internal state accordingly. The interpretation of edits and the\npolicies for proof document processing are determined by the prover. The editor\nfront-end merely takes care of visual rendering of formal document content.\n  Here we report on an experiment to connect Coq to the PIDE infrastructure of\nIsabelle. This requires to re-implement the core PIDE protocol layer of\nIsabelle/ML in OCaml. The payload for semantic processing of proof document\ncontent is restricted to lexical analysis in the sense of existing CoqIde\nfunctionality. This is sufficient as proof-of-concept for PIDE connectivity.\nActual proof processing is then a matter of improving Coq towards timeless and\nstateless proof processing, independently of PIDE technicalities. The\nimplementation worked out smoothly and required minimal changes to the refined\nPIDE architecture of Isabelle2013.\n  This experiment substantiates PIDE as general approach to prover interaction.\nIt illustrates how other provers of the greater ITP family can participate by\nfollowing similar reforms of the classic TTY loop as was done for Isabelle in\nthe past few years.", "journal": ""}
{"doi": "10.48550/arXiv.2412.14063", "date": "2024-12-18", "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification", "authors": "Kyle Thompson, Nuno Saavedra, Pedro Carrott, Kevin Fisher, Alex Sanchez-Stern, Yuriy Brun, Jo\u00e3o F. Ferreira, Sorin Lerner, Emily First", "abstract": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven.", "journal": ""}
{"doi": "10.48550/arXiv.2403.13312", "date": "2024-03-20", "title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean", "authors": "Dongwei Jiang, Marcio Fonseca, Shay B. Cohen", "abstract": "Large language models (LLMs) often struggle with complex logical reasoning\ndue to logical inconsistencies and the inherent difficulty of such reasoning.\nWe use Lean, a theorem proving framework, to address these challenges. By\nformalizing logical reasoning problems into theorems within Lean, we can solve\nthem by proving or disproving the corresponding theorems. This method reduces\nthe risk of logical inconsistencies with the help of Lean's symbolic solver. It\nalso enhances our ability to treat complex reasoning tasks by using Lean's\nextensive library of theorem proofs. Our method achieves state-of-the-art\nperformance on the FOLIO dataset and achieves performance near this level on\nProofWriter. Notably, these results were accomplished by fine-tuning on fewer\nthan 100 in-domain samples for each dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2303.04910", "date": "2023-03-08", "title": "Baldur: Whole-Proof Generation and Repair with Large Language Models", "authors": "Emily First, Markus N. Rabe, Talia Ringer, Yuriy Brun", "abstract": "Formally verifying software properties is a highly desirable but\nlabor-intensive task. Recent work has developed methods to automate formal\nverification using proof assistants, such as Coq and Isabelle/HOL, e.g., by\ntraining a model to predict one proof step at a time, and using that model to\nsearch through the space of possible proofs. This paper introduces a new method\nto automate formal verification: We use large language models, trained on\nnatural language text and code and fine-tuned on proofs, to generate whole\nproofs for theorems at once, rather than one step at a time. We combine this\nproof generation model with a fine-tuned repair model to repair generated\nproofs, further increasing proving power. As its main contributions, this paper\ndemonstrates for the first time that: (1) Whole-proof generation using\ntransformers is possible and is as effective as search-based techniques without\nrequiring costly search. (2) Giving the learned model additional context, such\nas a prior failed proof attempt and the ensuing error message, results in proof\nrepair and further improves automated proof generation. (3) We establish a new\nstate of the art for fully automated proof synthesis. We reify our method in a\nprototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL\ntheorems and their proofs. In addition to empirically showing the effectiveness\nof whole-proof generation, repair, and added context, we show that Baldur\nimproves on the state-of-the-art tool, Thor, by automatically generating proofs\nfor an additional 8.7% of the theorems. Together, Baldur and Thor can prove\n65.7% of the theorems fully automatically. This paper paves the way for new\nresearch into using large language models for automating formal verification.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17840", "date": "2025-02-25", "title": "A Combinatorial Identities Benchmark for Theorem Proving via Automated Theorem Generation", "authors": "Beibei Xiong, Hangyu Lv, Haojia Shan, Jianlin Wang, Zhengfeng Yang, Lihong Zhi", "abstract": "Large language models (LLMs) have significantly advanced formal theorem\nproving, yet the scarcity of high-quality training data constrains their\ncapabilities in complex mathematical domains. Combinatorics, a cornerstone of\nmathematics, provides essential tools for analyzing discrete structures and\nsolving optimization problems. However, its inherent complexity makes it\nparticularly challenging for automated theorem proving (ATP) for combinatorial\nidentities. To address this, we manually construct LeanComb, combinatorial\nidentities benchmark in Lean, which is, to our knowledge, the first formalized\ntheorem proving benchmark built for combinatorial identities. We develop an\nAutomated Theorem Generator for Combinatorial Identities, ATG4CI, which\ncombines candidate tactics suggested by a self-improving large language model\nwith a Reinforcement Learning Tree Search approach for tactic prediction. By\nutilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K\ncombinatorial identities theorems, each with a complete formal proof in Lean,\nand experimental evaluations demonstrate that models trained on this dataset\ncan generate more effective tactics, thereby improving success rates in\nautomated theorem proving for combinatorial identities.", "journal": ""}
{"doi": "10.48550/arXiv.2407.17227", "date": "2024-07-24", "title": "LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover", "authors": "Zijian Wu, Jiayu Wang, Dahua Lin, Kai Chen", "abstract": "Recently, large language models have presented promising results in aiding\nformal mathematical reasoning. However, their performance is restricted due to\nthe scarcity of formal theorem-proving data, which requires additional effort\nto be extracted from raw formal language corpora. Meanwhile, a significant\namount of human-written formal language corpora remains underutilized. To\naddress this issue, we propose LEAN-GitHub, a dataset consisting of large-scale\nformal data extracted from almost all Lean 4 repositories on GitHub. After\nfine-tuning InternLM-math-plus on this dataset, our model achieved accuracies\nof 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F\ntest, surpassing state-of-the-art method at 52%. And it also achieves\nstate-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting\ndifferent fields/levels of math. These results demonstrate that our proposed\ndataset is beneficial for formal reasoning on a wide range of math topics. We\nopen-source our model at https://GitHub. com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/ datasets/InternLM/Lean-GitHub", "journal": ""}
{"doi": "10.48550/arXiv.2410.19940", "date": "2024-10-25", "title": "Cobblestone: Iterative Automation for Formal Verification", "authors": "Saketh Ram Kasibatla, Arpan Agarwal, Yuriy Brun, Sorin Lerner, Talia Ringer, Emily First", "abstract": "Formal verification using proof assistants, such as Coq, is an effective way\nof improving software quality, but it is expensive. Writing proofs manually\nrequires both significant effort and expertise. Recent research has used\nmachine learning to automatically synthesize proofs, reducing verification\neffort, but these tools are able to prove only a fraction of the desired\nsoftware properties. We introduce Cobblestone, a new proof-synthesis approach\nthat improves on the state of the art by taking advantage of partial progress\nin proof synthesis attempts. Unlike prior tools, Cobblestone can produce\nmultiple unsuccessful proofs using a large language model (LLM), identify the\nworking portions of those proofs, and combine them into a single, successful\nproof, taking advantage of internal partial progress. We evaluate Cobblestone\non two benchmarks of open-source Coq projects, controlling for training data\nleakage in LLM datasets. Fully automatically, Cobblestone can prove 48% of the\ntheorems, while Proverbot9001, the previous state-of-the-art, learning-based,\nproof-synthesis tool, can prove 17%. Cobblestone establishes a new state of the\nart for fully automated proof synthesis tools for Coq. We also evaluate\nCobblestone in a setting where it is given external partial proof progress from\noracles, serving as proxies for a human proof engineer or another tool. When\nthe theorem is broken down into a set of subgoals and Cobblestone is given a\nset of relevant lemmas already proven in the project, it can prove up to 58% of\nthe theorems. We qualitatively study the theorems Cobblestone is and is not\nable to prove to outline potential future research directions to further\nimprove proof synthesis, including developing interactive, semi-automated\ntools. Our research shows that tools can make better use of partial progress\nmade during proof synthesis to more effectively automate formal verification.", "journal": ""}
{"doi": "10.48550/arXiv.2205.11491", "date": "2022-05-23", "title": "HyperTree Proof Search for Neural Theorem Proving", "authors": "Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aur\u00e9lien Rodriguez, Timoth\u00e9e Lacroix", "abstract": "We propose an online training procedure for a transformer-based automated\ntheorem prover. Our approach leverages a new search algorithm, HyperTree Proof\nSearch (HTPS), inspired by the recent success of AlphaZero. Our model learns\nfrom previous proof searches through online training, allowing it to generalize\nto domains far from the training distribution. We report detailed ablations of\nour pipeline's main components by studying performance on three environments of\nincreasing complexity. In particular, we show that with HTPS alone, a model\ntrained on annotated proofs manages to prove 65.4% of a held-out set of\nMetamath theorems, significantly outperforming the previous state of the art of\n56.5% by GPT-f. Online training on these unproved theorems increases accuracy\nto 82.6%. With a similar computational budget, we improve the state of the art\non the Lean-based miniF2F-curriculum dataset from 31% to 42% proving accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.1810.04828", "date": "2018-10-11", "title": "FEther: An Extensible Definitional Interpreter for Smart-contract Verifications in Coq", "authors": "Zheng Yang, Hang Lei", "abstract": "Blockchain technology adds records to a list using cryptographic links.\nTherefore, the security of blockchain smart contracts is among the most popular\ncontemporary research topics. To improve the theorem-proving technology in this\nfield, we are developing an extensible hybrid verification tool chain, denoted\nas FSPVM-E, for Ethereum smart contract verification. This hybrid system\nextends the proof assistants in Coq, a formal proof-management system.\nCombining symbolic execution with higher-order theorem-proving, it solves\nconsistency, automation, and reusability problems by standard theorem-proving\napproaches. This article completes the FSPVM-E by developing its proof engine.\nFSPVM-E is an extensible definitional interpreter based on our previous work\nFEther, which is totally developed in the Coq proof assistant. It supports\nalmost all semantics of the Solidity programing language, and simultaneously\nexecutes multiple types of symbols. FEther also contains a set of automatic\nstrategies that execute and verify the smart contracts in Coq with a high level\nof automation. The functional correctness of FEther was verified in Coq. In\nstandard tutorials, the execution efficiency of FEther far exceeded that of the\ninterpreters developed in Coq.", "journal": "IEEE Access, 2019"}
{"doi": "10.48550/arXiv.2405.13041", "date": "2024-05-17", "title": "Assessing Political Bias in Large Language Models", "authors": "Luca Rettenberger, Markus Reischl, Mark Schutera", "abstract": "The assessment of bias within Large Language Models (LLMs) has emerged as a\ncritical concern in the contemporary discourse surrounding Artificial\nIntelligence (AI) in the context of their potential impact on societal\ndynamics. Recognizing and considering political bias within LLM applications is\nespecially important when closing in on the tipping point toward performative\nprediction. Then, being educated about potential effects and the societal\nbehavior LLMs can drive at scale due to their interplay with human operators.\nIn this way, the upcoming elections of the European Parliament will not remain\nunaffected by LLMs. We evaluate the political bias of the currently most\npopular open-source LLMs (instruct or assistant models) concerning political\nissues within the European Union (EU) from a German voter's perspective. To do\nso, we use the \"Wahl-O-Mat,\" a voting advice application used in Germany. From\nthe voting advice of the \"Wahl-O-Mat\" we quantize the degree of alignment of\nLLMs with German political parties. We show that larger models, such as\nLlama3-70B, tend to align more closely with left-leaning political parties,\nwhile smaller models often remain neutral, particularly when prompted in\nEnglish. The central finding is that LLMs are similarly biased, with low\nvariances in the alignment concerning a specific party. Our findings underline\nthe importance of rigorously assessing and making bias transparent in LLMs to\nsafeguard the integrity and trustworthiness of applications that employ the\ncapabilities of performative prediction and the invisible hand of machine\nlearning prediction and language generation.", "journal": ""}
{"doi": "10.48550/arXiv.1803.01466", "date": "2018-03-05", "title": "Learning how to Prove: From the Coq Proof Assistant to Textbook Style", "authors": "Sebastian B\u00f6hne, Christoph Kreitz", "abstract": "We have developed an alternative approach to teaching computer science\nstudents how to prove. First, students are taught how to prove theorems with\nthe Coq proof assistant. In a second, more difficult, step students will\ntransfer their acquired skills to the area of textbook proofs. In this article\nwe present a realisation of the second step.\n  Proofs in Coq have a high degree of formality while textbook proofs have only\na medium one. Therefore our key idea is to reduce the degree of formality from\nthe level of Coq to textbook proofs in several small steps. For that purpose we\nintroduce three proof styles between Coq and textbook proofs, called line by\nline comments, weakened line by line comments, and structure faithful proofs.\n  While this article is mostly conceptional we also report on experiences with\nputting our approach into practise.", "journal": "EPTCS 267, 2018, pp. 1-18"}
{"doi": "10.48550/arXiv.2012.09388", "date": "2020-12-17", "title": "Formalization of PAL$\\cdot$S5 in Proof Assistant", "authors": "Jiatu Li", "abstract": "As an experiment to the application of proof assistant for logic research, we\nformalize the model and proof system for multi-agent modal logic S5 with\nPAL-style dynamic modality in Lean theorem prover. We provide a formal proof\nfor the reduction axiom of public announcement, and the soundness and\ncompleteness of modal logic S5, which can be typechecked with Lean 3.19.0. The\ncomplete proof is now available at Github.", "journal": ""}
{"doi": "10.48550/arXiv.2406.01940", "date": "2024-06-04", "title": "Process-Driven Autoformalization in Lean 4", "authors": "Jianqiao Lu, Yingjia Wan, Zhengying Liu, Yinya Huang, Jing Xiong, Chengwu Liu, Jianhao Shen, Hui Jin, Jipeng Zhang, Haiming Wang, Zhicheng Yang, Jing Tang, Zhijiang Guo", "abstract": "Autoformalization, the conversion of natural language mathematics into formal\nlanguages, offers significant potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to formal languages with substantial\nonline corpora and struggle to keep pace with rapidly evolving languages like\nLean 4. To bridge this gap, we propose a new benchmark \\textbf{Form}alization\nfor \\textbf{L}ean~\\textbf{4} (\\textbf{\\name}) designed to evaluate the\nautoformalization capabilities of large language models (LLMs). This benchmark\nencompasses a comprehensive assessment of questions, answers, formal\nstatements, and proofs. Additionally, we introduce a\n\\textbf{P}rocess-\\textbf{S}upervised \\textbf{V}erifier (\\textbf{PSV}) model\nthat leverages the precise feedback from Lean 4 compilers to enhance\nautoformalization. Our experiments demonstrate that the PSV method improves\nautoformalization, enabling higher accuracy using less filtered training data.\nFurthermore, when fine-tuned with data containing detailed process information,\nPSV can leverage the data more effectively, leading to more significant\nimprovements in autoformalization for Lean 4. Our dataset and code are\navailable at \\url{https://github.com/rookie-joe/PDA}.", "journal": ""}
{"doi": "10.48550/arXiv.1609.07127", "date": "2016-09-22", "title": "Social Network Processes in the Isabelle and Coq Theorem Proving Communities", "authors": "Jacques Fleuriot, Steven Obua, Phil Scott", "abstract": "We identify the main actors in the Isabelle and Coq communities and describe\nhow they affect and influence their peers. This work explores selected\nfoundations of social networking analysis that we expect to be useful in the\ncontext of the ProofPeer project, which is developing a new model for\ninteractive theorem proving based on collaboration and social interactions.", "journal": ""}
{"doi": "10.48550/arXiv.2410.23765", "date": "2024-10-31", "title": "Intuitionistic Propositional Logic in Lean", "authors": "Dafina Trufa\u015f", "abstract": "In this paper we present a formalization of Intuitionistic Propositional\nLogic in the Lean proof assistant. Our approach focuses on verifying two\ncompleteness proofs for the studied logical system, as well as exploring the\nrelation between the two analyzed semantical paradigms - Kripke and algebraic.\nIn addition, we prove a large number of theorems and derived deduction rules.", "journal": "EPTCS 410, 2024, pp. 133-149"}
{"doi": "10.48550/arXiv.2012.08990", "date": "2020-12-16", "title": "A Novice-Friendly Induction Tactic for Lean", "authors": "Jannis Limperg", "abstract": "In theorem provers based on dependent type theory such as Coq and Lean,\ninduction is a fundamental proof method and induction tactics are omnipresent\nin proof scripts. Yet the ergonomics of existing induction tactics are not\nideal: they do not reliably support inductive predicates and relations; they\nsometimes generate overly specific or unnecessarily complex induction\nhypotheses; and they occasionally choose confusing names for the hypotheses\nthey introduce.\n  This paper describes a new induction tactic, implemented in Lean 3, which\naddresses these issues. The tactic is particularly suitable for educational\nuse, but experts should also find it more convenient than existing induction\ntactics. In addition, the tactic serves as a moderately complex case study for\nthe metaprogramming framework of Lean 3. The paper describes some difficulties\nencountered during the implementation and suggests improvements to the\nframework.", "journal": ""}
{"doi": "10.48550/arXiv.2405.14414", "date": "2024-05-23", "title": "Proving Theorems Recursively", "authors": "Haiming Wang, Huajian Xin, Zhengying Liu, Wenda Li, Yinya Huang, Jianqiao Lu, Zhicheng Yang, Jing Tang, Jian Yin, Zhenguo Li, Xiaodan Liang", "abstract": "Recent advances in automated theorem proving leverages language models to\nexplore expanded search spaces by step-by-step proof generation. However, such\napproaches are usually based on short-sighted heuristics (e.g., log probability\nor value function scores) that potentially lead to suboptimal or even\ndistracting subgoals, preventing us from finding longer proofs. To address this\nchallenge, we propose POETRY (PrOvE Theorems RecursivelY), which proves\ntheorems in a recursive, level-by-level manner in the Isabelle theorem prover.\nUnlike previous step-by-step methods, POETRY searches for a verifiable sketch\nof the proof at each level and focuses on solving the current level's theorem\nor conjecture. Detailed proofs of intermediate conjectures within the sketch\nare temporarily replaced by a placeholder tactic called sorry, deferring their\nproofs to subsequent levels. This approach allows the theorem to be tackled\nincrementally by outlining the overall theorem at the first level and then\nsolving the intermediate conjectures at deeper levels. Experiments are\nconducted on the miniF2F and PISA datasets and significant performance gains\nare observed in our POETRY approach over state-of-the-art methods. POETRY on\nminiF2F achieves an average proving success rate improvement of 5.1%. Moreover,\nwe observe a substantial increase in the maximum proof length found by POETRY,\nfrom 10 to 26.", "journal": ""}
{"doi": "10.48550/arXiv.2408.15180", "date": "2024-08-27", "title": "Formalizing Mason-Stothers Theorem and its Corollaries in Lean 4", "authors": "Jineon Baek, Seewoo Lee", "abstract": "The ABC conjecture implies many conjectures and theorems in number theory,\nincluding the celebrated Fermat's Last Theorem. Mason-Stothers Theorem is a\nfunction field analogue of the ABC conjecture that admits a much more\nelementary proof with many interesting consequences, including a polynomial\nversion of Fermat's Last Theorem. While years of dedicated effort are expected\nfor a full formalization of Fermat's Last Theorem, the simple proof of\nMason-Stothers Theorem and its corollaries calls for an immediate\nformalization.\n  We formalize an elementary proof of by Snyder in Lean 4, and also formalize\nmany consequences of Mason-Stothers, including (i) non-solvability of\nFermat-Cartan equations in polynomials, (ii) non-parametrizability of a certain\nelliptic curve, and (iii) Davenport's Theorem. We compare our work to existing\nformalizations of Mason-Stothers by Eberl in Isabelle and Wagemaker in Lean 3\nrespectively. Our formalization is based on the mathlib4 library of Lean 4, and\nis currently being ported back to mathlib4.", "journal": ""}
{"doi": "10.48550/arXiv.1806.00608", "date": "2018-06-02", "title": "GamePad: A Learning Environment for Theorem Proving", "authors": "Daniel Huang, Prafulla Dhariwal, Dawn Song, Ilya Sutskever", "abstract": "In this paper, we introduce a system called GamePad that can be used to\nexplore the application of machine learning methods to theorem proving in the\nCoq proof assistant. Interactive theorem provers such as Coq enable users to\nconstruct machine-checkable proofs in a step-by-step manner. Hence, they\nprovide an opportunity to explore theorem proving with human supervision. We\nuse GamePad to synthesize proofs for a simple algebraic rewrite problem and\ntrain baseline models for a formalization of the Feit-Thompson theorem. We\naddress position evaluation (i.e., predict the number of proof steps left) and\ntactic prediction (i.e., predict the next proof step) tasks, which arise\nnaturally in tactic-based theorem proving.", "journal": ""}
{"doi": "10.48550/arXiv.2407.14521", "date": "2024-07-05", "title": "Towards Automated Functional Equation Proving: A Benchmark Dataset and A Domain-Specific In-Context Agent", "authors": "Mahdi Buali, Robert Hoehndorf", "abstract": "Automated Theorem Proving (ATP) faces challenges due to its complexity and\ncomputational demands. Recent work has explored using Large Language Models\n(LLMs) for ATP action selection, but these methods can be resource-intensive.\nThis study introduces FEAS, an agent that enhances the COPRA in-context\nlearning framework within Lean. FEAS refines prompt generation, response\nparsing, and incorporates domain-specific heuristics for functional equations.\nIt introduces FunEq, a curated dataset of functional equation problems with\nvarying difficulty. FEAS outperforms baselines on FunEq, particularly with the\nintegration of domain-specific heuristics. The results demonstrate FEAS's\neffectiveness in generating and formalizing high-level proof strategies into\nLean proofs, showcasing the potential of tailored approaches for specific ATP\nchallenges.", "journal": ""}
{"doi": "10.48550/arXiv.2502.13137", "date": "2025-02-18", "title": "Theorem Prover as a Judge for Synthetic Data Generation", "authors": "Joshua Ong Jun Leang, Giwon Hong, Wenda Li, Shay B. Cohen", "abstract": "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.", "journal": ""}
{"doi": "10.48550/arXiv.2406.14408", "date": "2024-06-20", "title": "FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving", "authors": "Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang", "abstract": "Formal verification (FV) has witnessed growing significance with current\nemerging program synthesis by the evolving large language models (LLMs).\nHowever, current formal verification mainly resorts to symbolic verifiers or\nhand-craft rules, resulting in limitations for extensive and flexible\nverification. On the other hand, formal languages for automated theorem\nproving, such as Isabelle, as another line of rigorous verification, are\nmaintained with comprehensive rules and theorems. In this paper, we propose\nFVEL, an interactive Formal Verification Environment with LLMs. Specifically,\nFVEL transforms a given code to be verified into Isabelle, and then conducts\nverification via neural automated theorem proving with an LLM. The joined\nparadigm leverages the rigorous yet abundant formulated and organized rules in\nIsabelle and is also convenient for introducing and adjusting cutting-edge\nLLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER\ndataset includes code dependencies and verification processes that are\nformulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646\nproof steps in total with in-depth dependencies. We benchmark FVELER in the\nFVEL environment by first fine-tuning LLMs with FVELER and then evaluating them\non Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned\nLlama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 ->\n84) more problems in SV-COMP. And the proportion of proof errors is reduced.\nProject page: https://fveler.github.io/.", "journal": ""}
{"doi": "10.48550/arXiv.2310.16595", "date": "2023-10-25", "title": "Encoding impredicative hierarchy of type universes with variables", "authors": "Yoan G\u00e9ran", "abstract": "Logical frameworks can be used to translate proofs from a proof system to\nanother one. For this purpose, we should be able to encode the theory of the\nproof system in the logical framework. The Lambda Pi calculus modulo theory is\none of these logical frameworks. Powerful theories such as pure type systems\nwith an infinite hierarchy of universes have been encoded, leading to partial\nencodings of proof systems such as Coq, Matita or Agda. In order to fully\nrepresent systems such as Coq and Lean, we introduce a representation of an\ninfinite universe hierarchy with an impredicative universe and universe\nvariables where universe equivalence is equality, and implement it as a\nterminating and confluent rewrite system.", "journal": ""}
{"doi": "10.48550/arXiv.2409.14274", "date": "2024-09-22", "title": "Proof Automation with Large Language Models", "authors": "Minghai Lu, Benjamin Delaware, Tianyi Zhang", "abstract": "Interactive theorem provers such as Coq are powerful tools to formally\nguarantee the correctness of software. However, using these tools requires\nsignificant manual effort and expertise. While Large Language Models (LLMs)\nhave shown promise in automatically generating informal proofs in natural\nlanguage, they are less effective at generating formal proofs in interactive\ntheorem provers. In this paper, we conduct a formative study to identify common\nmistakes made by LLMs when asked to generate formal proofs. By analyzing 520\nproof generation errors made by GPT-3.5, we found that GPT-3.5 often identified\nthe correct high-level structure of a proof, but struggled to get the\nlower-level details correct. Based on this insight, we propose PALM, a novel\ngenerate-then-repair approach that first prompts an LLM to generate an initial\nproof and then leverages targeted symbolic methods to iteratively repair\nlow-level problems. We evaluate PALM on a large dataset that includes more than\n10K theorems. Our results show that PALM significantly outperforms other\nstate-of-the-art approaches, successfully proving 76.6% to 180.4% more\ntheorems. Moreover, PALM proves 1270 theorems beyond the reach of existing\napproaches. We also demonstrate the generalizability of PALM across different\nLLMs.", "journal": "In Proceedings of the 39th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2024)"}
{"doi": "10.48550/arXiv.1906.03930", "date": "2019-06-10", "title": "Formalization of the Axiom of Choice and its Equivalent Theorems", "authors": "Tianyu Sun, Wensheng Yu", "abstract": "In this paper, we describe the formalization of the axiom of choice and\nseveral of its famous equivalent theorems in Morse-Kelley set theory. These\ntheorems include Tukey's lemma, the Hausdorff maximal principle, the maximal\nprinciple, Zermelo's postulate, Zorn's lemma and the well-ordering theorem. We\nprove the above theorems by the axiom of choice in turn, and finally prove the\naxiom of choice by Zermelo's postulate and the well-ordering theorem, thus\ncompleting the cyclic proof of equivalence between them. The proofs are checked\nformally using the Coq proof assistant in which Morse-Kelley set theory is\nformalized. The whole process of formal proof demonstrates that the Coq-based\nmachine proving of mathematics theorem is highly reliable and rigorous. The\nformal work of this paper is enough for most applications, especially in set\ntheory, topology and algebra.", "journal": ""}
{"doi": "10.48550/arXiv.0505034", "date": "2005-05-12", "title": "Essential Incompleteness of Arithmetic Verified by Coq", "authors": "Russell O'Connor", "abstract": "A constructive proof of the Goedel-Rosser incompleteness theorem has been\ncompleted using the Coq proof assistant. Some theory of classical first-order\nlogic over an arbitrary language is formalized. A development of primitive\nrecursive functions is given, and all primitive recursive functions are proved\nto be representable in a weak axiom system. Formulas and proofs are encoded as\nnatural numbers, and functions operating on these codes are proved to be\nprimitive recursive. The weak axiom system is proved to be essentially\nincomplete. In particular, Peano arithmetic is proved to be consistent in Coq's\ntype theory and therefore is incomplete.", "journal": "Russell O'Connor, Essential Incompleteness of Arithmetic Verified\n  by Coq, Lecture Notes in Computer Science, Volume 3603, Aug 2005, Pages 245 -\n  260"}
{"doi": "10.48550/arXiv.1405.3391", "date": "2014-05-14", "title": "A Vernacular for Coherent Logic", "authors": "Sana Stojanovic, Julien Narboux, Marc Bezem, Predrag Janicic", "abstract": "We propose a simple, yet expressive proof representation from which proofs\nfor different proof assistants can easily be generated. The representation uses\nonly a few inference rules and is based on a frag- ment of first-order logic\ncalled coherent logic. Coherent logic has been recognized by a number of\nresearchers as a suitable logic for many ev- eryday mathematical developments.\nThe proposed proof representation is accompanied by a corresponding XML format\nand by a suite of XSL transformations for generating formal proofs for\nIsabelle/Isar and Coq, as well as proofs expressed in a natural language form\n(formatted in LATEX or in HTML). Also, our automated theorem prover for\ncoherent logic exports proofs in the proposed XML format. All tools are\npublicly available, along with a set of sample theorems.", "journal": ""}
{"doi": "10.48550/arXiv.1701.07125", "date": "2017-01-25", "title": "jsCoq: Towards Hybrid Theorem Proving Interfaces", "authors": "Emilio Jes\u00fas Gallego Arias, Beno\u00eet Pin, Pierre Jouvelot", "abstract": "We describe jsCcoq, a new platform and user environment for the Coq\ninteractive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015\nspecification, and it is typically run inside a standards-compliant browser,\nwithout the need of external servers or services. Targeting educational use,\njsCoq allows the user to start interaction with proof scripts right away,\nthanks to its self-contained nature. Indeed, a full Coq environment is packed\nalong the proof scripts, easing distribution and installation. Starting to use\njsCoq is as easy as clicking on a link. The current release ships more than 10\npopular Coq libraries, and supports popular books such as Software Foundations\nor Certified Programming with Dependent Types. The new target platform has\nopened up new interaction and display possibilities. It has also fostered the\ndevelopment of some new Coq-related technology. In particular, we have\nimplemented a new serialization-based protocol for interaction with the proof\nassistant, as well as a new package format for library distribution.", "journal": "EPTCS 239, 2017, pp. 15-27"}
{"doi": "10.48550/arXiv.1909.11342", "date": "2019-09-25", "title": "A formal proof of Hensel's lemma over the p-adic integers", "authors": "Robert Y. Lewis", "abstract": "The field of $p$-adic numbers $\\mathbb{Q}_p$ and the ring of $p$-adic\nintegers $\\mathbb{Z}_p$ are essential constructions of modern number theory.\nHensel's lemma, described by Gouv\\^ea as the \"most important algebraic property\nof the $p$-adic numbers,\" shows the existence of roots of polynomials over\n$\\mathbb{Z}_p$ provided an initial seed point. The theorem can be proved for\nthe $p$-adics with significantly weaker hypotheses than for general rings. We\nconstruct $\\mathbb{Q}_p$ and $\\mathbb{Z}_p$ in the Lean proof assistant, with\nvarious associated algebraic properties, and formally prove a strong form of\nHensel's lemma. The proof lies at the intersection of algebraic and analytic\nreasoning and demonstrates how the Lean mathematical library handles such a\nheterogeneous topic.", "journal": ""}
{"doi": "10.48550/arXiv.2406.07222", "date": "2024-06-11", "title": "Improving Autoformalization using Type Checking", "authors": "Auguste Poiroux, Gail Weiss, Viktor Kun\u010dak, Antoine Bosselut", "abstract": "Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.", "journal": ""}
{"doi": "10.48550/arXiv.1803.00699", "date": "2018-03-02", "title": "QWIRE Practice: Formal Verification of Quantum Circuits in Coq", "authors": "Robert Rand, Jennifer Paykin, Steve Zdancewic", "abstract": "We describe an embedding of the QWIRE quantum circuit language in the Coq\nproof assistant. This allows programmers to write quantum circuits using\nhigh-level abstractions and to prove properties of those circuits using Coq's\ntheorem proving features. The implementation uses higher-order abstract syntax\nto represent variable binding and provides a type-checking algorithm for linear\nwire types, ensuring that quantum circuits are well-formed. We formalize a\ndenotational semantics that interprets QWIRE circuits as superoperators on\ndensity matrices, and prove the correctness of some simple quantum programs.", "journal": "EPTCS 266, 2018, pp. 119-132"}
{"doi": "10.48550/arXiv.2410.13224", "date": "2024-10-17", "title": "Proof Flow: Preliminary Study on Generative Flow Network Language Model Tuning for Formal Reasoning", "authors": "Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang", "abstract": "Reasoning is a fundamental substrate for solving novel and complex problems.\nDeliberate efforts in learning and developing frameworks around System 2\nreasoning have made great strides, yet problems of sufficient complexity remain\nlargely out of reach for open models. To address this gap, we examine the\npotential of Generative Flow Networks as a fine-tuning method for LLMs to\nunlock advanced reasoning capabilities. In this paper, we present a proof of\nconcept in the domain of formal reasoning, specifically in the Neural Theorem\nProving (NTP) setting, where proofs specified in a formal language such as Lean\ncan be deterministically and objectively verified. Unlike classical\nreward-maximization reinforcement learning, which frequently over-exploits\nhigh-reward actions and fails to effectively explore the state space, GFlowNets\nhave emerged as a promising approach for sampling compositional objects,\nimproving generalization, and enabling models to maintain diverse hypotheses.\nOur early results demonstrate GFlowNet fine-tuning's potential for enhancing\nmodel performance in a search setting, which is especially relevant given the\nparadigm shift towards inference time compute scaling and \"thinking slowly.\"", "journal": ""}
{"doi": "10.48550/arXiv.2502.12065", "date": "2025-02-17", "title": "Formalizing Complex Mathematical Statements with LLMs: A Study on Mathematical Definitions", "authors": "Lan Zhang, Marco Valentino, Andre Freitas", "abstract": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions -- a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalisation, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we guide LLMs through\nrelevant contextual elements from formal mathematical libraries. Our findings\nreveal that definitions present a greater challenge compared to existing\nbenchmarks, such as miniF2F. In particular, we found that LLMs still struggle\nwith self-correction, and aligning with relevant mathematical libraries. At the\nsame time, structured refinement methods and definition grounding strategies\nyield notable improvements of up to 16% on self-correction capabilities and 43%\non the reduction of undefined errors, highlighting promising directions for\nenhancing LLM-based autoformalization in real-world scenarios.", "journal": ""}
{"doi": "10.48550/arXiv.2401.02950", "date": "2024-01-05", "title": "The Tactician's Web of Large-Scale Formal Knowledge", "authors": "Lasse Blaauwbroek", "abstract": "The Tactician's Web is a platform offering a large web of strongly\ninterconnected, machine-checked, formal mathematical knowledge conveniently\npackaged for machine learning, analytics, and proof engineering. Built on top\nof the Coq proof assistant, the platform exports a dataset containing a wide\nvariety of formal theories, presented as a web of definitions, theorems, proof\nterms, tactics, and proof states. Theories are encoded both as a semantic graph\n(rendered below) and as human-readable text, each with a unique set of\nadvantages and disadvantages. Proving agents may interact with Coq through the\nsame rich data representation and can be automatically benchmarked on a set of\ntheorems. Tight integration with Coq provides the unique possibility to make\nagents available to proof engineers as practical tools.", "journal": ""}
{"doi": "10.48550/arXiv.2310.16005", "date": "2023-10-24", "title": "MLFMF: Data Sets for Machine Learning for Mathematical Formalization", "authors": "Andrej Bauer, Matej Petkovi\u0107, Ljup\u010do Todorovski", "abstract": "We introduce MLFMF, a collection of data sets for benchmarking recommendation\nsystems used to support formalization of mathematics with proof assistants.\nThese systems help humans identify which previous entries (theorems,\nconstructions, datatypes, and postulates) are relevant in proving a new theorem\nor carrying out a new construction. Each data set is derived from a library of\nformalized mathematics written in proof assistants Agda or Lean. The collection\nincludes the largest Lean~4 library Mathlib, and some of the largest Agda\nlibraries: the standard library, the library of univalent mathematics\nAgda-unimath, and the TypeTopology library. Each data set represents the\ncorresponding library in two ways: as a heterogeneous network, and as a list of\ns-expressions representing the syntax trees of all the entries in the library.\nThe network contains the (modular) structure of the library and the references\nbetween entries, while the s-expressions give complete and easily parsed\ninformation about every entry. We report baseline results using standard graph\nand word embeddings, tree ensembles, and instance-based learning algorithms.\nThe MLFMF data sets provide solid benchmarking support for further\ninvestigation of the numerous machine learning approaches to formalized\nmathematics. The methodology used to extract the networks and the s-expressions\nreadily applies to other libraries, and is applicable to other proof\nassistants. With more than $250\\,000$ entries in total, this is currently the\nlargest collection of formalized mathematical knowledge in machine learnable\nformat.", "journal": ""}
{"doi": "10.48550/arXiv.2404.07382", "date": "2024-04-10", "title": "Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving", "authors": "Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang", "abstract": "Recent advances in Automated Theorem Proving have shown the effectiveness of\nleveraging a (large) language model that generates tactics (i.e. proof steps)\nto search through proof states. The current model, while trained solely on\nsuccessful proof paths, faces a discrepancy at the inference stage, as it must\nsample and try various tactics at each proof state until finding success,\nunlike its training which does not incorporate learning from failed attempts.\nIntuitively, a tactic that leads to a failed search path would indicate that\nsimilar tactics should receive less attention during the following trials. In\nthis paper, we demonstrate the benefit of training models that additionally\nlearn from failed search paths. Facing the lack of such trial-and-error data in\nexisting open-source theorem-proving datasets, we curate a dataset on\nintuitionistic propositional logic theorems and formalize it in Lean, such that\nwe can reliably check the correctness of proofs. We compare our model trained\non relatively short trial-and-error information (TrialMaster) with models\ntrained only on the correct paths and discover that the former solves more\nunseen theorems with lower trial searches.", "journal": ""}
{"doi": "10.48550/arXiv.2110.08453", "date": "2021-10-16", "title": "Voting Theory in the Lean Theorem Prover", "authors": "Wesley H. Holliday, Chase Norman, Eric Pacuit", "abstract": "There is a long tradition of fruitful interaction between logic and social\nchoice theory. In recent years, much of this interaction has focused on\ncomputer-aided methods such as SAT solving and interactive theorem proving. In\nthis paper, we report on the development of a framework for formalizing voting\ntheory in the Lean theorem prover, which we have applied to verify properties\nof a recently studied voting method. While previous applications of interactive\ntheorem proving to social choice (using Isabelle/HOL and Mizar) have focused on\nthe verification of impossibility theorems, we aim to cover a variety of\nresults ranging from impossibility theorems to the verification of properties\nof specific voting methods (e.g., Condorcet consistency, independence of\nclones, etc.). In order to formalize voting theoretic axioms concerning adding\nor removing candidates and voters, we work in a variable-election setting whose\nformalization makes use of dependent types in Lean.", "journal": ""}
{"doi": "10.48550/arXiv.1811.11317", "date": "2018-11-28", "title": "Adventures in Formalisation: Financial Contracts, Modules, and Two-Level Type Theory", "authors": "Danil Annenkov", "abstract": "We present three projects concerned with applications of proof assistants in\nthe area of programming language theory and mathematics. The first project is\nabout a certified compilation technique for a domain-specific programming\nlanguage for financial contracts (the CL language). The code in CL is\ntranslated into a simple expression language well-suited for integration with\nsoftware components implementing Monte Carlo simulation techniques (pricing\nengines). The compilation procedure is accompanied with formal proofs of\ncorrectness carried out in Coq. The second project presents techniques that\nallow for formal reasoning with nested and mutually inductive structures built\nup from finite maps and sets. The techniques, which build on the theory of\nnominal sets combined with the ability to work with isomorphic representations\nof finite maps, make it possible to give a formal treatment, in Coq, of a\nhigher-order module system, including the ability to eliminate at compile time\nabstraction barriers introduced by the module system. The development is based\non earlier work on static interpretation of modules and provides the foundation\nfor a higher-order module language for Futhark, an optimising compiler\ntargeting data-parallel architectures. The third project presents an\nimplementation of two-level type theory, a version of Martin-Lof type theory\nwith two equality types: the first acts as the usual equality of homotopy type\ntheory, while the second allows us to reason about strict equality. In this\nsystem, we can formalise results of partially meta-theoretic nature. We develop\nand explore in details how two-level type theory can be implemented in a proof\nassistant, providing a prototype implementation in the proof assistant Lean. We\ndemonstrate an application of two-level type theory by developing some results\non the theory of inverse diagrams using our Lean implementation.", "journal": ""}
{"doi": "10.48550/arXiv.2408.09237", "date": "2024-08-17", "title": "QEDCartographer: Automating Formal Verification Using Reward-Free Reinforcement Learning", "authors": "Alex Sanchez-Stern, Abhishek Varghese, Zhanna Kaufman, Dylan Zhang, Talia Ringer, Yuriy Brun", "abstract": "Formal verification is a promising method for producing reliable software,\nbut the difficulty of manually writing verification proofs severely limits its\nutility in practice. Recent methods have automated some proof synthesis by\nguiding a search through the proof space using a theorem prover. Unfortunately,\nthe theorem prover provides only the crudest estimate of progress, resulting in\neffectively undirected search. To address this problem, we create\nQEDCartographer, an automated proof-synthesis tool that combines supervised and\nreinforcement learning to more effectively explore the proof space.\nQEDCartographer incorporates the proofs' branching structure, enabling\nreward-free search and overcoming the sparse reward problem inherent to formal\nverification. We evaluate QEDCartographer using the CoqGym benchmark of 68.5K\ntheorems from 124 open-source Coq projects. QEDCartographer fully automatically\nproves 21.4% of the test-set theorems. Previous search-based proof-synthesis\ntools Tok, Tac, ASTactic, Passport, and Proverbot9001, which rely only on\nsupervised learning, prove 9.6%, 9.8%, 10.9%, 12.5%, and 19.8%, respectively.\nDiva, which combines 62 tools, proves 19.2%. Comparing to the most effective\nprior tool, Proverbot9001, QEDCartographer produces 34% shorter proofs 29%\nfaster, on average over the theorems both tools prove. Together,\nQEDCartographer and non-learning-based CoqHammer prove 30.3% of the theorems,\nwhile CoqHammer alone proves 26.6%. Our work demonstrates that reinforcement\nlearning is a fruitful research direction for improving proof-synthesis tools'\nsearch mechanisms.", "journal": ""}
{"doi": "10.48550/arXiv.2501.18639", "date": "2025-01-28", "title": "A Comprehensive Survey of the Lean 4 Theorem Prover: Architecture, Applications, and Advances", "authors": "Xichen Tang", "abstract": "This comprehensive survey examines Lean 4, a state-of-the-art interactive\ntheorem prover and functional programming language. We analyze its\narchitectural design, type system, metaprogramming capabilities, and practical\napplications in formal verification and mathematics. Through detailed\ncomparisons with other proof assistants and extensive case studies, we\ndemonstrate Lean 4's unique advantages in proof automation, performance, and\nusability. The paper also explores recent developments in its ecosystem,\nincluding libraries, tools, and educational applications, providing insights\ninto its growing impact on formal methods and mathematical formalization.", "journal": ""}
{"doi": "10.48550/arXiv.1705.07318", "date": "2017-05-20", "title": "Formalized Lambek Calculus in Higher Order Logic (HOL4)", "authors": "Chun Tian", "abstract": "In this project, a rather complete proof-theoretical formalization of Lambek\nCalculus (non-associative with arbitrary extensions) has been ported from Coq\nproof assistent to HOL4 theorem prover, with some improvements and new\ntheorems.\n  Three deduction systems (Syntactic Calculus, Natural Deduction and Sequent\nCalculus) of Lambek Calculus are defined with many related theorems proved. The\nequivalance between these systems are formally proved. Finally, a formalization\nof Sequent Calculus proofs (where Coq has built-in supports) has been designed\nand implemented in HOL4. Some basic results including the sub-formula\nproperties of the so-called \"cut-free\" proofs are formally proved.\n  This work can be considered as the preliminary work towards a language parser\nbased on category grammars which is not multimodal but still has ability to\nsupport context-sensitive languages through customized extensions.", "journal": ""}
{"doi": "10.48550/arXiv.1805.08059", "date": "2018-05-21", "title": "One Monad to Prove Them All", "authors": "Sandra Dylus, Jan Christiansen, Finn Teegen", "abstract": "One Monad to Prove Them All is a modern fairy tale about curiosity and\nperseverance, two important properties of a successful PhD student. We follow\nthe PhD student Mona on her adventure of proving properties about Haskell\nprograms in the proof assistant Coq. On the one hand, as a PhD student in\ncomputer science Mona observes an increasing demand for correct software\nproducts. In particular, because of the large amount of existing software,\nverifying existing software products becomes more important. Verifying programs\nin the functional programming language Haskell is no exception. On the other\nhand, Mona is delighted to see that communities in the area of theorem proving\nare becoming popular. Thus, Mona sets out to learn more about the interactive\ntheorem prover Coq and verifying Haskell programs in Coq. To prove properties\nabout a Haskell function in Coq, Mona has to translate the function into Coq\ncode. As Coq programs have to be total and Haskell programs are often not, Mona\nhas to model partiality explicitly in Coq. In her quest for a solution Mona\nfinds an ancient manuscript that explains how properties about Haskell\nfunctions can be proven in the proof assistant Agda by translating Haskell\nprograms into monadic Agda programs. By instantiating the monadic program with\na concrete monad instance the proof can be performed in either a total or a\npartial setting. Mona discovers that the proposed transformation does not work\nin Coq due to a restriction in the termination checker. In fact the\ntransformation does not work in Agda anymore as well, as the termination\nchecker in Agda has been improved. We follow Mona on an educational journey\nthrough the land of functional programming where she learns about concepts like\nfree monads and containers as well as basics and restrictions of proof\nassistants like Coq. These concepts are well-known individually, but their\ninterplay gives rise to a solution for Mona's problem based on the originally\nproposed monadic tranformation that has not been presented before. When Mona\nstarts to test her approach by proving a statement about simple Haskell\nfunctions, she realizes that her approach has an additional advantage over the\noriginal idea in Agda. Mona's final solution not only works for a specific\nmonad instance but even allows her to prove monad-generic properties. Instead\nof proving properties over and over again for specific monad instances she is\nable to prove properties that hold for all monads representable by a\ncontainer-based instance of the free monad. In order to strengthen her\nconfidence in the practicability of her approach, Mona evaluates her approach\nin a case study that compares two implementations for queues. In order to share\nthe results with other functional programmers the fairy tale is available as a\nliterate Coq file. If you are a citizen of the land of functional programming\nor are at least familiar with its customs, had a journey that involved\nreasoning about functional programs of your own, or are just a curious soul\nlooking for the next story about monads and proofs, then this tale is for you.", "journal": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 8"}
{"doi": "10.48550/arXiv.2205.00862", "date": "2022-05-02", "title": "Accelerating Verified-Compiler Development with a Verified Rewriting Engine", "authors": "Jason Gross, Andres Erbsen, Jade Philipoom, Miraya Poddar-Agrawal, Adam Chlipala", "abstract": "Compilers are a prime target for formal verification, since compiler bugs\ninvalidate higher-level correctness guarantees, but compiler changes may become\nmore labor-intensive to implement, if they must come with proof patches. One\nappealing approach is to present compilers as sets of algebraic rewrite rules,\nwhich a generic engine can apply efficiently. Now each rewrite rule can be\nproved separately, with no need to revisit past proofs for other parts of the\ncompiler. We present the first realization of this idea, in the form of a\nframework for the Coq proof assistant. Our new Coq command takes normal proved\ntheorems and combines them automatically into fast compilers with proofs. We\napplied our framework to improve the Fiat Cryptography toolchain for generating\ncryptographic arithmetic, producing an extracted command-line compiler that is\nabout 1000$\\times$ faster while actually featuring simpler compiler-specific\nproofs.", "journal": ""}
{"doi": "10.48550/arXiv.2003.01685", "date": "2020-03-03", "title": "Sealing Pointer-Based Optimizations Behind Pure Functions", "authors": "Daniel Selsam, Simon Hudon, Leonardo de Moura", "abstract": "Functional programming languages are particularly well-suited for building\nautomated reasoning systems, since (among other reasons) a logical term is well\nmodeled by an inductive type, traversing a term can be implemented generically\nas a higher-order combinator, and backtracking is dramatically simplified by\npersistent datastructures. However, existing pure functional programming\nlanguages all suffer a major limitation in these domains: traversing a term\nrequires time proportional to the tree size of the term as opposed to its graph\nsize. This limitation would be particularly devastating when building\nautomation for interactive theorem provers such as Lean and Coq, for which the\nexponential blowup of term-tree sizes has proved to be both common and\ndifficult to prevent. All that is needed to recover the optimal scaling is the\nability to perform simple operations on the memory addresses of terms, and yet\nallowing these operations to be used freely would clearly violate the basic\npremise of referential transparency. We show how to use dependent types to seal\nthe necessary pointer-address manipulations behind pure functional interfaces\nwhile requiring only a negligible amount of additional trust. We have\nimplemented our approach for the upcoming version (v4) of Lean, and our\napproach could be adopted by other languages based on dependent type theory as\nwell.", "journal": ""}
{"doi": "10.48550/arXiv.1712.03894", "date": "2017-12-11", "title": "Coqatoo: Generating Natural Language Versions of Coq Proofs", "authors": "Andrew Bedford", "abstract": "Due to their numerous advantages, formal proofs and proof assistants, such as\nCoq, are becoming increasingly popular. However, one disadvantage of using\nproof assistants is that the resulting proofs can sometimes be hard to read and\nunderstand, particularly for less-experienced users. To address this issue, we\nhave implemented a tool capable of generating natural language versions of Coq\nproofs called Coqatoo, which we present in this paper.", "journal": ""}
{"doi": "10.48550/arXiv.2001.04301", "date": "2020-01-13", "title": "Tabled Typeclass Resolution", "authors": "Daniel Selsam, Sebastian Ullrich, Leonardo de Moura", "abstract": "Typeclasses provide an elegant and effective way of managing ad-hoc\npolymorphism in both programming languages and interactive proof assistants.\nHowever, the increasingly sophisticated uses of typeclasses within proof\nassistants, especially within Lean's burgeoning mathematics library, mathlib,\nhave elevated once-theoretical limitations of existing typeclass resolution\nprocedures into major impediments to ongoing progress. The two most devastating\nlimitations of existing procedures are exponential running times in the\npresence of diamonds and divergence in the presence of cycles. We present a new\nprocedure, tabled typeclass resolution, that solves both problems by tabling,\nwhich is a generalization of memoizing originally introduced to address similar\nlimitations of early logic programming systems. We have implemented our\nprocedure for the upcoming version (v4) of Lean, and have confirmed empirically\nthat our implementation is exponentially faster than existing systems in the\npresence of diamonds. Although tabling is notoriously difficult to implement,\nour procedure is notably lightweight and could easily be implemented in other\nsystems. We hope our new procedure facilitates even more sophisticated uses of\ntypeclasses in both software development and interactive theorem proving.", "journal": ""}
{"doi": "10.48550/arXiv.1201.3601", "date": "2012-01-17", "title": "A Synthesis of the Procedural and Declarative Styles of Interactive Theorem Proving", "authors": "Freek Wiedijk", "abstract": "We propose a synthesis of the two proof styles of interactive theorem\nproving: the procedural style (where proofs are scripts of commands, like in\nCoq) and the declarative style (where proofs are texts in a controlled natural\nlanguage, like in Isabelle/Isar). Our approach combines the advantages of the\ndeclarative style - the possibility to write formal proofs like normal\nmathematical text - and the procedural style - strong automation and help with\nshaping the proofs, including determining the statements of intermediate steps.\nOur approach is new, and differs significantly from the ways in which the\nprocedural and declarative proof styles have been combined before in the\nIsabelle, Ssreflect and Matita systems. Our approach is generic and can be\nimplemented on top of any procedural interactive theorem prover, regardless of\nits architecture and logical foundations. To show the viability of our proposed\napproach, we fully implemented it as a proof interface called miz3, on top of\nthe HOL Light interactive theorem prover. The declarative language that this\ninterface uses is a slight variant of the language of the Mizar system, and can\nbe used for any interactive theorem prover regardless of its logical\nfoundations. The miz3 interface allows easy access to the full set of tactics\nand formal libraries of HOL Light, and as such has \"industrial strength\". Our\napproach gives a way to automatically convert any procedural proof to a\ndeclarative counterpart, where the converted proof is similar in size to the\noriginal. As all declarative systems have essentially the same proof language,\nthis gives a straightforward way to port proofs between interactive theorem\nprovers.", "journal": "Logical Methods in Computer Science, Volume 8, Issue 1 (March 28,\n  2012) lmcs:1046"}
{"doi": "10.48550/arXiv.2307.09497", "date": "2023-07-18", "title": "Towards a geometry for syntax", "authors": "Jonathan Sterling", "abstract": "It often happens that free algebras for a given theory satisfy useful\nreasoning principles that are not preserved under homomorphisms of algebras,\nand hence need not hold in an arbitrary algebra. For instance, if $M$ is the\nfree monoid on a set $A$, then the scalar multiplication function $A\\times M\n\\to M$ is injective. Therefore, when reasoning in the formal theory of monoids\nunder $A$, it is possible to use this injectivity law to make sound deductions\neven about monoids under $A$ for which scalar multiplication is not injective\n-- a principle known in algebra as the permanence of identity. Properties of\nthis kind are of fundamental practical importance to the logicians and computer\nscientists who design and implement computerized proof assistants like Lean and\nCoq, as they enable the formal reductions of equational problems that make type\nchecking tractable.\n  As type theories have become increasingly more sophisticated, it has become\nmore and more difficult to establish the useful properties of their free models\nthat enable effective implementation. These obstructions have facilitated a\nfruitful return to foundational work in type theory, which has taken on a more\ngeometrical flavor than ever before. Here we expose a modern way to prove a\nhighly non-trivial injectivity law for free models of Martin-L\\\"of type theory,\npaying special attention to the ways that contemporary methods in type theory\nhave been influenced by three important ideas of the Grothendieck school: the\nrelative point of view, the language of universes, and the recollement of\ngeneralized spaces.", "journal": ""}
{"doi": "10.48550/arXiv.2502.03321", "date": "2025-02-05", "title": "Simplifying Formal Proof-Generating Models with ChatGPT and Basic Searching Techniques", "authors": "Sangjun Han, Taeil Hur, Youngmi Hur, Kathy Sangkyung Lee, Myungyoon Lee, Hyojae Lim", "abstract": "The challenge of formal proof generation has a rich history, but with modern\ntechniques, we may finally be at the stage of making actual progress in\nreal-life mathematical problems. This paper explores the integration of ChatGPT\nand basic searching techniques to simplify generating formal proofs, with a\nparticular focus on the miniF2F dataset. We demonstrate how combining a large\nlanguage model like ChatGPT with a formal language such as Lean, which has the\nadded advantage of being verifiable, enhances the efficiency and accessibility\nof formal proof generation. Despite its simplicity, our best-performing\nLean-based model surpasses all known benchmarks with a 31.15% pass rate. We\nextend our experiments to include other datasets and employ alternative\nlanguage models, showcasing our models' comparable performance in diverse\nsettings and allowing for a more nuanced analysis of our results. Our findings\noffer insights into AI-assisted formal proof generation, suggesting a promising\ndirection for future research in formal mathematical proof.", "journal": ""}
{"doi": "10.48550/arXiv.2408.04564", "date": "2024-08-08", "title": "First steps towards Computational Polynomials in Lean", "authors": "James Harold Davenport", "abstract": "The proof assistant Lean has support for abstract polynomials, but this is\nnot necessarily the same as support for computations with polynomials. Lean is\nalso a functional programming language, so it should be possible to implement\ncomputational polynomials in Lean. It turns out not to be as easy as the naive\nauthor thought.", "journal": ""}
{"doi": "10.48550/arXiv.2204.10370", "date": "2022-04-21", "title": "Passport: Improving Automated Formal Verification Using Identifiers", "authors": "Alex Sanchez-Stern, Emily First, Timothy Zhou, Zhanna Kaufman, Yuriy Brun, Talia Ringer", "abstract": "Formally verifying system properties is one of the most effective ways of\nimproving system quality, but its high manual effort requirements often render\nit prohibitively expensive. Tools that automate formal verification, by\nlearning from proof corpora to suggest proofs, have just begun to show their\npromise. These tools are effective because of the richness of the data the\nproof corpora contain. This richness comes from the stylistic conventions\nfollowed by communities of proof developers, together with the logical systems\nbeneath proof assistants. However, this richness remains underexploited, with\nmost work thus far focusing on architecture rather than making the most of the\nproof data.\n  In this paper, we develop Passport, a fully-automated proof-synthesis tool\nthat systematically explores how to most effectively exploit one aspect of that\nproof data: identifiers. Passport enriches a predictive Coq model with three\nnew encoding mechanisms for identifiers: category vocabulary indexing, subword\nsequence modeling, and path elaboration. We compare Passport to three existing\nbase tools which Passport can enhance: ASTactic, Tac, and Tok. In head-to-head\ncomparisons, Passport automatically proves 29% more theorems than the\nbest-performing of these base tools. Combining the three Passport-enhanced\ntools automatically proves 38% more theorems than the three base tools\ntogether, without Passport's enhancements. Finally, together, these base tools\nand Passport-enhanced tools prove 45% more theorems than the combined base\ntools without Passport's enhancements. Overall, our findings suggest that\nmodeling identifiers can play a significant role in improving proof synthesis,\nleading to higher-quality software.", "journal": "ACM Transactions on Programming Languages and Systems (TOPLAS),\n  45(2):12:1-12:30, June 2023"}
{"doi": "10.48550/arXiv.2107.07664", "date": "2021-07-16", "title": "SMLtoCoq: Automated Generation of Coq Specifications and Proof Obligations from SML Programs with Contracts", "authors": "Laila El-Beheiry, Giselle Reis, Ammar Karkour", "abstract": "Formally reasoning about functional programs is supposed to be\nstraightforward and elegant, however, it is not typically done as a matter of\ncourse. Reasoning in a proof assistant requires \"reimplementing\" the code in\nthose tools, which is far from trivial. SMLtoCoq provides an automatic\ntranslation of SML programs and function contracts into Coq. Programs are\ntranslated into Coq specifications, and function contracts into theorems, which\ncan then be formally proved. Using the Equations plugin and other well\nestablished Coq libraries, SMLtoCoq is able to translate SML programs without\nside-effects containing partial functions, structures, functors, records, among\nothers. Additionally, we provide a Coq version of many parts of SML's basis\nlibrary, so that calls to these libraries are kept almost as is.", "journal": "EPTCS 337, 2021, pp. 71-87"}
{"doi": "10.48550/arXiv.2401.02949", "date": "2024-01-05", "title": "Graph2Tac: Online Representation Learning of Formal Math Concepts", "authors": "Lasse Blaauwbroek, Miroslav Ol\u0161\u00e1k, Jason Rute, Fidel Ivan Schaposnik Massolo, Jelle Piepenbrock, Vasily Pestun", "abstract": "In proof assistants, the physical proximity between two formal mathematical\nconcepts is a strong predictor of their mutual relevance. Furthermore, lemmas\nwith close proximity regularly exhibit similar proof structures. We show that\nthis locality property can be exploited through online learning techniques to\nobtain solving agents that far surpass offline learners when asked to prove\ntheorems in an unseen mathematical setting. We extensively benchmark two such\nonline solvers implemented in the Tactician platform for the Coq proof\nassistant: First, Tactician's online $k$-nearest neighbor solver, which can\nlearn from recent proofs, shows a $1.72\\times$ improvement in theorems proved\nover an offline equivalent. Second, we introduce a graph neural network,\nGraph2Tac, with a novel approach to build hierarchical representations for new\ndefinitions. Graph2Tac's online definition task realizes a $1.5\\times$\nimprovement in theorems solved over an offline baseline. The $k$-NN and\nGraph2Tac solvers rely on orthogonal online data, making them highly\ncomplementary. Their combination improves $1.27\\times$ over their individual\nperformances. Both solvers outperform all other general-purpose provers for\nCoq, including CoqHammer, Proverbot9001, and a transformer baseline by at least\n$1.48\\times$ and are available for practical use by end-users.", "journal": ""}
{"doi": "10.48550/arXiv.1307.1944", "date": "2013-07-08", "title": "READ-EVAL-PRINT in Parallel and Asynchronous Proof-checking", "authors": "Makarius Wenzel", "abstract": "The LCF tradition of interactive theorem proving, which was started by Milner\nin the 1970-ies, appears to be tied to the classic READ-EVAL-PRINT-LOOP of\nsequential and synchronous evaluation of prover commands. We break up this loop\nand retrofit the read-eval-print phases into a model of parallel and\nasynchronous proof processing. Thus we explain some key concepts of the\nIsabelle/Scala approach to prover interaction and integration, and the\nIsabelle/jEdit Prover IDE as front-end technology. We hope to open up the\nscientific discussion about non-trivial interaction models for ITP systems\nagain, and help getting other old-school proof assistants on a similar track.", "journal": "EPTCS 118, 2013, pp. 57-71"}
{"doi": "10.48550/arXiv.1710.10258", "date": "2017-10-27", "title": "Temporal Type Theory: A topos-theoretic approach to systems and behavior", "authors": "Patrick Schultz, David I. Spivak", "abstract": "This book introduces a temporal type theory, the first of its kind as far as\nwe know. It is based on a standard core, and as such it can be formalized in a\nproof assistant such as Coq or Lean by adding a number of axioms. Well-known\ntemporal logics---such as Linear and Metric Temporal Logic (LTL and\nMTL)---embed within the logic of temporal type theory.\n  The types in this theory represent \"behavior types\". The language is rich\nenough to allow one to define arbitrary hybrid dynamical systems, which are\nmixtures of continuous dynamics---e.g. as described by a differential\nequation---and discrete jumps. In particular, the derivative of a continuous\nreal-valued function is internally defined.\n  We construct a semantics for the temporal type theory in the topos of sheaves\non a translation-invariant quotient of the standard interval domain. In fact,\ndomain theory plays a recurring role in both the semantics and the type theory.", "journal": ""}
{"doi": "10.48550/arXiv.1902.08726", "date": "2019-02-23", "title": "A Hybrid Formal Verification System in Coq for Ensuring the Reliability and Security of Ethereum-based Service Smart Contracts", "authors": "Zheng Yang, Hang Lei, Weizhong Qian", "abstract": "This paper reports on the development of a formal symbolic process virtual\nmachine (FSPVM) denoted as FSPVM-E for verifying the reliability and security\nof Ethereum-based services at the source code level of smart contracts, and a\nCoq proof assistant is employed for both programming the system and for proving\nits correctness. The current version of FSPVM-E adopts execution-verification\nisomorphism, which is an application extension of Curry-Howard isomorphism, as\nits fundamental theoretical framework to combine symbolic execution and\nhigher-order logic theorem proving. The four primary components of FSPVM-E\ninclude a general, extensible, and reusable formal memory framework, an\nextensible and universal formal intermediate programming language denoted as\nLolisa, which is a large subset of the Solidity programming language using\ngeneralized algebraic datatypes, the corresponding formally verified\ninterpreter of Lolisa, denoted as FEther, and assistant tools and libraries.\nThe self-correctness of all components is certified in Coq. Currently, FSPVM-E\nsupports the ERC20 token standard, and can automatically and symbolically\nexecute Ethereum-based smart contracts, scan their standard vulnerabilities,\nand verify their reliability and security properties with Hoare-style logic in\nCoq. To the best of authors' knowledge, the present work represents the first\nhybrid formal verification system implemented in Coq for Ethereum smart\ncontracts that is applied at the Solidity source code level.", "journal": "IEEE ACCESS (2020)"}
{"doi": "10.48550/arXiv.2409.15351", "date": "2024-09-11", "title": "Classification of Covering Spaces and Canonical Change of Basepoint", "authors": "Jelle Wemmenhove, Cosmin Manea, Jim Portegies", "abstract": "Using the language of homotopy type theory (HoTT), we 1) prove a synthetic\nversion of the classification theorem for covering spaces, and 2) explore the\nexistence of canonical change-of-basepoint isomorphisms between homotopy\ngroups. There is some freedom in choosing how to translate concepts from\nclassical algebraic topology into HoTT. The final translations we ended up with\nare easier to work with than the ones we started with. We discuss some earlier\nattempts to shed light on this translation process. The proofs are mechanized\nusing the Coq proof assistant and closely follow classical treatments like\nthose by Hatcher.", "journal": "LIPIcs, Volume 303, TYPES 2023"}
{"doi": "10.48550/arXiv.2403.13592", "date": "2024-03-20", "title": "Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs", "authors": "Ilias Chalkidis, Stephanie Brandl", "abstract": "Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.", "journal": ""}
{"doi": "10.48550/arXiv.0802.2853", "date": "2008-02-20", "title": "Discrete Jordan Curve Theorem: A proof formalized in Coq with hypermaps", "authors": "Jean-Fran\u00e7ois Dufourd", "abstract": "This paper presents a formalized proof of a discrete form of the Jordan Curve\nTheorem. It is based on a hypermap model of planar subdivisions, formal\nspecifications and proofs assisted by the Coq system. Fundamental properties\nare proven by structural or noetherian induction: Genus Theorem, Euler's\nFormula, constructive planarity criteria. A notion of ring of faces is\ninductively defined and a Jordan Curve Theorem is stated and proven for any\nplanar hypermap.", "journal": "Dans Proceedings of the 25th Annual Symposium on the Theoretical\n  Aspects of Computer Science - STACS 2008, Bordeaux : France (2008)"}
{"doi": "10.48550/arXiv.2408.11043", "date": "2024-08-20", "title": "Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research", "authors": "Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Anshul Mittal, Rutu Mulkar", "abstract": "Qualitative data collection and analysis approaches, such as those employing\ninterviews and focus groups, provide rich insights into customer attitudes,\nsentiment, and behavior. However, manually analyzing qualitative data requires\nextensive time and effort to identify relevant topics and thematic insights.\nThis study proposes a novel approach to address this challenge by leveraging\nRetrieval Augmented Generation (RAG) based Large Language Models (LLMs) for\nanalyzing interview transcripts. The novelty of this work lies in strategizing\nthe research inquiry as one that is augmented by an LLM that serves as a novice\nresearch assistant. This research explores the mental model of LLMs to serve as\nnovice qualitative research assistants for researchers in the talent management\nspace. A RAG-based LLM approach is extended to enable topic modeling of\nsemi-structured interview data, showcasing the versatility of these models\nbeyond their traditional use in information retrieval and search. Our findings\ndemonstrate that the LLM-augmented RAG approach can successfully extract topics\nof interest, with significant coverage compared to manually generated topics\nfrom the same dataset. This establishes the viability of employing LLMs as\nnovice qualitative research assistants. Additionally, the study recommends that\nresearchers leveraging such models lean heavily on quality criteria used in\ntraditional qualitative research to ensure rigor and trustworthiness of their\napproach. Finally, the paper presents key recommendations for industry\npractitioners seeking to reconcile the use of LLMs with established qualitative\nresearch paradigms, providing a roadmap for the effective integration of these\npowerful, albeit novice, AI tools in the analysis of qualitative datasets\nwithin talent", "journal": ""}
{"doi": "10.48550/arXiv.2102.03529", "date": "2021-02-06", "title": "Vampire With a Brain Is a Good ITP Hammer", "authors": "Martin Suda", "abstract": "Vampire has been for a long time the strongest first-order automatic theorem\nprover, widely used for hammer-style proof automation in ITPs such as Mizar,\nIsabelle, HOL, and Coq. In this work, we considerably improve the performance\nof Vampire in hammering over the full Mizar library by enhancing its saturation\nprocedure with efficient neural guidance. In particular, we employ a recently\nproposed recursive neural network classifying the generated clauses based only\non their derivation history. Compared to previous neural methods based on\nconsidering the logical content of the clauses, our architecture makes\nevaluating a single clause much less time consuming. The resulting system shows\ngood learning capability and improves on the state-of-the-art performance on\nthe Mizar library, while proving many theorems that the related ENIGMA system\ncould not prove in a similar hammering evaluation.", "journal": ""}
{"doi": "10.48550/arXiv.2211.07524", "date": "2022-11-14", "title": "Towards a Mathematics Formalisation Assistant using Large Language Models", "authors": "Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni Narayanan, Anand Tadipatri", "abstract": "Mathematics formalisation is the task of writing mathematics (i.e.,\ndefinitions, theorem statements, proofs) in natural language, as found in books\nand papers, into a formal language that can then be checked for correctness by\na program. It is a thriving activity today, however formalisation remains\ncumbersome. In this paper, we explore the abilities of a large language model\n(Codex) to help with formalisation in the Lean theorem prover. We find that\nwith careful input-dependent prompt selection and postprocessing, Codex is able\nto formalise short mathematical statements at undergrad level with nearly 75\\%\naccuracy for $120$ theorem statements. For proofs quantitative analysis is\ninfeasible and we undertake a detailed case study. We choose a diverse set of\n$13$ theorems at undergrad level with proofs that fit in two-three paragraphs.\nWe show that with a new prompting strategy Codex can formalise these proofs in\nnatural language with at least one out of twelve Codex completion being easy to\nrepair into a complete proof. This is surprising as essentially no aligned data\nexists for formalised mathematics, particularly for proofs. These results\nsuggest that large language models are a promising avenue towards fully or\npartially automating formalisation.", "journal": ""}
{"doi": "10.48550/arXiv.2110.04461", "date": "2021-10-09", "title": "Toward Hole-Driven Development with Liquid Haskell", "authors": "Patrick Redmond, Gan Shen, Lindsey Kuper", "abstract": "Liquid Haskell is an extension to the Haskell programming language that adds\nsupport for refinement types: data types augmented with SMT-decidable logical\npredicates that refine the set of values that can inhabit a type. Furthermore,\nLiquid Haskell's support for refinement reflection enables the use of Haskell\nfor general-purpose mechanized theorem proving. A growing list of large-scale\nmechanized proof developments in Liquid Haskell take advantage of this\ncapability. Adding theorem-proving capabilities to a \"legacy\" language like\nHaskell lets programmers directly verify properties of real-world Haskell\nprograms (taking advantage of the existing highly tuned compiler, run-time\nsystem, and libraries), just by writing Haskell. However, more established\nproof assistants like Agda and Coq offer far better support for interactive\nproof development and insight into the proof state (for instance, what subgoals\nstill need to be proved to finish a partially-complete proof). In contrast,\nLiquid Haskell provides only coarse-grained feedback to the user -- either it\nreports a type error, or not -- unfortunately hindering its usability as a\ntheorem prover.\n  In this paper, we propose improving the usability of Liquid Haskell by\nextending it with support for Agda-style typed holes and interactive editing\ncommands that take advantage of them. In Agda, typed holes allow programmers to\nindicate unfinished parts of a proof, and incrementally complete the proof in a\ndialogue with the compiler. While GHC Haskell already has its own Agda-inspired\nsupport for typed holes, we posit that typed holes would be especially powerful\nand useful if combined with Liquid Haskell's refinement types and SMT\nautomation. We discuss how typed holes might work in Liquid Haskell, and we\nconsider possible implementation approaches and next steps.", "journal": ""}
{"doi": "10.48550/arXiv.2502.15795", "date": "2025-02-18", "title": "Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization", "authors": "Willy Chan, Michael Souliman, Jakob Nordhagen, Brando Miranda, Elyas Obbad, Kai Fronsdal Sanmi Koyejo", "abstract": "Autoformalization, the process of transforming informal mathematical language\ninto formal specifications and proofs remains a difficult task for\nstate-of-the-art (large) language models. Existing works point to competing\nexplanations for the performance gap. To this end, we introduce a novel\nmethodology that leverages back-translation with hand-curated prompts to\nenhance the mathematical capabilities of language models, particularly\naddressing the challenge posed by the scarcity of labeled data. Specifically,\nwe evaluate three primary variations of this strategy: (1) on-the-fly (online)\nbacktranslation, (2) distilled (offline) backtranslation with few-shot\namplification, and (3) line-by-line proof analysis integrated with proof state\ninformation. Each variant is designed to optimize data quality over quantity,\nfocusing on the high fidelity of generated proofs rather than sheer data scale.\nOur findings provide evidence that employing our proposed approaches to\ngenerate synthetic data, which prioritizes quality over volume, improves the\nAutoformalization performance of LLMs as measured by standard benchmarks such\nas ProofNet. Crucially, our approach outperforms pretrained models using a\nminimal number of tokens. We also show, through strategic prompting and\nbacktranslation, that our approaches surpass the performance of fine-tuning\nwith extensive multilingual datasets such as MMA on ProofNet with only 1/150th\nof the tokens. Taken together, our methods show a promising new approach to\nsignificantly reduce the resources required to formalize proofs, thereby\naccelerating AI for math.", "journal": ""}
{"doi": "10.48550/arXiv.0707.4389", "date": "2007-07-30", "title": "Separation Logic for Small-step Cminor", "authors": "Andrew W. Appel, Sandrine Blazy", "abstract": "Cminor is a mid-level imperative programming language; there are\nproved-correct optimizing compilers from C to Cminor and from Cminor to machine\nlanguage. We have redesigned Cminor so that it is suitable for Hoare Logic\nreasoning and we have designed a Separation Logic for Cminor. In this paper, we\ngive a small-step semantics (instead of the big-step of the proved-correct\ncompiler) that is motivated by the need to support future concurrent\nextensions. We detail a machine-checked proof of soundness of our Separation\nLogic. This is the first large-scale machine-checked proof of a Separation\nLogic w.r.t. a small-step semantics. The work presented in this paper has been\ncarried out in the Coq proof assistant. It is a first step towards an\nenvironment in which concurrent Cminor programs can be verified using\nSeparation Logic and also compiled by a proved-correct compiler with formal\nend-to-end correctness guarantees.", "journal": "Dans 20th Int. Conference on Theorem Proving in Higher Order\n  Logics (TPHOLs 2007) 4732 (2007) 5-21"}
{"doi": "10.48550/arXiv.1907.01449", "date": "2019-07-02", "title": "Formalizing the Solution to the Cap Set Problem", "authors": "Sander R. Dahmen, Johannes H\u00f6lzl, Robert Y. Lewis", "abstract": "In 2016, Ellenberg and Gijswijt established a new upper bound on the size of\nsubsets of $\\mathbb{F}^n_q$ with no three-term arithmetic progression. This\nproblem has received much mathematical attention, particularly in the case $q =\n3$, where it is commonly known as the \\emph{cap set problem}. Ellenberg and\nGijswijt's proof was published in the \\emph{Annals of Mathematics} and is\nnoteworthy for its clever use of elementary methods. This paper describes a\nformalization of this proof in the Lean proof assistant, including both the\ngeneral result in $\\mathbb{F}^n_q$ and concrete values for the case $q = 3$. We\nfaithfully follow the pen and paper argument to construct the bound. Our work\nshows that (some) modern mathematics is within the range of proof assistants.", "journal": ""}
{"doi": "10.48550/arXiv.2009.00416", "date": "2020-09-01", "title": "Church's thesis and related axioms in Coq's type theory", "authors": "Yannick Forster", "abstract": "\"Church's thesis\" ($\\mathsf{CT}$) as an axiom in constructive logic states\nthat every total function of type $\\mathbb{N} \\to \\mathbb{N}$ is computable,\ni.e. definable in a model of computation. $\\mathsf{CT}$ is inconsistent in both\nclassical mathematics and in Brouwer's intuitionism since it contradicts Weak\nK\\\"onig's Lemma and the fan theorem, respectively. Recently, $\\mathsf{CT}$ was\nproved consistent for (univalent) constructive type theory.\n  Since neither Weak K\\\"onig's Lemma nor the fan theorem are a consequence of\njust logical axioms or just choice-like axioms assumed in constructive logic,\nit seems likely that $\\mathsf{CT}$ is inconsistent only with a combination of\nclassical logic and choice axioms. We study consequences of $\\mathsf{CT}$ and\nits relation to several classes of axioms in Coq's type theory, a constructive\ntype theory with a universe of propositions which does neither prove classical\nlogical axioms nor strong choice axioms.\n  We thereby provide a partial answer to the question which axioms may preserve\ncomputational intuitions inherent to type theory, and which certainly do not.\nThe paper can also be read as a broad survey of axioms in type theory, with all\nresults mechanised in the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1807.01873", "date": "2018-07-05", "title": "Sharing a Library between Proof Assistants: Reaching out to the HOL Family", "authors": "Fran\u00e7ois Thir\u00e9", "abstract": "We observe today a large diversity of proof systems. This diversity has the\nnegative consequence that a lot of theorems are proved many times. Unlike\nprogramming languages, it is difficult for these systems to co-operate because\nthey do not implement the same logic. Logical frameworks are a class of theorem\nprovers that overcome this issue by their capacity of implementing various\nlogics. In this work, we study the STTforall logic, an extension of Simple Type\nTheory that has been encoded in the logical framework Dedukti. We present a\ntranslation from this logic to OpenTheory, a proof system and interoperability\ntool between provers of the HOL family. We have used this translation to export\nan arithmetic library containing Fermat's little theorem to OpenTheory and to\ntwo other proof systems that are Coq and Matita.", "journal": "EPTCS 274, 2018, pp. 57-71"}
{"doi": "10.48550/arXiv.1909.00097", "date": "2019-08-31", "title": "VST-A: A Foundationally Sound Annotation Verifier", "authors": "Litao Zhou, Jianxing Qin, Qinshi Wang, Andrew W. Appel, Qinxiang Cao", "abstract": "Program verifiers for imperative languages such as C may be annotation-based,\nin which assertions and invariants are put into source files and then checked,\nor tactic-based, where proof scripts separate from programs are interactively\ndeveloped in a proof assistant such as Coq. Annotation verifiers have been more\nautomated and convenient, but some interactive verifiers have richer assertion\nlanguages and formal proofs of soundness. We present VST-A, an annotation\nverifier that uses the rich assertion language of VST, leverages the formal\nsoundness proof of VST, but allows users to describe functional correctness\nproofs intuitively by inserting assertions.\n  VST-A analyzes control flow graphs, decomposes every C function into control\nflow paths between assertions, and reduces program verification problems into\ncorresponding straightline Hoare triples. Compared to existing foundational\nprogram verification tools like VST and Iris, in VST-A, such decompositions and\nreductions are allowed to be nonstructural, which makes VST-A more flexible to\nuse.\n  VST-A's decomposition and reduction is defined in Coq, proved sound in Coq,\nand computed in a call-by-value way in Coq. The soundness proof for reduction\nis totally logical, independent of the complicated semantic model (and\nsoundness proof) of VST's Hoare triple. Because of the rich assertion language,\nnot all reduced proof goals can be automatically checked, but the system allows\nusers to prove residual proof goals using the full power of the Coq proof\nassistant.", "journal": ""}
{"doi": "10.48550/arXiv.2104.05207", "date": "2021-04-12", "title": "Online Machine Learning Techniques for Coq: A Comparison", "authors": "Liao Zhang, Lasse Blaauwbroek, Bartosz Piotrowski, Prokop \u010cern\u00fd, Cezary Kaliszyk, Josef Urban", "abstract": "We present a comparison of several online machine learning techniques for\ntactical learning and proving in the Coq proof assistant. This work builds on\ntop of Tactician, a plugin for Coq that learns from proofs written by the user\nto synthesize new proofs. Learning happens in an online manner, meaning that\nTactician's machine learning model is updated immediately every time the user\nperforms a step in an interactive proof. This has important advantages compared\nto the more studied offline learning systems: (1) it provides the user with a\nseamless, interactive experience with Tactician and, (2) it takes advantage of\nlocality of proof similarity, which means that proofs similar to the current\nproof are likely to be found close by. We implement two online methods, namely\napproximate k-nearest neighbors based on locality sensitive hashing forests and\nrandom decision forests. Additionally, we conduct experiments with gradient\nboosted trees in an offline setting using XGBoost. We compare the relative\nperformance of Tactician using these three learning methods on Coq's standard\nlibrary.", "journal": ""}
{"doi": "10.48550/arXiv.2003.09140", "date": "2020-03-20", "title": "Tactic Learning and Proving for the Coq Proof Assistant", "authors": "Lasse Blaauwbroek, Josef Urban, Herman Geuvers", "abstract": "We present a system that utilizes machine learning for tactic proof search in\nthe Coq Proof Assistant. In a similar vein as the TacticToe project for HOL4,\nour system predicts appropriate tactics and finds proofs in the form of tactic\nscripts. To do this, it learns from previous tactic scripts and how they are\napplied to proof states. The performance of the system is evaluated on the Coq\nStandard Library. Currently, our predictor can identify the correct tactic to\nbe applied to a proof state 23.4% of the time. Our proof searcher can fully\nautomatically prove 39.3% of the lemmas. When combined with the CoqHammer\nsystem, the two systems together prove 56.7% of the library's lemmas.", "journal": "In LPAR, volume 73 of EPiC Series in Computing, pages 138-150.\n  Easychair, 2020"}
{"doi": "10.48550/arXiv.2103.13534", "date": "2021-03-25", "title": "A formal proof of the Lax equivalence theorem for finite difference schemes", "authors": "Mohit Tekriwal, Karthik Duraisamy, Jean-Baptiste Jeannin", "abstract": "The behavior of physical systems is typically modeled using differential\nequations which are too complex to solve analytically. In practical problems,\nthese equations are discretized on a computational domain, and numerical\nsolutions are computed. A numerical scheme is called convergent, if in the\nlimit of infinitesimal discretization, the bounds on the discretization error\nis also infinitesimally small. The approximate solution converges to the \"true\nsolution\" in this limit. The Lax equivalence theorem enables a proof of\nconvergence given consistency and stability of the method. In this work, we\nformally prove the Lax equivalence theorem using the Coq Proof Assistant. We\nassume a continuous linear differential operator between complete normed\nspaces, and define an equivalent mapping in the discretized space. Given that\nthe numerical method is consistent (i.e., the discretization error tends to\nzero as the discretization step tends to zero), and the method is stable (i.e.,\nthe error is uniformly bounded), we formally prove that the approximate\nsolution converges to the true solution. We then demonstrate convergence of the\ndifference scheme on an example problem by proving both its consistency and\nstability, and then applying the Lax equivalence theorem. In order to prove\nconsistency, we use the Taylor-Lagrange theorem by formally showing that the\ndiscretization error is bounded above by the nth power of the discretization\nstep, where n is the order of the truncated Taylor polynomial.", "journal": ""}
{"doi": "10.48550/arXiv.1207.3441", "date": "2012-07-14", "title": "Isabelle/jEdit --- a Prover IDE within the PIDE framework", "authors": "Makarius Wenzel", "abstract": "PIDE is a general framework for document-oriented prover interaction and\nintegration, based on a bilingual architecture that combines ML and Scala. The\noverall aim is to connect LCF-style provers like Isabelle (or Coq or HOL) with\nsophisticated front-end technology on the JVM platform, overcoming command-line\ninteraction at last.\n  The present system description specifically covers Isabelle/jEdit as part of\nthe official release of Isabelle2011-1 (October 2011). It is a concrete Prover\nIDE implementation based on Isabelle/PIDE library modules (implemented in\nScala) on the one hand, and the well-known text editor framework of jEdit\n(implemented in Java) on the other hand.\n  The interaction model of our Prover IDE follows the idea of continuous proof\nchecking: the theory source text is annotated by semantic information by the\nprover as it becomes available incrementally. This works via an asynchronous\nprotocol that neither blocks the editor nor stops the prover from exploiting\nparallelism on multi-core hardware. The jEdit GUI provides standard metaphors\nfor augmented text editing (highlighting, squiggles, tooltips, hyperlinks etc.)\nthat we have instrumented to render the formal content from the prover context.\nFurther refinement of the jEdit display engine via suitable plugins and fonts\napproximates mathematical rendering in the text buffer, including symbols from\nthe TeX repertoire, and sub-/superscripts.\n  Isabelle/jEdit is presented here both as a usable interface for current\nIsabelle, and as a reference application to inspire further projects based on\nPIDE.", "journal": ""}
{"doi": "10.48550/arXiv.2403.18120", "date": "2024-03-26", "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization", "authors": "Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu", "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.", "journal": ""}
{"doi": "10.48550/arXiv.1105.4537", "date": "2011-05-20", "title": "Deciding Kleene Algebras in Coq", "authors": "Thomas Braibant, Damien Pous", "abstract": "We present a reflexive tactic for deciding the equational theory of Kleene\nalgebras in the Coq proof assistant. This tactic relies on a careful\nimplementation of efficient finite automata algorithms, so that it solves\ncasual equations instantaneously and properly scales to larger expressions. The\ndecision procedure is proved correct and complete: correctness is established\nw.r.t. any model by formalising Kozen's initiality theorem; a counter-example\nis returned when the given equation does not hold. The correctness proof is\nchallenging: it involves both a precise analysis of the underlying automata\nalgorithms and a lot of algebraic reasoning. In particular, we have to\nformalise the theory of matrices over a Kleene algebra. We build on the recent\naddition of firstorder typeclasses in Coq in order to work efficiently with the\ninvolved algebraic structures.", "journal": "Logical Methods in Computer Science, Volume 8, Issue 1 (March 2,\n  2012) lmcs:1043"}
{"doi": "10.48550/arXiv.2212.11630", "date": "2022-12-22", "title": "Towards Mechanised Proofs in Double-Pushout Graph Transformation", "authors": "Robert S\u00f6ldner, Detlef Plump", "abstract": "We formalise the basics of the double-pushout approach to graph\ntransformation in the proof assistant Isabelle/HOL and provide associated\nmachine-checked proofs. Specifically, we formalise graphs, graph morphisms and\nrules, and a definition of direct derivations based on deletion and gluing. We\nthen formalise graph pushouts and prove with Isabelle's help that both\ndeletions and gluings are pushouts. We also prove that pushouts are unique up\nto isomorphism. The formalisation comprises around 2000 lines of source text.\nOur motivation is to pave the way for rigorous, machine-checked proofs in the\ntheory of the double-pushout approach, and to lay the foundations for verifying\ngraph transformation systems and rule-based graph programs by interactive\ntheorem proving.", "journal": "EPTCS 374, 2022, pp. 59-75"}
{"doi": "10.48550/arXiv.2310.14022", "date": "2023-10-21", "title": "Trocq: Proof Transfer for Free, With or Without Univalence", "authors": "Cyril Cohen, Enzo Crance, Assia Mahboubi", "abstract": "Libraries of formalized mathematics use a possibly broad range of different\nrepresentations for a same mathematical concept. Yet light to major manual\ninput from users remains most often required for obtaining the corresponding\nvariants of theorems, when such obvious replacements are typically left\nimplicit on paper. This article presents Trocq, a new proof transfer framework\nfor dependent type theory. Trocq is based on a novel formulation of type\nequivalence, used to generalize the univalent parametricity translation. This\nframework takes care of avoiding dependency on the axiom of univalence when\npossible, and may be used with more relations than just equivalences. We have\nimplemented a corresponding plugin for the Coq proof assistant, in the CoqElpi\nmeta-language. We use this plugin on a gallery of representative examples of\nproof transfer issues in interactive theorem proving, and illustrate how Trocq\ncovers the spectrum of several existing tools, used in program verification as\nwell as in formalized mathematics in the broad sense.", "journal": ""}
{"doi": "10.48550/arXiv.2104.14260", "date": "2021-04-29", "title": "A Machine-Assisted Proof of G\u00f6del's Incompleteness Theorems for the Theory of Hereditarily Finite Sets", "authors": "Lawrence C. Paulson", "abstract": "A formalisation of G\\\"odel's incompleteness theorems using the Isabelle proof\nassistant is described. This is apparently the first mechanical verification of\nthe second incompleteness theorem. The work closely follows {\\'S}wierczkowski\n(2003), who gave a detailed proof using hereditarily finite set theory. The\nadoption of this theory is generally beneficial, but it poses certain technical\nissues that do not arise for Peano arithmetic. The formalisation itself should\nbe useful to logicians, particularly concerning the second incompleteness\ntheorem, where existing proofs are lacking in detail.", "journal": "Review of Symbolic Logic 7:3 (2014), 484-498"}
{"doi": "10.48550/arXiv.2502.09225", "date": "2025-02-13", "title": "A Coq Formalization of Unification Modulo Exclusive-Or", "authors": "Yichi Xu, Daniel J. Dougherty, Rose Bohrer", "abstract": "Equational Unification is a critical problem in many areas such as automated\ntheorem proving and security protocol analysis. In this paper, we focus on\nXOR-Unification, that is, unification modulo the theory of exclusive-or. This\ntheory contains an operator with the properties Associativity, Commutativity,\nNilpotency, and the presence of an identity. In the proof assistant Coq, we\nimplement an algorithm that solves XOR unification problems, whose design was\ninspired by Liu and Lynch, and prove it sound, complete, and terminating. Using\nCoq's code extraction capability we obtain an implementation in the programming\nlanguage OCaml.", "journal": "EPTCS 416, 2025, pp. 267-273"}
{"doi": "10.48550/arXiv.1910.12320", "date": "2019-10-27", "title": "Formalising perfectoid spaces", "authors": "Kevin Buzzard, Johan Commelin, Patrick Massot", "abstract": "Perfectoid spaces are sophisticated objects in arithmetic geometry introduced\nby Peter Scholze in 2012. We formalised enough definitions and theorems in\ntopology, algebra and geometry to define perfectoid spaces in the Lean theorem\nprover. This experiment confirms that a proof assistant can handle complexity\nin that direction, which is rather different from formalising a long proof\nabout simple objects. It also confirms that mathematicians with no computer\nscience training can become proficient users of a proof assistant in a\nrelatively short period of time. Finally, we observe that formalising a piece\nof mathematics that is a trending topic boosts the visibility of proof\nassistants amongst pure mathematicians.", "journal": "CPP 2020: Proceedings of the 9th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs, Pages 299-312"}
{"doi": "10.48550/arXiv.2403.14064", "date": "2024-03-21", "title": "Lean4Lean: Towards a Verified Typechecker for Lean, in Lean", "authors": "Mario Carneiro", "abstract": "In this paper we present a new \"external checker\" for the Lean theorem\nprover, written in Lean itself. This is the first complete typechecker for Lean\n4 other than the reference implementation in C++ used by Lean itself, and our\nnew checker is competitive with the original, running between 20% and 50%\nslower and usable to verify all of Lean's mathlib library, forming an\nadditional step in Lean's aim to self-host the full elaborator and compiler.\nMoreover, because the checker is written in a language which admits formal\nverification, it is possible to state and prove properties about the kernel\nitself, and we report on some initial steps taken in this direction to\nformalize the Lean type theory abstractly and express the relation between the\nkernel functions and the type theory. We plan to use this project to help\njustify any future changes to the kernel and type theory and ensure unsoundness\ndoes not sneak in through either the abstract theory or implementation bugs.", "journal": ""}
{"doi": "10.48550/arXiv.2411.06094", "date": "2024-11-09", "title": "Generically Automating Separation Logic by Functors, Homomorphisms and Modules", "authors": "Qiyuan Xu, David Sanan, Zhe Hou, Xiaokun Luan, Conrad Watt, Yang Liu", "abstract": "Foundational verification considers the functional correctness of programming\nlanguages with formalized semantics and uses proof assistants (e.g., Coq,\nIsabelle) to certify proofs. The need for verifying complex programs compels it\nto involve expressive Separation Logics (SLs) that exceed the scopes of\nwell-studied automated proof theories, e.g., symbolic heap. Consequently,\nautomation of SL in foundational verification relies heavily on ad-hoc\nheuristics that lack a systematic meta-theory and face scalability issues. To\nmitigate the gap, we propose a theory to specify SL predicates using abstract\nalgebras including functors, homomorphisms, and modules over rings. Based on\nthis theory, we develop a generic SL automation algorithm to reason about any\ndata structures that can be characterized by these algebras. In addition, we\nalso present algorithms for automatically instantiating the algebraic models to\nreal data structures. The instantiation reuses the algebraic models of\ncomponent structures and preserves their data abstractions. Case studies on\nformalized imperative semantics show our algorithm can instantiate the\nalgebraic models automatically for a variety of complex data structures.\nExperimental results indicate the automatically instantiated reasoners from our\ngeneric theory show similar results to the state-of-the-art systems made of\nspecifically crafted reasoning rules. The presented theories, proofs, and the\nverification framework are formalized in Isabelle/HOL.", "journal": ""}
{"doi": "10.48550/arXiv.2102.07636", "date": "2021-02-04", "title": "Formalized Haar Measure", "authors": "Floris van Doorn", "abstract": "We describe the formalization of the existence and uniqueness of Haar measure\nin the Lean theorem prover. The Haar measure is an invariant regular measure on\nlocally compact groups, and it has not been formalized in a proof assistant\nbefore. We will also discuss the measure theory library in Lean's mathematical\nlibrary \\textsf{mathlib}, and discuss the construction of product measures and\nthe proof of Fubini's theorem for the Bochner integral.", "journal": ""}
{"doi": "10.48550/arXiv.1601.05520", "date": "2016-01-21", "title": "COGENT: Certified Compilation for a Functional Systems Language", "authors": "Liam O'Connor, Christine Rizkallah, Zilin Chen, Sidney Amani, Japheth Lim, Yutaka Nagashima, Thomas Sewell, Alex Hixon, Gabriele Keller, Toby Murray, Gerwin Klein", "abstract": "We present a self-certifying compiler for the COGENT systems language. COGENT\nis a restricted, polymorphic, higher-order, and purely functional language with\nlinear types and without the need for a trusted runtime or garbage collector.\nIt compiles to efficient C code that is designed to interoperate with existing\nC functions. The language is suited for layered systems code with minimal\nsharing such as file systems or network protocol control code. For a well-typed\nCOGENT program, the compiler produces C code, a high-level shallow embedding of\nits semantics in Isabelle/HOL, and a proof that the C code correctly implements\nthis embedding. The aim is for proof engineers to reason about the full\nsemantics of real-world systems code productively and equationally, while\nretaining the interoperability and leanness of C. We describe the formal\nverification stages of the compiler, which include automated formal refinement\ncalculi, a switch from imperative update semantics to functional value\nsemantics formally justified by the linear type system, and a number of\nstandard compiler phases such as type checking and monomorphisation. The\ncompiler certificate is a series of language-level meta proofs and per-program\ntranslation validation phases, combined into one coherent top-level theorem in\nIsabelle/HOL.", "journal": ""}
{"doi": "10.48550/arXiv.1901.06022", "date": "2019-01-17", "title": "Path Spaces of Higher Inductive Types in Homotopy Type Theory", "authors": "Nicolai Kraus, Jakob von Raumer", "abstract": "The study of equality types is central to homotopy type theory.\nCharacterizing these types is often tricky, and various strategies, such as the\nencode-decode method, have been developed.\n  We prove a theorem about equality types of coequalizers and pushouts,\nreminiscent of an induction principle and without any restrictions on the\ntruncation levels. This result makes it possible to reason directly about\ncertain equality types and to streamline existing proofs by eliminating the\nnecessity of auxiliary constructions.\n  To demonstrate this, we give a very short argument for the calculation of the\nfundamental group of the circle (Licata and Shulman '13), and for the fact that\npushouts preserve embeddings. Further, our development suggests a higher\nversion of the Seifert-van Kampen theorem, and the set-truncation operator maps\nit to the standard Seifert-van Kampen theorem (due to Favonia and Shulman '16).\n  We provide a formalization of the main technical results in the proof\nassistant Lean.", "journal": ""}
{"doi": "10.48550/arXiv.2410.01466", "date": "2024-10-02", "title": "A complete formalization of Fermat's Last Theorem for regular primes in Lean", "authors": "Riccardo Brasca, Christopher Birkbeck, Eric Rodriguez Boidi, Alex Best, Ruben van De Velde, Andrew Yang", "abstract": "We formalize a complete proof of the regular case of Fermat's Last Theorem in\nthe Lean4 theorem prover. Our formalization includes a proof of Kummer's lemma,\nthat is the main obstruction to Fermat's Last Theorem for regular primes.\nRather than following the modern proof of Kummer's lemma via class field\ntheory, we prove it by using Hilbert's Theorems 90-94 in a way that is more\namenable to formalization.", "journal": ""}
{"doi": "10.48550/arXiv.1710.08444", "date": "2017-10-23", "title": "Locally Nameless Permutation Types", "authors": "Edsko de Vries, Vasileios Koutavas", "abstract": "We define \"Locally Nameless Permutation Types\", which fuse permutation types\nas used in Nominal Isabelle with the locally nameless representation. We show\nthat this combination is particularly useful when formalizing programming\nlanguages where bound names may become free during execution (\"extrusion\"),\ncommon in process calculi. It inherits the generic definition of permutations\nand support, and associated lemmas, from the Nominal approach, and the ability\nto stay close to pencil-and-paper proofs from the locally nameless approach. We\nexplain how to use cofinite quantification in this setting, show why reasoning\nabout renaming is more important here than in languages without extrusion, and\nprovide results about infinite support, necessary when reasoning about\ncountable choice.", "journal": ""}
{"doi": "10.48550/arXiv.0808.2220", "date": "2008-08-15", "title": "Every Computably Enumerable Random Real Is Provably Computably Enumerable Random", "authors": "Cristian S. Calude, Nicholas J. Hay", "abstract": "We prove that every computably enumerable (c.e.) random real is provable in\nPeano Arithmetic (PA) to be c.e. random. A major step in the proof is to show\nthat the theorem stating that \"a real is c.e. and random iff it is the halting\nprobability of a universal prefix-free Turing machine\" can be proven in PA. Our\nproof, which is simpler than the standard one, can also be used for the\noriginal theorem.\n  Our positive result can be contrasted with the case of computable functions,\nwhere not every computable function is provably computable in PA, or even more\ninterestingly, with the fact that almost all random finite strings are not\nprovably random in PA.\n  We also prove two negative results: a) there exists a universal machine whose\nuniversality cannot be proved in PA, b) there exists a universal machine $U$\nsuch that, based on $U$, PA cannot prove the randomness of its halting\nprobability.\n  The paper also includes a sharper form of the Kraft-Chaitin Theorem, as well\nas a formal proof of this theorem written with the proof assistant Isabelle.", "journal": ""}
{"doi": "10.48550/arXiv.2502.03432", "date": "2025-02-05", "title": "A formalization of Borel determinacy in Lean", "authors": "Sven Manthe", "abstract": "We present a formalization of Borel determinacy in the Lean 4 theorem prover.\nThe formalization includes a definition of Gale-Stewart games and a proof of\nMartin's theorem stating that Borel games are determined. The proof closely\nfollows Martin's \"A purely inductive proof of Borel determinacy\".", "journal": ""}
{"doi": "10.48550/arXiv.1703.03756", "date": "2017-03-10", "title": "A unified treatment of linked and lean tree-decompositions", "authors": "Joshua Erde", "abstract": "There are many results asserting the existence of tree-decompositions of\nminimal width which still represent local connectivity properties of the\nunderlying graph, perhaps the best-known being Thomas' theorem that proves for\nevery graph $G$ the existence of a linked tree-decompositon of width tw$(G)$.\nWe prove a general theorem on the existence of linked and lean\ntree-decompositions, providing a unifying proof of many known results in the\nfield, as well as implying some new results. In particular we prove that every\nmatroid $M$ admits a lean tree-decomposition of width tw$(M)$, generalizing the\nresult of Thomas.", "journal": ""}
{"doi": "10.48550/arXiv.2211.13513", "date": "2022-11-24", "title": "Waterproof: Educational Software for Learning How to Write Mathematical Proofs", "authors": "Jelle Wemmenhove, Dick Arends, Thijs Beurskens, Maitreyee Bhaid, Sean McCarren, Jan Moraal, Diego Rivera Garrido, David Tuin, Malcolm Vassallo, Pieter Wils, Jim Portegies", "abstract": "In order to help students learn how to write mathematical proofs, we adapt\nthe Coq proof assistant into an educational tool we call Waterproof. Like with\nother interactive theorem provers, students write out their proofs inside the\nsoftware using a specific syntax, and the software provides feedback on the\nlogical validity of each step. Waterproof consists of two components: a custom\nproof language that allows formal, machine-verified proofs to be written in a\nstyle that closely resembles handwritten proofs, and a custom editor that\nallows these proofs to be combined with formatted text to improve readability.\nThe editor can be used for Coq documents in general, but also offers special\nfeatures designed for use in education. Student input, for example, can be\nlimited to specific parts of the document to prevent exercises from being\naccidentally deleted. Waterproof has been used to supplement teaching the\nAnalysis 1 course at Eindhoven University of Technology (TU/e) for the last\nfour years. Students started using the specific formulations of proof steps\nfrom the custom proof language in their handwritten proofs; the explicit\nphrasing of these sentences helped to clarify the logical structure of their\narguments.", "journal": "EPTCS 400, 2024, pp. 96-119"}
{"doi": "10.48550/arXiv.2502.05714", "date": "2025-02-08", "title": "Proving the Coding Interview: A Benchmark for Formally Verified Code Generation", "authors": "Quinn Dougherty, Ronak Mehta", "abstract": "We introduce the Formally Verified Automated Programming Progress Standards,\nor FVAPPS, a benchmark of 4715 samples for writing programs and proving their\ncorrectness, the largest formal verification benchmark, including 1083 curated\nand quality controlled samples. Previously, APPS provided a benchmark and\ndataset for programming puzzles to be completed in Python and checked against\nunit tests, of the kind seen in technical assessments in the software\nengineering industry. Building upon recent approaches for benchmarks in\ninteractive theorem proving, we generalize the unit tests to Lean 4 theorems\ngiven without proof (i.e., using Lean's \"sorry\" keyword). On the 406 theorems\nof 100 randomly selected samples, Sonnet correctly proves 30% and Gemini\ncorrectly proves 18%. We challenge the machine learning and program synthesis\ncommunities to solve both each general purpose programming problem and its\nassociated correctness specifications. The benchmark is available at\nhttps://huggingface.co/datasets/quinn-dougherty/fvapps.", "journal": ""}
{"doi": "10.48550/arXiv.2104.12224", "date": "2021-04-25", "title": "Isabelle's Metalogic: Formalization and Proof Checker", "authors": "Tobias Nipkow, Simon Ro\u00dfkopf", "abstract": "Isabelle is a generic theorem prover with a fragment of higher-order logic as\na metalogic for defining object logics. Isabelle also provides proof terms. We\nformalize this metalogic and the language of proof terms in Isabelle/HOL,\ndefine an executable (but inefficient) proof term checker and prove its\ncorrectness w.r.t. the metalogic. We integrate the proof checker with Isabelle\nand run it on a range of logics and theories to check the correctness of all\nthe proofs in those theories.", "journal": "Lecture Notes in Computer Science 12699 (2021) 93-110"}
{"doi": "10.48550/arXiv.1805.00808", "date": "2018-05-01", "title": "Formal Process Virtual Machine for Smart Contracts Verification", "authors": "Zheng Yang, Hang Lei", "abstract": "This paper reports on the development and verification of a novel formal\nsymbolic process virtual machine (FSPVM) for verifying the reliability and\nsecurity of Ethereum smart contracts, denoted as FSPVM-E, completely in Coq\nproof assistant. It adopts execution-verification isomorphism (EVI), an\nextension of Curry-Howard isomorphism (CHI), as its fundamental theoretical\nframework. The current version of FSPVM-E is constructed on a general,\nextensible, and reusable formal memory (GERM) framework, an extensible and\nuniversal formal intermediate programming language Lolisa, which is a large\nsubset of the Solidity programming language using generalized algebraic\ndatatypes, and the corresponding formally verified interpreter of Lolisa,\ndenoted as FEther. It supports the ERC20 standard and can automatically\nsimultaneously symbolically execute the smart contract programs of Ethereum and\nverify their reliability and security properties using Hoare logic in Coq. In\naddition, this work, contributes to solving the problems of automation,\ninconsistency and reusability in higher-order logic theorem proving.", "journal": "International Journal of Performability Engineering, 2018"}
{"doi": "10.48550/arXiv.2104.05348", "date": "2021-04-12", "title": "Quotients of Bounded Natural Functors", "authors": "Basil F\u00fcrer, Andreas Lochbihler, Joshua Schneider, Dmitriy Traytel", "abstract": "The functorial structure of type constructors is the foundation for many\ndefinition and proof principles in higher-order logic (HOL). For example,\ninductive and coinductive datatypes can be built modularly from bounded natural\nfunctors (BNFs), a class of well-behaved type constructors. Composition,\nfixpoints, and, under certain conditions, subtypes are known to preserve the\nBNF structure. In this article, we tackle the preservation question for\nquotients, the last important principle for introducing new types in HOL. We\nidentify sufficient conditions under which a quotient inherits the BNF\nstructure from its underlying type. Surprisingly, lifting the structure in the\nobvious manner fails for some quotients, a problem that also affects the\nquotients of polynomial functors used in the Lean proof assistant. We provide a\nstrictly more general lifting scheme that supports such problematic quotients.\nWe extend the Isabelle/HOL proof assistant with a command that automates the\nregistration of a quotient type as a BNF, reducing the proof burden on the user\nfrom the full set of BNF axioms to our inheritance conditions. We demonstrate\nthe command's usefulness through several case studies.", "journal": "Logical Methods in Computer Science, Volume 18, Issue 1 (February\n  1, 2022) lmcs:7354"}
{"doi": "10.48550/arXiv.1505.07987", "date": "2015-05-29", "title": "SEPIA: Search for Proofs Using Inferred Automata", "authors": "Thomas Gransden, Neil Walkinshaw, Rajeev Raman", "abstract": "This paper describes SEPIA, a tool for automated proof generation in Coq.\nSEPIA combines model inference with interactive theorem proving. Existing proof\ncorpora are modelled using state-based models inferred from tactic sequences.\nThese can then be traversed automatically to identify proofs. The SEPIA system\nis described and its performance evaluated on three Coq datasets. Our results\nshow that SEPIA provides a useful complement to existing automated tactics in\nCoq.", "journal": ""}
{"doi": "10.48550/arXiv.1907.02594", "date": "2019-06-29", "title": "Domain-Specific Language to Encode Induction Heuristics", "authors": "Yutaka Nagashima", "abstract": "Proof assistants, such as Isabelle/HOL, offer tools to facilitate inductive\ntheorem proving. Isabelle experts know how to use these tools effectively;\nhowever, they did not have a systematic way to encode their expertise. To\naddress this problem, we present our domain-specific language, LiFtEr. LiFtEr\nallows experienced Isabelle users to encode their induction heuristics in a\nstyle independent of any problem domain. LiFtEr's interpreter mechanically\nchecks if a given application of induction tool matches the heuristics\nspecified by experienced users, thus systematically transferring experienced\nusers' expertise to new Isabelle users.", "journal": ""}
{"doi": "10.48550/arXiv.1010.5582", "date": "2010-10-27", "title": "Mechanized semantics", "authors": "Xavier Leroy", "abstract": "The goal of this lecture is to show how modern theorem provers---in this\ncase, the Coq proof assistant---can be used to mechanize the specification of\nprogramming languages and their semantics, and to reason over individual\nprograms and over generic program transformations, as typically found in\ncompilers. The topics covered include: operational semantics (small-step,\nbig-step, definitional interpreters); a simple form of denotational semantics;\naxiomatic semantics and Hoare logic; generation of verification conditions,\nwith application to program proof; compilation to virtual machine code and its\nproof of correctness; an example of an optimizing program transformation (dead\ncode elimination) and its proof of correctness.", "journal": "Logics and languages for reliability and security, J. Esparza and\n  B. Spanfelner and O. Grumberg (Ed.) (2010) 195-224"}
{"doi": "10.48550/arXiv.2501.15639", "date": "2025-01-26", "title": "The continuous functional calculus in Lean", "authors": "Anatole Dedecker, Jireh Loreaux", "abstract": "The continuous functional calculus is perhaps the most fundamental\nconstruction in the theory of operator algebras, especially $C^{*}$-algebras.\nHere we document our formalization of the continuous functional calculus in\nLean, which constitutes the first such formalization in any proof assistant.\nOur implementation is already merged into Lean's mathematical library, Mathlib.\nWe provide a brief introduction to the mathematical theory for those unfamiliar\nwith the subject, and then highlight the design decisions in our formalization\nwhich proved to be important for usability. Our exposition is aimed at a\ngeneral mathematical audience and provides a glimpse into the world of\nformalization by laying bare the discovery process.", "journal": ""}
{"doi": "10.48550/arXiv.2405.04282", "date": "2024-05-07", "title": "CoqPyt: Proof Navigation in Python in the Era of LLMs", "authors": "Pedro Carrott, Nuno Saavedra, Kyle Thompson, Sorin Lerner, Jo\u00e3o F. Ferreira, Emily First", "abstract": "Proof assistants enable users to develop machine-checked proofs regarding\nsoftware-related properties. Unfortunately, the interactive nature of these\nproof assistants imposes most of the proof burden on the user, making formal\nverification a complex, and time-consuming endeavor. Recent automation\ntechniques based on neural methods address this issue, but require good\nprogrammatic support for collecting data and interacting with proof assistants.\nThis paper presents CoqPyt, a Python tool for interacting with the Coq proof\nassistant. CoqPyt improves on other Coq-related tools by providing novel\nfeatures, such as the extraction of rich premise data. We expect our work to\naid development of tools and techniques, especially LLM-based, designed for\nproof synthesis and repair. A video describing and demonstrating CoqPyt is\navailable at: https://youtu.be/fk74o0rePM8.", "journal": ""}
{"doi": "10.48550/arXiv.2410.19605", "date": "2024-10-25", "title": "CoqPilot, a plugin for LLM-based generation of proofs", "authors": "Andrei Kozyrev, Gleb Solovev, Nikita Khramov, Anton Podkopaev", "abstract": "We present CoqPilot, a VS Code extension designed to help automate writing of\nCoq proofs. The plugin collects the parts of proofs marked with the admit\ntactic in a Coq file, i.e., proof holes, and combines LLMs along with\nnon-machine-learning methods to generate proof candidates for the holes. Then,\nCoqPilot checks if each proof candidate solves the given subgoal and, if\nsuccessful, replaces the hole with it. The focus of CoqPilot is twofold.\nFirstly, we want to allow users to seamlessly combine multiple Coq generation\napproaches and provide a zero-setup experience for our tool. Secondly, we want\nto deliver a platform for LLM-based experiments on Coq proof generation. We\ndeveloped a benchmarking system for Coq generation methods, available in the\nplugin, and conducted an experiment using it, showcasing the framework's\npossibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo.\nCode at: https://github.com/JetBrains-Research/coqpilot", "journal": ""}
{"doi": "10.48550/arXiv.2412.16689", "date": "2024-12-21", "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation", "authors": "Majd Zayyad, Yossi Adi", "abstract": "The integration of retrieval-augmented techniques with LLMs has shown promise\nin improving performance across various domains. However, their utility in\ntasks requiring advanced reasoning, such as generating and evaluating\nmathematical statements and proofs, remains underexplored. This study explores\nthe use of Lean, a programming language for writing mathematical proofs, to\npopulate the knowledge corpus used by RAG systems. We hope for this to lay the\nfoundation to exploring different methods of using RAGs to improve the\nperformance of LLMs in advanced logical reasoning tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2003.10623", "date": "2020-03-24", "title": "Computer-Assisted Verification of Four Interval Arithmetic Operators", "authors": "Daisuke Ishii, Tomohito Yabu", "abstract": "Interval arithmetic libraries provide the four elementary arithmetic\noperators for operand intervals bounded by floating-point numbers. Actual\nimplementations need to make a large case analysis that considers, e.g.,\nmagnitude relations between all pairs of argument bounds, positional relations\nbetween the arguments and zero, and handling of the special values, infinities\nand NaN. Their correctness is not obvious as they are implemented by human\nhands, which comes to be critical for the reliability. This work provides a\nmechanically-verified interval arithmetic library. For this purpose, we utilize\nthe Why3 platform equipped with a specification language for annotated programs\nand back-end theorem provers. We conduct several proof tasks for each of three\nproperties of the target code: validity, soundness, and tightness; zero\ndivision exception handling is also verified for the division code. To\naccomplish the proof, we propose several techniques for\nspecification/verification. First, we specify additional lemmas that support\ndeductions made by back-end SMT solvers, which enable to discharge proof\nobligations in floating-point arithmetic containing nonlinear terms. Second, we\nexamine the annotation of tightness, which requires to assume that a\ncomputation may result in NaN; we propose specific extremum operators for this\npurpose. In the experiments, applying the techniques in conjunction with the\nAlt-Ergo SMT solver and the Coq proof assistant proved the entire code.", "journal": "Journal of Computational and Applied Mathematics 377, 112893\n  (2020)"}
{"doi": "10.48550/arXiv.1909.05027", "date": "2019-09-11", "title": "The Marriage of Univalence and Parametricity", "authors": "Nicolas Tabareau, \u00c9ric Tanter, Matthieu Sozeau", "abstract": "Reasoning modulo equivalences is natural for everyone, including\nmathematicians. Unfortunately, in proof assistants based on type theory,\nequality is appallingly syntactic and, as a result, exploiting equivalences is\ncumbersome at best. Parametricity and univalence are two major concepts that\nhave been explored to transport programs and proofs across type equivalences,\nbut they fall short of achieving seamless, automatic transport. This work first\nclarifies the limitations of these two concepts in isolation, and then devises\na fruitful marriage between both. The resulting concept, univalent\nparametricity, is an heterogeneous extension of parametricity strengthened with\nunivalence that fully realizes programming and proving modulo equivalences. In\naddition to the theory of univalent parametricity, we present a lightweight\nframework implemented in Coq that allows the user to transparently transfer\ndefinitions and theorems for a type to an equivalent one, as if they were\nequal. For instance, this makes it possible to conveniently switch between an\neasy-to-reason-about representation and a computationally-efficient\nrepresentation, as soon as they are proven equivalent. The combination of\nparametricity and univalence supports transport \\`a la carte: basic univalent\ntransport, which stems from a type equivalence, can be complemented with\nadditional proofs of equivalences between functions over these types, in order\nto be able to lift more programs and proofs, as well as to yield more efficient\nterms. We illustrate the use of univalent parametricity on several examples,\nincluding a recent integration of native integers in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1707.01747", "date": "2017-07-06", "title": "Verifying Strong Eventual Consistency in Distributed Systems", "authors": "Victor B. F. Gomes, Martin Kleppmann, Dominic P. Mulligan, Alastair R. Beresford", "abstract": "Data replication is used in distributed systems to maintain up-to-date copies\nof shared data across multiple computers in a network. However, despite decades\nof research, algorithms for achieving consistency in replicated systems are\nstill poorly understood. Indeed, many published algorithms have later been\nshown to be incorrect, even some that were accompanied by supposed mechanised\nproofs of correctness. In this work, we focus on the correctness of\nConflict-free Replicated Data Types (CRDTs), a class of algorithm that provides\nstrong eventual consistency guarantees for replicated data. We develop a\nmodular and reusable framework in the Isabelle/HOL interactive proof assistant\nfor verifying the correctness of CRDT algorithms. We avoid correctness issues\nthat have dogged previous mechanised proofs in this area by including a network\nmodel in our formalisation, and proving that our theorems hold in all possible\nnetwork behaviours. Our axiomatic network model is a standard abstraction that\naccurately reflects the behaviour of real-world computer networks. Moreover, we\nidentify an abstract convergence theorem, a property of order relations, which\nprovides a formal definition of strong eventual consistency. We then obtain the\nfirst machine-checked correctness theorems for three concrete CRDTs: the\nReplicated Growable Array, the Observed-Remove Set, and an Increment-Decrement\nCounter. We find that our framework is highly reusable, developing proofs of\ncorrectness for the latter two CRDTs in a few hours and with relatively little\nCRDT-specific code.", "journal": "Proceedings of the ACM on Programming Languages (PACMPL), Vol. 1,\n  No. OOPSLA, Article 109, October 2017"}
{"doi": "10.48550/arXiv.2106.12973", "date": "2021-06-24", "title": "Making Tezos smart contracts more reliable with Coq", "authors": "Bruno Bernardo, Rapha\u00ebl Cauderlier, Guillaume Claret, Arvid Jakobsson, Basile Pesin, Julien Tesson", "abstract": "Tezos is a smart-contract blockchain. Tezos smart contracts are written in a\nlow-level stack-based language called Michelson. This article gives an overview\nof efforts using the Coq proof assistant to have stronger guarantees on\nMichelson smart contracts: the Mi-Cho-Coq framework, a Coq library defining\nformal semantics of Michelson, as well as an interpreter, a simple optimiser\nand a weakest-precondition calculus to reason about Michelson smart contracts;\nAlbert, an intermediate language that abstracts Michelson stacks with a\ncompiler written in Coq that targets Mi-Cho-Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2101.07758", "date": "2021-01-17", "title": "A bi-directional extensible interface between Lean and Mathematica", "authors": "Robert Y. Lewis, Minchao Wu", "abstract": "We implement a user-extensible ad hoc connection between the Lean proof\nassistant and the computer algebra system Mathematica. By reflecting the syntax\nof each system in the other and providing a flexible interface for extending\ntranslation, our connection allows for the exchange of arbitrary information\nbetween the two systems.\n  We show how to make use of the Lean metaprogramming framework to verify\ncertain Mathematica computations, so that the rigor of the proof assistant is\nnot compromised. We also use Mathematica as an untrusted oracle to guide proof\nsearch in the proof assistant and interact with a Mathematica notebook from\nwithin a Lean session. In the other direction, we import and process Lean\ndeclarations from within Mathematica. The proof assistant library serves as a\ndatabase of mathematical knowledge that the CAS can display and explore.", "journal": ""}
{"doi": "10.48550/arXiv.2201.05716", "date": "2022-01-15", "title": "Mechanizing Matching Logic In Coq", "authors": "P\u00e9ter Bereczky, Xiaohong Chen, D\u00e1niel Horp\u00e1csi, Lucas Pe\u00f1a, Jan Tu\u0161il", "abstract": "Matching logic is a formalism for specifying, and reasoning about,\nmathematical structures, using patterns and pattern matching. Growing in\npopularity, it has been used to define many logical systems such as separation\nlogic with recursive definitions and linear temporal logic. In addition, it\nserves as the logical foundation of the K semantic framework, which was used to\nbuild practical verifiers for a number of real-world languages. Despite being a\nfundamental formal system accommodating substantial theories, matching logic\nlacks a general-purpose, machine-checked formalization. Hence, we formalize\nmatching logic using the Coq proof assistant. Specifically, we create a new\nrepresentation of matching logic that uses a locally nameless encoding, and we\nformalize the syntax, semantics, and proof system of this representation in the\nCoq proof assistant. Crucially, we prove the soundness of the formalized proof\nsystem and provide a means to carry out interactive matching logic reasoning in\nCoq. We believe this work provides a previously unexplored avenue for reasoning\nabout matching logic, its models, and the proof system.", "journal": "EPTCS 369, 2022, pp. 17-36"}
{"doi": "10.48550/arXiv.0908.0494", "date": "2009-08-04", "title": "A Formalization of the Semantics of Functional-Logic Programming in Isabelle", "authors": "Francisco L\u00f3pez Fraguas, Stephan Merz, Juan Rodr\u00edguez Hortal\u00e1", "abstract": "Modern functional-logic programming languages like Toy or Curry feature\nnon-strict non-deterministic functions that behave under call-time choice\nsemantics. A standard formulation for this semantics is the CRWL logic, that\nspecifies a proof calculus for computing the set of possible results for each\nexpression. In this paper we present a formalization of that calculus in the\nIsabelle/HOL proof assistant. We have proved some basic properties of CRWL:\nclosedness under c-substitutions, polarity and compositionality. We also\ndiscuss some insights that have been gained, such as the fact that left\nlinearity of program rules is not needed for any of these results to hold.", "journal": "22nd International Conference Theorem Proving for Higher-Order\n  Logics (TPHOLs 2009): emerging trends session (2009)"}
{"doi": "10.48550/arXiv.2108.13647", "date": "2021-08-31", "title": "A Mechanically Verified Theory of Contracts", "authors": "St\u00e9phane Kastenbaum, Beno\u00eet Boyer, Jean-Pierre Talpin", "abstract": "Cyber-physical systems (CPS) are assemblies of networked, heterogeneous,\nhardware, and software components sensing, evaluating, and actuating a physical\nenvironment. This heterogeneity induces complexity that makes CPSs challenging\nto model correctly. Since CPSs often have critical functions, it is however of\nutmost importance to formally verify them in order to provide the highest\nguarantees of safety. Faced with CPS complexity, model abstraction becomes\nparamount to make verification attainable. To this end, assume/guarantee\ncontracts enable component model abstraction to support a sound, structured,\nand modular verification process. While abstractions of models by contracts are\nusually proved sound, none of the related contract frameworks themselves have,\nto the best of our knowledge, been formally proved correct so far. In this aim,\nwe present the formalization of a generic assume/guarantee contract theory in\nthe proof assistant Coq. We identify and prove theorems that ensure its\ncorrectness. Our theory is generic, or parametric, in that it can be\ninstantiated and used with any given logic, in particular hybrid logics, in\nwhich highly complex cyber-physical systems can uniformly be described.", "journal": "International Colloquium on Theoretical Aspects of Computing, Sep\n  2021, Nur-Sultan, Kazakhstan. pp.134-151"}
{"doi": "10.48550/arXiv.1906.08084", "date": "2019-06-19", "title": "LiFtEr: Language to Encode Induction Heuristics for Isabelle/HOL", "authors": "Yutaka Nagashima", "abstract": "Proof assistants, such as Isabelle/HOL, offer tools to facilitate inductive\ntheorem proving. Isabelle experts know how to use these tools effectively;\nhowever, there is a little tool support for transferring this expert knowledge\nto a wider user audience. To address this problem, we present our\ndomain-specific language, LiFtEr. LiFtEr allows experienced Isabelle users to\nencode their induction heuristics in a style independent of any problem domain.\nLiFtEr's interpreter mechanically checks if a given application of induction\ntool matches the heuristics, thus automating the knowledge transfer loop.", "journal": ""}
{"doi": "10.48550/arXiv.2202.05959", "date": "2022-02-12", "title": "Formalization of a Stochastic Approximation Theorem", "authors": "Koundinya Vajjha, Barry Trager, Avraham Shinnar, Vasily Pestun", "abstract": "Stochastic approximation algorithms are iterative procedures which are used\nto approximate a target value in an environment where the target is unknown and\ndirect observations are corrupted by noise. These algorithms are useful, for\ninstance, for root-finding and function minimization when the target function\nor model is not directly known. Originally introduced in a 1951 paper by\nRobbins and Monro, the field of Stochastic approximation has grown enormously\nand has come to influence application domains from adaptive signal processing\nto artificial intelligence. As an example, the Stochastic Gradient Descent\nalgorithm which is ubiquitous in various subdomains of Machine Learning is\nbased on stochastic approximation theory. In this paper, we give a formal proof\n(in the Coq proof assistant) of a general convergence theorem due to Aryeh\nDvoretzky, which implies the convergence of important classical methods such as\nthe Robbins-Monro and the Kiefer-Wolfowitz algorithms. In the process, we build\na comprehensive Coq library of measure-theoretic probability theory and\nstochastic processes.", "journal": ""}
{"doi": "10.48550/arXiv.2303.12404", "date": "2023-03-22", "title": "Un experimento de demostraci\u00f3n formal de un teorema de nivel intermedio en \u00e1lgebra (Formalizing the proof of an intermediate-level algebra theorem -- An experiment)", "authors": "Antoine Chambert-Loir", "abstract": "Proof assistants are computer softwares that allow us to write mathematical\nproofs so as to assess their correctness. In November 2021, I started the\nproject of checking the simplicity of the alternating groups within the Lean\ntheorem prover and its mathlib library. This text aims at reviewing this\nexperiment.\n  --\n  (French) Les assistants de preuves sont des logiciels qui permettent de\nr\\'ediger des d\\'emonstrations math\\'ematiques et d'en garantir leur\ncorrection. En novembre 2021, j'ai d\\'ebut\\'e un projet de v\\'erification de la\nsimplicit\\'e des groupes altern\\'es au sein de l'assistant de preuve Lean, et\nde sa librairie mathlib. Ce texte est un essai de compte rendu de cette\nexp\\'erience.\n  --\n  Published version in Spanish", "journal": "La Gaceta de la RSME, Vol. 26 (2023), N\\'um. 3, P\\'ags. 535--553"}
{"doi": "10.48550/arXiv.1803.00403", "date": "2018-02-23", "title": "A general formal memory framework in Coq for verifying the properties of programs based on higher-order logic theorem proving with increased automation, consistency, and reusability", "authors": "Zheng Yang, Hang Lei", "abstract": "In recent years, a number of lightweight programs have been deployed in\ncritical domains, such as in smart contracts based on blockchain technology.\nTherefore, the security and reliability of such programs should be guaranteed\nby the most credible technology. Higher-order logic theorem proving is one of\nthe most reliable technologies for verifying the properties of programs.\nHowever, programs may be developed by different high-level programming\nlanguages, and a general, extensible, and reusable formal memory (GERM)\nframework that can simultaneously support different formal verification\nspecifications, particularly at the code level, is presently unavailable for\nverifying the properties of programs. Therefore, the present work proposes a\nGERM framework to fill this gap. The framework simulates physical memory\nhardware structure, including a low-level formal memory space, and provides a\nset of simple, nonintrusive application programming interfaces and assistant\ntools using Coq that can support different formal verification specifications\nsimultaneously. The proposed GERM framework is independent and customizable,\nand was verified entirely in Coq. We also present an extension of Curry-Howard\nisomorphism, denoted as execution-verification isomorphism (EVI), which\ncombines symbolic execution and theorem proving for increasing the degree of\nautomation in higher-order logic theorem proving assistant tools. We also\nimplement a toy functional programming language in a generalized algebraic\ndatatypes style and a formal interpreter in Coq based on the GERM framework.\nThese implementations are then employed to demonstrate the application of EVI\nto a simple code segment.", "journal": ""}
{"doi": "10.48550/arXiv.0811.1914", "date": "2008-11-12", "title": "A TLA+ Proof System", "authors": "Kaustuv C. Chaudhuri, Damien Doligez, Leslie Lamport, Stephan Merz", "abstract": "We describe an extension to the TLA+ specification language with constructs\nfor writing proofs and a proof environment, called the Proof Manager (PM), to\nchecks those proofs. The language and the PM support the incremental\ndevelopment and checking of hierarchically structured proofs. The PM translates\na proof into a set of independent proof obligations and calls upon a collection\nof back-end provers to verify them. Different provers can be used to verify\ndifferent obligations. The currently supported back-ends are the tableau prover\nZenon and Isabelle/TLA+, an axiomatisation of TLA+ in Isabelle/Pure. The proof\nobligations for a complete TLA+ proof can also be used to certify the theorem\nin Isabelle/TLA+.", "journal": "Knowledge Exchange: Automated Provers and Proof Assistants\n  (KEAPPA) (2008)"}
{"doi": "10.48550/arXiv.2301.12893", "date": "2023-01-30", "title": "Formalizing Piecewise Affine Activation Functions of Neural Networks in Coq", "authors": "Andrei Aleksandrov, Kim V\u00f6llinger", "abstract": "Verification of neural networks relies on activation functions being\npiecewise affine (pwa) -- enabling an encoding of the verification problem for\ntheorem provers. In this paper, we present the first formalization of pwa\nactivation functions for an interactive theorem prover tailored to verifying\nneural networks within Coq using the library Coquelicot for real analysis. As a\nproof-of-concept, we construct the popular pwa activation function ReLU. We\nintegrate our formalization into a Coq model of neural networks, and devise a\nverified transformation from a neural network N to a pwa function representing\nN by composing pwa functions that we construct for each layer. This\nrepresentation enables encodings for proof automation, e.g. Coq's tactic lra --\na decision procedure for linear real arithmetic. Further, our formalization\npaves the way for integrating Coq in frameworks of neural network verification\nas a fallback prover when automated proving fails.", "journal": ""}
{"doi": "10.48550/arXiv.2206.03358", "date": "2022-06-07", "title": "Towards a Coq formalization of a quantified modal logic", "authors": "Ana de Almeida Borges", "abstract": "We present a Coq formalization of the Quantified Reflection Calculus with one\nmodality, or $\\mathsf{QRC}_1$. This is a decidable, strictly positive, and\nquantified modal logic previously studied for its applications in proof theory.\nThe highlights are a deep embedding of $\\mathsf{QRC}_1$ in the Coq proof\nassistant, a mechanization of the notion of Kripke model with varying domains\nand a formalization of the soundness theorem. We focus on the design decisions\ninherent to the formalization and the insights that led to new and simplified\nproofs.", "journal": "Proceedings of the 4th Workshop on Automated Reasoning in\n  Quantified Non-Classical Logics (ARQNL 2022): 13-27 (2022)"}
{"doi": "10.48550/arXiv.2001.08983", "date": "2020-01-04", "title": "A Formal Development Cycle for Security Engineering in Isabelle", "authors": "Florian Kamm\u00fcller", "abstract": "In this paper, we show a security engineering process based on a formal\nnotion of refinement fully formalized in the proof assistant Isabelle. This\nRefinement-Risk Cycle focuses on attack analysis and security refinement\nsupported by interactive theorem proving. Since we use a fully formalized model\nof infrastructures with actors and policies we can support a novel way of\nformal security refinement for system specifications. This formal process is\nbuilt practically as an extension to the Isabelle Infrastructure framework with\nattack trees. We define a formal notion of refinement on infrastructure models.\nThanks to the formal foundation of Kripke structures and branching time\ntemporal logic in the Isabelle Infrastructure framework, these stepwise\ntransformations can be interleaved with attack tree analysis thus providing a\nfully formal security engineering framework. The process is illustrated on an\nIoT healthcare case study introducing GDPR requirements and blockchain.", "journal": ""}
{"doi": "10.48550/arXiv.1406.4291", "date": "2014-06-17", "title": "Vector Clocks in Coq: An Experience Report", "authors": "Christopher Meiklejohn", "abstract": "This report documents the process of implementing vector clocks in the Coq\nproof assistant for extraction and use in the distributed Dynamo-inspired data\nstore, Riak. In this report, we focus on the technical challenges of using Core\nErlang code extracted from the proof assistant in a production-grade Erlang\napplication, as opposed to verification of the model itself.", "journal": ""}
{"doi": "10.48550/arXiv.2407.03685", "date": "2024-07-04", "title": "Verifying Peephole Rewriting In SSA Compiler IRs", "authors": "Siddharth Bhat, Alex Keizer, Chris Hughes, Andr\u00e9s Goens, Tobias Grosser", "abstract": "There is an increasing need for domain-specific reasoning in modern\ncompilers. This has fueled the use of tailored intermediate representations\n(IRs) based on static single assignment (SSA), like in the MLIR compiler\nframework. Interactive theorem provers (ITPs) provide strong guarantees for the\nend-to-end verification of compilers (e.g., CompCert). However, modern\ncompilers and their IRs evolve at a rate that makes proof engineering alongside\nthem prohibitively expensive. Nevertheless, well-scoped push-button automated\nverification tools such as the Alive peephole verifier for LLVM-IR gained\nrecognition in domains where SMT solvers offer efficient (semi) decision\nprocedures. In this paper, we aim to combine the convenience of automation with\nthe versatility of ITPs for verifying peephole rewrites across domain-specific\nIRs. We formalize a core calculus for SSA-based IRs that is generic over the IR\nand covers so-called regions (nested scoping used by many domain-specific IRs\nin the MLIR ecosystem). Our mechanization in the Lean proof assistant provides\na user-friendly frontend for translating MLIR syntax into our calculus. We\nprovide scaffolding for defining and verifying peephole rewrites, offering\ntactics to eliminate the abstraction overhead of our SSA calculus. We prove\ncorrectness theorems about peephole rewriting, as well as two classical program\ntransformations. To evaluate our framework, we consider three use cases from\nthe MLIR ecosystem that cover different levels of abstractions: (1) bitvector\nrewrites from LLVM, (2) structured control flow, and (3) fully homomorphic\nencryption. We envision that our mechanization provides a foundation for\nformally verified rewrites on new domain-specific IRs.", "journal": ""}
{"doi": "10.48550/arXiv.1408.1364", "date": "2014-08-06", "title": "Constructing categories and setoids of setoids in type theory", "authors": "Erik Palmgren, Olov Wilander", "abstract": "In this paper we consider the problem of building rich categories of setoids,\nin standard intensional Martin-L\\\"of type theory (MLTT), and in particular how\nto handle the problem of equality on objects in this context. Any\n(proof-irrelevant) family F of setoids over a setoid A gives rise to a category\nC(A, F) of setoids with objects A. We may regard the family F as a setoid of\nsetoids, and a crucial issue in this article is to construct rich or large\nenough such families. Depending on closure conditions of F, the category C(A,\nF) has corresponding categorical constructions. We exemplify this with finite\nlimits. A very large family F may be obtained from Aczel's model construction\nof CZF in type theory. It is proved that the category so obtained is isomorphic\nto the internal category of sets in this model. Set theory can thus establish\n(categorical) properties of C(A, F) which may be used in type theory. We also\nshow that Aczel's model construction may be extended to include the elements of\nany setoid as atoms or urelements. As a byproduct we obtain a natural extension\nof CZF, adding atoms. This extension, CZFU, is validated by the extended model.\nThe main theorems of the paper have been checked in the proof assistant Coq\nwhich is based on MLTT. A possible application of this development is to\nintegrate set-theoretic and type-theoretic reasoning in proof assistants.", "journal": "Logical Methods in Computer Science, Volume 10, Issue 3 (September\n  23, 2014) lmcs:964"}
{"doi": "10.48550/arXiv.1711.09286", "date": "2017-11-25", "title": "Total Haskell is Reasonable Coq", "authors": "Antal Spector-Zabusky, Joachim Breitner, Christine Rizkallah, Stephanie Weirich", "abstract": "We would like to use the Coq proof assistant to mechanically verify\nproperties of Haskell programs. To that end, we present a tool, named\nhs-to-coq, that translates total Haskell programs into Coq programs via a\nshallow embedding. We apply our tool in three case studies -- a lawful Monad\ninstance, \"Hutton's razor\", and an existing data structure library -- and prove\ntheir correctness. These examples show that this approach is viable: both that\nhs-to-coq applies to existing Haskell code, and that the output it produces is\namenable to verification.", "journal": ""}
{"doi": "10.48550/arXiv.2308.14608", "date": "2023-08-28", "title": "AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics", "authors": "Vahid Ghafouri, Vibhor Agarwal, Yong Zhang, Nishanth Sastry, Jose Such, Guillermo Suarez-Tangil", "abstract": "The introduction of ChatGPT and the subsequent improvement of Large Language\nModels (LLMs) have prompted more and more individuals to turn to the use of\nChatBots, both for information and assistance with decision-making. However,\nthe information the user is after is often not formulated by these ChatBots\nobjectively enough to be provided with a definite, globally accepted answer.\n  Controversial topics, such as \"religion\", \"gender identity\", \"freedom of\nspeech\", and \"equality\", among others, can be a source of conflict as partisan\nor biased answers can reinforce preconceived notions or promote disinformation.\nBy exposing ChatGPT to such debatable questions, we aim to understand its level\nof awareness and if existing models are subject to socio-political and/or\neconomic biases. We also aim to explore how AI-generated answers compare to\nhuman ones. For exploring this, we use a dataset of a social media platform\ncreated for the purpose of debating human-generated claims on polemic subjects\namong users, dubbed Kialo.\n  Our results show that while previous versions of ChatGPT have had important\nissues with controversial topics, more recent versions of ChatGPT\n(gpt-3.5-turbo) are no longer manifesting significant explicit biases in\nseveral knowledge areas. In particular, it is well-moderated regarding economic\naspects. However, it still maintains degrees of implicit libertarian leaning\ntoward right-winged ideals which suggest the need for increased moderation from\nthe socio-political point of view. In terms of domain knowledge on\ncontroversial topics, with the exception of the \"Philosophical\" category,\nChatGPT is performing well in keeping up with the collective human level of\nknowledge. Finally, we see that sources of Bing AI have slightly more tendency\nto the center when compared to human answers. All the analyses we make are\ngeneralizable to other types of biases and domains.", "journal": ""}
{"doi": "10.48550/arXiv.2201.01649", "date": "2022-01-05", "title": "WebSpec: Towards Machine-Checked Analysis of Browser Security Mechanisms", "authors": "Lorenzo Veronese, Benjamin Farinier, Pedro Bernardo, Mauro Tempesta, Marco Squarcina, Matteo Maffei", "abstract": "The complexity of browsers has steadily increased over the years, driven by\nthe continuous introduction and update of Web platform components, such as\nnovel Web APIs and security mechanisms. Their specifications are manually\nreviewed by experts to identify potential security issues. However, this\nprocess has proved to be error-prone due to the extensiveness of modern browser\nspecifications and the interplay between new and existing Web platform\ncomponents. To tackle this problem, we developed WebSpec, the first formal\nsecurity framework for the analysis of browser security mechanisms, which\nenables both the automatic discovery of logical flaws and the development of\nmachine-checked security proofs. WebSpec, in particular, includes a\ncomprehensive semantic model of the browser in the Coq proof assistant, a\nformalization in this model of ten Web security invariants, and a toolchain\nturning the Coq model and the Web invariants into SMT-lib formulas to enable\nmodel checking with the Z3 theorem prover. If a violation is found, the\ntoolchain automatically generates executable tests corresponding to the\ndiscovered attack trace, which is validated across major browsers. We showcase\nthe effectiveness of WebSpec by discovering two new logical flaws caused by the\ninteraction of different browser mechanisms and by identifying three previously\ndiscovered logical flaws in the current Web platform, as well as five in old\nversions. Finally, we show how WebSpec can aid the verification of our proposed\nchanges to amend the reported inconsistencies affecting the current Web\nplatform.", "journal": ""}
{"doi": "10.48550/arXiv.2006.12789", "date": "2020-06-23", "title": "Modelling Value-oriented Legal Reasoning in LogiKEy", "authors": "Christoph Benzm\u00fcller, David Fuenmayor, Bertram Lomfeld", "abstract": "The logico-pluralist LogiKEy knowledge engineering methodology and framework\nis applied to the modelling of a theory of legal balancing in which legal\nknowledge (cases and laws) is encoded by utilising context-dependent value\npreferences. The theory obtained is then used to formalise, automatically\nevaluate, and reconstruct illustrative property law cases (involving\nappropriation of wild animals) within the Isabelle proof assistant system,\nillustrating how LogiKEy can harness interactive and automated theorem proving\ntechnology to provide a testbed for the development and formal verification of\nlegal domain-specific languages and theories. Modelling value-oriented legal\nreasoning in that framework, we establish novel bridges between latest research\nin knowledge representation and reasoning in non-classical logics, automated\ntheorem proving, and applications in legal reasoning.", "journal": ""}
{"doi": "10.48550/arXiv.2309.13802", "date": "2023-09-25", "title": "While Loops in Coq", "authors": "David Nowak, Vlad Rusu", "abstract": "While loops are present in virtually all imperative programming languages.\nThey are important both for practical reasons (performing a number of\niterations not known in advance) and theoretical reasons (achieving Turing\ncompleteness). In this paper we propose an approach for incorporating while\nloops in an imperative language shallowly embedded in the Coq proof assistant.\nThe main difficulty is that proving the termination of while loops is\nnontrivial, or impossible in the case of non-termination, whereas Coq only\naccepts programs endowed with termination proofs. Our solution is based on a\nnew, general method for defining possibly non-terminating recursive functions\nin Coq. We illustrate the approach by proving termination and partial\ncorrectness of a program on linked lists.", "journal": "EPTCS 389, 2023, pp. 96-109"}
{"doi": "10.48550/arXiv.2405.13036", "date": "2024-05-16", "title": "Can formal argumentative reasoning enhance LLMs performances?", "authors": "Federico Castagna, Isabel Sassoon, Simon Parsons", "abstract": "Recent years witnessed significant performance advancements in\ndeep-learning-driven natural language models, with a strong focus on the\ndevelopment and release of Large Language Models (LLMs). These improvements\nresulted in better quality AI-generated output but rely on resource-expensive\ntraining and upgrading of models. Although different studies have proposed a\nrange of techniques to enhance LLMs without retraining, none have considered\ncomputational argumentation as an option. This is a missed opportunity since\ncomputational argumentation is an intuitive mechanism that formally captures\nagents' interactions and the information conflict that may arise during such\ninterplays, and so it seems well-suited for boosting the reasoning and\nconversational abilities of LLMs in a seamless manner. In this paper, we\npresent a pipeline (MQArgEng) and preliminary study to evaluate the effect of\nintroducing computational argumentation semantics on the performance of LLMs.\nOur experiment's goal was to provide a proof-of-concept and a feasibility\nanalysis in order to foster (or deter) future research towards a fully-fledged\nargumentation engine plugin for LLMs. Exploratory results using the MT-Bench\nindicate that MQArgEng provides a moderate performance gain in most of the\nexamined topical categories and, as such, show promise and warrant further\nresearch.", "journal": ""}
{"doi": "10.48550/arXiv.2208.14260", "date": "2022-08-30", "title": "Program Equivalence in an Untyped, Call-by-value Lambda Calculus with Uncurried Recursive Functions", "authors": "D\u00e1niel Horp\u00e1csi, P\u00e9ter Bereczky, Simon Thompson", "abstract": "We aim to reason about the correctness of behaviour-preserving\ntransformations of Erlang programs. Behaviour preservation is characterised by\nsemantic equivalence. Based upon our existing formal semantics for Core Erlang,\nwe investigate potential definitions of suitable equivalence relations. In\nparticular we adapt a number of existing approaches of expression equivalence\nto a simple functional programming language that carries the main features of\nsequential Core Erlang; we then examine the properties of the equivalence\nrelations and formally establish connections between them. The results\npresented in this paper, including all theorems and their proofs, have been\nmachine checked using the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2303.11445", "date": "2023-03-20", "title": "Infinite Words and Morphic Languages Formalized in Isabelle/HOL", "authors": "\u0160t\u011bp\u00e1n Starosta", "abstract": "We present a formalization of basics related to infinite words in the generic\nproof assistant Isabelle/HOL. Furthermore, we present a formalization of purely\nmorphic and morphic languages. Finally, we present a formalized definition of\nSturmian words as lower mechanical words and prove some very elementary facts.\nThe formalization is based on an ongoing larger project of formalization of\ncombinatorics on words.", "journal": ""}
{"doi": "10.48550/arXiv.2005.13954", "date": "2020-05-28", "title": "Adding an Abstraction Barrier to ZF Set Theory", "authors": "Ciar\u00e1n Dunne, J. B. Wells, Fairouz Kamareddine", "abstract": "Much mathematical writing exists that is, explicitly or implicitly, based on\nset theory, often Zermelo-Fraenkel set theory (ZF) or one of its variants. In\nZF, the domain of discourse contains only sets, and hence every mathematical\nobject must be a set. Consequently, in ZF, with the usual encoding of an\nordered pair ${\\langle a, b\\rangle}$, formulas like ${\\{a\\} \\in \\langle a, b\n\\rangle}$ have truth values, and operations like ${\\mathcal P (\\langle a,\nb\\rangle)}$ have results that are sets. Such 'accidental theorems' do not match\nhow people think about the mathematics and also cause practical difficulties\nwhen using set theory in machine-assisted theorem proving. In contrast, in a\nnumber of proof assistants, mathematical objects and concepts can be built of\ntype-theoretic stuff so that many mathematical objects can be, in essence,\nterms of an extended typed ${\\lambda}$-calculus. However, dilemmas and\nfrustration arise when formalizing mathematics in type theory.\n  Motivated by problems of formalizing mathematics with (1) purely\nset-theoretic and (2) type-theoretic approaches, we explore an option with much\nof the flexibility of set theory and some of the useful features of type\ntheory. We present ZFP: a modification of ZF that has ordered pairs as\nprimitive, non-set objects. ZFP has a more natural and abstract axiomatic\ndefinition of ordered pairs free of any notion of representation. This paper\npresents axioms for ZFP, and a proof in ZF (machine-checked in Isabelle/ZF) of\nthe existence of a model for ZFP, which implies that ZFP is consistent if ZF\nis. We discuss the approach used to add this abstraction barrier to ZF.", "journal": ""}
{"doi": "10.48550/arXiv.2004.13312", "date": "2020-04-28", "title": "Certifying Certainty and Uncertainty in Approximate Membership Query Structures -- Extended Version", "authors": "Kiran Gopinathan, Ilya Sergey", "abstract": "Approximate Membership Query structures (AMQs) rely on randomisation for\ntime- and space-efficiency, while introducing a possibility of false positive\nand false negative answers. Correctness proofs of such structures involve\nsubtle reasoning about bounds on probabilities of getting certain outcomes.\nBecause of these subtleties, a number of unsound arguments in such proofs have\nbeen made over the years. In this work, we address the challenge of building\nrigorous and reusable computer-assisted proofs about probabilistic\nspecifications of AMQs. We describe the framework for systematic decomposition\nof AMQs and their properties into a series of interfaces and reusable\ncomponents. We implement our framework as a library in the Coq proof assistant\nand showcase it by encoding in it a number of non-trivial AMQs, such as Bloom\nfilters, counting filters, quotient filters and blocked constructions, and\nmechanising the proofs of their probabilistic specifications. We demonstrate\nhow AMQs encoded in our framework guarantee the absence of false negatives by\nconstruction. We also show how the proofs about probabilities of false\npositives for complex AMQs can be obtained by means of verified reduction to\nthe implementations of their simpler counterparts. Finally, we provide a\nlibrary of domain-specific theorems and tactics that allow a high degree of\nautomation in probabilistic proofs.", "journal": ""}
{"doi": "10.48550/arXiv.2211.06747", "date": "2022-11-12", "title": "Formally Verified Samplers From Probabilistic Programs With Loops and Conditioning", "authors": "Alexander Bagnall, Gordon Stewart, Anindya Banerjee", "abstract": "We present Zar: a formally verified compiler pipeline from discrete\nprobabilistic programs with unbounded loops in the conditional probabilistic\nguarded command language (cpGCL) to proved-correct executable samplers in the\nrandom bit model. We exploit the key idea that all discrete probability\ndistributions can be reduced to unbiased coin-flipping schemes. The compiler\npipeline first translates a cpGCL program into choice-fix trees, an\nintermediate representation suitable for reduction of biased probabilistic\nchoices. Choice-fix trees are then translated to coinductive interaction trees\nfor execution within the random bit model. The correctness of the composed\ntranslations establishes the sampling equidistribution theorem: compiled\nsamplers are correct wrt. the conditional weakest pre-expectation semantics of\ncpGCL source programs. Zar is implemented and fully verified in the Coq proof\nassistant. We extract verified samplers to OCaml and Python and empirically\nvalidate them on a number of illustrative examples.", "journal": ""}
{"doi": "10.48550/arXiv.1905.01735", "date": "2019-05-05", "title": "Interaction with Formal Mathematical Documents in Isabelle/PIDE", "authors": "Makarius Wenzel", "abstract": "Isabelle/PIDE has emerged over more than 10 years as the standard Prover IDE\nfor interactive theorem proving in Isabelle. The well-established Archive of\nFormal Proofs (AFP) testifies the success of such applications of formalized\nmathematics in Isabelle/HOL. More recently, the scope of PIDE has widened\ntowards languages that are not connected to logic and proof in Isabelle, but\ntaken from a broader repertoire of mathematics on the computer. The present\npaper provides a general overview of the PIDE project and its underlying\ndocument model, with built-in parallel evaluation and asynchronous interaction.\nThere is also some discussion of original aims and approaches, successes and\nfailures, later changes to the plan, and ideas for the future.", "journal": ""}
{"doi": "10.48550/arXiv.0808.0586", "date": "2008-08-05", "title": "Coinductive big-step operational semantics", "authors": "Xavier Leroy, Herv\u00e9 Grall", "abstract": "Using a call-by-value functional language as an example, this article\nillustrates the use of coinductive definitions and proofs in big-step\noperational semantics, enabling it to describe diverging evaluations in\naddition to terminating evaluations. We formalize the connections between the\ncoinductive big-step semantics and the standard small-step semantics, proving\nthat both semantics are equivalent. We then study the use of coinductive\nbig-step semantics in proofs of type soundness and proofs of semantic\npreservation for compilers. A methodological originality of this paper is that\nall results have been proved using the Coq proof assistant. We explain the\nproof-theoretic presentation of coinductive definitions and proofs offered by\nCoq, and show that it facilitates the discovery and the presentation of the\nresults.", "journal": "Information and Computation (2007)"}
{"doi": "10.48550/arXiv.2210.13078", "date": "2022-10-24", "title": "Study of a division-like property", "authors": "Robin Khanfir, B\u00e9ranger Seguin", "abstract": "We introduce a weak division-like property for noncommutative rings: a\nnontrivial ring is fadelian if for all nonzero $a,x$ there exist $b,c$ such\nthat $x=ab+ca$. We prove properties of fadelian rings, and construct examples\nof such rings which are not division rings, as well as non-Noetherian and\nnon-Ore examples. We have also formalized some of these results in the Lean\nproof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2103.01346", "date": "2021-03-01", "title": "Roosterize: Suggesting Lemma Names for Coq Verification Projects Using Deep Learning", "authors": "Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric", "abstract": "Naming conventions are an important concern in large verification projects\nusing proof assistants, such as Coq. In particular, lemma names are used by\nproof engineers to effectively understand and modify Coq code. However,\nproviding accurate and informative lemma names is a complex task, which is\ncurrently often carried out manually. Even when lemma naming is automated using\nrule-based tools, generated names may fail to adhere to important conventions\nnot specified explicitly. We demonstrate a toolchain, dubbed Roosterize, which\nautomatically suggests lemma names in Coq projects. Roosterize leverages a\nneural network model trained on existing Coq code, thus avoiding manual\nspecification of naming conventions. To allow proof engineers to conveniently\naccess suggestions from Roosterize during Coq project development, we\nintegrated the toolchain into the popular Visual Studio Code editor. Our\nevaluation shows that Roosterize substantially outperforms strong baselines for\nsuggesting lemma names and is useful in practice. The demo video for Roosterize\ncan be viewed at: https://youtu.be/HZ5ac7Q14rc.", "journal": ""}
{"doi": "10.48550/arXiv.2404.11638", "date": "2024-04-17", "title": "Chain Bounding, the leanest proof of Zorn's lemma, and an illustration of computerized proof formalization", "authors": "Guillermo L. Incatasciato, Pedro S\u00e1nchez Terraf", "abstract": "We present an exposition of the *Chain Bounding Lemma*, which is a common\ngeneralization of both Zorn's Lemma and the Bourbaki-Witt fixed point theorem.\nThe proofs of these results through the use of Chain Bounding are amongst the\nsimplest ones that we are aware of. As a by-product, we show that for every\nposet $P$ and function $f$ from the powerset of $P$ into $P$, there exists a\nmaximal well-ordered chain whose family of initial segments is appropriately\nclosed under $f$.\n  We also provide an introduction to the process of \"computer formalization\" of\nmathematical proofs by using *proofs assistants*. As an illustration, we verify\nour main results with the Lean proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1712.09288", "date": "2017-12-05", "title": "An Extensible Ad Hoc Interface between Lean and Mathematica", "authors": "Robert Y. Lewis", "abstract": "We implement a user-extensible ad hoc connection between the Lean proof\nassistant and the computer algebra system Mathematica. By reflecting the syntax\nof each system in the other and providing a flexible interface for extending\ntranslation, our connection allows for the exchange of arbitrary information\nbetween the two systems. We show how to make use of the Lean metaprogramming\nframework to verify certain Mathematica computations, so that the rigor of the\nproof assistant is not compromised.", "journal": "EPTCS 262, 2017, pp. 23-37"}
{"doi": "10.48550/arXiv.2310.10180", "date": "2023-10-16", "title": "TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models", "authors": "Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng, Xiaodan Liang, Ming Zhang, Qun Liu", "abstract": "Automated theorem proving (ATP) has become an appealing domain for exploring\nthe reasoning ability of the recent successful generative language models.\nHowever, current ATP benchmarks mainly focus on symbolic inference, but rarely\ninvolve the understanding of complex number combination reasoning. In this\nwork, we propose TRIGO, an ATP benchmark that not only requires a model to\nreduce a trigonometric expression with step-by-step proofs but also evaluates a\ngenerative LM's reasoning ability on formulas and its capability to manipulate,\ngroup, and factor number terms. We gather trigonometric expressions and their\nreduced forms from the web, annotate the simplification process manually, and\ntranslate it into the Lean formal language system. We then automatically\ngenerate additional examples from the annotated samples to expand the dataset.\nFurthermore, we develop an automatic generator based on Lean-Gym to create\ndataset splits of varying difficulties and distributions in order to thoroughly\nanalyze the model's generalization ability. Our extensive experiments show our\nproposed TRIGO poses a new challenge for advanced generative LM's including\nGPT-4 which is pre-trained on a considerable amount of open-source formal\ntheorem-proving language data, and provide a new tool to study the generative\nLM's ability on both formal and mathematical reasoning.", "journal": ""}
{"doi": "10.48550/arXiv.1107.4751", "date": "2011-07-24", "title": "Extended Initiality for Typed Abstract Syntax", "authors": "Benedikt Ahrens", "abstract": "Initial Semantics aims at interpreting the syntax associated to a signature\nas the initial object of some category of 'models', yielding induction and\nrecursion principles for abstract syntax. Zsid\\'o proves an initiality result\nfor simply-typed syntax: given a signature S, the abstract syntax associated to\nS constitutes the initial object in a category of models of S in monads.\nHowever, the iteration principle her theorem provides only accounts for\ntranslations between two languages over a fixed set of object types. We\ngeneralize Zsid\\'o's notion of model such that object types may vary, yielding\na larger category, while preserving initiality of the syntax therein. Thus we\nobtain an extended initiality theorem for typed abstract syntax, in which\ntranslations between terms over different types can be specified via the\nassociated category-theoretic iteration operator as an initial morphism. Our\ndefinitions ensure that translations specified via initiality are type-safe,\ni.e. compatible with the typing in the source and target language in the\nobvious sense. Our main example is given via the propositions-as-types\nparadigm: we specify propositions and inference rules of classical and\nintuitionistic propositional logics through their respective typed signatures.\nAfterwards we use the category--theoretic iteration operator to specify a\ndouble negation translation from the former to the latter. A second example is\ngiven by the signature of PCF. For this particular case, we formalize the\ntheorem in the proof assistant Coq. Afterwards we specify, via the\ncategory-theoretic iteration operator, translations from PCF to the untyped\nlambda calculus.", "journal": "Logical Methods in Computer Science, Volume 8, Issue 2 (April 6,\n  2012) lmcs:1193"}
{"doi": "10.48550/arXiv.2303.08894", "date": "2023-03-15", "title": "A Formalization of Operads in Coq", "authors": "Zachary Flores, Angelo Taranto, Eric Bond, Yakir Forman", "abstract": "What provides the highest level of assurance for correctness of execution\nwithin a programming language? One answer, and our solution in particular, to\nthis problem is to provide a formalization for, if it exists, the denotational\nsemantics of a programming language. Achieving such a formalization provides a\ngold standard for ensuring a programming language is correct-by-construction.\nIn our effort on the DARPA V-SPELLS program, we worked to provide a foundation\nfor the denotational semantics of a meta-language using a mathematical object\nknown as an operad. This object has compositional properties which are vital to\nbuilding languages from smaller pieces. In this paper, we discuss our\nformalization of an operad in the proof assistant Coq. Moreover, our definition\nwithin Coq is capable of providing proofs that objects specified within Coq are\noperads. This work within Coq provides a formal mathematical basis for our\nmeta-language development within V-SPELLS. Our work also provides, to our\nknowledge, the first known formalization of operads within a proof assistant\nthat has significant automation, as well as a model that can be replicated\nwithout knowledge of Homotopy Type Theory.", "journal": ""}
{"doi": "10.48550/arXiv.1412.6781", "date": "2014-12-21", "title": "Polarities & Focussing: a journey from Realisability to Automated Reasoning", "authors": "St\u00e9phane Graham-Lengrand", "abstract": "This dissertation explores the roles of polarities and focussing in various\naspects of Computational Logic. These concepts play a key role in the the\ninterpretation of proofs as programs, a.k.a. the Curry-Howard correspondence,\nin the context of classical logic. Arising from linear logic, they allow the\nconstruction of meaningful semantics for cut-elimination in classical logic,\nsome of which relate to the Call-by-Name and Call-by-Value disciplines of\nfunctional programming. The first part of this dissertation provides an\nintroduction to these interpretations, highlighting the roles of polarities and\nfocussing. For instance: proofs of positive formulae provide structured data,\nwhile proofs of negative formulae consume such data; focussing allows the\ndescription of the interaction between the two kinds of proofs as pure\npattern-matching. This idea is pushed further in the second part of this\ndissertation, and connected to realisability semantics, where the structured\ndata is interpreted algebraically, and the consumption of such data is modelled\nwith the use of an orthogonality relation. Most of this part has been proved in\nthe Coq proof assistant. Polarities and focussing were also introduced with\napplications to logic programming in mind, where computation is proof-search.\nIn the third part of this dissertation, we push this idea further by exploring\nthe roles that these concepts can play in other applications of proof-search,\nsuch as theorem proving and more particularly automated reasoning. We use these\nconcepts to describe the main algorithm of SAT-solvers and SMT-solvers: DPLL.\nWe then describe the implementation of a proof-search engine called Psyche. Its\narchitecture, based on the concept of focussing, offers a platform where smart\ntechniques from automated reasoning (or a user interface) can safely and\ntrustworthily be implemented via the use of an API.", "journal": ""}
{"doi": "10.48550/arXiv.1012.4892", "date": "2010-12-22", "title": "A Machine Checked Model of Idempotent MGU Axioms For Lists of Equational Constraints", "authors": "Sunil Kothari, James Caldwell", "abstract": "We present formalized proofs verifying that the first-order unification\nalgorithm defined over lists of satisfiable constraints generates a most\ngeneral unifier (MGU), which also happens to be idempotent. All of our proofs\nhave been formalized in the Coq theorem prover. Our proofs show that finite\nmaps produced by the unification algorithm provide a model of the axioms\ncharacterizing idempotent MGUs of lists of constraints. The axioms that serve\nas the basis for our verification are derived from a standard set by extending\nthem to lists of constraints. For us, constraints are equalities between terms\nin the language of simple types. Substitutions are formally modeled as finite\nmaps using the Coq library Coq.FSets.FMapInterface. Coq's method of functional\ninduction is the main proof technique used in proving many of the axioms.", "journal": "EPTCS 42, 2010, pp. 24-38"}
{"doi": "10.48550/arXiv.0904.3789", "date": "2009-04-24", "title": "Formally Specifying and Proving Operational Aspects of Forensic Lucid in Isabelle", "authors": "Serguei A. Mokhov, Joey Paquet", "abstract": "A Forensic Lucid intensional programming language has been proposed for\nintensional cyberforensic analysis. In large part, the language is based on\nvarious predecessor and codecessor Lucid dialects bound by the higher-order\nintensional logic (HOIL) that is behind them. This work formally specifies the\noperational aspects of the Forensic Lucid language and compiles a theory of its\nconstructs using Isabelle, a proof assistant system.", "journal": ""}
{"doi": "10.48550/arXiv.2011.00720", "date": "2020-10-29", "title": "Towards a certified reference monitor of the Android 10 permission system", "authors": "Guido De Luca, Carlos Luna", "abstract": "Android is a platform for mobile devices that captures more than 85% of the\ntotal market-share. Currently, mobile devices allow people to develop multiple\ntasks in different areas. Regrettably, the benefits of using mobile devices are\ncounteracted by increasing security risks. The important and critical role of\nthese systems makes them a prime target for formal verification. In our\nprevious work (LNCS 10855, https://doi.org/10.1007/978-3-319-94460-9_16), we\nexhibited a formal specification of an idealized formulation of the permission\nmodel of version \\texttt{6} of Android. In this paper we present an enhanced\nversion of the model in the proof-assistant Coq, including the most relevant\nchanges concerning the permission system introduced on versions Nougat, Oreo,\nPie and 10. The properties that we had proved earlier for the security model\nhas been either revalidated or refuted, and new ones have been formulated and\nproved. Additionally, we make observations on the security of the most recent\nversions of Android. Using the programming language of Coq we have developed a\nfunctional implementation of a reference validation mechanism and certified its\ncorrectness. The formal development is about 23k LOC of Coq, including proofs.", "journal": ""}
{"doi": "10.48550/arXiv.2312.08897", "date": "2023-12-14", "title": "Syntax Monads for the Working Formal Metatheorist", "authors": "Lawrence Dunn, Val Tannen, Steve Zdancewic", "abstract": "Formally verifying the properties of formal systems using a proof assistant\nrequires justifying numerous minor lemmas about capture-avoiding substitution.\nDespite work on category-theoretic accounts of syntax and variable binding,\nraw, first-order representations of syntax, the kind considered by many\npractitioners and compiler frontends, have received relatively little\nattention. Therefore applications miss out on the benefits of category theory,\nmost notably the promise of reusing formalized infrastructural lemmas between\nimplementations of different systems. Our Coq framework Tealeaves provides\nlibraries of reusable infrastructure for a raw, locally nameless representation\nand can be extended to other representations in a modular fashion. In this\npaper we give a string-diagrammatic account of decorated traversable monads\n(DTMs), the key abstraction implemented by Tealeaves. We define DTMs as monoids\nof structured endofunctors before proving a representation theorem a la\nKleisli, yielding a recursion combinator for finitary tree-like datatypes.", "journal": "EPTCS 397, 2023, pp. 98-117"}
{"doi": "10.48550/arXiv.1907.10674", "date": "2019-07-24", "title": "ConCert: A Smart Contract Certification Framework in Coq", "authors": "Danil Annenkov, Jakob Botsch Nielsen, Bas Spitters", "abstract": "We present a new way of embedding functional languages into the Coq proof\nassistant by using meta-programming. This allows us to develop the meta-theory\nof the language using the deep embedding and provides a convenient way for\nreasoning about concrete programs using the shallow embedding. We connect the\ndeep and the shallow embeddings by a soundness theorem. As an instance of our\napproach, we develop an embedding of a core smart contract language into Coq\nand verify several important properties of a crowdfunding contract based on a\nprevious formalisation of smart contract execution in blockchains.", "journal": "CPP 2020: Proceedings of the 9th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs, January 2020, Pages 215-228"}
{"doi": "10.48550/arXiv.2112.14809", "date": "2021-12-29", "title": "Explanation by Automated Reasoning Using the Isabelle Infrastructure Framework", "authors": "Florian Kamm\u00fcller", "abstract": "In this paper, we propose the use of interactive theorem proving for\nexplainable machine learning. After presenting our proposition, we illustrate\nit on the dedicated application of explaining security attacks using the\nIsabelle Infrastructure framework and its process of dependability engineering.\nThis formal framework and process provides the logics for specification and\nmodeling. Attacks on security of the system are explained by specification and\nproofs in the Isabelle Infrastructure framework. Existing case studies of\ndependability engineering in Isabelle are used as feasibility studies to\nillustrate how different aspects of explanations are covered by the Isabelle\nInfrastructure framework.", "journal": ""}
{"doi": "10.48550/arXiv.2205.01959", "date": "2022-05-04", "title": "Birkhoff-von Neumann Quantum Logic as an Assertion Language for Quantum Programs", "authors": "Mingsheng Ying", "abstract": "A first-order logic with quantum variables is needed as an assertion language\nfor specifying and reasoning about various properties (e.g. correctness) of\nquantum programs. Surprisingly, such a logic is missing in the literature, and\nthe existing first-order Birkhoff-von Neumann quantum logic deals with only\nclassical variables and quantifications over them. In this paper, we fill in\nthis gap by introducing a first-order extension of Birkhoff-von Neumann quantum\nlogic with universal and existential quantifiers over quantum variables.\nExamples are presented to show our logic is particularly suitable for\nspecifying some important properties studied in quantum computation and quantum\ninformation. We further incorporate this logic into quantum Hoare logic as an\nassertion logic so that it can play a role similar to that of first-order logic\nfor classical Hoare logic and BI-logic for separation logic. In particular, we\nshow how it can be used to define and derive quantum generalisations of some\nadaptation rules that have been applied to significantly simplify verification\nof classical programs. It is expected that the assertion logic defined in this\npaper - first-order quantum logic with quantum variables - can be combined with\nvarious quantum program logics to serve as a solid logical foundation upon\nwhich verification tools can be built using proof assistants such as Coq and\nIsabelle/HOL.", "journal": ""}
{"doi": "10.48550/arXiv.1108.4253", "date": "2011-08-22", "title": "Coquet: a Coq library for verifying hardware", "authors": "Thomas Braibant", "abstract": "We propose a new library to model and verify hardware circuits in the Coq\nproof assistant. This library allows one to easily build circuits by following\nthe usual pen-and-paper diagrams. We define a deep-embedding: we use a\n(dependently typed) data-type that models the architecture of circuits, and a\nmeaning function. We propose tactics that ease the reasoning about the behavior\nof the circuits, and we demonstrate that our approach is practicable by proving\nthe correctness of various circuits: a text-book divide and conquer adder of\nparametric size, some higher-order combinators of circuits, and some sequential\ncircuits: a buffer, and a register.", "journal": ""}
{"doi": "10.48550/arXiv.2302.12433", "date": "2023-02-24", "title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics", "authors": "Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, Jeremy Avigad", "abstract": "We introduce ProofNet, a benchmark for autoformalization and formal proving\nof undergraduate-level mathematics. The ProofNet benchmarks consists of 371\nexamples, each consisting of a formal theorem statement in Lean 3, a natural\nlanguage theorem statement, and a natural language proof. The problems are\nprimarily drawn from popular undergraduate pure mathematics textbooks and cover\ntopics such as real and complex analysis, linear algebra, abstract algebra, and\ntopology. We intend for ProofNet to be a challenging benchmark that will drive\nprogress in autoformalization and automatic theorem proving. We report baseline\nresults on statement autoformalization via in-context learning. Moreover, we\nintroduce two novel statement autoformalization methods: prompt retrieval and\ndistilled backtranslation.", "journal": ""}
{"doi": "10.48550/arXiv.1809.01572", "date": "2018-09-05", "title": "A Safe Computational Framework for Integer Programming applied to Chv\u00e1tal's Conjecture", "authors": "Leon Eifler, Ambros Gleixner, Jonad Pulaj", "abstract": "We describe a general and safe computational framework that provides integer\nprogramming results with the degree of certainty that is required for\nmachine-assisted proofs of mathematical theorems. At its core, the framework\nrelies on a rational branch-and-bound certificate produced by an exact integer\nprogramming solver, SCIP, in order to circumvent floating-point roundoff errors\npresent in most state-of-the-art solvers for mixed-integer programs. The\nresulting certificates are self-contained and checker software exists that can\nverify their correctness independently of the integer programming solver used\nto produce the certificate. This acts as a safeguard against programming errors\nthat may be present in complex solver software. The viability of this approach\nis tested by applying it to finite cases of Chv\\'atal's conjecture, a\nlong-standing open question in extremal combinatorics. We take particular care\nto verify also the correctness of the input for this specific problem, using\nthe Coq formal proof assistant. As a result, we are able to provide a first\nmachine-assisted proof that Chv\\'atal's conjecture holds for all downsets whose\nunion of sets contains seven elements or less.", "journal": ""}
{"doi": "10.48550/arXiv.0902.2137", "date": "2009-02-12", "title": "A formally verified compiler back-end", "authors": "Xavier Leroy", "abstract": "This article describes the development and formal verification (proof of\nsemantic preservation) of a compiler back-end from Cminor (a simple imperative\nintermediate language) to PowerPC assembly code, using the Coq proof assistant\nboth for programming the compiler and for proving its correctness. Such a\nverified compiler is useful in the context of formal methods applied to the\ncertification of critical software: the verification of the compiler guarantees\nthat the safety properties proved on the source code hold for the executable\ncompiled code as well.", "journal": "Journal of Automated Reasoning 43, 4 (2009) 363-446"}
{"doi": "10.48550/arXiv.2207.11350", "date": "2022-07-22", "title": "CoqQ: Foundational Verification of Quantum Programs", "authors": "Li Zhou, Gilles Barthe, Pierre-Yves Strub, Junyi Liu, Mingsheng Ying", "abstract": "CoqQ is a framework for reasoning about quantum programs in the Coq proof\nassistant. Its main components are: a deeply embedded quantum programming\nlanguage, in which classic quantum algorithms are easily expressed, and an\nexpressive program logic for proving properties of programs. CoqQ is\nfoundational: the program logic is formally proved sound with respect to a\ndenotational semantics based on state-of-art mathematical libraries (mathcomp\nand mathcomp analysis). CoqQ is also practical: assertions can use Dirac\nexpressions, which eases concise specifications, and proofs can exploit local\nand parallel reasoning, which minimizes verification effort. We illustrate the\napplicability of CoqQ with many examples from the literature.", "journal": ""}
{"doi": "10.48550/arXiv.1607.01539", "date": "2016-07-06", "title": "Translating Scala Programs to Isabelle/HOL", "authors": "Lars Hupel, Viktor Kuncak", "abstract": "We present a trustworthy connection between the Leon verification system and\nthe Isabelle proof assistant. Leon is a system for verifying functional Scala\nprograms. It uses a variety of automated theorem provers (ATPs) to check\nverification conditions (VCs) stemming from the input program. Isabelle, on the\nother hand, is an interactive theorem prover used to verify mathematical\nspecifications using its own input language Isabelle/Isar. Users specify\n(inductive) definitions and write proofs about them manually, albeit with the\nhelp of semi-automated tactics. The integration of these two systems allows us\nto exploit Isabelle's rich standard library and give greater confidence\nguarantees in the correctness of analysed programs.", "journal": "IJCAR 2016: Automated Reasoning Volume 9706 of the series Lecture\n  Notes in Computer Science pp 568-577, Springer"}
{"doi": "10.48550/arXiv.2310.07957", "date": "2023-10-12", "title": "A New Approach Towards Autoformalization", "authors": "Nilay Patel, Rahul Saha, Jeffrey Flanigan", "abstract": "Verifying mathematical proofs is difficult, but can be automated with the\nassistance of a computer. Autoformalization is the task of automatically\ntranslating natural language mathematics into a formal language that can be\nverified by a program. This is a challenging task, and especially for\nhigher-level mathematics found in research papers. Research paper mathematics\nrequires large amounts of background and context. In this paper, we propose an\navenue towards tackling autoformalization for research-level mathematics, by\nbreaking the task into easier and more approachable subtasks: unlinked\nformalization (formalization with unlinked definitions and theorems), entity\nlinking (linking to the proper theorems and definitions), and finally adjusting\ntypes so it passes the type checker. In addition, we present arXiv2Formal, a\nbenchmark dataset for unlinked formalization consisting of 50 theorems\nformalized for the Lean theorem prover sampled from papers on arXiv.org. We\nwelcome any contributions from the community to future versions of this\ndataset.", "journal": ""}
{"doi": "10.48550/arXiv.1708.08542", "date": "2017-08-28", "title": "Verified Correctness and Security of mbedTLS HMAC-DRBG", "authors": "Katherine Q. Ye, Matthew Green, Naphat Sanguansin, Lennart Beringer, Adam Petcher, Andrew W. Appel", "abstract": "We have formalized the functional specification of HMAC-DRBG (NIST 800-90A),\nand we have proved its cryptographic security--that its output is\npseudorandom--using a hybrid game-based proof. We have also proved that the\nmbedTLS implementation (C program) correctly implements this functional\nspecification. That proof composes with an existing C compiler correctness\nproof to guarantee, end-to-end, that the machine language program gives strong\npseudorandomness. All proofs (hybrid games, C program verification, compiler,\nand their composition) are machine-checked in the Coq proof assistant. Our\nproofs are modular: the hybrid game proof holds on any implementation of\nHMAC-DRBG that satisfies our functional specification. Therefore, our\nfunctional specification can serve as a high-assurance reference.", "journal": ""}
{"doi": "10.48550/arXiv.2501.15002", "date": "2025-01-25", "title": "A Proof-Producing Compiler for Blockchain Applications", "authors": "Jeremy Avigad, Lior Goldberg, David Levit, Yoav Seginer, Alon Titelman", "abstract": "CairoZero is a programming language for running decentralized applications\n(dApps) at scale. Programs written in the CairoZero language are compiled to\nmachine code for the Cairo CPU architecture and cryptographic protocols are\nused to verify the results of execution efficiently on blockchain. We explain\nhow we have extended the CairoZero compiler with tooling that enables users to\nprove, in the Lean 3 proof assistant, that compiled code satisfies high-level\nfunctional specifications. We demonstrate the success of our approach by\nverifying primitives for computation with the secp256k1 and secp256r1 curves\nover a large finite field as well as the validation of cryptographic signatures\nusing the former. We also verify a mechanism for simulating a read-write\ndictionary data structure in a read-only setting. Finally, we reflect on our\nmethodology and discuss some of the benefits of our approach.", "journal": ""}
{"doi": "10.48550/arXiv.2004.10667", "date": "2020-04-21", "title": "Simple Dataset for Proof Method Recommendation in Isabelle/HOL (Dataset Description)", "authors": "Yutaka Nagashima", "abstract": "Recently, a growing number of researchers have applied machine learning to\nassist users of interactive theorem provers. However, the expressive nature of\nunderlying logics and esoteric structures of proof documents impede machine\nlearning practitioners, who often do not have much expertise in formal logic,\nlet alone Isabelle/HOL, from achieving a large scale success in this field. In\nthis data description, we present a simple dataset that contains data on over\n400k proof method applications along with over 100 extracted features for each\nin a format that can be processed easily without any knowledge about formal\nlogic. Our simple data format allows machine learning practitioners to try\nmachine learning tools to predict proof methods in Isabelle/HOL without\nrequiring domain expertise in logic.", "journal": ""}
{"doi": "10.48550/arXiv.1812.11108", "date": "2018-12-28", "title": "Towards a constructive formalization of Perfect Graph Theorems", "authors": "Abhishek Kr Singh, Raja Natarajan", "abstract": "Interaction between clique number $\\omega(G) $ and chromatic number $\\chi(G)\n$ of a graph is a well studied topic in graph theory. Perfect Graph Theorems\nare probably the most important results in this direction. Graph $G$ is called\n\\emph{perfect} if $\\chi(H)=\\omega(H)$ for every induced subgraph $H$ of $G$.\nThe Strong Perfect Graph Theorem (SPGT) states that a graph is perfect if and\nonly if it does not contain an odd hole (or an odd anti-hole) as its induced\nsubgraph. The Weak Perfect Graph Theorem (WPGT) states that a graph is perfect\nif and only if its complement is perfect. In this paper, we present a formal\nframework for verifying these results. We model finite simple graphs in the\nconstructive type theory of Coq Proof Assistant without adding any axiom to it.\nFinally, we use this framework to present a constructive proof of the\nLov\\'{a}sz Replication Lemma, which is the central idea in the proof of Weak\nPerfect Graph Theorem.", "journal": ""}
{"doi": "10.48550/arXiv.1909.08789", "date": "2019-09-19", "title": "Proof Pearl: Magic Wand as Frame", "authors": "Qinxiang Cao, Shengyi Wang, Aquinas Hobor, Andrew W. Appel", "abstract": "Separation logic adds two connectives to assertion languages: separating\nconjunction * (\"star\") and its adjoint, separating implication -* (\"magic\nwand\"). Comparatively, separating implication is less widely used.\n  This paper demonstrates that by using magic wand to express frames that\nrelate mutable local portions of data structures to global portions, we can\nexploit its power while proofs are still easily understandable. Many useful\nseparation logic theorems about partial data structures can now be proved by\nsimple automated tactics, which were usually proved by induction. This\nmagic-wand-as-frame technique is especially useful when formalizing the proofs\nby a high order logic. We verify binary search tree insert in Coq as an example\nto demonstrate this proof technique.", "journal": ""}
{"doi": "10.48550/arXiv.1401.7694", "date": "2014-01-29", "title": "Experience Implementing a Performant Category-Theory Library in Coq", "authors": "Jason Gross, Adam Chlipala, David I. Spivak", "abstract": "We describe our experience implementing a broad category-theory library in\nCoq. Category theory and computational performance are not usually mentioned in\nthe same breath, but we have needed substantial engineering effort to teach Coq\nto cope with large categorical constructions without slowing proof script\nprocessing unacceptably. In this paper, we share the lessons we have learned\nabout how to represent very abstract mathematical objects and arguments in Coq\nand how future proof assistants might be designed to better support such\nreasoning. One particular encoding trick to which we draw attention allows\ncategory-theoretic arguments involving duality to be internalized in Coq's\nlogic with definitional equality. Ours may be the largest Coq development to\ndate that uses the relatively new Coq version developed by homotopy type\ntheorists, and we reflect on which new features were especially helpful.", "journal": ""}
{"doi": "10.48550/arXiv.2403.04651", "date": "2024-03-07", "title": "Cedar: A New Language for Expressive, Fast, Safe, and Analyzable Authorization (Extended Version)", "authors": "Joseph W. Cutler, Craig Disselkoen, Aaron Eline, Shaobo He, Kyle Headley, Michael Hicks, Kesha Hietala, Eleftherios Ioannidis, John Kastner, Anwar Mamat, Darin McAdams, Matt McCutchen, Neha Rungta, Emina Torlak, Andrew Wells", "abstract": "Cedar is a new authorization policy language designed to be ergonomic, fast,\nsafe, and analyzable. Rather than embed authorization logic in an application's\ncode, developers can write that logic as Cedar policies and delegate access\ndecisions to Cedar's evaluation engine. Cedar's simple and intuitive syntax\nsupports common authorization use-cases with readable policies, naturally\nleveraging concepts from role-based, attribute-based, and relation-based access\ncontrol models. Cedar's policy structure enables access requests to be decided\nquickly. Cedar's policy validator leverages optional typing to help policy\nwriters avoid mistakes, but not get in their way. Cedar's design has been\nfinely balanced to allow for a sound and complete logical encoding, which\nenables precise policy analysis, e.g., to ensure that when refactoring a set of\npolicies, the authorized permissions do not change. We have modeled Cedar in\nthe Lean programming language, and used Lean's proof assistant to prove\nimportant properties of Cedar's design. We have implemented Cedar in Rust, and\nreleased it open-source. Comparing Cedar to two open-source languages, OpenFGA\nand Rego, we find (subjectively) that Cedar has equally or more readable\npolicies, but (objectively) performs far better.", "journal": ""}
{"doi": "10.48550/arXiv.1107.5252", "date": "2011-07-26", "title": "Modules over relative monads for syntax and semantics", "authors": "Benedikt Ahrens", "abstract": "We give an algebraic characterization of the syntax and semantics of a class\nof languages with variable binding.\n  We introduce a notion of 2-signature: such a signature specifies not only the\nterms of a language, but also reduction rules on those terms. To any\n2-signature $S$ we associate a category of \"models\" of $S$. This category has\nan initial object, which integrates the terms freely generated by $S$, and\nwhich is equipped with reductions according to the inequations given in $S$. We\ncall this initial object the language generated by $S$. Models of a\n2--signature are built from relative monads and modules over such monads.\nThrough the use of monads, the models---and in particular, the initial\nmodel---come equipped with a substitution operation that is compatible with\nreduction in a suitable sense.\n  The initiality theorem is formalized in the proof assistant Coq, yielding a\nmachinery which, when fed with a 2-signature, provides the associated\nprogramming language with reduction relation and certified substitution.", "journal": "Math. Struct. Comp. Sci. 26 (2016) 3-37"}
{"doi": "10.48550/arXiv.2210.12150", "date": "2022-10-21", "title": "Formalizing Chemical Physics using the Lean Theorem Prover", "authors": "Maxwell P. Bobbin, Samiha Sharlin, Parivash Feyzishendi, An Hong Dang, Catherine M. Wraback, Tyler R. Josephson", "abstract": "Chemical theory can be made more rigorous using the Lean theorem prover, an\ninteractive theorem prover for complex mathematics. We formalize the Langmuir\nand BET theories of adsorption, making each scientific premise clear and every\nstep of the derivations explicit. Lean's math library, mathlib, provides\nformally verified theorems for infinite geometries series, which are central to\nBET theory. While writing these proofs, Lean prompts us to include mathematical\nconstraints that were not originally reported. We also illustrate how Lean\nflexibly enables the reuse of proofs that build on more complex theories\nthrough the use of functions, definitions, and structures. Finally, we\nconstruct scientific frameworks for interoperable proofs, by creating\nstructures for classical thermodynamics and kinematics, using them to formalize\ngas law relationships like Boyle's Law and equations of motion underlying\nNewtonian mechanics, respectively. This approach can be extended to other\nfields, enabling the formalization of rich and complex theories in science and\nengineering.", "journal": ""}
{"doi": "10.48550/arXiv.2310.03885", "date": "2023-10-05", "title": "Trustworthy Formal Natural Language Specifications", "authors": "Colin S. Gordon, Sergey Matskevich", "abstract": "Interactive proof assistants are computer programs carefully constructed to\ncheck a human-designed proof of a mathematical claim with high confidence in\nthe implementation. However, this only validates truth of a formal claim, which\nmay have been mistranslated from a claim made in natural language. This is\nespecially problematic when using proof assistants to formally verify the\ncorrectness of software with respect to a natural language specification. The\ntranslation from informal to formal remains a challenging, time-consuming\nprocess that is difficult to audit for correctness.\n  This paper shows that it is possible to build support for specifications\nwritten in expressive subsets of natural language, within existing proof\nassistants, consistent with the principles used to establish trust and\nauditability in proof assistants themselves. We implement a means to provide\nspecifications in a modularly extensible formal subset of English, and have\nthem automatically translated into formal claims, entirely within the Lean\nproof assistant. Our approach is extensible (placing no permanent restrictions\non grammatical structure), modular (allowing information about new words to be\ndistributed alongside libraries), and produces proof certificates explaining\nhow each word was interpreted and how the sentence's structure was used to\ncompute the meaning.\n  We apply our prototype to the translation of various English descriptions of\nformal specifications from a popular textbook into Lean formalizations; all can\nbe translated correctly with a modest lexicon with only minor modifications\nrelated to lexicon size.", "journal": "Proceedings of the 2023 ACM SIGPLAN International Symposium on New\n  Ideas, New Paradigms, and Reflections on Programming and Software (Onward!\n  '23)"}
{"doi": "10.48550/arXiv.2401.16233", "date": "2024-01-25", "title": "Incremental Proof Development in Dafny with Module-Based Induction", "authors": "Son Ho, Cl\u00e9ment Pit-Claudel", "abstract": "Highly automated theorem provers like Dafny allow users to prove simple\nproperties with little effort, making it easy to quickly sketch proofs. The\ndrawback is that such provers leave users with little control about the proof\nsearch, meaning that the small changes inherent to the iterative process of\nwriting a proof often lead to unpredictable variations in verification time,\nand eventually hard-to-diagnose proof failures. This sometimes turns the boon\nof high automation into a curse, as instead of breaking early and showing\nunsolved goals to the user like in Coq, proofs tend to gradually become\nunstable until their verification time explodes. At this point, the absence of\na proof context to investigate often leaves the user to a painful debugging\nsession. In this paper, we show how to use Dafny modules to encode Coq-like\ninduction principles to dramatically improve the stability and maintainability\nof proofs about inductive data structures.", "journal": ""}
{"doi": "10.48550/arXiv.2501.03352", "date": "2025-01-06", "title": "Teaching \"Foundations of Mathematics\" with the Lean Theorem Prover", "authors": "Mattia Luciano Bottoni, Alberto S. Cattaneo, Elif Sacikara", "abstract": "This study aims to observe if the theorem prover Lean positively influences\nstudents' understanding of mathematical proving. To this end, we perform a\npilot study concerning freshmen students at the University of Zurich (UZH).\nWhile doing so, we apply certain teaching methods and gather data from the\nvolunteer students enrolled in the ``Foundations of Mathematics'' course. After\neleven weeks of study covering some exercise questions implemented with Lean,\nwe measure Lean students' performances in proving mathematical statements,\ncompared to other students who are not engaged with Lean. For this measurement,\nwe interview five Lean and four Non-Lean students and we analyze the scores of\nall students in the final exam. Finally, we check significance by performing a\n$t$-test for independent samples and the Mann-Whitney $U$-test.", "journal": ""}
{"doi": "10.48550/arXiv.2207.12699", "date": "2022-07-26", "title": "Teaching Functional Programmers Logic and Metatheory", "authors": "Frederik Krogsdal Jacobsen, J\u00f8rgen Villadsen", "abstract": "We present a novel approach for teaching logic and the metatheory of logic to\nstudents who have some experience with functional programming. We define\nconcepts in logic as a series of functional programs in the language of the\nproof assistant Isabelle/HOL. This allows us to make notions which are often\nunclear in textbooks precise, to experiment with definitions by executing them,\nand to prove metatheoretical theorems in full detail. We have surveyed student\nperceptions of our teaching approach to determine its usefulness and found that\nstudents felt that our formalizations helped them understand concepts in logic,\nand that they experimented with them as a learning tool. However, the approach\nwas not enough to make students feel confident in their abilities to design and\nimplement their own formal systems. Further studies are needed to confirm and\ngeneralize the results of our survey, but our initial results seem promising.", "journal": "EPTCS 363, 2022, pp. 74-92"}
{"doi": "10.48550/arXiv.2010.02595", "date": "2020-10-06", "title": "Formalizing the Ring of Witt Vectors", "authors": "Johan Commelin, Robert Y. Lewis", "abstract": "The ring of Witt vectors $\\mathbb{W} R$ over a base ring $R$ is an important\ntool in algebraic number theory and lies at the foundations of modern $p$-adic\nHodge theory. $\\mathbb{W} R$ has the interesting property that it constructs a\nring of characteristic $0$ out of a ring of characteristic $p > 1$, and it can\nbe used more specifically to construct from a finite field containing\n$\\mathbb{Z}/p\\mathbb{Z}$ the corresponding unramified field extension of the\n$p$-adic numbers $\\mathbb{Q}_p$ (which is unique up to isomorphism).\n  We formalize the notion of a Witt vector in the Lean proof assistant, along\nwith the corresponding ring operations and other algebraic structure. We prove\nin Lean that, for prime $p$, the ring of Witt vectors over\n$\\mathbb{Z}/p\\mathbb{Z}$ is isomorphic to the ring of $p$-adic integers\n$\\mathbb{Z}_p$. In the process we develop idioms to cleanly handle calculations\nof identities between operations on the ring of Witt vectors. These\ncalculations are intractable with a naive approach, and require a proof\ntechnique that is usually skimmed over in the informal literature. Our proofs\nresemble the informal arguments while being fully rigorous.", "journal": "Proceedings of Certified Programs and Proofs (CPP 2021)"}
{"doi": "10.48550/arXiv.0707.0926", "date": "2007-07-06", "title": "Theorem proving support in programming language semantics", "authors": "Yves Bertot", "abstract": "We describe several views of the semantics of a simple programming language\nas formal documents in the calculus of inductive constructions that can be\nverified by the Coq proof system. Covered aspects are natural semantics,\ndenotational semantics, axiomatic semantics, and abstract interpretation.\nDescriptions as recursive functions are also provided whenever suitable, thus\nyielding a a verification condition generator and a static analyser that can be\nrun inside the theorem prover for use in reflective proofs. Extraction of an\ninterpreter from the denotational semantics is also described. All different\naspects are formally proved sound with respect to the natural semantics\nspecification.", "journal": ""}
{"doi": "10.48550/arXiv.1704.03104", "date": "2017-04-11", "title": "On the Underapproximation of Reach Sets of Abstract Continuous-Time Systems", "authors": "Ievgen Ivanov", "abstract": "We consider the problem of proving that each point in a given set of states\n(\"target set\") can indeed be reached by a given nondeterministic\ncontinuous-time dynamical system from some initial state. We consider this\nproblem for abstract continuous-time models that can be concretized as various\nkinds of continuous and hybrid dynamical systems.\n  The approach to this problem proposed in this paper is based on finding a\nsuitable superset S of the target set which has the property that each partial\ntrajectory of the system which lies entirely in S either is defined as the\ninitial time moment, or can be locally extended backward in time, or can be\nlocally modified in such a way that the resulting trajectory can be locally\nextended back in time.\n  This reformulation of the problem has a relatively simple logical expression\nand is convenient for applying various local existence theorems and local\ndynamics analysis methods to proving reachability which makes it suitable for\nreasoning about the behavior of continuous and hybrid dynamical systems in\nproof assistants such as Mizar, Isabelle, etc.", "journal": "EPTCS 247, 2017, pp. 46-51"}
{"doi": "10.48550/arXiv.2402.06064", "date": "2024-02-08", "title": "Formalizing Automated Market Makers in the Lean 4 Theorem Prover", "authors": "Daniele Pusceddu, Massimo Bartoletti", "abstract": "Automated Market Makers (AMMs) are an integral component of the decentralized\nfinance (DeFi) ecosystem, as they allow users to exchange crypto-assets without\nthe need for trusted authorities or external price oracles. Although these\nprotocols are based on relatively simple mechanisms, e.g., to algorithmically\ndetermine the exchange rate between crypto-assets, they give rise to complex\neconomic behaviours. This complexity is witnessed by the proliferation of\nmodels that study their structural and economic properties. Currently, most of\ntheoretical results obtained on these models are supported by pen-and-paper\nproofs. This work proposes a formalization of constant-product AMMs in the Lean\n4 Theorem Prover. To demonstrate the utility of our model, we provide\nmechanized proofs of key economic properties like arbitrage, that at the best\nof our knowledge have only been proved by pen-and-paper before.", "journal": ""}
{"doi": "10.48550/arXiv.2110.03551", "date": "2021-10-07", "title": "Formalizing Geometric Algebra in Lean", "authors": "Eric Wieser, Utensil Song", "abstract": "This paper explores formalizing Geometric (or Clifford) algebras into the\nLean 3 theorem prover, building upon the substantial body of work that is the\nLean mathematics library, mathlib. As we use Lean source code to demonstrate\nmany of our ideas, we include a brief introduction to the Lean language\ntargeted at a reader with no prior experience with Lean or theorem provers in\ngeneral.\n  We formalize the multivectors as the quotient of the tensor algebra by a\nsuitable relation, which provides the ring structure automatically, then go on\nto establish the universal property of the Clifford algebra. We show that this\nis quite different to the approach taken by existing formalizations of\nGeometric algebra in other theorem provers; most notably, our approach does not\nrequire a choice of basis.\n  We go on to show how operations and structure such as involutions, versors,\nand the $\\mathbb{Z}_2$-grading can be defined using the universal property\nalone, and how to recover an induction principle from the universal property\nsuitable for proving statements about these definitions. We outline the steps\nneeded to formalize the wedge product and $\\mathbb{N}$-grading, and some of the\ngaps in mathlib that currently make this challenging.", "journal": "Advances in Applied Clifford Algebras 32, Article 28 (2022)"}
{"doi": "10.48550/arXiv.1405.7012", "date": "2014-05-27", "title": "A formally verified proof of the Central Limit Theorem", "authors": "Jeremy Avigad, Johannes H\u00f6lzl, Luke Serafin", "abstract": "We describe a proof of the Central Limit Theorem that has been formally\nverified in the Isabelle proof assistant. Our formalization builds upon and\nextends Isabelle's libraries for analysis and measure-theoretic probability.\nThe proof of the theorem uses characteristic functions, which are a kind of\nFourier transform, to demonstrate that, under suitable hypotheses, sums of\nrandom variables converge weakly to the standard normal distribution. We also\ndiscuss the libraries and infrastructure that supported the formalization, and\nreflect on some of the lessons we have learned from the effort.", "journal": ""}
{"doi": "10.48550/arXiv.1904.08468", "date": "2019-04-17", "title": "Towards Evolutionary Theorem Proving for Isabelle/HOL", "authors": "Yutaka Nagashima", "abstract": "Mechanized theorem proving is becoming the basis of reliable systems\nprogramming and rigorous mathematics. Despite decades of progress in proof\nautomation, writing mechanized proofs still requires engineers' expertise and\nremains labor intensive. Recently, researchers have extracted heuristics of\ninteractive proof development from existing large proof corpora using\nsupervised learning. However, such existing proof corpora present only one way\nof proving conjectures, while there are often multiple equivalently effective\nways to prove one conjecture. In this abstract, we identify challenges in\ndiscovering heuristics for automatic proof search and propose our novel\napproach to improve heuristics of automatic proof search in Isabelle/HOL using\nevolutionary computation.", "journal": ""}
{"doi": "10.48550/arXiv.1305.6543", "date": "2013-05-28", "title": "MirrorShard: Proof by Computational Reflection with Verified Hints", "authors": "Gregory Malecha, Adam Chlipala, Thomas Braibant, Patrick Hulin, Edward Z. Yang", "abstract": "We describe a method for building composable and extensible verification\nprocedures within the Coq proof assistant. Unlike traditional methods that rely\non run-time generation and checking of proofs, we use verified-correct\nprocedures with Coq soundness proofs. Though they are internalized in Coq's\nlogic, our provers support sound extension by users with hints over new\ndomains, enabling automated reasoning about user-defined abstract predicates.\nWe maintain soundness by developing an architecture for modular packaging,\nconstruction, and composition of hint databases, which had previously only been\nimplemented in Coq at the level of its dynamically typed, proof-generating\ntactic language. Our provers also include rich handling of unification\nvariables, enabling integration with other tactic-based deduction steps within\nCoq. We have implemented our techniques in MirrorShard, an open-source\nframework for reflective verification. We demonstrate its applicability by\ninstantiating it to separation logic in order to reason about imperative\nprogram verification.", "journal": ""}
{"doi": "10.48550/arXiv.2010.00774", "date": "2020-10-02", "title": "Proof Repair across Type Equivalences", "authors": "Talia Ringer, RanDair Porter, Nathaniel Yazdani, John Leo, Dan Grossman", "abstract": "We describe a new approach to automatically repairing broken proofs in the\nCoq proof assistant in response to changes in types. Our approach combines a\nconfigurable proof term transformation with a decompiler from proof terms to\ntactic scripts. The proof term transformation implements transport across\nequivalences in a way that removes references to the old version of the changed\ntype and does not rely on axioms beyond those Coq assumes.\n  We have implemented this approach in PUMPKIN Pi, an extension to the PUMPKIN\nPATCH Coq plugin suite for proof repair. We demonstrate PUMPKIN Pi's\nflexibility on eight case studies, including supporting a benchmark from a user\nstudy, easing development with dependent types, porting functions and proofs\nbetween unary and binary numbers, and supporting an industrial proof engineer\nto interoperate between Coq and other verification tools more easily.", "journal": ""}
{"doi": "10.48550/arXiv.1802.04315", "date": "2018-02-12", "title": "Higher Groups in Homotopy Type Theory", "authors": "Ulrik Buchholtz, Floris van Doorn, Egbert Rijke", "abstract": "We present a development of the theory of higher groups, including infinity\ngroups and connective spectra, in homotopy type theory. An infinity group is\nsimply the loops in a pointed, connected type, where the group structure comes\nfrom the structure inherent in the identity types of Martin-L\\\"of type theory.\nWe investigate ordinary groups from this viewpoint, as well as higher\ndimensional groups and groups that can be delooped more than once. A major\nresult is the stabilization theorem, which states that if an $n$-type can be\ndelooped $n+2$ times, then it is an infinite loop type. Most of the results\nhave been formalized in the Lean proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2308.02540", "date": "2023-08-01", "title": "Top-down Automated Theorem Proving (Notes for Sir Timothy)", "authors": "C. E. Larson, N. Van Cleemput", "abstract": "We describe a \"top down\" approach for automated theorem proving (ATP).\nResearchers might usefully investigate the forms of the theorems mathematicians\nuse in practice, carefully examine how they differ and are proved in practice,\nand code all relevant domain concepts. These concepts encode a large portion of\nthe knowledge in any domain. Furthermore, researchers should write programs\nthat produce proofs of the kind that human mathematicians write (and publish);\nthis means proofs that might sometimes have mistakes; and this means making\ninferences that are sometimes invalid.\n  This approach is meant to contrast with the historically dominant \"bottom up\"\napproach: coding fundamental types (typically sets), axioms and rules for\n(valid) inference, and building up from this foundation to the theorems of\nmathematical practice and to their outstanding questions. It is an important\nfact that the actual proofs that mathematicians publish in math journals do not\nlook like the formalized proofs of Russell & Whitehead's Principia Mathematica\n(or modern computer systems like Lean that automate some of this\nformalization). We believe some \"lack of rigor\" (in mathematical practice) is\nhuman-like, and can and should be leveraged for ATP.", "journal": ""}
{"doi": "10.48550/arXiv.2407.02704", "date": "2024-07-02", "title": "Monads, Comonads, and Transducers", "authors": "Rafa\u0142 Stefa\u0144ski", "abstract": "This paper proposes a definition of recognizable transducers over monads and\ncomonads, which bridges two important ongoing efforts in the current research\non regularity. The first effort is the study of regular transductions, which\nextends the notion of regularity from languages into word-to-word functions.\nThe other important effort is generalizing the notion of regular languages from\nwords to arbitrary monads, introduced in arXiv:1502.04898. In this paper, we\npresent a number of examples of transducer classes that fit the proposed\nframework. In particular we show that our class generalizes the classes of\nMealy machines and rational transductions. We also present examples of\nrecognizable transducers for infinite words and a specific type of trees called\nterms. The main result of this paper is a theorem, which states the class of\nrecognizable transductions is closed under composition, subject to some\ncoherence axioms between the structure of a monad and the structure of a\ncomonad. Due to its complexity, we formalize the proof of the theorem in Coq\nProof Assistant. In the proof, we introduce the concepts of a context and a\ngeneralized wreath product for Eilenberg-Moore algebras, which could be\nvaluable tools for studying these algebras.", "journal": ""}
{"doi": "10.48550/arXiv.1912.02211", "date": "2019-12-04", "title": "A Constructive Formalization of the Weak Perfect Graph Theorem", "authors": "Abhishek Kr Singh, Raja Natarajan", "abstract": "The Perfect Graph Theorems are important results in graph theory describing\nthe relationship between clique number $\\omega(G) $ and chromatic number\n$\\chi(G) $ of a graph $G$. A graph $G$ is called \\emph{perfect} if\n$\\chi(H)=\\omega(H)$ for every induced subgraph $H$ of $G$. The Strong Perfect\nGraph Theorem (SPGT) states that a graph is perfect if and only if it does not\ncontain an odd hole (or an odd anti-hole) as its induced subgraph. The Weak\nPerfect Graph Theorem (WPGT) states that a graph is perfect if and only if its\ncomplement is perfect. In this paper, we present a formal framework for working\nwith finite simple graphs. We model finite simple graphs in the Coq Proof\nAssistant by representing its vertices as a finite set over a countably\ninfinite domain. We argue that this approach provides a formal framework in\nwhich it is convenient to work with different types of graph constructions (or\nexpansions) involved in the proof of the Lov\\'{a}sz Replication Lemma (LRL),\nwhich is also the key result used in the proof of Weak Perfect Graph Theorem.\nFinally, we use this setting to develop a constructive formalization of the\nWeak Perfect Graph Theorem.", "journal": ""}
{"doi": "10.48550/arXiv.2011.03463", "date": "2020-11-06", "title": "Extending Equational Monadic Reasoning with Monad Transformers", "authors": "Reynald Affeldt, David Nowak", "abstract": "There is a recent interest for the verification of monadic programs using\nproof assistants. This line of research raises the question of the integration\nof monad transformers, a standard technique to combine monads. In this paper,\nwe extend Monae, a Coq library for monadic equational reasoning, with monad\ntransformers and we explain the benefits of this extension. Our starting point\nis the existing theory of modular monad transformers, which provides a uniform\ntreatment of operations. Using this theory, we simplify the formalization of\nmodels in Monae and we propose an approach to support monadic equational\nreasoning in the presence of monad transformers. We also use Monae to revisit\nthe lifting theorems of modular monad transformers by providing equational\nproofs and explaining how to patch a known bug using a non-standard use of Coq\nthat combines impredicative polymorphism and parametricity.", "journal": ""}
{"doi": "10.48550/arXiv.2001.10594", "date": "2020-01-28", "title": "Simplifying Casts and Coercions", "authors": "Robert Y. Lewis, Paul-Nicolas Madelaine", "abstract": "This paper introduces norm_cast, a toolbox of tactics for the Lean proof\nassistant designed to manipulate expressions containing coercions and casts.\nThese expressions can be frustrating for beginning and expert users alike; the\npresence of coercions can cause seemingly identical expressions to fail to\nunify and rewrites to fail. The norm_cast tactics aim to make reasoning with\nsuch expressions as transparent as possible. They are used extensively to\neliminate boilerplate arguments in the Lean mathematical library and in\nexternal developments.", "journal": ""}
{"doi": "10.48550/arXiv.1206.4556", "date": "2012-06-20", "title": "Initiality for Typed Syntax and Semantics", "authors": "Benedikt Ahrens", "abstract": "In this thesis we give an algebraic characterization of the syntax and\nsemantics of simply-typed languages. More precisely, we characterize\nsimply-typed binding syntax equipped with reduction rules via a universal\nproperty, namely as the initial object of some category. We specify a language\nby a 2-signature ({\\Sigma}, A), that is, a signature on two levels: the\nsyntactic level {\\Sigma} specifies the sorts and terms of the language, and\nassociates a sort to each term. The semantic level A specifies, through\ninequations, reduction rules on the terms of the language. To any given\n2-signature ({\\Sigma}, A) we associate a category of \"models\" of ({\\Sigma}, A).\nWe prove that this category has an initial object, which integrates the terms\nfreely generated by {\\Sigma} and the reduction relation - on those terms -\ngenerated by A. We call this object the programming language generated by\n({\\Sigma}, A).\n  Initiality provides an iteration principle which allows to specify\ntranslations on the syntax, possibly to a language over different sorts.\nFurthermore, translations specified via the iteration principle are by\nconstruction type-safe and faithful with respect to reduction.\n  To illustrate our results, we consider two examples extensively: firstly, we\nspecify a double negation translation from classical to intuitionistic\npropositional logic via the category-theoretic iteration principle. Secondly,\nwe specify a translation from PCF to the untyped lambda calculus which is\nfaithful with respect to reduction in the source and target languages.\n  In a second part, we formalize some of our initiality theorems in the proof\nassistant Coq. The implementation yields a machinery which, when given a\n2-signature, returns an implementation of its associated abstract syntax\ntogether with certified substitution operation, iteration operator and a\nreduction relation generated by the specified reduction rules.", "journal": ""}
{"doi": "10.48550/arXiv.2411.03417", "date": "2024-11-05", "title": "Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment", "authors": "Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar B. Shah", "abstract": "Large language models (LLMs) represent a promising, but controversial, tool\nin aiding scientific peer review. This study evaluates the usefulness of LLMs\nin a conference setting as a tool for vetting paper submissions against\nsubmission standards. We conduct an experiment at the 2024 Neural Information\nProcessing Systems (NeurIPS) conference, where 234 papers were voluntarily\nsubmitted to an \"LLM-based Checklist Assistant.\" This assistant validates\nwhether papers adhere to the author checklist used by NeurIPS, which includes\nquestions to ensure compliance with research and manuscript preparation\nstandards. Evaluation of the assistant by NeurIPS paper authors suggests that\nthe LLM-based assistant was generally helpful in verifying checklist\ncompletion. In post-usage surveys, over 70% of authors found the assistant\nuseful, and 70% indicate that they would revise their papers or checklist\nresponses based on its feedback. While causal attribution to the assistant is\nnot definitive, qualitative evidence suggests that the LLM contributed to\nimproving some submissions. Survey responses and analysis of re-submissions\nindicate that authors made substantive revisions to their submissions in\nresponse to specific feedback from the LLM. The experiment also highlights\ncommon issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)\nwere the most frequent issues flagged by authors. We also conduct experiments\nto understand potential gaming of the system, which reveal that the assistant\ncould be manipulated to enhance scores through fabricated justifications,\nhighlighting potential vulnerabilities of automated review tools.", "journal": ""}
{"doi": "10.48550/arXiv.2201.10443", "date": "2022-01-25", "title": "Certifying algorithms and relevant properties of Reversible Primitive Permutations with Lean", "authors": "Giacomo Maletto, Luca Roversi", "abstract": "Reversible Primitive Permutations (RPP) are recursively defined functions\ndesigned to model Reversible Computation. We illustrate a proof, fully\ndeveloped with the proof-assistant Lean, certifying that: \"RPP can encode every\nPrimitive Recursive Function\". Our reworking of the original proof of that\nstatement is conceptually simpler, fixes some bugs, suggests a new more\nprimitive reversible iteration scheme for RPP, and, in order to keep\nformalization and semi-automatic proofs simple, led us to identify a single\npattern that can generate some useful reversible algorithms in RPP: Cantor\nPairing, Quotient/Reminder of integer division, truncated Square Root. Our Lean\nsource code is available for experiments on Reversible Computation whose\nproperties can be certified.", "journal": ""}
{"doi": "10.48550/arXiv.1507.04002", "date": "2015-07-14", "title": "NaDeA: A Natural Deduction Assistant with a Formalization in Isabelle", "authors": "J\u00f8rgen Villadsen, Alexander Birch Jensen, Anders Schlichtkrull", "abstract": "We present a new software tool for teaching logic based on natural deduction.\nIts proof system is formalized in the proof assistant Isabelle such that its\ndefinition is very precise. Soundness of the formalization has been proved in\nIsabelle. The tool is open source software developed in TypeScript / JavaScript\nand can thus be used directly in a browser without any further installation.\nAlthough developed for undergraduate computer science students who are used to\nstudy and program concrete computer code in a programming language we consider\nthe approach relevant for a broader audience and for other proof systems as\nwell.", "journal": ""}
{"doi": "10.48550/arXiv.2001.07655", "date": "2020-01-21", "title": "Coherence via Well-Foundedness: Taming Set-Quotients in Homotopy Type Theory", "authors": "Nicolai Kraus, Jakob von Raumer", "abstract": "Suppose we are given a graph and want to show a property for all its cycles\n(closed chains). Induction on the length of cycles does not work since\nsub-chains of a cycle are not necessarily closed. This paper derives a\nprinciple reminiscent of induction for cycles for the case that the graph is\ngiven as the symmetric closure of a locally confluent and (co-)well-founded\nrelation. We show that, assuming the property in question is sufficiently nice,\nit is enough to prove it for the empty cycle and for cycles given by local\nconfluence.\n  Our motivation and application is in the field of homotopy type theory, which\nallows us to work with the higher-dimensional structures that appear in\nhomotopy theory and in higher category theory, making coherence a central\nissue. This is in particular true for quotienting - a natural operation which\ngives a new type for any binary relation on a type and, in order to be\nwell-behaved, cuts off higher structure (set-truncates). The latter makes it\nhard to characterise the type of maps from a quotient into a higher type, and\nseveral open problems stem from this difficulty.\n  We prove our theorem on cycles in a type-theoretic setting and use it to show\ncoherence conditions necessary to eliminate from set-quotients into 1-types,\nderiving approximations to open problems on free groups and pushouts. We have\nformalised the main result in the proof assistant Lean.", "journal": "Logic in Computer Science 2020 (LICS'20)"}
{"doi": "10.48550/arXiv.2103.02209", "date": "2021-03-03", "title": "SciviK: A Versatile Framework for Specifying and Verifying Smart Contracts", "authors": "Shaokai Lin, Xinyuan Sun, Jianan Yao, Ronghui Gu", "abstract": "The growing adoption of smart contracts on blockchains poses new security\nrisks that can lead to significant monetary loss, while existing approaches\neither provide no (or partial) security guarantees for smart contracts or\nrequire huge proof effort. To address this challenge, we present SciviK, a\nversatile framework for specifying and verifying industrial-grade smart\ncontracts. SciviK's versatile approach extends previous efforts with three key\ncontributions: (i) an expressive annotation system enabling built-in directives\nfor vulnerability pattern checking, neural-based loop invariant inference, and\nthe verification of rich properties of real-world smart contracts (ii) a\nfine-grained model for the Ethereum Virtual Machine (EVM) that provides\nlow-level execution semantics, (iii) an IR-level verification framework\nintegrating both SMT solvers and the Coq proof assistant.\n  We use SciviK to specify and verify security properties for 12 benchmark\ncontracts and a real-world Decentralized Finance (DeFi) smart contract. Among\nall 158 specified security properties (in six types), 151 properties can be\nautomatically verified within 2 seconds, five properties can be automatically\nverified after moderate modifications, and two properties are manually proved\nwith around 200 lines of Coq code.", "journal": ""}
{"doi": "10.48550/arXiv.2411.07667", "date": "2024-11-12", "title": "Formalization of physics index notation in Lean 4", "authors": "Joseph Tooby-Smith", "abstract": "The physics community relies on index notation to effectively manipulate\ntypes of tensors. This paper introduces the first formally verified\nimplementation of index notation in the interactive theorem prover Lean 4. By\nintegrating index notation into Lean, we bridge the gap between traditional\nphysics notation and formal verification tools, making it more accessible for\nphysicists to write and prove results within Lean. We also open up a new avenue\nthrough which AI tools can be used to prove results related to tensors in\nphysics. Behind the scenes our implementation leverages a novel application of\ncategory theory.", "journal": ""}
{"doi": "10.48550/arXiv.2209.10278", "date": "2022-09-21", "title": "An Automatically Verified Prototype of the Android Permissions System", "authors": "Maximiliano Cristi\u00e1, Guido De Luca, Carlos Luna", "abstract": "In a previous work De Luca and Luna presented formal specifications of\nidealized formulations of the permission model of Android in the Coq proof\nassistant. This formal development is about 23 KLOC of Coq code, including\nproofs. This work aims at showing that {log} (`setlog') -- a satisfiability\nsolver and a constraint logic programming language -- can be used as an\neffective automated prover for the class of proofs that must be discharged in\nthe formal verification of systems such as the one carried out by De Luca and\nLuna. We show how the Coq model is encoded in {log} and how automated proofs\nare performed. The resulting {log} model is an automatically verified\nexecutable prototype of the Android permissions system. Detailed data on the\nempirical evaluation resulting after executing all the proofs in {log} is\nprovided. The integration of Coq and {log} as to provide a framework featuring\nautomated proof and prototype generation is discussed.", "journal": ""}
{"doi": "10.48550/arXiv.1910.09336", "date": "2019-10-21", "title": "The Lean mathematical library", "authors": "The mathlib Community", "abstract": "This paper describes mathlib, a community-driven effort to build a unified\nlibrary of mathematics formalized in the Lean proof assistant. Among proof\nassistant libraries, it is distinguished by its dependently typed foundations,\nfocus on classical mathematics, extensive hierarchy of structures, use of\nlarge- and small-scale automation, and distributed organization. We explain the\narchitecture and design decisions of the library and the social organization\nthat has led us here.", "journal": ""}
{"doi": "10.48550/arXiv.2107.07670", "date": "2021-07-16", "title": "Touring the MetaCoq Project (Invited Paper)", "authors": "Matthieu Sozeau", "abstract": "Proof assistants are getting more widespread use in research and industry to\nprovide certified and independently checkable guarantees about theories,\ndesigns, systems and implementations. However, proof assistant implementations\nthemselves are seldom verified, although they take a major share of the trusted\ncode base in any such certification effort. In this area, proof assistants\nbased on Higher-Order Logic enjoy stronger guarantees, as self-certified\nimplementations have been available for some years. One cause of this\ndifference is the inherent complexity of dependent type theories together with\ntheir extensions with inductive types, universe polymorphism and complex sort\nsystems, and the gap between theory on paper and practical implementations in\nefficient programming languages. MetaCoq is a collaborative project that aims\nto tackle these difficulties to provide the first fully-certified realistic\nimplementation of a type checker for the full calculus underlying the Coq proof\nassistant. To achieve this, we refined the sometimes blurry, if not incorrect,\nspecification and implementation of the system. We show how theoretical tools\nfrom this community such as bidirectional type-checking,\nTait-Martin-L\\\"of/Takahashi's confluence proof technique and monadic and\ndependently-typed programming can help construct the following artefacts: a\nspecification of Coq's syntax and type theory, the Polymorphic Cumulative\nCalculus of (Co)-Inductive Constructions (PCUIC); a monad for the manipulation\nof raw syntax and interaction with the Coq system; a verification of PCUIC's\nmetatheory, whose main results are the confluence of reduction, type\npreservation and principality of typing; a realistic, correct and complete\ntype-checker for PCUIC; a sound type and proof erasure procedure from PCUIC to\nuntyped lambda-calculus, i.e., the core of the extraction mechanism of Coq.", "journal": "EPTCS 337, 2021, pp. 13-29"}
{"doi": "10.48550/arXiv.2008.00120", "date": "2020-07-31", "title": "The Tactician (extended version): A Seamless, Interactive Tactic Learner and Prover for Coq", "authors": "Lasse Blaauwbroek, Josef Urban, Herman Geuvers", "abstract": "We present Tactician, a tactic learner and prover for the Coq Proof\nAssistant. Tactician helps users make tactical proof decisions while they\nretain control over the general proof strategy. To this end, Tactician learns\nfrom previously written tactic scripts and gives users either suggestions about\nthe next tactic to be executed or altogether takes over the burden of proof\nsynthesis. Tactician's goal is to provide users with a seamless, interactive,\nand intuitive experience together with robust and adaptive proof automation. In\nthis paper, we give an overview of Tactician from the user's point of view,\nregarding both day-to-day usage and issues of package dependency management\nwhile learning in the large. Finally, we give a peek into Tactician's\nimplementation as a Coq plugin and machine learning platform.", "journal": "In CICM. volume 12236 of Lecture Notes in Computer Science, pages\n  271-277. Springer, 2020"}
{"doi": "10.48550/arXiv.2302.06420", "date": "2023-02-13", "title": "Closure Properties of General Grammars -- Formally Verified", "authors": "Martin Dvorak, Jasmin Blanchette", "abstract": "We formalized general (i.e., type-0) grammars using the Lean 3 proof\nassistant. We defined basic notions of rewrite rules and of words derived by a\ngrammar, and used grammars to show closure of the class of type-0 languages\nunder four operations: union, reversal, concatenation, and the Kleene star. The\nliterature mostly focuses on Turing machine arguments, which are possibly more\ndifficult to formalize. For the Kleene star, we could not follow the literature\nand came up with our own grammar-based construction.", "journal": ""}
{"doi": "10.48550/arXiv.1410.3735", "date": "2014-10-14", "title": "The Foundational Cryptography Framework", "authors": "Adam Petcher, Greg Morrisett", "abstract": "We present the Foundational Cryptography Framework (FCF) for developing and\nchecking complete proofs of security for cryptographic schemes within a proof\nassistant. This is a general-purpose framework that is capable of modeling and\nreasoning about a wide range of cryptographic schemes, security definitions,\nand assumptions. Security is proven in the computational model, and the proof\nprovides concrete bounds as well as asymptotic conclusions. FCF provides a\nlanguage for probabilistic programs, a theory that is used to reason about\nprograms, and a library of tactics and definitions that are useful in proofs\nabout cryptography. The framework is designed to leverage fully the existing\ntheory and capabilities of the Coq proof assistant in order to reduce the\neffort required to develop proofs.", "journal": ""}
{"doi": "10.48550/arXiv.2102.02627", "date": "2021-02-04", "title": "Formalising a Turing-Complete Choreographic Language in Coq", "authors": "Lu\u00eds Cruz-Filipe, Fabrizio Montesi, Marco Peressotti", "abstract": "Theory of choreographic languages typically includes a number of complex\nresults that are proved by structural induction. The high number of cases and\nthe subtle details in some of them lead to long reviewing processes, and\noccasionally to errors being found in published proofs. In this work, we take a\npublished proof of Turing completeness of a choreographic language and\nformalise it in Coq. Our development includes formalising the choreographic\nlanguage and its basic properties, Kleene's theory of partial recursive\nfunctions, the encoding of these functions as choreographies, and proving this\nencoding correct.\n  With this effort, we show that theorem proving can be a very useful tool in\nthe field of choreographic languages: besides the added degree of confidence\nthat we get from a mechanised proof, the formalisation process led us to a\nsignificant simplification of the underlying theory. Our results offer a\nfoundation for the future formal development of choreographic languages.", "journal": ""}
{"doi": "10.48550/arXiv.1812.04088", "date": "2018-12-04", "title": "Towards Machine Learning Induction", "authors": "Yutaka Nagashima", "abstract": "Induction lies at the heart of mathematics and computer science. However,\nautomated theorem proving of inductive problems is still limited in its power.\nIn this abstract, we first summarize our progress in automating inductive\ntheorem proving for Isabelle/HOL. Then, we present MeLoId, our approach to\nsuggesting promising applications of induction without completing a proof\nsearch.", "journal": ""}
{"doi": "10.48550/arXiv.1912.02250", "date": "2019-12-04", "title": "A Verified Optimizer for Quantum Circuits", "authors": "Kesha Hietala, Robert Rand, Shih-Han Hung, Xiaodi Wu, Michael Hicks", "abstract": "We present VOQC, the first fully verified optimizer for quantum circuits,\nwritten using the Coq proof assistant. Quantum circuits are expressed as\nprograms in a simple, low-level language called SQIR, a simple quantum\nintermediate representation, which is deeply embedded in Coq. Optimizations and\nother transformations are expressed as Coq functions, which are proved correct\nwith respect to a semantics of SQIR programs. SQIR uses a semantics of matrices\nof complex numbers, which is the standard for quantum computation, but treats\nmatrices symbolically in order to reason about programs that use an arbitrary\nnumber of quantum bits. SQIR's careful design and our provided automation make\nit possible to write and verify a broad range of optimizations in VOQC,\nincluding full-circuit transformations from cutting-edge optimizers.", "journal": ""}
{"doi": "10.48550/arXiv.2404.05458", "date": "2024-04-08", "title": "Teaching Higher-Order Logic Using Isabelle", "authors": "Simon Tobias Lund, J\u00f8rgen Villadsen", "abstract": "We present a formalization of higher-order logic in the Isabelle proof\nassistant, building directly on the foundational framework Isabelle/Pure and\ndeveloped to be as small and readable as possible. It should therefore serve as\na good introduction for someone looking into learning about higher-order logic\nand proof assistants, without having to study the much more complex\nIsabelle/HOL with heavier automation. To showcase our development and approach\nwe explain a sample proof, describe the axioms and rules of our higher-order\nlogic, and discuss our experience with teaching the subject in a classroom\nsetting.", "journal": "EPTCS 400, 2024, pp. 59-78"}
{"doi": "10.48550/arXiv.2006.04399", "date": "2020-06-08", "title": "Completeness Theorems for First-Order Logic Analysed in Constructive Type Theory (Extended Version)", "authors": "Yannick Forster, Dominik Kirst, Dominik Wehr", "abstract": "We study various formulations of the completeness of first-order logic\nphrased in constructive type theory and mechanised in the Coq proof assistant.\nSpecifically, we examine the completeness of variants of classical and\nintuitionistic natural deduction and sequent calculi with respect to\nmodel-theoretic, algebraic, and game-theoretic semantics. As completeness with\nrespect to the standard model-theoretic semantics \\`a la Tarski and Kripke is\nnot readily constructive, we analyse connections of completeness theorems to\nMarkov's Principle and Weak K\\\"onig's Lemma and discuss non-standard semantics\nadmitting assumption-free completeness. We contribute a reusable Coq library\nfor first-order logic containing all results covered in this paper.", "journal": "Journal of Logic and Computation, Volume 31, Issue 1, January\n  2021, Pages 112-151"}
{"doi": "10.48550/arXiv.1706.03814", "date": "2017-06-12", "title": "A Simple Soundness Proof for Dependent Object Types", "authors": "Marianna Rapoport, Ifaz Kabir, Paul He, Ond\u0159ej Lhot\u00e1k", "abstract": "Dependent Object Types (DOT) is intended to be a core calculus for modelling\nScala. Its distinguishing feature is abstract type members, fields in objects\nthat hold types rather than values. Proving soundness of DOT has been\nsurprisingly challenging, and existing proofs are complicated, and reason about\nmultiple concepts at the same time (e.g. types, values, evaluation). To serve\nas a core calculus for Scala, DOT should be easy to experiment with and extend,\nand therefore its soundness proof needs to be easy to modify.\n  This paper presents a simple and modular proof strategy for reasoning in DOT.\nThe strategy separates reasoning about types from other concerns. It is centred\naround a theorem that connects the full DOT type system to a restricted variant\nin which the challenges and paradoxes caused by abstract type members are\neliminated. Almost all reasoning in the proof is done in the intuitive world of\nthis restricted type system. Once we have the necessary results about types, we\nobserve that the other aspects of DOT are mostly standard and can be\nincorporated into a soundness proof using familiar techniques known from other\ncalculi.\n  Our paper comes with a machine-verified version of the proof in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2009.00583", "date": "2020-09-01", "title": "Formally Verified Transformation of Non-binary Constraints into Binary Constraints", "authors": "Catherine Dubois", "abstract": "It is well known in the Constraint Programming community that any non-binary\nconstraint satisfaction problem (with finite domains) can be transformed into\nan equivalent binary one. One of the most well-known translations is the Hidden\nVariable Encoding. In this paper we formalize this encoding in the proof\nassistant Coq and prove that any solution of the binary constraint satisfaction\nproblem makes it possible to build a solution of the original problem and\nvice-versa. This formal development is used to complete the formally verified\nconstraint solver developed in Coq by Carlier, Dubois and Gotlieb in 2012,\nmaking it a tool able to solve any n-ary constraint satisfaction problem, The\nkey of success of the connection between the translator and the Coq binary\nsolver is the genericity of the latter.", "journal": ""}
{"doi": "10.48550/arXiv.1003.0773", "date": "2010-03-03", "title": "S-Program Calculus", "authors": "Aleksandar Kupusinac, Dusan Malbaski", "abstract": "This paper presents a special subset of the first-order predicate logic named\nS-program calculus (briefly S-calculus). The S-calculus is a calculus\nconsisting of so-called S-formulas that are defined over the abstract state\nspace of a virtual machine. We show that S-formulas are a highly general tool\nfor analyzing program semantics inasmuch as Hoare triplets of total and partial\ncorrectness are not more than two S-formulas. Moreover, all the rules of Hoare\nlogic can be derived using S-formulas and axioms/theorems of first-order\npredicate calculus. The S-calculus is a powerful mechanism for proving program\ncorrectness as well as for building additional proving tools using theorems of\nthe predicate logic. Every proof is based on deriving the validity of some\nS-formula, so the procedure may be automated using automatic theorem provers\n(we will use Coq in this paper). As an example of the use of S-calculus, we\nwill prove the four basic properties of Dijsktra's operator wp. The proofs\ngiven by Dijkstra are not completely formalized and we will show that a full\nformalization can be achieved using S-calculus. Finally, we add one more\ntheorem to the above-mentioned four, namely the law of negation.", "journal": ""}
{"doi": "10.48550/arXiv.2307.16270", "date": "2023-07-30", "title": "Formalizing Monoidal Categories and Actions for Syntax with Binders", "authors": "Benedikt Ahrens, Ralph Matthes, Kobe Wullaert", "abstract": "We discuss some aspects of our work on the mechanization of syntax and\nsemantics in the UniMath library, based on the proof assistant Coq. We focus on\nexperiences where Coq (as a type-theoretic proof assistant with decidable\ntypechecking) made us use more theory or helped us to see theory more clearly.", "journal": ""}
{"doi": "10.48550/arXiv.2112.11781", "date": "2021-12-22", "title": "Parametric Church's Thesis: Synthetic Computability without Choice", "authors": "Yannick Forster", "abstract": "In synthetic computability, pioneered by Richman, Bridges, and Bauer, one\ndevelops computability theory without an explicit model of computation. This is\nenabled by assuming an axiom equivalent to postulating a function $\\phi$ to be\nuniversal for the space $\\mathbb{N}\\to\\mathbb{N}$ ($\\mathsf{CT}_\\phi$, a\nconsequence of the constructivist axiom $\\mathsf{CT}$), Markov's principle, and\nat least the axiom of countable choice. Assuming $\\mathsf{CT}$ and countable\nchoice invalidates the law of excluded middle, thereby also invalidating\nclassical intuitions prevalent in textbooks on computability. On the other\nhand, results like Rice's theorem are not provable without a form of choice. In\ncontrast to existing work, we base our investigations in constructive type\ntheory with a separate, impredicative universe of propositions where countable\nchoice does not hold and thus a priori $\\mathsf{CT}_{\\phi}$ and the law of\nexcluded middle seem to be consistent. We introduce various parametric\nstrengthenings of $\\mathsf{CT}_{\\phi}$, which are equivalent to assuming\n$\\mathsf{CT}_\\phi$ and an $S^m_n$ operator for $\\phi$ like in the $S^m_n$\ntheorem. The strengthened axioms allow developing synthetic computability\ntheory without choice, as demonstrated by elegant synthetic proofs of Rice's\ntheorem. Moreover, they seem to be not in conflict with classical intuitions\nsince they are consequences of the traditional analytic form of $\\mathsf{CT}$.\n  Besides explaining the novel axioms and proofs of Rice's theorem we\ncontribute machine-checked proofs of all results in the Coq proof assistant.", "journal": "S. Artemov and A. Nerode (Eds.): LFCS 2022, LNCS 13137, pp. 70-89,\n  2022"}
{"doi": "10.48550/arXiv.2310.06959", "date": "2023-10-10", "title": "Proof Repair across Quotient Type Equivalences", "authors": "Cosmo Viola, Max Fan, Talia Ringer", "abstract": "Proofs in proof assistants like Coq can be brittle, breaking easily in\nresponse to changes. To address this, recent work introduced an algorithm and\ntool in Coq to automatically repair broken proofs in response to changes that\ncorrespond to type equivalences. However, many changes remained out of the\nscope of this algorithm and tool -- especially changes in underlying behavior.\nWe extend this proof repair algorithm so that it can express certain changes in\nbehavior that were previously out of scope. We focus in particular on\nequivalences between quotient types -- types equipped with a relation that\ndescribes what it means for any two elements of that type to be equal. Quotient\ntype equivalences can be used to express interesting changes in representations\nof mathematical structures, as well as changes in the underlying\nimplementations of data structures.\n  We extend this algorithm and tool to support quotient type equivalences in\nCoq. Notably, since Coq lacks quotient types entirely, our extensions use Coq's\nsetoid machinery to represent quotients externally. Specifically, (1) our\nextension to the algorithm supports new changes corresponding to setoids, and\n(2) our extension to the tool supports this new class of changes and further\nautomates away some of the new proof obligations. We ground our setoid\nextensions by way of a discussion of a corresponding manual proof repair\napproach in Cubical Agda, which supports quotient types and allows for some\ninternalization of the correctness criteria for proof repair. We demonstrate\nour extensions on proof repair case studies for previously unsupported changes.", "journal": ""}
{"doi": "10.48550/arXiv.1301.3047", "date": "2013-01-14", "title": "On Formal Reasoning on the Semantics of PLC using Coq", "authors": "Jan Olaf Blech, Sidi Ould Biha", "abstract": "Programmable Logic Controllers (PLC) and its programming standard IEC 61131-3\nare widely used in embedded systems for the industrial automation domain. We\npropose a framework for the formal treatment of PLC based on the IEC 61131-3\nstandard. A PLC system description typically combines code written in different\nlanguages that are defined in IEC 61131-3. For the top-level specification we\nregard the Sequential Function Charts (SFC) language, a graphical high-level\nlanguage that allows to describe the main control-flow of the system. In\naddition to this, we describe the Instruction List (IL) language -- an assembly\nlike language -- and two other graphical languages: Ladder Diagrams (LD) and\nFunction Block Diagrams (FBD). IL, LD, and FBD are used to describe more low\nlevel structures of a PLC. We formalize the semantics of these languages and\ndescribe and prove relations between them. Formalization and associated proofs\nare carried out using the proof assistant Coq. In addition to this, we present\nwork on a tool for automatically generating SFC representations from a\ngraphical description -- the IL and LD languages can be handled in Coq directly\n-- and its usage for verification purposes. We sketch possible usages of our\nformal framework, and present an example application for a PLC in a project\ndemonstrator and prove safety properties.", "journal": ""}
{"doi": "10.48550/arXiv.1901.03313", "date": "2019-01-10", "title": "Mechanization of Separation in Generic Extensions", "authors": "Emmanuel Gunther, Miguel Pagano, Pedro S\u00e1nchez Terraf", "abstract": "We mechanize, in the proof assistant Isabelle, a proof of the axiom-scheme of\nSeparation in generic extensions of models of set theory by using the\nfundamental theorems of forcing. We also formalize the satisfaction of the\naxioms of Extensionality, Foundation, Union, and Powerset. The axiom of\nInfinity is likewise treated, under additional assumptions on the ground model.\nIn order to achieve these goals, we extended Paulson's library on\nconstructibility with renaming of variables for internalized formulas, improved\nresults on definitions by recursion on well-founded relations, and sharpened\nhypotheses in his development of relativization and absoluteness.", "journal": ""}
{"doi": "10.48550/arXiv.1905.13706", "date": "2019-05-31", "title": "A Role for Dependent Types in Haskell (Extended version)", "authors": "Stephanie Weirich, Pritam Choudhury, Antoine Voizard, Richard A. Eisenberg", "abstract": "Modern Haskell supports zero-cost coercions, a mechanism where types that\nshare the same run-time representation may be freely converted between. To make\nsure such conversions are safe and desirable, this feature relies on a\nmechanism of roles to prohibit invalid coercions. In this work, we show how to\nintegrate roles with dependent type systems and prove, using the Coq proof\nassistant, that the resulting system is sound. We have designed this work as a\nfoundation for the addition of dependent types to the Glasgow Haskell Compiler,\nbut we also expect that it will be of use to designers of other\ndependently-typed languages who might want to adopt Haskell's safe coercions\nfeature.", "journal": ""}
{"doi": "10.48550/arXiv.2307.05539", "date": "2023-07-08", "title": "Concerto Grosso for Sessions: Fair Termination of Sessions", "authors": "Luca Ciccone", "abstract": "Sessions are a fundamental notion in message-passing systems. A session is an\nabstract notion of communication between parties where each one owns an\nendpoint. Session types are types that are assigned to the endpoints and that\nare used to statically and dynamically enforce some desired properties of the\ncommunications, such as the absence of deadlocks. Properties of concurrent\nsystems are usually divided in safety and liveness ones and depending on the\nclass it belongs to, a property is defined using different (dual) techniques.\nHowever, there exist some properties that require to mix both techniques and\nthe challenges in defining them are exacerbated in proof assistants (e.g. Agda,\nCoq, . . . ), that is, tools that allow users to formally characterize and\nprove theorems. In particular, we mechanize the meta-theory of inference\nsystems in Agda. Among the interesting properties that can be studied in the\nsession-based context, we study fair termination which is the property of those\nsessions that can always eventually reach successful termination under a\nfairness assumption. Fair termination implies many desirable and well known\nproperties, such as lock freedom. Moreover, a lock free session does not imply\nthat other sessions are lock free as well. On the other hand, if we consider a\nsession and we assume that all the other ones are fairly terminating, we can\nconclude that the one under analysis is fairly terminating as well.", "journal": ""}
{"doi": "10.48550/arXiv.1811.00796", "date": "2018-11-02", "title": "Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning", "authors": "Mitsuru Kusumoto, Keisuke Yahata, Masahiro Sakai", "abstract": "The problem-solving in automated theorem proving (ATP) can be interpreted as\na search problem where the prover constructs a proof tree step by step. In this\npaper, we propose a deep reinforcement learning algorithm for proof search in\nintuitionistic propositional logic. The most significant challenge in the\napplication of deep learning to the ATP is the absence of large, public theorem\ndatabase. We, however, overcame this issue by applying a novel data\naugmentation procedure at each iteration of the reinforcement learning. We also\nimprove the efficiency of the algorithm by representing the syntactic structure\nof formulas by a novel compact graph representation. Using the large volume of\naugmented data, we train highly accurate graph neural networks that approximate\nthe value function for the set of the syntactic structures of formulas. Our\nmethod is also cost-efficient in terms of computational time. We will show that\nour prover outperforms Coq's $\\texttt{tauto}$ tactic, a prover based on\nhuman-engineered heuristics. Within the specified time limit, our prover solved\n84% of the theorems in a benchmark library, while $\\texttt{tauto}$ was able to\nsolve only 52%.", "journal": ""}
{"doi": "10.48550/arXiv.2110.11034", "date": "2021-10-21", "title": "Certifying C program correctness with respect to CompCert with VeriFast", "authors": "Stefan Wils, Bart Jacobs", "abstract": "VeriFast is a powerful tool for verification of various correctness\nproperties of C programs using symbolic execution. However, VeriFast itself has\nnot been verified. We present a proof-of-concept extension which generates a\ncorrectness certificate for each successful verification run individually. This\ncertificate takes the form of a Coq script containing two proofs which, when\nsuccessfully checked by Coq, together remove the need for trusting in the\ncorrectness of VeriFast itself.\n  The first proves a lemma expressing the correctness of the program with\nrespect to a big step operational semantics developed by ourselves, intended to\nreflect VeriFast's interpretation of C. We have formalized this semantics in\nCoq as cbsem. This lemma is proven by symbolic execution in Coq, which in turn\nis implemented by transforming the exported AST of the program into a Coq\nproposition representing the symbolic execution performed by VeriFast itself.\n  The second proves the correctness of the same C program with respect to\nCompCert's Clight big step semantics. This proof simply applies our proof of\nthe soundness of cbsem with respect to CompCert Clight to the first proof.", "journal": ""}
{"doi": "10.48550/arXiv.1907.02836", "date": "2019-07-05", "title": "From LCF to Isabelle/HOL", "authors": "Lawrence C. Paulson, Tobias Nipkow, Makarius Wenzel", "abstract": "Interactive theorem provers have developed dramatically over the past four\ndecades, from primitive beginnings to today's powerful systems. Here, we focus\non Isabelle/HOL and its distinctive strengths. They include automatic proof\nsearch, borrowing techniques from the world of first order theorem proving, but\nalso the automatic search for counterexamples. They include a highly readable\nstructured language of proofs and a unique interactive development environment\nfor editing live proof documents. Everything rests on the foundation conceived\nby Robin Milner for Edinburgh LCF: a proof kernel, using abstract types to\nensure soundness and eliminate the need to store proofs. Compared with the\nresearch prototypes of the 1970s, Isabelle is a practical and versatile tool.\nIt is used by system designers, mathematicians and many others.", "journal": "Formal Aspects of Computing 31:6 (2019), 675-698"}
{"doi": "10.48550/arXiv.2411.06636", "date": "2024-11-11", "title": "The internal languages of univalent categories", "authors": "Niels van der Weide", "abstract": "Internal language theorems are fundamental in categorical logic, since they\nexpress an equivalence between syntax and semantics. One of such theorems was\nproven by Clairambault and Dybjer, who corrected the result originally by\nSeely. More specifically, they constructed a biequivalence between the\nbicategory of locally Cartesian closed categories and the bicategory of\ndemocratic categories with families with extensional identity types,\n$\\sum$-types, and $\\prod$-types. This theorem expresses that the internal\nlanguage of locally Cartesian closed is extensional Martin-L\\\"of type theory\nwith dependent sums and products. In this paper, we study the theorem by\nClairambault and Dybjer for univalent categories, and we extend it to various\nclasses of toposes, among which are $\\prod$-pretoposes and elementary toposes.\nThe results in this paper have been formalized using the proof assistant Coq\nand the UniMath library.", "journal": ""}
{"doi": "10.48550/arXiv.1801.08766", "date": "2018-01-26", "title": "Relational Equivalence Proofs Between Imperative and MapReduce Algorithms", "authors": "Bernhard Beckert, Timo Bingmann, Moritz Kiefer, Peter Sanders, Mattias Ulbrich, Alexander Weigl", "abstract": "MapReduce frameworks are widely used for the implementation of distributed\nalgorithms. However, translating imperative algorithms into these frameworks\nrequires significant structural changes to the algorithm. As the costs of\nrunning faulty algorithms at scale can be severe, it is highly desirable to\nverify the correctness of the translation, i.e., to prove that the MapReduce\nversion is equivalent to the imperative original. We present a novel approach\nfor proving equivalence between imperative and MapReduce algorithms based on\npartitioning the equivalence proof into a sequence of equivalence proofs\nbetween intermediate programs with smaller differences. Our approach is based\non the insight that two kinds of sub-proofs are required: (1) uniform\ntransformations changing the controlflow structure that are mostly independent\nof the particular context in which they are applied; and (2) context-dependent\ntransformations that are not uniform but that preserve the overall structure\nand can be proved correct using coupling invariants. We demonstrate the\nfeasibility of our approach by evaluating it on two prototypical algorithms\ncommonly used as examples in MapReduce frameworks: k-means and PageRank. To\ncarry out the proofs, we use the interactive theorem prover Coq with partial\nproof automation. The results show that our approach and its prototypical\nimplementation based on Coq enables equivalence proofs of non-trivial\nalgorithms and could be automated to a large degree.", "journal": ""}
{"doi": "10.48550/arXiv.2404.08163", "date": "2024-04-11", "title": "ViCAR: Visualizing Categories with Automated Rewriting in Coq", "authors": "Bhakti Shah, William Spencer, Laura Zielinski, Ben Caldwell, Adrian Lehmann, Robert Rand", "abstract": "We present ViCAR, a library for working with monoidal categories in the Coq\nproof assistant. ViCAR provides definitions for categorical structures that\nusers can instantiate with their own verification projects. Upon verifying\nrelevant coherence conditions, ViCAR gives a set of lemmas and tactics for\nmanipulating categorical structures. We also provide a visualizer that can\ndisplay any composition and tensor product of morphisms as a string diagram,\nshowing its categorical structure. This enables graphical reasoning and\nautomated rewriting for Coq projects with monoidal structures.", "journal": ""}
{"doi": "10.48550/arXiv.2207.09486", "date": "2022-07-19", "title": "Formalising the Krull Topology in Lean", "authors": "Sebastian Monnet", "abstract": "The Galois group of an infinite Galois extension has a natural topology,\ncalled the Krull topology, which has the important property of being profinite.\nIt is impossible to talk about Galois representations, and hence the Langlands\nProgram, without first defining the Krull topology. We explain our\nformalisation of this topology, and our proof that it is profinite, in the Lean\n3 theorem prover.", "journal": ""}
{"doi": "10.48550/arXiv.2503.00959", "date": "2025-03-02", "title": "Formalizing zeta and L-functions in Lean", "authors": "David Loeffler, Michael Stoll", "abstract": "The Riemann zeta function, and more generally the L-functions of Dirichlet\ncharacters, are among the central objects of study in number theory. We report\non a project to formalize the theory of these objects in Lean's \"Mathlib\"\nlibrary, including a proof of Dirichlet's theorem on primes in arithmetic\nprogressions and a formal statement of the Riemann hypothesis", "journal": ""}
{"doi": "10.48550/arXiv.1410.5467", "date": "2014-10-20", "title": "Machine Learning of Coq Proof Guidance: First Experiments", "authors": "Cezary Kaliszyk, Lionel Mamane, Josef Urban", "abstract": "We report the results of the first experiments with learning proof\ndependencies from the formalizations done with the Coq system. We explain the\nprocess of obtaining the dependencies from the Coq proofs, the characterization\nof formulas that is used for the learning, and the evaluation method. Various\nmachine learning methods are compared on a dataset of 5021 toplevel Coq proofs\ncoming from the CoRN repository. The best resulting method covers on average\n75% of the needed proof dependencies among the first 100 predictions, which is\na comparable performance of such initial experiments on other large-theory\ncorpora.", "journal": ""}
{"doi": "10.48550/arXiv.1912.05601", "date": "2019-12-11", "title": "Is Sized Typing for Coq Practical?", "authors": "Jonathan Chan, Yufeng Li, William J. Bowman", "abstract": "Contemporary proof assistants such as Coq require that recursive functions be\nterminating and corecursive functions be productive to maintain logical\nconsistency of their type theories, and some ensure these properties using\nsyntactic checks. However, being syntactic, they are inherently delicate and\nrestrictive, preventing users from easily writing obviously terminating or\nproductive functions at their whim.\n  Meanwhile, there exist many sized type theories that perform type-based\ntermination and productivity checking, including theories based on the Calculus\nof (Co)Inductive Constructions (CIC), the core calculus underlying Coq. These\ntheories are more robust and compositional in comparison. So why haven't they\nbeen adapted to Coq?\n  In this paper, we venture to answer this question with CIC$\\widehat{*}$, a\nsized type theory based on CIC. It extends past work on sized types in CIC with\nadditional Coq features such as global and local definitions. We also present a\ncorresponding size inference algorithm and implement it within Coq's kernel;\nfor maximal backward compatibility with existing Coq developments, it requires\nno additional annotations from the user.\n  In our evaluation of the implementation, we find a severe performance\ndegradation when compiling parts of the Coq standard library, inherent to the\nalgorithm itself. We conclude that if we wish to maintain backward\ncompatibility, using size inference as a replacement for syntactic checking is\nwildly impractical in terms of performance.", "journal": ""}
{"doi": "10.48550/arXiv.2309.04295", "date": "2023-09-08", "title": "FIMO: A Challenge Formal Dataset for Automated Theorem Proving", "authors": "Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, Ming Zhang, Qun Liu", "abstract": "We present FIMO, an innovative dataset comprising formal mathematical problem\nstatements sourced from the International Mathematical Olympiad (IMO)\nShortlisted Problems. Designed to facilitate advanced automated theorem proving\nat the IMO level, FIMO is currently tailored for the Lean formal language. It\ncomprises 149 formal problem statements, accompanied by both informal problem\ndescriptions and their corresponding LaTeX-based informal proofs. Through\ninitial experiments involving GPT-4, our findings underscore the existing\nlimitations in current methodologies, indicating a substantial journey ahead\nbefore achieving satisfactory IMO-level automated theorem proving outcomes.", "journal": ""}
{"doi": "10.48550/arXiv.1509.03534", "date": "2015-09-11", "title": "Premise Selection and External Provers for HOL4", "authors": "Thibault Gauthier, Cezary Kaliszyk", "abstract": "Learning-assisted automated reasoning has recently gained popularity among\nthe users of Isabelle/HOL, HOL Light, and Mizar. In this paper, we present an\nadd-on to the HOL4 proof assistant and an adaptation of the HOLyHammer system\nthat provides machine learning-based premise selection and automated reasoning\nalso for HOL4. We efficiently record the HOL4 dependencies and extract features\nfrom the theorem statements, which form a basis for premise selection.\nHOLyHammer transforms the HOL4 statements in the various TPTP-ATP proof\nformats, which are then processed by the ATPs. We discuss the different\nevaluation settings: ATPs, accessible lemmas, and premise numbers. We measure\nthe performance of HOLyHammer on the HOL4 standard library. The results are\ncombined accordingly and compared with the HOL Light experiments, showing a\ncomparably high quality of predictions. The system directly benefits HOL4 users\nby automatically finding proofs dependencies that can be reconstructed by\nMetis.", "journal": ""}
{"doi": "10.48550/arXiv.1611.09473", "date": "2016-11-29", "title": "Proust: A Nano Proof Assistant", "authors": "Prabhakar Ragde", "abstract": "Proust is a small Racket program offering rudimentary interactive assistance\nin the development of verified proofs for propositional and predicate logic. It\nis constructed in stages, some of which are done by students before using it to\ncomplete proof exercises, and in parallel with the study of its theoretical\nunderpinnings, including elements of Martin-Lof type theory. The goal is\ntwofold: to demystify some of the machinery behind full-featured proof\nassistants such as Coq and Agda, and to better integrate the study of formal\nlogic with other core elements of an undergraduate computer science curriculum.", "journal": "EPTCS 230, 2016, pp. 63-75"}
{"doi": "10.48550/arXiv.1209.6336", "date": "2012-09-27", "title": "Parametricity in an Impredicative Sort", "authors": "Chantal Keller, Marc Lasson", "abstract": "Reynold's abstraction theorem is now a well-established result for a large\nclass of type systems. We propose here a definition of relational parametricity\nand a proof of the abstraction theorem in the Calculus of Inductive\nConstructions (CIC), the underlying formal language of Coq, in which\nparametricity relations' codomain is the impredicative sort of propositions. To\nproceed, we need to refine this calculus by splitting the sort hierarchy to\nseparate informative terms from non-informative terms. This refinement is very\nclose to CIC, but with the property that typing judgments can distinguish\ninformative terms. Among many applications, this natural encoding of\nparametricity inside CIC serves both theoretical purposes (proving the\nindependence of propositions with respect to the logical system) as well as\npractical aspirations (proving properties of finite algebraic structures). We\nfinally discuss how we can simply build, on top of our calculus, a new\nreflexive Coq tactic that constructs proof terms by parametricity.", "journal": "CSL - 26th International Workshop/21st Annual Conference of the\n  EACSL, CSL 2012 16 (2012) 381-395"}
{"doi": "10.48550/arXiv.2308.15567", "date": "2023-08-29", "title": "Certifying C program correctness with respect to CH2O with VeriFast", "authors": "Stefan Wils, Bart Jacobs", "abstract": "VeriFast is a powerful tool for verification of various correctness\nproperties of C programs using symbolic execution. However, VeriFast itself has\nnot been verified. We present a proof-of-concept extension which generates a\ncorrectness certificate for each successful verification run individually. This\ncertificate takes the form of a Coq script which, when successfully checked by\nCoq, removes the need for trusting in the correctness of VeriFast itself.\n  The Coq script achieves this by applying a chain of soundness results,\nallowing us to prove correctness of the program with regards to the third-party\nCH2O small step semantics for C11 by proving correctness in terms of symbolic\nexecution in Coq. This proof chain includes two intermediate auxiliary big step\nsemantics, the most important of which describes VeriFast's interpretation of\nC. Finally, symbolic execution in Coq is implemented by transforming the\nexported AST of the program into a Coq proposition representing the symbolic\nexecution performed by VeriFast itself.", "journal": ""}
{"doi": "10.48550/arXiv.2005.09452", "date": "2020-05-18", "title": "PubSub implementation in Haskell with formal verification in Coq", "authors": "Boro Sitnikovski, Biljana Stojcevska, Lidija Goracinova-Ilieva, Irena Stojmenovska", "abstract": "In the cloud, the technology is used on-demand without the need to install\nanything on the desktop. Software as a Service is one of the many cloud\narchitectures. The PubSub messaging pattern is a cloud-based Software as a\nService solution used in complex systems, especially in the notifications part\nwhere there is a need to send a message from one unit to another single unit or\nmultiple units. Haskell is a generic typed programming language which has\npioneered several advanced programming language features. Based on the lambda\ncalculus system, it belongs to the family of functional programming languages.\nCoq, also based on a stricter version of lambda calculus, is a programming\nlanguage that has a more advanced type system than Haskell and is mainly used\nfor theorem proving i.e. proving software correctness. This paper aims to show\nhow PubSub can be used in conjunction with cloud computing (Software as a\nService), as well as to present an example implementation in Haskell and proof\nof correctness in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1402.0081", "date": "2014-02-01", "title": "Proof Pattern Search in Coq/SSReflect", "authors": "J\u00f3nathan Heras, Ekaterina Komendantskaya", "abstract": "ML4PG is an extension of the Proof General interface, allowing the user to\ninvoke machine-learning algorithms and find proof similarities in Coq/SSReect\nlibraries. In this paper, we present three new improvements to ML4PG. First, a\nnew method of \"recurrent clustering\" is introduced to collect statistical\nfeatures from Coq terms. Now the user can receive suggestions about similar\ndefinitions, types and lemma statements, in addition to proof strategies.\nSecond, Coq proofs are split into patches to capture proof strategies that\ncould arise at different stages of a proof. Finally, we improve ML4PG's output\nintroducing an automaton-shape representation for proof patterns.", "journal": ""}
{"doi": "10.48550/arXiv.2107.07663", "date": "2021-07-16", "title": "Countability of Inductive Types Formalized in the Object-Logic Level", "authors": "Qinxiang Cao, Xiwei Wu", "abstract": "The set of integer number lists with finite length, and the set of binary\ntrees with integer labels are both countably infinite. Many inductively defined\ntypes also have countably many elements. In this paper, we formalize the syntax\nof first order inductive definitions in Coq and prove them countable, under\nsome side conditions. Instead of writing a proof generator in a meta language,\nwe develop an axiom-free proof in the Coq object logic. In other words, our\nproof is a dependently typed Coq function from the syntax of the inductive\ndefinition to the countability of the type. Based on this proof, we provide a\nCoq tactic to automatically prove the countability of concrete inductive types.\nWe also developed Coq libraries for countability and for the syntax of\ninductive definitions, which have value on their own.", "journal": "EPTCS 337, 2021, pp. 55-70"}
{"doi": "10.48550/arXiv.2004.07761", "date": "2020-04-16", "title": "Deep Generation of Coq Lemma Names Using Elaborated Terms", "authors": "Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric", "abstract": "Coding conventions for naming, spacing, and other essentially stylistic\nproperties are necessary for developers to effectively understand, review, and\nmodify source code in large software projects. Consistent conventions in\nverification projects based on proof assistants, such as Coq, increase in\nimportance as projects grow in size and scope. While conventions can be\ndocumented and enforced manually at high cost, emerging approaches\nautomatically learn and suggest idiomatic names in Java-like languages by\napplying statistical language models on large code corpora. However, due to its\npowerful language extension facilities and fusion of type checking and\ncomputation, Coq is a challenging target for automated learning techniques. We\npresent novel generation models for learning and suggesting lemma names for Coq\nprojects. Our models, based on multi-input neural networks, are the first to\nleverage syntactic and semantic information from Coq's lexer (tokens in lemma\nstatements), parser (syntax trees), and kernel (elaborated terms) for naming;\nthe key insight is that learning from elaborated terms can substantially boost\nmodel performance. We implemented our models in a toolchain, dubbed Roosterize,\nand applied it on a large corpus of code derived from the Mathematical\nComponents family of projects, known for its stringent coding conventions. Our\nresults show that Roosterize substantially outperforms baselines for suggesting\nlemma names, highlighting the importance of using multi-input models and\nelaborated terms.", "journal": ""}
{"doi": "10.48550/arXiv.1710.00787", "date": "2017-10-02", "title": "Proof-checking Euclid", "authors": "Michael Beeson, Julien Narboux, Freek Wiedijk", "abstract": "We used computer proof-checking methods to verify the correctness of our\nproofs of the propositions in Euclid Book I. We used axioms as close as\npossible to those of Euclid, in a language closely related to that used in\nTarski's formal geometry. We used proofs as close as possible to those given by\nEuclid, but filling Euclid's gaps and correcting errors. Euclid Book I has 48\npropositions, we proved 235 theorems. The extras were partly \"Book Zero\",\npreliminaries of a very fundamental nature, partly propositions that Euclid\nomitted but were used implicitly, partly advanced theorems that we found\nnecessary to fill Euclid's gaps, and partly just variants of Euclid's\npropositions. We wrote these proofs in a simple fragment of first-order logic\ncorresponding to Euclid's logic, debugged them using a custom software tool,\nand then checked them in the well-known and trusted proof checkers HOL Light\nand Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1711.07023", "date": "2017-11-19", "title": "Verification of PCP-Related Computational Reductions in Coq", "authors": "Yannick Forster, Edith Heiter, Gert Smolka", "abstract": "We formally verify several computational reductions concerning the Post\ncorrespondence problem (PCP) using the proof assistant Coq. Our verifications\ninclude a reduction of a string rewriting problem generalising the halting\nproblem for Turing machines to PCP, and reductions of PCP to the intersection\nproblem and the palindrome problem for context-free grammars. Interestingly,\nrigorous correctness proofs for some of the reductions are missing in the\nliterature.", "journal": ""}
{"doi": "10.48550/arXiv.2412.19463", "date": "2024-12-27", "title": "Laws of Quantum Programming", "authors": "Mingsheng Ying, Li Zhou, Gilles Barthe", "abstract": "In this paper, we investigate the fundamental laws of quantum programming. We\nextend a comprehensive set of Hoare et al.'s basic laws of classical\nprogramming to the quantum setting. These laws characterise the algebraic\nproperties of quantum programs, such as the distributivity of sequential\ncomposition over (quantum) if-statements and the unfolding of nested (quantum)\nif-statements. At the same time, we clarify some subtle differences between\ncertain laws of classical programming and their quantum counterparts.\nAdditionally, we derive a fixpoint characterization of quantum while-loops and\na loop-based realisation of tail recursion in quantum programming. Furthermore,\nwe establish two normal form theorems: one for quantum circuits and one for\nfinite quantum programs. The theory in which these laws are established is\nformalised in the Coq proof assistant, and all of these laws are mechanically\nverified. As an application case of our laws, we present a formal derivation of\nthe principle of deferred measurements in dynamic quantum circuits.\n  We expect that these laws can be utilized in correctness-preserving\ntransformation, compilation, and automatic code optimization in quantum\nprogramming. In particular, because these laws are formally verified in Coq,\nthey can be confidently applied in quantum program development.", "journal": ""}
{"doi": "10.48550/arXiv.2501.10802", "date": "2025-01-18", "title": "Logical Relations for Formally Verified Authenticated Data Structures", "authors": "Simon Oddershede Gregersen, Chaitanya Agarwal, Joseph Tassarotti", "abstract": "Authenticated data structures allow untrusted third parties to carry out\noperations which produce proofs that can be used to verify an operation's\noutput. Such data structures are challenging to develop and implement\ncorrectly. This paper gives a formal proof of security and correctness for a\nlibrary that generates authenticated versions of data structures automatically.\nThe proof is based on a new relational separation logic for reasoning about\nprograms that use collision-resistant cryptographic hash functions. This logic\nprovides a basis for constructing two semantic models of a type system, which\nare used to justify how the library makes use of type abstraction to enforce\nsecurity and correctness. Using these models, we also prove the correctness of\nseveral optimizations to the library and then show how optimized, hand-written\nimplementations of authenticated data structures can be soundly linked with\nautomatically generated code. All of the results in this paper have been\nmechanized in the Coq proof assistant using the Iris framework.", "journal": ""}
{"doi": "10.48550/arXiv.2402.14485", "date": "2024-02-22", "title": "Machine-Checked Categorical Diagrammatic Reasoning", "authors": "Beno\u00eet Guillemet, Assia Mahboubi, Matthieu Piquerez", "abstract": "This paper describes a formal proof library, developed using the Coq proof\nassistant, designed to assist users in writing correct diagrammatic proofs, for\n1-categories. This library proposes a deep-embedded, domain-specific formal\nlanguage, which features dedicated proof commands to automate the synthesis,\nand the verification, of the technical parts often eluded in the literature.", "journal": ""}
{"doi": "10.48550/arXiv.2404.01234", "date": "2024-04-01", "title": "GFLean: An Autoformalisation Framework for Lean via GF", "authors": "Shashank Pathak", "abstract": "We present an autoformalisation framework for the Lean theorem prover, called\nGFLean. GFLean uses a high-level grammar writing tool called Grammatical\nFramework (GF) for parsing and linearisation. GFLean is implemented in Haskell.\nWe explain the functionalities of GFLean, its inner working and discuss its\nlimitations. We also discuss how we can use neural network based translation\nprograms and rule based translation programs together complimenting each other\nto build robust autoformalisation frameworks.", "journal": ""}
{"doi": "10.48550/arXiv.2102.02901", "date": "2021-02-04", "title": "A Formal Proof of the Independence of the Continuum Hypothesis", "authors": "Jesse Michael Han, Floris van Doorn", "abstract": "We describe a formal proof of the independence of the continuum hypothesis\n($\\mathsf{CH}$) in the Lean theorem prover. We use Boolean-valued models to\ngive forcing arguments for both directions, using Cohen forcing for the\nconsistency of $\\neg \\mathsf{CH}$ and a $\\sigma$-closed forcing for the\nconsistency of $\\mathsf{CH}$.", "journal": ""}
{"doi": "10.48550/arXiv.2405.06423", "date": "2024-05-10", "title": "Carleson Operators on Doubling Metric Measure Spaces", "authors": "Lars Becker, Floris van Doorn, Asgar Jamneshan, Rajula Srivastava, Christoph Thiele", "abstract": "We prove a new generalization of a theorem of Carleson, namely bounds for a\ngeneralized Carleson operator on doubling metric measure spaces. Additionally,\nwe explicitly reduce Carleson's classical result on pointwise convergence of\nFourier series to this new theorem. Both proofs are presented in great detail,\nsuitable as a blueprint for computer verification using the current\ncapabilities of the software package Lean. Note that even Carleson's classical\nresult has not yet been computer-verified.", "journal": ""}
{"doi": "10.48550/arXiv.2309.07252", "date": "2023-09-13", "title": "Towards solid abelian groups: A formal proof of N\u00f6beling's theorem", "authors": "Dagur Asgeirsson", "abstract": "Condensed mathematics, developed by Clausen and Scholze over the last few\nyears, is a new way of studying the interplay between algebra and geometry. It\nreplaces the concept of a topological space by a more sophisticated but\nbetter-behaved idea, namely that of a condensed set. Central to the theory are\nsolid abelian groups and liquid vector spaces, analogues of complete\ntopological groups.\n  N\\\"obeling's theorem, a surprising result from the 1960s about the structure\nof the abelian group of continuous maps from a profinite space to the integers,\nis a crucial ingredient in the theory of solid abelian groups; without it one\ncannot give any nonzero examples of solid abelian groups. We discuss a recently\ncompleted formalisation of this result in the Lean theorem prover, and give a\nmore detailed proof than those previously available in the literature. The\nproof is somewhat unusual in that it requires induction over ordinals -- a\ntechnique which has not previously been used to a great extent in formalised\nmathematics.", "journal": "15th International Conference on Interactive Theorem Proving (ITP\n  2024)"}
{"doi": "10.48550/arXiv.1103.2246", "date": "2011-03-11", "title": "Formal verification of a time-triggered hardware interface", "authors": "Julien Schmaltz", "abstract": "We present a formal proof of a time-triggered hardware interface. The design\nimplements the bit-clock synchronization mechanism specified by the FlexRay\nstandard for automotive embedded systems. The design is described at the\ngate-level. It can be translated to Verilog and synthesized on FPGA. The proof\nis based on a general model of asynchronous communications and combines\ninteractive theorem proving in Isabelle/HOL and automatic model-checking using\nNuSMV together with a model-reduction procedure, IHaVeIt. Our general model of\nasynchronous communications defines a clear separation between analog and\ndigital concerns. This separation enables the combination of theorem proving\nand model-checking for an efficient methodology. The analog phenomena are\nformalized in the logic of Isabelle/HOL. The gate-level hardware is\nautomatically analyzed using IHaVeIt. Our proof reveals the correct values of a\ncrucial parameter of the bit-clock synchronization mechanism. Our main theorem\nproves the functional correctness as well as the maximum number of cycles of\nthe transmission.", "journal": ""}
{"doi": "10.48550/arXiv.2108.10259", "date": "2021-08-23", "title": "The Multiverse: Logical Modularity for Proof Assistants", "authors": "Kenji Maillard, Nicolas Margulies, Matthieu Sozeau, Nicolas Tabareau, \u00c9ric Tanter", "abstract": "Proof assistants play a dual role as programming languages and logical\nsystems. As programming languages, proof assistants offer standard modularity\nmechanisms such as first-class functions, type polymorphism and modules. As\nlogical systems, however, modularity is lacking, and understandably so:\nincompatible reasoning principles -- such as univalence and uniqueness of\nidentity proofs -- can indirectly lead to logical inconsistency when used in a\ngiven development, even when they appear to be confined to different modules.\nThe lack of logical modularity in proof assistants also hinders the adoption of\nricher programming constructs, such as effects. We propose the multiverse, a\ngeneral type-theoretic approach to endow proof assistants with logical\nmodularity. The multiverse consists of multiple universe hierarchies that\nstatically describe the reasoning principles and effects available to define a\nterm at a given type. We identify sufficient conditions for this structuring to\nmodularly ensure that incompatible principles do not interfere, and to locally\nrestrict the power of dependent elimination when necessary. This extensible\napproach generalizes the ad-hoc treatment of the sort of propositions in the\nCoq proof assistant. We illustrate the power of the multiverse by describing\nthe inclusion of Coq-style propositions, the strict propositions of Gilbert et\nal., the exceptional type theory of P\\'edrot and Tabareau, and general\naxiomatic extensions of the logic.", "journal": ""}
{"doi": "10.48550/arXiv.2412.03125", "date": "2024-12-04", "title": "Gradual Guarantee via Step-Indexed Logical Relations in Agda", "authors": "Jeremy G. Siek", "abstract": "The gradual guarantee is an important litmus test for gradually typed\nlanguages, that is, languages that enable a mixture of static and dynamic\ntyping. The gradual guarantee states that changing the precision of a type\nannotation does not change the behavior of the program, except perhaps to\ntrigger an error if the type annotation is incorrect. Siek et al. (2015) proved\nthat the Gradually Typed Lambda Calculus (GTLC) satisfies the gradual guarantee\nusing a simulation-based proof and mechanized their proof in Isabelle. In the\nfollowing decade, researchers have proved the gradual guarantee for more\nsophisticated calculi, using step-indexed logical relations. However, given the\ncomplexity of that style of proof, there has not yet been a mechanized proof of\nthe gradual guarantee using step-indexed logical relations. This paper reports\non a mechanized proof of the gradual guarantee for the GTLC carried out in the\nAgda proof assistant.", "journal": "EPTCS 413, 2024, pp. 27-42"}
{"doi": "10.48550/arXiv.2302.10640", "date": "2023-02-21", "title": "An Elementary Formal Proof of the Group Law on Weierstrass Elliptic Curves in any Characteristic", "authors": "David Kurniadi Angdinata, Junyan Xu", "abstract": "Elliptic curves are fundamental objects in number theory and algebraic\ngeometry, whose points over a field form an abelian group under a geometric\naddition law. Any elliptic curve over a field admits a Weierstrass model, but\nprior formal proofs that the addition law is associative in this model involve\neither advanced algebraic geometry or tedious computation, especially in\ncharacteristic two. We formalise in the Lean theorem prover, the type of\nnonsingular points of a Weierstrass curve over a field of any characteristic\nand a purely algebraic proof that it forms an abelian group.", "journal": ""}
{"doi": "10.48550/arXiv.1709.03652", "date": "2017-09-12", "title": "A certified reference validation mechanism for the permission model of Android", "authors": "Gustavo Betarte, Juan Campo, Felipe Gorostiaga, Carlos Luna", "abstract": "Android embodies security mechanisms at both OS and application level. In\nthis platform application security is built primarily upon a system of\npermissions which specify restrictions on the operations a particular process\ncan perform. The critical role of these security mechanisms makes them a prime\ntarget for (formal) verification. We present an idealized model of a reference\nmonitor of the novel mechanisms of Android 6 (and further), where it is\npossible to grant permissions at run time. Using the programming language of\nthe proof-assistant Coq we have developed a functional implementation of the\nreference validation mechanism and certified its correctness with respect to\nthe specified reference monitor. Several properties concerning the permission\nmodel of Android 6 and its security mechanisms have been formally formulated\nand proved. Applying the program extraction mechanism provided by Coq we have\nalso derived a certified Haskell prototype of the reference validation\nmechanism.", "journal": ""}
{"doi": "10.48550/arXiv.2407.06222", "date": "2024-07-05", "title": "Formalization of the Filter Extension Principle (FEP) in Coq", "authors": "Guowei Dou, Wensheng Yu", "abstract": "The Filter Extension Principle (FEP) asserts that every filter can be\nextended to an ultrafilter, which plays a crucial role in the quest for\nnon-principal ultrafilters. Non-principal ultrafilters find widespread\napplications in logic, set theory, topology, model theory, and especially\nnon-standard extensions of algebraic structures. Since non-principal\nultrafilters are challenging to construct directly, the Filter Extension\nPrinciple, stemming from the Axiom of Choice, holds significant value in\nobtaining them. This paper presents the formal verification of the Filter\nExtension Principle, implemented using the Coq proof assistant and grounded in\naxiomatic set theory. It offers formal descriptions for the concepts related to\nfilter base, filter, ultrafilter and more. All relevant theorems, propositions,\nand the Filter Extension Principle itself are rigorously and formally verified.\nThis work sets the stage for the formalization of non-standard analysis and a\nspecific real number theory.", "journal": ""}
{"doi": "10.48550/arXiv.2411.00860", "date": "2024-10-30", "title": "Survey of Cultural Awareness in Language Models: Text and Beyond", "authors": "Siddhesh Pawar, Junyeong Park, Jiho Jin, Arnav Arora, Junho Myung, Srishti Yadav, Faiz Ghifari Haznitrama, Inhwa Song, Alice Oh, Isabelle Augenstein", "abstract": "Large-scale deployment of large language models (LLMs) in various\napplications, such as chatbots and virtual assistants, requires LLMs to be\nculturally sensitive to the user to ensure inclusivity. Culture has been widely\nstudied in psychology and anthropology, and there has been a recent surge in\nresearch on making LLMs more culturally inclusive in LLMs that goes beyond\nmultilinguality and builds on findings from psychology and anthropology. In\nthis paper, we survey efforts towards incorporating cultural awareness into\ntext-based and multimodal LLMs. We start by defining cultural awareness in\nLLMs, taking the definitions of culture from anthropology and psychology as a\npoint of departure. We then examine methodologies adopted for creating\ncross-cultural datasets, strategies for cultural inclusion in downstream tasks,\nand methodologies that have been used for benchmarking cultural awareness in\nLLMs. Further, we discuss the ethical implications of cultural alignment, the\nrole of Human-Computer Interaction in driving cultural inclusion in LLMs, and\nthe role of cultural alignment in driving social science research. We finally\nprovide pointers to future research based on our findings about gaps in the\nliterature.", "journal": ""}
{"doi": "10.48550/arXiv.2307.15543", "date": "2023-07-28", "title": "Oracle Computability and Turing Reducibility in the Calculus of Inductive Constructions", "authors": "Yannick Forster, Dominik Kirst, Niklas M\u00fcck", "abstract": "We develop synthetic notions of oracle computability and Turing reducibility\nin the Calculus of Inductive Constructions (CIC), the constructive type theory\nunderlying the Coq proof assistant. As usual in synthetic approaches, we employ\na definition of oracle computations based on meta-level functions rather than\nobject-level models of computation, relying on the fact that in constructive\nsystems such as CIC all definable functions are computable by construction.\nSuch an approach lends itself well to machine-checked proofs, which we carry\nout in Coq.\n  There is a tension in finding a good synthetic rendering of the higher-order\nnotion of oracle computability. On the one hand, it has to be informative\nenough to prove central results, ensuring that all notions are faithfully\ncaptured. On the other hand, it has to be restricted enough to benefit from\naxioms for synthetic computability, which usually concern first-order objects.\nDrawing inspiration from a definition by Andrej Bauer based on continuous\nfunctions in the effective topos, we use a notion of sequential continuity to\ncharacterise valid oracle computations.\n  As main technical results, we show that Turing reducibility forms an upper\nsemilattice, transports decidability, and is strictly more expressive than\ntruth-table reducibility, and prove that whenever both a predicate $p$ and its\ncomplement are semi-decidable relative to an oracle $q$, then $p$\nTuring-reduces to $q$.", "journal": ""}
{"doi": "10.48550/arXiv.2404.05459", "date": "2024-04-08", "title": "A Coq Library of Sets for Teaching Denotational Semantics", "authors": "Qinxiang Cao, Xiwei Wu, Yalun Liang", "abstract": "Sets and relations are very useful concepts for defining denotational\nsemantics. In the Coq proof assistant, curried functions to Prop are used to\nrepresent sets and relations, e.g. A -> Prop, A -> B -> Prop, A -> B -> C ->\nProp, etc. Further, the membership relation can be encoded by function\napplications, e.g. X a represents a in X if X: A -> Prop. This is very\nconvenient for developing formal definitions and proofs for professional users,\nbut it makes propositions more difficult to read for non-professional users,\ne.g. students of a program semantics course. We develop a small Coq library of\nsets and relations so that standard math notations can be used when teaching\ndenotational semantics of simple imperative languages. This library is\ndeveloped using Coq's type class system. It brings about zero proof-term\noverhead comparing with the existing formalization of sets.", "journal": "EPTCS 400, 2024, pp. 79-95"}
{"doi": "10.48550/arXiv.1307.8211", "date": "2013-07-31", "title": "Formal verification of a proof procedure for the description logic ALC", "authors": "Mohamed Chaabani, Mohamed Mezghiche, Martin Strecker", "abstract": "Description Logics (DLs) are a family of languages used for the\nrepresentation and reasoning on the knowledge of an application domain, in a\nstructured and formal manner. In order to achieve this objective, several\nprovers, such as RACER and FaCT++, have been implemented, but these provers\nthemselves have not been yet certified. In order to ensure the soundness of\nderivations in these DLs, it is necessary to formally verify the deductions\napplied by these reasoners. Formal methods offer powerful tools for the\nspecification and verification of proof procedures, among them there are\nmethods for proving properties such as soundness, completeness and termination\nof a proof procedure. In this paper, we present the definition of a proof\nprocedure for the Description Logic ALC, based on a semantic tableau method. We\nensure validity of our prover by proving its soundness, completeness and\ntermination properties using Isabelle proof assistant. The proof proceeds in\ntwo phases, first by establishing these properties on an abstract level, and\nthen by instantiating them for an implementation based on lists.", "journal": "EPTCS 122, 2013, pp. 51-61"}
{"doi": "10.48550/arXiv.2209.00975", "date": "2022-09-02", "title": "A Reasonably Gradual Type Theory", "authors": "Kenji Maillard, Meven Lennon-Bertrand, Nicolas Tabareau, \u00c9ric Tanter", "abstract": "Gradualizing the Calculus of Inductive Constructions (CIC) involves dealing\nwith subtle tensions between normalization, graduality, and conservativity with\nrespect to CIC. Recently, GCIC has been proposed as a parametrized gradual type\ntheory that admits three variants, each sacrificing one of these properties.\nFor devising a gradual proof assistant based on CIC, normalization and\nconservativity with respect to CIC are key, but the tension with graduality\nneeds to be addressed. Additionally, several challenges remain: (1) The\npresence of two wildcard terms at any type-the error and unknown terms-enables\ntrivial proofs of any theorem, jeopardizing the use of a gradual type theory in\na proof assistant; (2) Supporting general indexed inductive families, most\nprominently equality, is an open problem; (3) Theoretical accounts of gradual\ntyping and graduality so far do not support handling type mismatches detected\nduring reduction; (4) Precision and graduality are external notions not\namenable to reasoning within a gradual type theory. All these issues manifest\nprimally in CastCIC, the cast calculus used to define GCIC. In this work, we\npresent an extension of CastCIC called GRIP. GRIP is a reasonably gradual type\ntheory that addresses the issues above, featuring internal precision and\ngeneral exception handling. GRIP features an impure (gradual) sort of types\ninhabited by errors and unknown terms, and a pure (non-gradual) sort of strict\npropositions for consistent reasoning about gradual terms. Internal precision\nsupports reasoning about graduality within GRIP itself, for instance to\ncharacterize gradual exception-handling terms, and supports gradual subset\ntypes. We develop the metatheory of GRIP using a model formalized in Coq, and\nprovide a prototype implementation of GRIP in Agda.", "journal": "Proceedings of the ACM on Programming Languages Volume 6 Issue\n  ICFP August 2022 Article No 124 pp 931-959"}
{"doi": "10.48550/arXiv.2412.13398", "date": "2024-12-18", "title": "Pattern Matching in AI Compilers and its Formalization (Extended Version)", "authors": "Joseph W. Cutler, Alex Collins, Bin Fan, Mahesh Ravishankar, Vinod Grover", "abstract": "PyPM is a Python-based domain specific language (DSL) for building\nrewrite-based optimization passes on machine learning computation graphs. Users\ndefine individual optimizations by writing (a) patterns that match subgraphs of\na computation graph and (b) corresponding rules which replace a matched\nsubgraph with an optimized kernel. PyPM is distinguished from the many other\nDSLs for defining rewriting passes by its complex and novel pattern language\nwhich borrows concepts from logic programming. PyPM patterns can be recursive,\nnondeterminstic, and can require checking domain-specific constraints such as\nthe shapes of tensors. The PyPM implementation is thus similarly complicated,\nconsisting of thousands of lines of C++ code. In this paper, we present our\nwork on building PyPM, as well as formalizing and distilling and this\ncomplexity to an understandable mathematical core. We have developed a formal\ncore calculus expressing the main operations of the PyPM pattern language. We\ndefine both a declarative semantics - describing which patterns match which\nterms - and an algorithmic semantics - an idealized version of the PyPM pattern\ninterpreter - and prove their equivalence. The development is fully mechanized\nin the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2302.14699", "date": "2023-02-28", "title": "An Analysis of Tennenbaum's Theorem in Constructive Type Theory", "authors": "Marc Hermes, Dominik Kirst", "abstract": "Tennenbaum's theorem states that the only countable model of Peano arithmetic\n(PA) with computable arithmetical operations is the standard model of natural\nnumbers. In this paper, we use constructive type theory as a framework to\nrevisit, analyze and generalize this result. The chosen framework allows for a\nsynthetic approach to computability theory, exploiting that, externally, all\nfunctions definable in constructive type theory can be shown computable. We\nthen build on this viewpoint, and furthermore internalize it by assuming a\nversion of Church's thesis, which expresses that any function on natural\nnumbers is representable by a formula in PA. This assumption provides for a\nconveniently abstract setup to carry out rigorous computability arguments, even\nin the theorem's mechanization. Concretely, we constructivize several classical\nproofs and present one inherently constructive rendering of Tennenbaum's\ntheorem, all following arguments from the literature. Concerning the classical\nproofs in particular, the constructive setting allows us to highlight\ndifferences in their assumptions and conclusions which are not visible\nclassically. All versions are accompanied by a unified mechanization in the Coq\nproof assistant.", "journal": "Logical Methods in Computer Science, Volume 20, Issue 1 (March 7,\n  2024) lmcs:11042"}
{"doi": "10.48550/arXiv.1907.03723", "date": "2019-07-08", "title": "APML: An Architecture Proof Modeling Language", "authors": "Diego Marmsoler, Genc Blakqori", "abstract": "To address the increasing size and complexity of modern software systems,\ncompositional verification separates the verification of single components from\nthe verification of their composition. In architecture-based verification, the\nformer is done using Model Checking, while this does not seem to be the case in\ngeneral the latter is done using interactive theorem proving (ITP). As of\ntoday, however, architects are usually not trained in using a full-fledged\ninteractive theorem prover. Thus, to bridge the gap between ITP and the\narchitecture domain, we developed APML: an architecture proof modeling\nlanguage. APML allows one to sketch proofs about component composition at the\nlevel of architecture using notations similar to Message Sequence Charts. With\nthis paper, we introduce APML: We describe the language, show its soundness and\ncompleteness for the verification of architecture contracts, and provide an\nalgorithm to map an APML proof to a corresponding proof for the interactive\ntheorem prover Isabelle. Moreover, we describe its implementation in terms of\nan Eclipse/EMF modeling application, demonstrate it by means of a running\nexample, and evaluate it in terms of a larger case study. Although our results\nare promising, the case study also reveals some limitations, which lead to new\ndirections for future work.", "journal": ""}
{"doi": "10.48550/arXiv.2403.13700", "date": "2024-03-20", "title": "Taming Differentiable Logics with Coq Formalisation", "authors": "Reynald Affeldt, Alessandro Bruni, Ekaterina Komendantskaya, Natalia \u015alusarz, Kathrin Stark", "abstract": "For performance and verification in machine learning, new methods have\nrecently been proposed that optimise learning systems to satisfy formally\nexpressed logical properties. Among these methods, differentiable logics (DLs)\nare used to translate propositional or first-order formulae into loss functions\ndeployed for optimisation in machine learning. At the same time, recent\nattempts to give programming language support for verification of neural\nnetworks showed that DLs can be used to compile verification properties to\nmachine-learning backends. This situation is calling for stronger guarantees\nabout the soundness of such compilers, the soundness and compositionality of\nDLs, and the differentiability and performance of the resulting loss functions.\nIn this paper, we propose an approach to formalise existing DLs using the\nMathematical Components library in the Coq proof assistant. Thanks to this\nformalisation, we are able to give uniform semantics to otherwise disparate\nDLs, give formal proofs to existing informal arguments, find errors in previous\nwork, and provide formal proofs to missing conjectured properties. This work is\nmeant as a stepping stone for the development of programming language support\nfor verification of machine learning.", "journal": ""}
{"doi": "10.48550/arXiv.1512.02274", "date": "2015-12-07", "title": "Constructing the Propositional Truncation using Non-recursive HITs", "authors": "Floris van Doorn", "abstract": "In homotopy type theory, we construct the propositional truncation as a\ncolimit, using only non-recursive higher inductive types (HITs). This is a\nfirst step towards reducing recursive HITs to non-recursive HITs. This\nconstruction gives a characterization of functions from the propositional\ntruncation to an arbitrary type, extending the universal property of the\npropositional truncation. We have fully formalized all the results in a new\nproof assistant, Lean.", "journal": ""}
{"doi": "10.48550/arXiv.1902.08048", "date": "2019-02-21", "title": "A complete axiomatisation of reversible Kleene lattices", "authors": "Paul Brunet", "abstract": "We consider algebras of languages over the signature of reversible Kleene\nlattices, that is the regular operations (empty and unit languages, union,\nconcatenation and Kleene star) together with intersection and mirror image. We\nprovide a complete set of axioms for the equational theory of these algebras.\nThis proof was developed in the proof assistant Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2205.05781", "date": "2022-05-11", "title": "VyZX : A Vision for Verifying the ZX Calculus", "authors": "Adrian Lehmann, Ben Caldwell, Robert Rand", "abstract": "Optimizing quantum circuits is a key challenge for quantum computing. The\nPyZX compiler broke new ground by optimizing circuits via the ZX calculus, a\npowerful graphical alternative to the quantum circuit model. Still, it carries\nno guarantee of its correctness. To address this, we developed VyZX, a verified\nZX-calculus in the Coq proof assistant. VyZX provides two distinct\nrepresentations of ZX diagrams for ease of programming and proof: A graph-based\nrepresentation for writing high-level functions on diagrams and a block-based\nrepresentation for proving ZX diagrams equivalent. Through these two different\nviews, VyZX provides the tools necessary to verify properties and\ntransformations of ZX diagrams. This paper explores the proofs and design\nchoices underlying VyZX and its application and the challenges of verifying a\ngraphical programming language.", "journal": ""}
{"doi": "10.48550/arXiv.2007.12105", "date": "2020-07-23", "title": "Formalizing Nakamoto-Style Proof of Stake", "authors": "S\u00f8ren Eller Thomsen, Bas Spitters", "abstract": "Fault-tolerant distributed systems move the trust in a single party to a\nmajority of parties participating in the protocol. This makes blockchain based\ncrypto-currencies possible: they allow parties to agree on a total order of\ntransactions without a trusted third party. To trust a distributed system, the\nsecurity of the protocol and the correctness of the implementation must be\nindisputable.\n  We present the first machine checked proof that guarantees both safety and\nliveness for a consensus algorithm. We verify a Proof of Stake (PoS)\nNakamoto-style blockchain (NSB) protocol, using the foundational proof\nassistant Coq.\n  In particular, we consider a PoS NSB in a synchronous network with a static\nset of corrupted parties. We define execution semantics for this setting and\nprove chain growth, chain quality, and common prefix which together imply both\nsafety and liveness.", "journal": "2021 IEEE 34th Computer Security Foundations Symposium (CSF),\n  Pages: 1-15"}
{"doi": "10.48550/arXiv.2308.06970", "date": "2023-08-14", "title": "ProofBuddy: A Proof Assistant for Learning and Monitoring", "authors": "Nadine Karsten, Frederik Krogsdal Jacobsen, Kim Jana Eiken, Uwe Nestmann, J\u00f8rgen Villadsen", "abstract": "Proof competence, i.e. the ability to write and check (mathematical) proofs,\nis an important skill in Computer Science, but for many students it represents\na difficult challenge. The main issues are the correct use of formal language\nand the ascertainment of whether proofs, especially the students' own, are\ncomplete and correct. Many authors have suggested using proof assistants to\nassist in teaching proof competence, but the efficacy of the approach is\nunclear. To improve the state of affairs, we introduce ProofBuddy: a web-based\ntool using the Isabelle proof assistant which enables researchers to conduct\nstudies of the efficacy of approaches to using proof assistants in education by\ncollecting fine-grained data about the way students interact with proof\nassistants. We have performed a preliminary usability study of ProofBuddy at\nthe Technical University of Denmark.", "journal": "EPTCS 382, 2023, pp. 1-21"}
{"doi": "10.48550/arXiv.1404.0853", "date": "2014-04-03", "title": "Correct-by-construction model composition: Application to the Invasive Software Composition method", "authors": "Mounira Kezadri Hamiaz, Marc Pantel, Beno\u00eet Combemale, Xavier Thirioux", "abstract": "Composition technologies improve reuse in the development of large-scale\ncomplex systems. Safety critical systems require intensive validation and\nverification activities. These activities should be compositional in order to\nreduce the amount of residual verification activities that must be conducted on\nthe composite in addition to the ones conducted on each components. In order to\nensure the correctness of compositional verification and assess the minimality\nof the residual verification, the contribution proposes to use formal\nspecification and verification at the composition operator level. A first\nexperiment was conducted in [15] using proof assistants to formalize the\ngeneric composition technology ISC and prove that type checking was\ncompositional. This contribution extends our early work to handle full model\nconformance and study the mandatory residual verification. It shows that ISC\noperators are not fully compositional with respect to conformance and provides\nthe minimal preconditions on the operators mandatory to ensure compositional\nconformance. The appropriate operators from ISC (especially bind) have been\nimplemented in the COQ4MDE framework that provides a full implementation of MOF\nin the COQ proof assistant. Expected properties, respectively residual\nverification, are expressed as post, respectfully pre, conditions for the\ncomposition operators. The correctness of the compositional verification is\nproven in COQ.", "journal": "EPTCS 147, 2014, pp. 108-122"}
{"doi": "10.48550/arXiv.1701.03037", "date": "2017-01-10", "title": "Towards Smart Proof Search for Isabelle", "authors": "Yutaka Nagashima", "abstract": "Despite the recent progress in automatic theorem provers, proof engineers are\nstill suffering from the lack of powerful proof automation. In this position\npaper we first report our proof strategy language based on a meta-tool\napproach. Then, we propose an AI-based approach to drastically improve proof\nautomation for Isabelle, while identifying three major challenges we plan to\naddress for this objective.", "journal": ""}
{"doi": "10.48550/arXiv.2106.12351", "date": "2021-06-23", "title": "Beginners' Quest to Formalize Mathematics: A Feasibility Study in Isabelle", "authors": "Jonas Bayer, Marco David, Abhik Pal, Benedikt Stock", "abstract": "How difficult are interactive theorem provers to use? We respond by reviewing\nthe formalization of Hilbert's tenth problem in Isabelle/HOL carried out by an\nundergraduate research group at Jacobs University Bremen. We argue that, as\ndemonstrated by our example, proof assistants are feasible for beginners to\nformalize mathematics. With the aim to make the field more accessible, we also\nsurvey hurdles that arise when learning an interactive theorem prover. Broadly,\nwe advocate for an increased adoption of interactive theorem provers in\nmathematical research and curricula.", "journal": "Intelligent Computer Mathematics (CICM 2019). Lecture Notes in\n  Computer Science, vol 11617, pp. 16-27. Springer, Cham"}
{"doi": "10.48550/arXiv.1907.05523", "date": "2019-07-11", "title": "Towards a Verified Model of the Algorand Consensus Protocol in Coq", "authors": "Musab A. Alturki, Jing Chen, Victor Luchangco, Brandon Moore, Karl Palmskog, Lucas Pe\u00f1a, Grigore Ro\u015fu", "abstract": "The Algorand blockchain is a secure and decentralized public ledger based on\npure proof of stake rather than proof of work. At its core it is a novel\nconsensus protocol with exactly one block certified in each round: that is, the\nprotocol guarantees that the blockchain does not fork. In this paper, we report\non our effort to model and formally verify the Algorand consensus protocol in\nthe Coq proof assistant. Similar to previous consensus protocol verification\nefforts, we model the protocol as a state transition system and reason over\nreachable global states. However, in contrast to previous work, our model\nexplicitly incorporates timing issues (e.g., timeouts and network delays) and\nadversarial actions, reflecting a more realistic environment faced by a public\nblockchain. Thus far, we have proved asynchronous safety of the protocol: two\ndifferent blocks cannot be certified in the same round, even when the adversary\nhas complete control of message delivery in the network. We believe that our\nmodel is sufficiently general and other relevant properties of the protocol\nsuch as liveness can be proved for the same model.", "journal": "Formal Methods. FM 2019 International Workshops. Lecture Notes in\n  Computer Science, vol 12232, pp. 362-367"}
{"doi": "10.48550/arXiv.2301.13867", "date": "2023-01-31", "title": "Mathematical Capabilities of ChatGPT", "authors": "Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Julius Berner", "abstract": "We investigate the mathematical capabilities of two iterations of ChatGPT\n(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on\npublicly available datasets, as well as hand-crafted ones, using a novel\nmethodology. In contrast to formal mathematics, where large databases of formal\nproofs are available (e.g., the Lean Mathematical Library), current datasets of\nnatural-language mathematics, used to benchmark language models, either cover\nonly elementary mathematics or are very small. We address this by publicly\nreleasing two new datasets: GHOSTS and miniGHOSTS. These are the first\nnatural-language datasets curated by working researchers in mathematics that\n(1) aim to cover graduate-level mathematics, (2) provide a holistic overview of\nthe mathematical capabilities of language models, and (3) distinguish multiple\ndimensions of mathematical reasoning. These datasets also test whether ChatGPT\nand GPT-4 can be helpful assistants to professional mathematicians by emulating\nuse cases that arise in the daily professional activities of mathematicians. We\nbenchmark the models on a range of fine-grained performance metrics. For\nadvanced mathematics, this is the most detailed evaluation effort to date. We\nfind that ChatGPT can be used most successfully as a mathematical assistant for\nquerying facts, acting as a mathematical search engine and knowledge base\ninterface. GPT-4 can additionally be used for undergraduate-level mathematics\nbut fails on graduate-level difficulty. Contrary to many positive reports in\nthe media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of\nselection bias), their overall mathematical performance is well below the level\nof a graduate student. Hence, if your goal is to use ChatGPT to pass a\ngraduate-level math exam, you would be better off copying from your average\npeer!", "journal": "NeurIPS 2023 Datasets and Benchmarks"}
{"doi": "10.48550/arXiv.2212.02425", "date": "2022-12-05", "title": "Leroy and Blazy were right: their memory model soundness proof is automatable (Extended Version)", "authors": "Pedro Barroso, M\u00e1rio Pereira, Ant\u00f3nio Ravara", "abstract": "Xavier Leroy and Sandrine Blazy in 2007 conducted a formal verification,\nusing the Coq proof assistant, of a memory model for low-level imperative\nlanguages such as C. Considering their formalization was performed essentially\nin first-order logic, one question left open by the authors was whether their\nproofs could be automated using a verification framework for first-order logic.\nWe took the challenge and automated their formalization using Why3,\nsignificantly reducing the proof effort. We systematically followed the Coq\nproofs and realized that in many cases at around one third of the way Why3 was\nable to discharge all VCs. Furthermore, the proofs still requiring interactions\n(e.g. induction, witnesses for existential proofs, assertions) were factorized\nisolating auxiliary results that we stated explicitly. In this way, we achieved\nan almost-automatic soundness and safety proof of the memory model.\nNonetheless, our development allows an extraction of a correct-by-construction\nconcrete memory model, going thus further than the preliminary Why version of\nLeroy and Blazy.", "journal": ""}
{"doi": "10.48550/arXiv.2104.04284", "date": "2021-04-09", "title": "Semantical Investigations on Non-classical Logics with Recovery Operators: Negation", "authors": "David Fuenmayor", "abstract": "We investigate mathematical structures that provide natural semantics for\nfamilies of (quantified) non-classical logics featuring special unary\nconnectives, known as recovery operators, that allow us to 'recover' the\nproperties of classical logic in a controlled manner. These structures are\nknown as topological Boolean algebras, which are Boolean algebras extended with\nadditional operations subject to specific conditions of a topological nature.\nIn this study we focus on the paradigmatic case of negation. We demonstrate how\nthese algebras are well-suited to provide a semantics for some families of\nparaconsistent Logics of Formal Inconsistency and paracomplete Logics of Formal\nUndeterminedness. These logics feature recovery operators used to earmark\npropositions that behave 'classically' when interacting with non-classical\nnegations. Unlike traditional semantical investigations, which are carried out\nin natural language (extended with mathematical shorthand), our formal\nmeta-language is a system of higher-order logic (HOL) for which automated\nreasoning tools exist. In our approach, topological Boolean algebras are\nencoded as algebras of sets via their Stone-type representation. We use our\nhigher-order meta-logic to define and interrelate several transformations on\nunary set operations, which naturally give rise to a topological cube of\nopposition. Additionally, our approach enables a uniform characterization of\npropositional, first-order, and higher-order quantification, including\nrestrictions to constant and varying domains. With this work, we aim to make a\ncase for the utilization of automated theorem proving technology for conducting\ncomputer-supported research in non-classical logics. All the results presented\nin this paper have been formally verified, and in many cases obtained, using\nthe Isabelle/HOL proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2312.06506", "date": "2023-12-11", "title": "The Directed Van Kampen Theorem in Lean", "authors": "Henning Basold, Peter Bruin, Dominique Lawson", "abstract": "Directed topology is an area of mathematics with applications in concurrency.\nIt extends the concept of a topological space by adding a notion of\ndirectedness, which restricts how paths can evolve through a space and enables\nthereby a faithful representation of computation with their direction. In this\npaper, we present a Lean formalisation of directed spaces and a Van Kampen\ntheorem for them. This theorem allows the calculation of the homotopy type of a\nspace by combining local knowledge the homotopy type of subspaces. With this\ntheorem, the reasoning about spaces can be reduced to subspaces and, by\nrepresenting concurrent systems as directed spaces, we can reduce the deduction\nof properties of a composed system to that of subsystems. The formalisation in\nLean can serve to support computer-assisted reasoning about the behaviour of\nconcurrent systems.", "journal": ""}
{"doi": "10.48550/arXiv.1907.07794", "date": "2019-07-17", "title": "Generating Correctness Proofs with Neural Networks", "authors": "Alex Sanchez-Stern, Yousef Alhessi, Lawrence Saul, Sorin Lerner", "abstract": "Foundational verification allows programmers to build software which has been\nempirically shown to have high levels of assurance in a variety of important\ndomains. However, the cost of producing foundationally verified software\nremains prohibitively high for most projects,as it requires significant manual\neffort by highly trained experts. In this paper we present Proverbot9001,a\nproof search system using machine learning techniques to produce proofs of\nsoftware correctness in interactive theorem provers. We demonstrate\nProverbot9001 on the proof obligations from a large practical proof project,the\nCompCert verified C compiler,and show that it can effectively automate what\nwere previously manual proofs,automatically producing proofs for 27.5% of\ntheorem statements in our test dataset, when combined with solver-based\ntooling. Without any additional solvers,we exhibit a proof completion rate that\nis a 4X improvement over prior state-of-the-art machine learning models for\ngenerating proofs in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1112.1795", "date": "2011-12-08", "title": "Wave Equation Numerical Resolution: a Comprehensive Mechanized Proof of a C Program", "authors": "Sylvie Boldo, Francois Clement, Jean-Christophe Filli\u00e2tre, Micaela Mayero, Guillaume Melquiond, Pierre Weis", "abstract": "We formally prove correct a C program that implements a numerical scheme for\nthe resolution of the one-dimensional acoustic wave equation. Such an\nimplementation introduces errors at several levels: the numerical scheme\nintroduces method errors, and floating-point computations lead to round-off\nerrors. We annotate this C program to specify both method error and round-off\nerror. We use Frama-C to generate theorems that guarantee the soundness of the\ncode. We discharge these theorems using SMT solvers, Gappa, and Coq. This\ninvolves a large Coq development to prove the adequacy of the C program to the\nnumerical scheme and to bound errors. To our knowledge, this is the first time\nsuch a numerical analysis program is fully machine-checked.", "journal": "Journal of Automated Reasoning 50, 4 (2013) 423-456"}
{"doi": "10.48550/arXiv.2501.12906", "date": "2025-01-22", "title": "Certified Knowledge Compilation with Application to Formally Verified Model Counting", "authors": "Randal E. Bryant, Wojciech Nawrocki, Jeremy Avigad, Marijn J. H. Heule", "abstract": "Computing many useful properties of Boolean formulas, such as their weighted\nor unweighted model count, is intractable on general representations. It can\nbecome tractable when formulas are expressed in a special form, such as the\ndecision decomposable negation normal form (decision-DNNF). Knowledge\ncompilation is the process of converting a formula into such a form.\nUnfortunately existing knowledge compilers provide no guarantee that their\noutput correctly represents the original formula, and therefore they cannot\nvalidate a model count, or any other computed value.\n  We present Partitioned-Operation Graphs (POGs), a form that can encode all of\nthe representations used by existing knowledge compilers. We have designed\nCPOG, a framework that can express proofs of equivalence between a POG and a\nBoolean formula in conjunctive normal form (CNF).\n  We have developed a program that generates POG representations from the\ndecision-DNNF graphs produced by the state-of-the-art knowledge compiler D4, as\nwell as checkable CPOG proofs certifying that the output POGs are equivalent to\nthe input CNF formulas. Our toolchain for generating and verifying POGs scales\nto all but the largest graphs produced by D4 for formulas from a recent model\ncounting competition. Additionally, we have developed a formally verified CPOG\nchecker and model counter for POGs in the Lean 4 proof assistant. In doing so,\nwe proved the soundness of our proof framework. These programs comprise the\nfirst formally verified toolchain for weighted and unweighted model counting.", "journal": ""}
{"doi": "10.48550/arXiv.1506.03428", "date": "2015-06-10", "title": "Formalization of closure properties for context-free grammars", "authors": "Marcus V. M. Ramos, Ruy J. G. B. de Queiroz", "abstract": "Context-free language theory is a well-established area of mathematics,\nrelevant to computer science foundations and technology. This paper presents\nthe preliminary results of an ongoing formalization project using context-free\ngrammars and the Coq proof assistant. The results obtained so far include the\nrepresentation of context-free grammars, the description of algorithms for some\noperations on them (union, concatenation and closure) and the proof of related\ntheorems (e.g. the correctness of these algorithms). A brief survey of related\nworks is presented, as well as plans for further development.", "journal": "Preliminary Proceedings of the 9th Workshop on Logical and\n  Semantic Frameworks, with Applications, LSFA'14 (2014), pp. 187-198"}
{"doi": "10.48550/arXiv.2410.17847", "date": "2024-10-23", "title": "A formal characterization of discrete condensed objects", "authors": "Dagur Asgeirsson", "abstract": "Condensed mathematics, developed by Clausen and Scholze over the last few\nyears, proposes a generalization of topology with better categorical\nproperties. It replaces the concept of a topological space by that of a\ncondensed set, which can be defined as a sheaf on a certain site of compact\nHausdorff spaces. Since condensed sets are supposed to be a generalization of\ntopological spaces, one would like to be able to study the notion of\ndiscreteness. There are various ways to define what it means for a condensed\nset to be discrete. In this paper we describe them, and prove that they are\nequivalent. The results have been fully formalized in the Lean proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1907.07885", "date": "2019-07-18", "title": "Formal verification of trading in financial markets", "authors": "Suneel Sarswat, Abhishek Kr Singh", "abstract": "We introduce a formal framework for analyzing trades in financial markets. An\nexchange is where multiple buyers and sellers participate to trade. These days,\nall big exchanges use computer algorithms that implement double sided auctions\nto match buy and sell requests and these algorithms must abide by certain\nregulatory guidelines. For example, market regulators enforce that a matching\nproduced by exchanges should be \\emph{fair}, \\emph{uniform} and\n\\emph{individual rational}. To verify these properties of trades, we first\nformally define these notions in a theorem prover and then give formal proofs\nof relevant results on matchings. Finally, we use this framework to verify\nproperties of two important classes of double sided auctions. All the\ndefinitions and results presented in this paper are completely formalised in\nthe Coq proof assistant without adding any additional axioms to it.", "journal": ""}
{"doi": "10.48550/arXiv.1802.06493", "date": "2018-02-19", "title": "A Method to Translate Order-Sorted Algebras to Many-Sorted Algebras", "authors": "Liyi Li, Elsa Gunter", "abstract": "Order-sorted algebras and many sorted algebras exist in a long history with\nmany different implementations and applications. A lot of language\nspecifications have been defined in order-sorted algebra frameworks such as the\nlanguage specifications in K (an order-sorted algebra framework). The biggest\nproblem in a lot of the order-sorted algebra frameworks is that even if they\nmight allow developers to write programs and language specifications easily,\nbut they do not have a large set of tools to provide reasoning infrastructures\nto reason about the specifications built on the frameworks, which are very\ncommon in some many-sorted algebra framework such as Isabelle/HOL, Coq and FDR.\nThis fact brings us the necessity to marry the worlds of order-sorted algebras\nand many sorted algebras. In this paper, we propose an algorithm to translate a\nstrictly sensible order-sorted algebra to a many-sorted one in a restricted\ndomain by requiring the order-sorted algebra to be strictly sensible. The key\nidea of the translation is to add an equivalence relation called core equality\nto the translated many-sorted algebras. By defining this relation, we reduce\nthe complexity of translating a strictly sensible order-sorted algebra to a\nmany-sorted one, make the translated many-sorted algebra equations only\nincreasing by a very small amount of new equations, and keep the number of\nrewrite rules in the algebra in the same amount. We then prove the order-sorted\nalgebra and its translated many-sorted algebra are bisimilar. To the best of\nour knowledge, our translation and bisimilar proof is the first attempt in\ntranslating and relating an order-sorted algebra with a many-sorted one in a\nway that keeps the size of the translated many-sorted algebra relatively small.", "journal": "EPTCS 265, 2018, pp. 20-34"}
{"doi": "10.48550/arXiv.2403.19790", "date": "2024-03-28", "title": "Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care", "authors": "Niall Taylor, Andrey Kormilitzin, Isabelle Lorge, Alejo Nevado-Holgado, Dan W Joyce", "abstract": "Contemporary large language models (LLMs) may have utility for processing\nunstructured, narrative free-text clinical data contained in electronic health\nrecords (EHRs) -- a particularly important use-case for mental health where a\nmajority of routinely-collected patient data lacks structured, machine-readable\ncontent.\n  A significant problem for the the United Kingdom's National Health Service\n(NHS) are the long waiting lists for specialist mental healthcare. According to\nNHS data, in each month of 2023, there were between 370,000 and 470,000\nindividual new referrals into secondary mental healthcare services. Referrals\nmust be triaged by clinicians, using clinical information contained in the\npatient's EHR to arrive at a decision about the most appropriate mental\nhealthcare team to assess and potentially treat these patients.\n  The ability to efficiently recommend a relevant team by ingesting potentially\nvoluminous clinical notes could help services both reduce referral waiting\ntimes and with the right technology, improve the evidence available to justify\ntriage decisions.\n  We present and evaluate three different approaches for LLM-based, end-to-end\ningestion of variable-length clinical EHR data to assist clinicians when\ntriaging referrals. Our model is able to deliver triage recommendations\nconsistent with existing clinical practices and it's architecture was\nimplemented on a single GPU, making it practical for implementation in\nresource-limited NHS environments where private implementations of LLM\ntechnology will be necessary to ensure confidential clinical data is\nappropriately controlled and governed.", "journal": ""}
{"doi": "10.48550/arXiv.1704.06781", "date": "2017-04-22", "title": "Homotopy Type Theory in Lean", "authors": "Floris van Doorn, Jakob von Raumer, Ulrik Buchholtz", "abstract": "We discuss the homotopy type theory library in the Lean proof assistant. The\nlibrary is especially geared toward synthetic homotopy theory. Of particular\ninterest is the use of just a few primitive notions of higher inductive types,\nnamely quotients and truncations, and the use of cubical methods.", "journal": ""}
{"doi": "10.48550/arXiv.1907.07801", "date": "2019-07-17", "title": "Iterated chromatic localisation", "authors": "Neil Strickland, Nicola Bellumat", "abstract": "We study a certain monoid of endofunctors of the stable homotopy category\nthat includes localizations with respect to finite unions of Morava\n$K$-theories. We work in an axiomatic framework that can also be applied to\nanalogous questions in equivariant stable homotopy theory. Our results should\nbe helpful for the study of transchromatic phenomena, including the Chromatic\nSplitting Conjecture. The combinatorial parts of this work have been formalised\nin the Lean proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1706.05271", "date": "2017-06-16", "title": "A Coq-based synthesis of Scala programs which are correct-by-construction", "authors": "Youssef El Bakouny, Tristan Crolard, Dani Mezher", "abstract": "The present paper introduces Scala-of-Coq, a new compiler that allows a\nCoq-based synthesis of Scala programs which are \"correct-by-construction\". A\ntypical workflow features a user implementing a Coq functional program, proving\nthis program's correctness with regards to its specification and making use of\nScala-of-Coq to synthesize a Scala program that can seamlessly be integrated\ninto an existing industrial Scala or Java application.", "journal": ""}
{"doi": "10.48550/arXiv.2411.19397", "date": "2024-11-28", "title": "Tail Modulo Cons, OCaml, and Relational Separation Logic", "authors": "Cl\u00e9ment Allain, Fr\u00e9d\u00e9ric Bour, Basile Cl\u00e9ment, Fran\u00e7ois Pottier, Gabriel Scherer", "abstract": "Common functional languages incentivize tail-recursive functions, as opposed\nto general recursive functions that consume stack space and may not scale to\nlarge inputs.\n  This distinction occasionally requires writing functions in a tail-recursive\nstyle that may be more complex and slower than the natural, non-tail-recursive\ndefinition.\n  This work describes our implementation of the *tail modulo constructor* (TMC)\ntransformation in the OCaml compiler, an optimization that provides\nstack-efficiency for a larger class of functions -- tail-recursive *modulo\nconstructors* -- which includes in particular the natural definition of\n`List.map` and many similar recursive data-constructing functions.\n  We prove the correctness of this program transformation in a simplified\nsetting -- a small untyped calculus -- that captures the salient aspects of the\nOCaml implementation. Our proof is mechanized in the Coq proof assistant, using\nthe Iris base logic.\n  An independent contribution of our work is an extension of the Simuliris\napproach to define simulation relations that support different calling\nconventions. To our knowledge, this is the first use of Simuliris to prove the\ncorrectness of a compiler transformation.", "journal": ""}
{"doi": "10.48550/arXiv.2312.06103", "date": "2023-12-11", "title": "A Practical Formalization of Monadic Equational Reasoning in Dependent-type Theory", "authors": "Reynald Affeldt, Jacques Garrigue, Takafumi Saikawa", "abstract": "One can perform equational reasoning about computational effects with a\npurely functional programming language thanks to monads. Even though equational\nreasoning for effectful programs is desirable, it is not yet mainstream. This\nis partly because it is difficult to maintain pencil-and-paper proofs of large\nexamples. We propose a formalization of a hierarchy of effects using monads in\nthe Coq proof assistant that makes monadic equational reasoning practical. Our\nmain idea is to formalize the hierarchy of effects and algebraic laws as\ninterfaces like it is done when formalizing hierarchy of algebras in dependent\ntype theory. Thanks to this approach, we clearly separate equational laws from\nmodels. We can then take advantage of the sophisticated rewriting capabilities\nof Coq and build libraries of lemmas to achieve concise proofs of programs. We\ncan also use the resulting framework to leverage on Coq's mathematical theories\nand formalize models of monads. In this article, we explain how we formalize a\nrich hierarchy of effects (nondeterminism, state, probability, etc.), how we\nmechanize examples of monadic equational reasoning from the literature, and how\nwe apply our framework to the design of equational laws for a subset of ML with\nreferences.", "journal": "J. Funct. Prog. 35 (2025) e1"}
{"doi": "10.48550/arXiv.2208.11241", "date": "2022-08-24", "title": "Your Blockchain Needn't Care How the Message is Spread", "authors": "Wolfgang Jeltsch, Javier D\u00edaz", "abstract": "In a blockchain system, nodes regularly distribute data to other nodes. The\nideal perspective taken in the scientific literature is that data is broadcast\nto all nodes directly, while in practice data is distributed by repeated\nmulticast. Since correctness and security typically have been established for\nthe ideal setting only, it is vital to show that these properties carry over to\nreal-world implementations. This can be done by proving that the ideal and the\nreal behavior are equivalent.\n  In the work described in this paper, we take an important step towards such a\nproof by proving a simpler variant of the above equivalence statement. The\nsimplification is that we consider only a concrete pair of network topologies,\nwhich nevertheless illustrates important phenomena encountered with arbitrary\ntopologies. For describing systems that distribute data, we use a\ndomain-specific language of processes that is embedded in a general-purpose\nprocess calculus. This allows us to leverage the rich theory of process calculi\nin our proof, which is machine-checked using the Isabelle proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1404.5439", "date": "2014-04-22", "title": "A Logical Framework for Systems Biology", "authors": "Elisabetta De Maria, Joelle Despeyroux, Amy Felty", "abstract": "We propose a novel approach for the formal verification of biological systems\nbased on the use of a modal linear logic. We show how such a logic can be used,\nwith worlds as instants of time, as an unified framework to encode both\nbiological systems and temporal properties of their dynamic behaviour. To\nillustrate our methodology, we consider a model of the P53/Mdm2 DNA-damage\nrepair mechanism. We prove several properties that are important for such a\nmodel to satisfy and serve to illustrate the promise of our approach. We\nformalize the proofs of these properties in the Coq Proof Assistant, with the\nhelp of a Lambda Prolog prover for partial automation of the proofs.", "journal": ""}
{"doi": "10.48550/arXiv.1602.08361", "date": "2016-02-26", "title": "Certified Universal Gathering in $R^2$ for Oblivious Mobile Robots", "authors": "Pierre Courtieu, Lionel Rieg, S\u00e9bastien Tixeuil, Xavier Urbain", "abstract": "We present a unified formal framework for expressing mobile robots models,\nprotocols, and proofs, and devise a protocol design/proof methodology dedicated\nto mobile robots that takes advantage of this formal framework. As a case\nstudy, we present the first formally certified protocol for oblivious mobile\nrobots evolving in a two-dimensional Euclidean space. In more details, we\nprovide a new algorithm for the problem of universal gathering mobile oblivious\nrobots (that is, starting from any initial configuration that is not bivalent,\nusing any number of robots, the robots reach in a finite number of steps the\nsame position, not known beforehand) without relying on a common orientation\nnor chirality. We give very strong guaranties on the correctness of our\nalgorithm by proving formally that it is correct, using the COQ proof\nassistant. This result demonstrates both the effectiveness of the approach to\nobtain new algorithms that use as few assumptions as necessary, and its\nmanageability since the amount of developed code remains human readable.", "journal": ""}
{"doi": "10.48550/arXiv.2308.12403", "date": "2023-08-23", "title": "A Frame Stack Semantics for Sequential Core Erlang", "authors": "P\u00e9ter Bereczky, D\u00e1niel Horp\u00e1csi, Simon Thompson", "abstract": "We present a small-step, frame stack style, semantics for sequential Core\nErlang, a dynamically typed, impure functional programming language. The\nsemantics and the properties that we prove are machine-checked with the Coq\nproof assistant. We improve on previous work by including exceptions and\nexception handling, as well as built-in data types and functions. Based on the\nsemantics, we define multiple concepts of program equivalence (contextual, CIU\nequivalence, and equivalence based on logical relations) and prove that the\ndefinitions are all equivalent. Using this we are able to give a correctness\ncriterion for refactorings by means of contextually equivalent symbolic\nexpression pairs, which is one of the main motivations of this work.", "journal": ""}
{"doi": "10.48550/arXiv.2302.00448", "date": "2023-02-01", "title": "A formalisation of Gallagher's ergodic theorem", "authors": "Oliver Nash", "abstract": "Gallagher's ergodic theorem is a result in metric number theory. It states\nthat the approximation of real numbers by rational numbers obeys a striking\n'all or nothing' behaviour. We discuss a formalisation of this result in the\nLean theorem prover. As well as being notable in its own right, the result is a\nkey preliminary, required for Koukoulopoulos and Maynard's stunning recent\nproof of the Duffin-Schaeffer conjecture.", "journal": ""}
{"doi": "10.48550/arXiv.2403.17370", "date": "2024-03-26", "title": "Formal Verification of the Empty Hexagon Number", "authors": "Bernardo Subercaseaux, Wojciech Nawrocki, James Gallicchio, Cayden Codel, Mario Carneiro, Marijn J. H. Heule", "abstract": "A recent breakthrough in computer-assisted mathematics showed that every set\nof $30$ points in the plane in general position (i.e., without three on a\ncommon line) contains an empty convex hexagon, thus closing a line of research\ndating back to the 1930s. Through a combination of geometric insights and\nautomated reasoning techniques, Heule and Scheucher constructed a CNF formula\n$\\phi_n$, with $O(n^4)$ clauses, whose unsatisfiability implies that no set of\n$n$ points in general position can avoid an empty convex hexagon. An\nunsatisfiability proof for n = 30 was then found with a SAT solver using 17300\nCPU hours of parallel computation, thus implying that the empty hexagon number\nh(6) is equal to 30. In this paper, we formalize and verify this result in the\nLean theorem prover. Our formalization covers discrete computational geometry\nideas and SAT encoding techniques that have been successfully applied to\nsimilar Erd\\H{o}s-Szekeres-type problems. In particular, our framework provides\ntools to connect standard mathematical objects to propositional assignments,\nwhich represents a key step towards the formal verification of other SAT-based\nmathematical results. Overall, we hope that this work sets a new standard for\nverification when extensive computation is used for discrete geometry problems,\nand that it increases the trust the mathematical community has in\ncomputer-assisted proofs in this area.", "journal": ""}
{"doi": "10.48550/arXiv.2202.13833", "date": "2022-02-28", "title": "Formally verified asymptotic consensus in robust networks", "authors": "Mohit Tekriwal, Avi Tachna-Fram, Jean-Baptiste Jeannin, Manos Kapritsos, Dimitra Panagou", "abstract": "Distributed architectures are used to improve performance and reliability of\nvarious systems. Examples include drone swarms and load-balancing servers. An\nimportant capability of a distributed architecture is the ability to reach\nconsensus among all its nodes. Several consensus algorithms have been proposed,\nand many of these algorithms come with intricate proofs of correctness, that\nare not mechanically checked. In the controls community, algorithms often\nachieve consensus asymptotically, e.g., for problems such as the design of\nhuman control systems, or the analysis of natural systems like bird flocking.\nThis is in contrast to exact consensus algorithm such as Paxos, which have\nreceived much more recent attention in the formal methods community.\n  This paper presents the first formal proof of an asymptotic consensus\nalgorithm, and addresses various challenges in its formalization. Using the Coq\nproof assistant, we verify the correctness of a widely used consensus algorithm\nin the distributed controls community, the Weighted-Mean Subsequence Reduced\n(W-MSR) algorithm. We formalize the necessary and sufficient conditions\nrequired to achieve resilient asymptotic consensus under the assumed attacker\nmodel. During the formalization, we clarify several imprecisions in the paper\nproof, including an imprecision on quantifiers in the main theorem.", "journal": ""}
{"doi": "10.48550/arXiv.1302.1737", "date": "2013-02-07", "title": "Kleene Algebra with Tests and Coq Tools for While Programs", "authors": "Damien Pous", "abstract": "We present a Coq library about Kleene algebra with tests, including a proof\nof their completeness over the appropriate notion of languages, a decision\nprocedure for their equational theory, and tools for exploiting hypotheses of a\nparticular shape in such a theory. Kleene algebra with tests make it possible\nto represent if-then-else statements and while loops in most imperative\nprogramming languages. They were actually introduced by Kozen as an alternative\nto propositional Hoare logic. We show how to exploit the corresponding Coq\ntools in the context of program verification by proving equivalences of while\nprograms, correctness of some standard compiler optimisations, Hoare rules for\npartial correctness, and a particularly challenging equivalence of flowchart\nschemes.", "journal": ""}
{"doi": "10.48550/arXiv.2109.14534", "date": "2021-09-29", "title": "A verified algebraic representation of Cairo program execution", "authors": "Jeremy Avigad, Lior Goldberg, David Levit, Yoav Seginer, Alon Titelman", "abstract": "Cryptographic interactive proof systems provide an efficient and scalable\nmeans of verifying the results of computation on blockchain. A prover\nconstructs a proof, off-chain, that the execution of a program on a given input\nterminates with a certain result. The prover then publishes a certificate that\ncan be verified efficiently and reliably modulo commonly accepted cryptographic\nassumptions. The method relies on an algebraic encoding of execution traces of\nprograms. Here we report on a verification of the correctness of such an\nencoding of the Cairo model of computation with respect to the STARK\ninteractive proof system, using the Lean 3 proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1808.10690", "date": "2018-08-31", "title": "On the Formalization of Higher Inductive Types and Synthetic Homotopy Theory", "authors": "Floris van Doorn", "abstract": "The goal of this dissertation is to present synthetic homotopy theory in the\nsetting of homotopy type theory. We will present various results in this\nframework, most notably the construction of the Atiyah-Hirzebruch and Serre\nspectral sequences for cohomology, which have been fully formalized in the Lean\nproof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2108.13660", "date": "2021-08-31", "title": "Formalizing the Gromov-Hausdorff space", "authors": "S\u00e9bastien Gou\u00ebzel", "abstract": "The Gromov-Hausdorff space is usually defined in textbooks as \"the space of\nall compact metric spaces up to isometry\". We describe a formalization of this\nnotion in the Lean proof assistant, insisting on how we need to depart from the\nusual informal viewpoint of mathematicians on this object to get a rigorous\nformalization.", "journal": ""}
{"doi": "10.48550/arXiv.2111.06807", "date": "2021-11-12", "title": "Verified Optimization", "authors": "Alexander Bentkamp, Jeremy Avigad", "abstract": "Optimization is used extensively in engineering, industry, and finance, and\nvarious methods are used to transform problems to the point where they are\namenable to solution by numerical methods. We describe progress towards\ndeveloping a framework, based on the Lean interactive proof assistant, for\ndesigning and applying such reductions in reliable and flexible ways.", "journal": ""}
{"doi": "10.48550/arXiv.2301.02195", "date": "2023-01-05", "title": "Towards Autoformalization of Mathematics and Code Correctness: Experiments with Elementary Proofs", "authors": "Garett Cunningham, Razvan C. Bunescu, David Juedes", "abstract": "The ever-growing complexity of mathematical proofs makes their manual\nverification by mathematicians very cognitively demanding. Autoformalization\nseeks to address this by translating proofs written in natural language into a\nformal representation that is computer-verifiable via interactive theorem\nprovers. In this paper, we introduce a semantic parsing approach, based on the\nUniversal Transformer architecture, that translates elementary mathematical\nproofs into an equivalent formalization in the language of the Coq interactive\ntheorem prover. The same architecture is also trained to translate simple\nimperative code decorated with Hoare triples into formally verifiable proofs of\ncorrectness in Coq. Experiments on a limited domain of artificial and\nhuman-written proofs show that the models generalize well to intermediate\nlengths not seen during training and variations in natural language.", "journal": ""}
{"doi": "10.48550/arXiv.1105.2751", "date": "2011-05-13", "title": "Computer certified efficient exact reals in Coq", "authors": "Robbert Krebbers, Bas Spitters", "abstract": "Floating point operations are fast, but require continuous effort on the part\nof the user in order to ensure that the results are correct. This burden can be\nshifted away from the user by providing a library of exact analysis in which\nthe computer handles the error estimates. We provide an implementation of the\nexact real numbers in the Coq proof assistant. This improves on the earlier\nCoq-implementation by O'Connor in two ways: we use dyadic rationals built from\nthe machine integers and we optimize computation of power series by using\napproximate division. Moreover, we use type classes for clean mathematical\ninterfaces. This appears to be the first time that type classes are used in\nheavy computation. We obtain over a 100 times speed up of the basic operations\nand indications for improving the Coq system.", "journal": "Proceedings of CICM11, vol 6824, Springer LNAI, 90-106, 2011"}
{"doi": "10.48550/arXiv.2003.04604", "date": "2020-03-10", "title": "Hilbert's Tenth Problem in Coq (Extended Version)", "authors": "Dominique Larchey-Wendling, Yannick Forster", "abstract": "We formalise the undecidability of solvability of Diophantine equations, i.e.\npolynomial equations over natural numbers, in Coq's constructive type theory.\nTo do so, we give the first full mechanisation of the\nDavis-Putnam-Robinson-Matiyasevich theorem, stating that every recursively\nenumerable problem -- in our case by a Minsky machine -- is Diophantine. We\nobtain an elegant and comprehensible proof by using a synthetic approach to\ncomputability and by introducing Conway's FRACTRAN language as intermediate\nlayer. Additionally, we prove the reverse direction and show that every\nDiophantine relation is recognisable by $\\mu$-recursive functions and give a\ncertified compiler from $\\mu$-recursive functions to Minsky machines.", "journal": "Logical Methods in Computer Science, Volume 18, Issue 1 (March 1,\n  2022) lmcs:6195"}
{"doi": "10.48550/arXiv.2303.05866", "date": "2023-03-10", "title": "On Exams with the Isabelle Proof Assistant", "authors": "Frederik Krogsdal Jacobsen, J\u00f8rgen Villadsen", "abstract": "We present an approach for testing student learning outcomes in a course on\nautomated reasoning using the Isabelle proof assistant. The approach allows us\nto test both general understanding of formal proofs in various logical proof\nsystems and understanding of proofs in the higher-order logic of Isabelle/HOL\nin particular. The use of Isabelle enables almost automatic grading of large\nparts of the exam. We explain our approach through a number of example\nproblems, and explain why we believe that each of the kinds of problems we have\nselected are adequate measures of our intended learning outcomes. Finally, we\ndiscuss our experiences using the approach for the exam of a course on\nautomated reasoning and suggest potential future work.", "journal": "EPTCS 375, 2023, pp. 63-76"}
{"doi": "10.48550/arXiv.1102.3529", "date": "2011-02-17", "title": "A Tool for the Certification of PLCs based on a Coq Semantics for Sequential Function Charts", "authors": "Jan Olaf Blech", "abstract": "In this report we describe a tool framework for certifying properties of\nPLCs: CERTPLC. CERTPLC can handle PLC descriptions provided in the Sequential\nFunction Chart (SFC) language of the IEC 61131-3 standard. It provides routines\nto certify properties of systems by delivering an independently checkable\nformal system description and proof (called certificate) for the desired\nproperties. We focus on properties that can be described as inductive\ninvariants. System descriptions and certificates are generated and handled\nusing the COQ proof assistant. Our tool framework is used to provide supporting\nevidence for the safety of embedded systems in the industrial automation domain\nto third-party authorities. In this document we describe the tool framework:\nusage scenarios, the archi-tecture, semantics of PLCs and their realization in\nCOQ, proof generation and the construction of certificates.", "journal": ""}
{"doi": "10.48550/arXiv.2205.06150", "date": "2022-05-12", "title": "Direct Foundations for Compositional Programming", "authors": "Andong Fan, Xuejing Huang, Han Xu, Yaozhu Sun, Bruno C. d. S. Oliveira", "abstract": "The recently proposed CP language adopts Compositional Programming: a new\nmodular programming style that solves challenging problems such as the\nExpression Problem. CP is implemented on top of a polymorphic core language\nwith disjoint intersection types called Fi+. The semantics of Fi+ employs an\nelaboration to a target language and relies on a sophisticated proof technique\nto prove the coherence of the elaboration. Unfortunately, the proof technique\nis technically challenging and hard to scale to many common features, including\nrecursion or impredicative polymorphism. Thus, the original formulation of Fi+\ndoes not support the two later features, which creates a gap between theory and\npractice, since CP fundamentally relies on them.\n  This paper presents a new formulation of Fi+ based on a type-directed\noperational semantics (TDOS). The TDOS approach was recently proposed to model\nthe semantics of languages with disjoint intersection types (but without\npolymorphism). Our work shows that the TDOS approach can be extended to\nlanguages with disjoint polymorphism and model the full Fi+ calculus. Unlike\nthe elaboration semantics, which gives the semantics to Fi+ indirectly via a\ntarget language, the TDOS approach gives a semantics to Fi+ directly. With a\nTDOS, there is no need for a coherence proof. Instead, we can simply prove that\nthe semantics is deterministic. The proof of determinism only uses simple\nreasoning techniques, such as straightforward induction, and is able to handle\nproblematic features such as recursion and impredicative polymorphism. This\nremoves the gap between theory and practice and validates the original proofs\nof correctness for CP. We formalized the TDOS variant of the Fi+ calculus and\nall its proofs in the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2405.01687", "date": "2024-05-02", "title": "Compactness via Pattern Stepping Bisimulation", "authors": "Matias Scharager", "abstract": "The compactness lemma in programming language theory states that any\nrecursive function can be simulated by a finite unrolling of the function. One\nimportant use case it has is in the logical relations proof technique for\nproving properties of typed programs, such as strong normalization. The\nrelation between recursive functions and their finite counterparts is a special\nvariant of the class of bisimulation relations. However, standard bisimulation\nproof approaches do not apply to the compactness lemma as properties of the\nrelation vary over execution. As a result, the proof of compactness is often\nmessy because the multiple copies made of the recursive function during\nexecution can be unrolled an inconsistent number of times. We present a new\nproof technique by indexing the bisimulation relation over the step transitions\nand utilizing an intermediate \"pattern\" language to mechanize bookkeeping. This\ngeneralization of \"pattern stepping bisimulation\" obviates the need for\ncontextual approximation within the compactness lemma, and thus extends the\ncompactness lemma to a wider range of programming languages, including those\nthat incorporate control flow effects. We demonstrate this approach by formally\nverifying the compactness lemma within the Coq theorem prover in the setting of\nexplicit control flow and polymorphism.", "journal": ""}
{"doi": "10.48550/arXiv.1711.00113", "date": "2017-10-31", "title": "Proving Soundness of Extensional Normal-Form Bisimilarities", "authors": "Dariusz Biernacki, Serguei Lenglet, Piotr Polesiuk", "abstract": "Normal-form bisimilarity is a simple, easy-to-use behavioral equivalence that\nrelates terms in $\\lambda$-calculi by decomposing their normal forms into\nbisimilar subterms. Moreover, it typically allows for powerful up-to\ntechniques, such as bisimulation up to context, which simplify bisimulation\nproofs even further. However, proving soundness of these relations becomes\ncomplicated in the presence of $\\eta$-expansion and usually relies on ad hoc\nproof methods which depend on the language. In this paper we propose a more\nsystematic proof method to show that an extensional normal-form bisimilarity\nalong with its corresponding up to context technique are sound. We illustrate\nour technique with three calculi: the call-by-value $\\lambda$-calculus, the\ncall-by-value $\\lambda$-calculus with the delimited-control operators shift and\nreset, and the call-by-value $\\lambda$-calculus with the abortive control\noperators call/cc and abort. In the first two cases, there was previously no\nsound up to context technique validating the $\\eta$-law, whereas no theory of\nnormal-form bisimulations for a calculus with call/cc and abort has been\npresented before. Our results have been fully formalized in the Coq proof\nassistant.", "journal": "Logical Methods in Computer Science, Volume 15, Issue 1 (March 29,\n  2019) lmcs:4041"}
{"doi": "10.48550/arXiv.2108.03018", "date": "2021-08-06", "title": "Conditional Separation as a Binary Relation. A Coq Assisted Proof", "authors": "Jean-Philippe Chancelier, Michel de Lara, Benjamin Heymann", "abstract": "The concept of d-separation holds a pivotal role in causality theory, serving\nas a fundamental tool for deriving conditional independence properties from\ncausal graphs. Pearl defined the d-separation of two subsets conditionally on a\nthird one. In this study, we present a novel perspective by showing i) how the\nd-separation can be extended beyond acyclic graphs, possibly infinite, and ii)\nhow it can be expressed and characterized as a binary relation between\nvertices. Compared to the typical perspectives in causality theory, our\nequivalence opens the door to more compact and computational proofing\ntechniques, because the language of binary relations is well adapted to\nequational reasoning. Additionally, and of independent interest, the proofs of\nthe results presented in this paper are checked with the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1910.01697", "date": "2019-10-03", "title": "A Henkin-style completeness proof for the modal logic S5", "authors": "Bruno Bentzen", "abstract": "This paper presents a recent formalization of a Henkin-style completeness\nproof for the propositional modal logic S5 using the Lean theorem prover. The\nproof formalized is close to that of Hughes and Cresswell, but the system,\nbased on a different choice of axioms, is better described as a Mendelson\nsystem augmented with axiom schemes for K, T, S4, and B, and the necessitation\nrule as a rule of inference. The language has the false and implication as the\nonly primitive logical connectives and necessity as the only primitive modal\noperator. The full source code is available online at\nhttps://github.com/bbentzen/mpl/ and has been typechecked with Lean 3.4.2.", "journal": "Logic and Argumentation: Fourth International Conference, CLAR\n  2021, Hangzhou, China, October 20--22. P. Baroni, C. Benzm\\\"uller, Y. N.\n  W\\'ang (Eds.), pp. 459--467, 2021"}
{"doi": "10.48550/arXiv.1705.04680", "date": "2017-05-12", "title": "Proof Mining with Dependent Types", "authors": "Ekaterina Komendantskaya, Jonathan Heras", "abstract": "Several approaches exist to data-mining big corpora of formal proofs. Some of\nthese approaches are based on statistical machine learning, and some -- on\ntheory exploration. However, most are developed for either untyped or\nsimply-typed theorem provers. In this paper, we present a method that combines\nstatistical data mining and theory exploration in order to analyse and automate\nproofs in dependently typed language of Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2409.11946", "date": "2024-09-18", "title": "An Imperative Language for Verified Exact Real-Number Computation", "authors": "Andrej Bauer, Sewon Park, Alex Simpson", "abstract": "We introduce Clerical, a programming language for exact real-number\ncomputation that combines first-order imperative-style programming with a limit\noperator for computation of real numbers as limits of Cauchy sequences. We\naddress the semidecidability of the linear ordering of the reals by\nincorporating nondeterministic guarded choice, through which decisions based on\npartial comparison operations on reals can be patched together to give total\nprograms. The interplay between mutable state, nondeterminism, and computation\nof limits is controlled by the requirement that expressions computing limits\nand guards modify only local state. We devise a domain-theoretic denotational\nsemantics that uses a variant of Plotkin powerdomain construction tailored to\nour specific version of nondeterminism. We formulate a Hoare-style\nspecification logic, show that it is sound for the denotational semantics, and\nillustrate the setup by implementing and proving correct a program for\ncomputation of $\\pi$ as the least positive zero of $\\sin$. The modular\ncharacter of Clerical allows us to compose the program from smaller parts, each\nof which is shown to be correct on its own. We provide a proof-of-concept OCaml\nimplementation of Clerical, and formally verify parts of the development,\nnotably the soundness of specification logic, in the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2403.08173", "date": "2024-03-13", "title": "A bargain for mergesorts (functional pearl) -- How to prove your mergesort correct and stable, almost for free", "authors": "Cyril Cohen, Kazuhiko Sakaguchi", "abstract": "We present a novel characterization of stable mergesort functions using\nrelational parametricity, and show that it implies the correctness of\nmergesort. As a result, one can prove the correctness of several variations of\nmergesort (e.g., top-down, bottom-up, tail-recursive, non-tail-recursive,\nsmooth, and non-smooth mergesorts) by proving the characterization property for\neach variation. To further motivate this work, we show a performance trade-off\nbetween tail-recursive and non-tail-recursive mergesorts that (1) the former in\ncall-by-value evaluation avoids using up stack space and is efficient and (2)\nthe latter in call-by-need evaluation is an optimal incremental sort, meaning\nthat it performs only $\\mathcal{O}(n + k \\log k)$ comparisons to compute the\nleast (or greatest) $k$ items of a list of length $n$. Thanks to our\ncharacterization and the parametricity translation, we deduced the correctness\nresults, including stability, of various implementations of mergesort for\nlists, including highly optimized ones, in the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2502.11344", "date": "2025-02-17", "title": "A Coq implementation of a Theory of Tagged Objects", "authors": "Matthew Gates, Alex Potanin", "abstract": "We present a first step towards the Coq implementation of the Theory of\nTagged Objects formalism. The concept of tagged types is encoded, and the\nsoundness proofs are discussed with some future work suggestions.", "journal": ""}
{"doi": "10.48550/arXiv.2012.10668", "date": "2020-12-19", "title": "FraCaS: Temporal Analysis", "authors": "Jean-Philippe Bernardy, Stergios Chatzikyriakidis", "abstract": "In this paper, we propose an implementation of temporal semantics which is\nsuitable for inference problems. This implementation translates syntax trees to\nlogical formulas, suitable for consumption by the Coq proof assistant. We\nsupport several phenomena including: temporal references, temporal adverbs,\naspectual classes and progressives. We apply these semantics to the complete\nFraCaS testsuite. We obtain an accuracy of 81 percent overall and 73 percent\nfor problems explicitly marked as related to temporal reference.", "journal": ""}
{"doi": "10.48550/arXiv.2301.09347", "date": "2023-01-23", "title": "Verified reductions for optimization", "authors": "Alexander Bentkamp, Ramon Fern\u00e1ndez Mir, Jeremy Avigad", "abstract": "Numerical and symbolic methods for optimization are used extensively in\nengineering, industry, and finance. Various methods are used to reduce problems\nof interest to ones that are amenable to solution by such software. We develop\na framework for designing and applying such reductions, using the Lean\nprogramming language and interactive proof assistant. Formal verification makes\nthe process more reliable, and the availability of an interactive framework and\nambient mathematical library provides a robust environment for constructing the\nreductions and reasoning about them.", "journal": ""}
{"doi": "10.48550/arXiv.2103.04287", "date": "2021-03-07", "title": "Reduction Free Normalisation for a proof irrelevant type of propositions", "authors": "Thierry Coquand", "abstract": "We show normalisation and decidability of convertibility for a type theory\nwith a hierarchy of universes and a proof irrelevant type of propositions,\nclose to the type system used in the proof assistant Lean. Contrary to previous\narguments, the proof does not require explicitly to introduce a notion of\nneutral and normal forms.", "journal": "Logical Methods in Computer Science, Volume 19, Issue 3 (July 13,\n  2023) lmcs:8818"}
{"doi": "10.48550/arXiv.1704.08909", "date": "2017-04-28", "title": "A Constructive Framework for Galois Connections", "authors": "Francesco Ranzato", "abstract": "Abstract interpretation-based static analyses rely on abstract domains of\nprogram properties, such as intervals or congruences for integer variables.\nGalois connections (GCs) between posets provide the most widespread and useful\nformal tool for mathematically specifying abstract domains. Recently, Darais\nand Van Horn [2016] put forward a notion of constructive Galois connection for\nunordered sets (rather than posets), which allows to define abstract domains in\na so-called mechanized and calculational proof style and therefore enables the\nuse of proof assistants like Coq and Agda for automatically extracting verified\nalgorithms of static analysis. We show here that constructive GCs are\nisomorphic, in a precise and comprehensive meaning including sound abstract\nfunctions, to so-called partitioning GCs--an already known class of GCs which\nallows to cast standard set partitions as an abstract domain. Darais and Van\nHorn [2016] also provide a notion of constructive GC for posets, which we prove\nto be isomorphic to plain GCs and therefore lose their constructive attribute.\nDrawing on these findings, we put forward and advocate the use of purely\npartitioning GCs, a novel class of constructive abstract domains for a\nmechanized approach to abstract interpretation. We show that this class of\nabstract domains allows us to represent a set partition with more flexibility\nwhile retaining a constructive approach to Galois connections.", "journal": ""}
{"doi": "10.48550/arXiv.1503.00948", "date": "2015-03-03", "title": "Hilbert-Post completeness for the state and the exception effects", "authors": "Jean-Guillaume Dumas, Dominique Duval, Burak Ekici, Damien Pous, Jean-Claude Reynaud", "abstract": "In this paper, we present a novel framework for studying the syntactic\ncompleteness of computational effects and we apply it to the exception effect.\nWhen applied to the states effect, our framework can be seen as a\ngeneralization of Pretnar's work on this subject. We first introduce a relative\nnotion of Hilbert-Post completeness, well-suited to the composition of effects.\nThen we prove that the exception effect is relatively Hilbert-Post complete, as\nwell as the \"core\" language which may be used for implementing it; these proofs\nhave been formalized and checked with the proof assistant Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1804.10565", "date": "2018-04-27", "title": "Certified Graph View Maintenance with Regular Datalog", "authors": "Angela Bonifati, Stefania Dumbrava, Emilio Jesus Gallego Arias", "abstract": "We employ the Coq proof assistant to develop a mechanically-certified\nframework for evaluating graph queries and incrementally maintaining\nmaterialized graph instances, also called views. The language we use for\ndefining queries and views is Regular Datalog (RD) -- a notable fragment of\nnon-recursive Datalog that can express complex navigational queries, with\ntransitive closure as native operator. We first design and encode the theory of\nRD and then mechanize a RD-specific evaluation algorithm capable of\nfine-grained, incremental graph view computation, which we prove sound with\nrespect to the declarative RD semantics. By using the Coq extraction mechanism,\nwe test an Ocaml version of the verified engine on a set of preliminary\nbenchmarks. Our development is particularly focused on leveraging existing\nverification and notational techniques to: a) define mechanized properties that\ncan be easily understood by logicians and database researchers and b) attain\nformal verification with limited effort. Our work is the first step towards a\nunified, machine-verified, formal framework for dynamic graph query languages\nand their evaluation engines. This paper is under consideration for acceptance\nin TPLP.", "journal": ""}
{"doi": "10.48550/arXiv.1407.3519", "date": "2014-07-14", "title": "Showing invariance compositionally for a process algebra for network protocols", "authors": "Timothy Bourke, Robert J. van Glabbeek, Peter H\u00f6fner", "abstract": "This paper presents the mechanization of a process algebra for Mobile Ad hoc\nNetworks and Wireless Mesh Networks, and the development of a compositional\nframework for proving invariant properties. Mechanizing the core process\nalgebra in Isabelle/HOL is relatively standard, but its layered structure\nnecessitates special treatment. The control states of reactive processes, such\nas nodes in a network, are modelled by terms of the process algebra. We propose\na technique based on these terms to streamline proofs of inductive invariance.\nThis is not sufficient, however, to state and prove invariants that relate\nstates across multiple processes (entire networks). To this end, we propose a\nnovel compositional technique for lifting global invariants stated at the level\nof individual nodes to networks of nodes.", "journal": "Proc. Interactive Theorem Proving, ITP '14 (G. Klein & R. Gamboa,\n  eds.), LNCS 8558, Springer, 2014, pp. 144-159"}
{"doi": "10.48550/arXiv.2005.00745", "date": "2020-05-02", "title": "Predicting the Path Loss of Wireless Channel Models Using Machine Learning Techniques in MmWave Urban Communications", "authors": "Saud Aldossari, Kwang-Cheng Chen", "abstract": "The classic wireless communication channel modeling is performed using\nDeterministic and Stochastic channel methodologies. Machine learning (ML)\nemerges to revolutionize system design for 5G and beyond. ML techniques such as\nsupervise leaning methods will be used to predict the wireless channel path\nloss of a variate of environments base on a certain dataset. The propagation\nsignal of communication systems fundamentals is focusing on channel modeling\nparticularly for new frequency bands such as MmWave. Machine learning can\nfacilitate rapid channel modeling for 5G and beyond wireless communication\nsystems due to the availability of partially relevant channel measurement data\nand model. When irregularity of the wireless channels lead to a complex\nmethodology to achieve accurate models, appropriate machine learning\nmethodology explores to reduce the complexity and increase the accuracy. In\nthis paper, we demonstrate alternative procedures beyond traditional channel\nmodeling to enhance the path loss models using machine learning techniques, to\nalleviate the dilemma of channel complexity and time-consuming process that the\nmeasurements were taken. This demonstrated regression uses the measurement data\nof a certain scenario to successfully assist the prediction of path loss model\nof a different operating environment.", "journal": ""}
{"doi": "10.48550/arXiv.2112.05515", "date": "2021-12-10", "title": "Semantic Cut Elimination for the Logic of Bunched Implications, Formalized in Coq", "authors": "Dan Frumin", "abstract": "The logic of bunched implications (BI) is a substructural logic that forms\nthe backbone of separation logic, the much studied logic for reasoning about\nheap-manipulating programs. Although the proof theory and metatheory of BI are\nmathematically involved, the formalization of important metatheoretical results\nis still incipient. In this paper we present a self-contained formalized, in\nthe Coq proof assistant, proof of a central metatheoretical property of BI: cut\nelimination for its sequent calculus.\n  The presented proof is *semantic*, in the sense that is obtained by\ninterpreting sequents in a particular \"universal\" model. This results in a more\nmodular and elegant proof than a standard Gentzen-style cut elimination\nargument, which can be subtle and error-prone in manual proofs for BI. In\nparticular, our semantic approach avoids unnecessary inversions on proof\nderivations, or the uses of cut reductions and the multi-cut rule.\n  Besides modular, our approach is also robust: we demonstrate how our method\nscales, with minor modifications, to (i) an extension of BI with an arbitrary\nset of \\emph{simple structural rules}, and (ii) an extension with an S4-like\n$\\Box$ modality.", "journal": ""}
{"doi": "10.48550/arXiv.1909.12582", "date": "2019-09-27", "title": "Towards a Coq-verified Chain of Esterel Semantics", "authors": "G\u00e9rard Berry, Lionel Rieg", "abstract": "This paper focuses on formally specifying and verifying the chain of formal\nsemantics of the Esterel synchronous programming language using the Coq proof\nassistant. In particular, in addition to the standard logical (LBS) semantics,\nconstructive semantics (CBS) and constructive state semantics (CSS), we\nintroduce a novel microstep semantics that gets rid of the Must/Can potential\nfunction pair of the constructive semantics and can be viewed as an abstract\nversion of Esterel's circuit semantics used by compilers to generate software\ncode and hardware designs. Excluding the loop construct from Esterel, the paper\nalso provides formal proofs in Coq of the equivalence between the CBS and CSS\nsemantics and of the refinement of the CSS by the microstep semantics.", "journal": ""}
{"doi": "10.48550/arXiv.2402.02449", "date": "2024-02-04", "title": "Surfing the modeling of PoS taggers in low-resource scenarios", "authors": "Manuel Vilares Ferro, V\u00edctor M. Darriba Bilbao, Francisco J. Ribadas-Pena, Jorge Gra\u00f1a Gil", "abstract": "The recent trend towards the application of deep structured techniques has\nrevealed the limits of huge models in natural language processing. This has\nreawakened the interest in traditional machine learning algorithms, which have\nproved still to be competitive in certain contexts, in particular low-resource\nsettings. In parallel, model selection has become an essential task to boost\nperformance at reasonable cost, even more so when we talk about processes\ninvolving domains where the training and/or computational resources are scarce.\nAgainst this backdrop, we evaluate the early estimation of learning curves as a\npractical mechanism for selecting the most appropriate model in scenarios\ncharacterized by the use of non-deep learners in resource-lean settings. On the\nbasis of a formal approximation model previously evaluated under conditions of\nwide availability of training and validation resources, we study the\nreliability of such an approach in a different and much more demanding\noperationalenvironment. Using as case study the generation of PoS taggers for\nGalician, a language belonging to the Western Ibero-Romance group, the\nexperimental results are consistent with our expectations.", "journal": "Mathematics 2022, 10(19), 3526"}
{"doi": "10.48550/arXiv.2305.05570", "date": "2023-05-09", "title": "Engineering a Formally Verified Automated Bug Finder", "authors": "Arthur Correnson, Dominic Steinhoefel", "abstract": "Symbolic execution is a program analysis technique executing programs with\nsymbolic instead of concrete inputs. This principle allows for exploring many\nprogram paths at once. Despite its wide adoption -- in particular for program\ntesting -- little effort was dedicated to studying the semantic foundations of\nsymbolic execution. Without these foundations, critical questions regarding the\ncorrectness of symbolic executors cannot be satisfyingly answered: Can a\nreported bug be reproduced, or is it a false positive (soundness)? Can we be\nsure to find all bugs if we let the testing tool run long enough\n(completeness)? This paper presents a systematic approach for engineering\nprovably sound and complete symbolic execution-based bug finders by relating a\nprogramming language's operational semantics with a symbolic semantics. In\ncontrast to prior work on symbolic execution semantics, we address the\ncorrectness of critical implementation details of symbolic bug finders,\nincluding the search strategy and the role of constraint solvers to prune the\nsearch space. We showcase our approach by implementing WiSE, a prototype of a\nverified bug finder for an imperative language, in the Coq proof assistant and\nproving it sound and complete. We demonstrate that the design principles of\nWiSE survive outside the ecosystem of interactive proof assistants by (1)\nautomatically extracting an OCaml implementation and (2) transforming WiSE to\nPyWiSE, a functionally equivalent Python version.", "journal": ""}
{"doi": "10.48550/arXiv.1505.00061", "date": "2015-05-01", "title": "Context-Free Language Theory Formalization", "authors": "Marcus Vin\u00edcius Midena Ramos, Ruy J. G. B. de Queiroz", "abstract": "Proof assistants are software-based tools that are used in the mechanization\nof proof construction and validation in mathematics and computer science, and\nalso in certified program development. Different tools are being increasingly\nused in order to accelerate and simplify proof checking. Context-free language\ntheory is a well-established area of mathematics, relevant to computer science\nfoundations and technology. This proposal aims at formalizing parts of\ncontext-free language theory in the Coq proof assistant. This report presents\nthe underlying theory and general characteristics of proof assistants,\nincluding Coq itself, discusses its use in relevant formalization projects,\npresents the current status of the implementation, addresses related projects\nand the contributions of this work. The results obtained so far include the\nformalization of closure properties for context-free grammars (under union,\nconcatenation and closure) and the formalization of grammar simplification.\nGrammar simplification is a subject of high importance in computer language\nprocessing technology as well as in formal language theory, and the\nformalization refers to the fact that general context-free grammars generate\nlanguages that can be also generated by simpler and equivalent context-free\ngrammars. Namely, useless symbol elimination, inaccessible symbol elimination,\nunit rules elimination and empty rules elimination operations were described\nand proven correct with respect to the preservation of the language generated\nby the original grammar.", "journal": ""}
{"doi": "10.48550/arXiv.1005.0824", "date": "2010-05-05", "title": "Formal Proof of a Wave Equation Resolution Scheme: the Method Error", "authors": "Sylvie Boldo, Fran\u00e7ois Cl\u00e9ment, Jean-Christophe Filli\u00e2tre, Micaela Mayero, Guillaume Melquiond, Pierre Weis", "abstract": "Popular finite difference numerical schemes for the resolution of the\none-dimensional acoustic wave equation are well-known to be convergent. We\npresent a comprehensive formalization of the simplest one and formally prove\nits convergence in Coq. The main difficulties lie in the proper definition of\nasymptotic behaviors and the implicit way they are handled in the mathematical\npen-and-paper proofs. To our knowledge, this is the first time such kind of\nmathematical proof is machine-checked.", "journal": "Interactive Theorem Proving 6172 (2010) 147-162"}
{"doi": "10.48550/arXiv.1802.01795", "date": "2018-02-06", "title": "A Lean formalization of Matiyasevi\u010d's Theorem", "authors": "Mario Carneiro", "abstract": "In this paper, we present a formalization of Matiyasevi\\v{c}'s theorem, which\nstates that the power function is Diophantine, forming the last and hardest\npiece of the MRDP theorem of the unsolvability of Hilbert's 10th problem. The\nformalization is performed within the Lean theorem prover, and necessitated the\ndevelopment of a small number theory library, including in particular the\nsolution to Pell's equation and properties of the Pell $x,y$ sequences.", "journal": ""}
{"doi": "10.48550/arXiv.1112.0215", "date": "2011-12-01", "title": "A Framework for Automated and Certified Refinement Steps", "authors": "Andreas Griesmayer, Zhiming Liu, Charles Morisset, Shuling Wang", "abstract": "The refinement calculus provides a methodology for transforming an abstract\nspecification into a concrete implementation, by following a succession of\nrefinement rules. These rules have been mechanized in theorem-provers, thus\nproviding a formal and rigorous way to prove that a given program refines\nanother one. In a previous work, we have extended this mechanization for\nobject-oriented programs, where the memory is represented as a graph, and we\nhave integrated our approach within the rCOS tool, a model-driven software\ndevelopment tool providing a refinement language. Hence, for any refinement\nstep, the tool automatically generates the corresponding proof obligations and\nthe user can manually discharge them, using a provided library of refinement\nlemmas. In this work, we propose an approach to automate the search of possible\nrefinement rules from a program to another, using the rewriting tool Maude.\nEach refinement rule in Maude is associated with the corresponding lemma in\nIsabelle, thus allowing the tool to automatically generate the Isabelle proof\nwhen a refinement rule can be automatically found. The user can add a new\nrefinement rule by providing the corresponding Maude rule and Isabelle lemma.", "journal": ""}
{"doi": "10.48550/arXiv.2501.16207", "date": "2025-01-27", "title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs", "authors": "Jialun Cao, Yaojie Lu, Meiziniu Li, Haoyang Ma, Haokun Li, Mengda He, Cheng Wen, Le Sun, Hongyu Zhang, Shengchao Qin, Shing-Chi Cheung, Cong Tian", "abstract": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe.", "journal": ""}
{"doi": "10.48550/arXiv.2310.06376", "date": "2023-10-10", "title": "Martin-L\u00f6f \u00e0 la Coq", "authors": "Arthur Adjedj, Meven Lennon-Bertrand, Kenji Maillard, Pierre-Marie P\u00e9drot, Lo\u00efc Pujet", "abstract": "We present an extensive mechanization of the meta-theory of Martin-L\\\"of Type\nTheory (MLTT) in the Coq proof assistant. Our development builds on\npre-existing work in Agda to show not only the decidability of conversion, but\nalso the decidability of type checking, using an approach guided by\nbidirectional type checking. From our proof of decidability, we obtain a\ncertified and executable type checker for a full-fledged version of MLTT with\nsupport for $\\Pi$, $\\Sigma$, $\\mathbb{N}$, and identity types, and one\nuniverse. Furthermore, our development does not rely on impredicativity,\ninduction-recursion or any axiom beyond MLTT with a schema for indexed\ninductive types and a handful of predicative universes, narrowing the gap\nbetween the object theory and the meta-theory to a mere difference in\nuniverses. Finally, we explain our formalization choices, geared towards a\nmodular development relying on Coq's features, e.g. meta-programming facilities\nprovided by tactics and universe polymorphism.", "journal": ""}
{"doi": "10.48550/arXiv.2004.10655", "date": "2020-04-06", "title": "Formal Verification of Flow Equivalence in Desynchronized Designs", "authors": "Jennifer Paykin, Brian Huffman, Daniel M. Zimmerman, Peter A. Beerel", "abstract": "Seminal work by Cortadella, Kondratyev, Lavagno, and Sotiriou includes a\nhand-written proof that a particular handshaking protocol preserves flow\nequivalence, a notion of equivalence between synchronous latch-based\nspecifications and their desynchronized bundled-data asynchronous\nimplementations. In this work we identify a counterexample to Cortadella et\nal.'s proof illustrating how their protocol can in fact lead to a violation of\nflow equivalence. However, two of the less concurrent protocols identified in\ntheir paper do preserve flow equivalence. To verify this fact, we formalize\nflow equivalence in the Coq proof assistant and provide mechanized,\nmachine-checkable proofs of our results.", "journal": ""}
{"doi": "10.48550/arXiv.2411.09347", "date": "2024-11-14", "title": "The Denotational Semantics of SSA", "authors": "Jad Elkhaleq Ghalayini, Neel Krishnaswami", "abstract": "Static single assignment form, or SSA, has been the dominant compiler\nintermediate representation for decades. In this paper, we give a type theory\nfor a variant of SSA, including its equational theory, which are strong enough\nto validate a variety of control and data flow transformations. We also give a\ncategorical semantics for SSA, and show that the type theory is sound and\ncomplete with respect to the categorical axiomatization. We demonstrate the\nutility of our model by exhibiting a variety of concrete models satisfying our\naxioms, including in particular a model of TSO weak memory. The correctness of\nthe syntactic metatheory, as well as the completeness proof has been mechanized\nin the Lean proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.0903.3850", "date": "2009-03-23", "title": "Using Structural Recursion for Corecursion", "authors": "Yves Bertot, Ekaterina Komendantskaya", "abstract": "We propose a (limited) solution to the problem of constructing stream values\ndefined by recursive equations that do not respect the guardedness condition.\nThe guardedness condition is imposed on definitions of corecursive functions in\nCoq, AGDA, and other higher-order proof assistants. In this paper, we\nconcentrate in particular on those non-guarded equations where recursive calls\nappear under functions. We use a correspondence between streams and functions\nover natural numbers to show that some classes of non-guarded definitions can\nbe modelled through the encoding as structural recursive functions. In\npractice, this work extends the class of stream values that can be defined in a\nconstructive type theory-based theorem prover with inductive and coinductive\ntypes, structural recursion and guarded corecursion", "journal": "Types 2008 5497 (2008) 220-236"}
{"doi": "10.48550/arXiv.2108.03076", "date": "2021-08-06", "title": "Certified Compilation of Financial Contracts", "authors": "Danil Annenkov, Martin Elsman", "abstract": "We present an extension to a certified financial contract management system\nthat allows for templated declarative financial contracts and for integration\nwith financial stochastic models through verified compilation into so-called\npayoff-expressions. Such expressions readily allow for determining the value of\na contract in a given evaluation context, such as contexts created for\nstochastic simulations. The templating mechanism is useful both at the contract\nspecification level, for writing generic reusable contracts, and for reuse of\ncode that, without the templating mechanism, needs to be recompiled for\ndifferent evaluation contexts. We report on the effect of using the certified\nsystem in the context of a GPGPU-based Monte Carlo simulation engine for\npricing various over-the-counter (OTC) financial contracts. The full\ncontract-management system, including the payoff-language compilation, is\nverified in the Coq proof assistant and certified Haskell code is extracted\nfrom our Coq development along with Futhark code for use in a data-parallel\npricing engine.", "journal": "20th International Symposium on Principles and Practice of\n  Declarative Programming 2018 (PPDP'18)"}
{"doi": "10.48550/arXiv.2412.15177", "date": "2024-12-19", "title": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying", "authors": "Federico Castagna, Isabel Sassoon, Simon Parsons", "abstract": "Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided.", "journal": ""}
{"doi": "10.48550/arXiv.2202.08017", "date": "2022-02-16", "title": "Mechanization of LAGC Semantics in Isabelle", "authors": "Niklas Heidler", "abstract": "Formal programming language semantics are imperative when trying to verify\nproperties of programs in an automated manner. Using a new approach, Din et al.\nstrengthen the ability of reasoning about concurrent programs by proposing a\nmodular trace semantics, which can flexibly adapt to the most prominent\nimperative programming language paradigms. These semantics decouple the\nevaluation in the local environments from the evaluation in the global\nenvironment by generating abstract, symbolic traces for the individual, local\nsystems. The traces are then composed and concretized, resulting in global\ntraces for the global system. Hence, these semantics are called Locally\nAbstract, Globally Concrete (LAGC).\n  In this work, we present a formalization of the LAGC semantics in the popular\ntheorem proving environment Isabelle/HOL. The given model is based on the prior\nwork on the theory of LAGC semantics by Din et al. and includes formalizations\nof the basic theorems, the LAGC semantics for the While Language (WL), as well\nas the LAGC semantics for an extended version of the While Language (WLEXT). We\nfurthermore use our Isabelle model in order to provide formal proofs for\nseveral advanced properties of the LAGC semantics, which have not been analyzed\nin the original paper.\n  Whilst the main goal of the work was to formalize the LAGC semantics in a\nmathematically rigorous manner, we also achieve a high level of proof\nautomatization and manage to contribute an efficient code-generation for the\ncomputation of program traces. As the formalization of the semantics is highly\nmodular, the given theories could in the future be extended with even more\nsophisticated programming language paradigms.", "journal": ""}
{"doi": "10.48550/arXiv.1909.05464", "date": "2019-09-12", "title": "A Formal Semantics of Findel in Coq (Short Paper)", "authors": "Andrei Arusoaie", "abstract": "We present the first formal semantics of Findel - a DSL for specifying\nfinancial derivatives. The semantics is encoded in Coq, and we use it to prove\nproperties of several Findel contracts.", "journal": ""}
{"doi": "10.48550/arXiv.2209.01886", "date": "2022-09-05", "title": "A Formal Theory of Choreographic Programming", "authors": "Lu\u00eds Cruz-Filipe, Fabrizio Montesi, Marco Peressotti", "abstract": "Choreographic programming is a paradigm for writing coordination plans for\ndistributed systems from a global point of view, from which\ncorrect-by-construction decentralised implementations can be generated\nautomatically.\n  Theory of choreographies typically includes a number of complex results that\nare proved by structural induction. The high number of cases and the subtle\ndetails in some of these proofs has led to important errors being found in\npublished works.\n  In this work, we formalise the theory of a choreographic programming language\nin Coq. Our development includes the basic properties of this language, a proof\nof its Turing completeness, a compilation procedure to a process language, and\nan operational characterisation of the correctness of this procedure.\n  Our formalisation experience illustrates the benefits of using a theorem\nprover: we get both an additional degree of confidence from the mechanised\nproof, and a significant simplification of the underlying theory. Our results\noffer a foundation for the future formal development of choreographic\nlanguages.", "journal": ""}
{"doi": "10.48550/arXiv.2409.11530", "date": "2024-09-17", "title": "Minuska: Towards a Formally Verified Programming Language Framework", "authors": "Jan Tu\u0161il, Jan Obdr\u017e\u00e1lek", "abstract": "Programming language frameworks allow us to generate language tools (e.g.,\ninterpreters) just from a formal description of the syntax and semantics of a\nprogramming language. As these frameworks tend to be quite complex, an issue\narises whether we can trust the generated tools. To address this issue, we\nintroduce a practical formal programming language framework called Minuska,\nwhich always generates a provably correct interpreter given a valid language\ndefinition. This is achieved by (1) defining a language MinusLang for\nexpressing programming language definitions and giving it formal semantics and\n(2) using the Coq proof assistant to implement an interpreter parametric in a\nMinusLang definition and to prove it correct. Minuska provides strong\ncorrectness guarantees and can support nontrivial languages while performing\nwell. This is the extended version of the SEFM24 paper of the same name.", "journal": ""}
{"doi": "10.48550/arXiv.1511.01568", "date": "2015-11-05", "title": "Formalization of Quantum Protocols using Coq", "authors": "Jaap Boender, Florian Kamm\u00fcller, Rajagopal Nagarajan", "abstract": "Quantum Information Processing, which is an exciting area of research at the\nintersection of physics and computer science, has great potential for\ninfluencing the future development of information processing systems. The\nbuilding of practical, general purpose Quantum Computers may be some years into\nthe future. However, Quantum Communication and Quantum Cryptography are well\ndeveloped. Commercial Quantum Key Distribution systems are easily available and\nseveral QKD networks have been built in various parts of the world. The\nsecurity of the protocols used in these implementations rely on\ninformation-theoretic proofs, which may or may not reflect actual system\nbehaviour. Moreover, testing of implementations cannot guarantee the absence of\nbugs and errors. This paper presents a novel framework for modelling and\nverifying quantum protocols and their implementations using the proof assistant\nCoq. We provide a Coq library for quantum bits (qubits), quantum gates, and\nquantum measurement. As a step towards verifying practical quantum\ncommunication and security protocols such as Quantum Key Distribution, we\nsupport multiple qubits, communication and entanglement. We illustrate these\nconcepts by modelling the Quantum Teleportation Protocol, which communicates\nthe state of an unknown quantum bit using only a classical channel.", "journal": "EPTCS 195, 2015, pp. 71-83"}
{"doi": "10.48550/arXiv.2407.10345", "date": "2024-07-14", "title": "PLACIDUS: Engineering Product Lines of Rigorous Assurance Cases", "authors": "Logan Murphy, Torin Viger, Alessio Di Sandro, Marsha Chechik", "abstract": "In critical software engineering, structured assurance cases (ACs) are used\nto demonstrate how key properties (e.g., safety, security) are supported by\nevidence artifacts (e.g., test results, proofs). ACs can also be studied as\nformal objects in themselves, such that formal methods can be used to establish\ntheir correctness. Creating rigorous ACs is particularly challenging in the\ncontext of software product lines (SPLs), wherein a family of related software\nproducts is engineered simultaneously. Since creating individual ACs for each\nproduct is infeasible, AC development must be lifted to the level of product\nlines. In this work, we propose PLACIDUS, a methodology for integrating formal\nmethods and software product line engineering to develop provably correct ACs\nfor SPLs. To provide rigorous foundations for PLACIDUS, we define a\nvariability-aware AC language and formalize its semantics using the proof\nassistant Lean. We provide tool support for PLACIDUS as part of an\nEclipse-based model management framework. Finally, we demonstrate the\nfeasibility of PLACIDUS by developing an AC for a product line of medical\ndevices.", "journal": ""}
{"doi": "10.48550/arXiv.2211.11797", "date": "2022-11-21", "title": "Multi-Spectral Image Classification with Ultra-Lean Complex-Valued Models", "authors": "Utkarsh Singhal, Stella X. Yu, Zackery Steck, Scott Kangas, Aaron A. Reite", "abstract": "Multi-spectral imagery is invaluable for remote sensing due to different\nspectral signatures exhibited by materials that often appear identical in\ngreyscale and RGB imagery. Paired with modern deep learning methods, this\nmodality has great potential utility in a variety of remote sensing\napplications, such as humanitarian assistance and disaster recovery efforts.\nState-of-the-art deep learning methods have greatly benefited from large-scale\nannotations like in ImageNet, but existing MSI image datasets lack annotations\nat a similar scale. As an alternative to transfer learning on such data with\nfew annotations, we apply complex-valued co-domain symmetric models to classify\nreal-valued MSI images. Our experiments on 8-band xView data show that our\nultra-lean model trained on xView from scratch without data augmentations can\noutperform ResNet with data augmentation and modified transfer learning on\nxView. Our work is the first to demonstrate the value of complex-valued deep\nlearning on real-valued MSI data.", "journal": ""}
{"doi": "10.48550/arXiv.1611.09633", "date": "2016-11-29", "title": "Formal Languages, Formally and Coinductively", "authors": "Dmitriy Traytel", "abstract": "Traditionally, formal languages are defined as sets of words. More recently,\nthe alternative coalgebraic or coinductive representation as infinite tries,\ni.e., prefix trees branching over the alphabet, has been used to obtain compact\nand elegant proofs of classic results in language theory. In this article, we\nstudy this representation in the Isabelle proof assistant. We define regular\noperations on infinite tries and prove the axioms of Kleene algebra for those\noperations. Thereby, we exercise corecursion and coinduction and confirm the\ncoinductive view being profitable in formalizations, as it improves over the\nset-of-words view with respect to proof automation.", "journal": "Logical Methods in Computer Science, Volume 13, Issue 3 (September\n  19, 2017) lmcs:2564"}
{"doi": "10.48550/arXiv.2010.16016", "date": "2020-10-30", "title": "Lucas-Interpretation on Isabelle's Functions", "authors": "Walther Neuper", "abstract": "Software tools of Automated Reasoning are too sophisticated for general use\nin mathematics education and respective reasoning, while Lucas-Interpretation\nprovides a general concept for integrating such tools into educational software\nwith the purpose to reliably and flexibly check formal input of students. This\npaper gives the first technically concise description of Lucas-Interpretation\nat the occasion of migrating a prototype implementation to the function package\nof the proof assistant Isabelle. The description shows straightforward\nadaptations of Isabelle's programming language and shows, how simple migration\nof the interpreter was, since the design (before the function package has been\nintroduced to Isabelle) recognised appropriateness of Isabelle's terms as\nmiddle end. The paper gives links into the code in an open repository as\ninvitation to readers for re-using the prototyped code or adopt the general\nconcept. And since the prototype has been designed before the function package\nwas implemented, the paper is an opportunity for recording lessons learned from\nIsabelle's development of code structure.", "journal": "EPTCS 328, 2020, pp. 79-95"}
{"doi": "10.48550/arXiv.1807.08588", "date": "2018-07-23", "title": "Automating Verification of State Machines with Reactive Designs and Isabelle/UTP", "authors": "Simon Foster, James Baxter, Ana Cavalcanti, Alvaro Miyazawa, Jim Woodcock", "abstract": "State-machine based notations are ubiquitous in the description of component\nsystems, particularly in the robotic domain. To ensure these systems are safe\nand predictable, formal verification techniques are important, and can be\ncost-effective if they are both automated and scalable. In this paper, we\npresent a verification approach for a diagrammatic state machine language that\nutilises theorem proving and a denotational semantics based on Unifying\nTheories of Programming (UTP). We provide the necessary theory to underpin\nstate machines (including induction theorems for iterative processes),\nmechanise an action language for states and transitions, and use these to\nformalise the semantics. We then describe the verification approach, which\nsupports infinite state systems, and exemplify it with a fully automated\ndeadlock-freedom check. The work has been mechanised in our proof tool,\nIsabelle/UTP, and so also illustrates the use of UTP to build practical\nverification tools.", "journal": ""}
{"doi": "10.48550/arXiv.1210.2094", "date": "2012-10-07", "title": "Type Directed Partial Evaluation for Level-1 Shift and Reset", "authors": "Danko Ilik", "abstract": "We present an implementation in the Coq proof assistant of type directed\npartial evaluation (TDPE) algorithms for call-by-name and call-by-value\nversions of shift and reset delimited control operators, and in presence of\nstrong sum types. We prove that the algorithm transforms well-typed programs to\nones in normal form. These normal forms can not always be arrived at using the\nso far known equational theories. The typing system does not allow answer-type\nmodification for function types and allows delimiters to be set on at most one\natomic type. The semantic domain for evaluation is expressed in Constructive\nType Theory as a dependently typed monadic structure combining Kripke models\nand continuation passing style translations.", "journal": "EPTCS 127, 2013, pp. 86-100"}
{"doi": "10.48550/arXiv.2309.03642", "date": "2023-05-20", "title": "Formal Verification of Chase-Lev Deque in Concurrent Separation Logic", "authors": "Jaemin Choi", "abstract": "Chase-Lev deque is a concurrent data structure designed for efficient load\nbalancing in multiprocessor scheduling. It employs a work-stealing strategy,\nwhere each thread possesses its own work-stealing deque to store tasks, and\nidle threads steal tasks from other threads. However, given the inherent risk\nof bugs in software, particularly in a multiprocessor environment, it is\ncrucial to formally establish the correctness of programs and data structures.\nTo our knowledge, no formal verification work for the Chase-Lev deque has met\nthree key criteria: (1) utilizing a minimal trusted computing base, (2) using a\nrealistic and unrestricted implementation, and (3) proving a strong\nspecification.\n  In this thesis, we address this gap by presenting the formal verification of\nthe Chase-Lev deque using a concurrent separation logic. Our work is mechanized\nin the Coq proof assistant, and our verified implementation is both realistic\nand unbounded in terms of the number of tasks it can handle. Also, we adopt\nlinearizability as the specification, as it is widely recognized as a strong\nspecification for concurrent data structures. Consequently, our work satisfies\nall three aforementioned criteria for formal verification. Additionally, we\nextend our verification to support safe memory reclamation, and provide a basis\nfor verifying the Chase-Lev deque in the relaxed memory model.", "journal": ""}
{"doi": "10.48550/arXiv.1506.04205", "date": "2015-06-13", "title": "Gradual Certified Programming in Coq", "authors": "\u00c9ric Tanter, Nicolas Tabareau", "abstract": "Expressive static typing disciplines are a powerful way to achieve\nhigh-quality software. However, the adoption cost of such techniques should not\nbe under-estimated. Just like gradual typing allows for a smooth transition\nfrom dynamically-typed to statically-typed programs, it seems desirable to\nsupport a gradual path to certified programming. We explore gradual certified\nprogramming in Coq, providing the possibility to postpone the proofs of\nselected properties, and to check \"at runtime\" whether the properties actually\nhold. Casts can be integrated with the implicit coercion mechanism of Coq to\nsupport implicit cast insertion a la gradual typing. Additionally, when\nextracting Coq functions to mainstream languages, our encoding of casts\nsupports lifting assumed properties into runtime checks. Much to our surprise,\nit is not necessary to extend Coq in any way to support gradual certified\nprogramming. A simple mix of type classes and axioms makes it possible to bring\ngradual certified programming to Coq in a straightforward manner.", "journal": ""}
{"doi": "10.48550/arXiv.1705.01163", "date": "2017-05-02", "title": "Revisiting Parametricity: Inductives and Uniformity of Propositions", "authors": "Abhishek Anand, Greg Morrisett", "abstract": "Reynold's parametricity theory captures the property that parametrically\npolymorphic functions behave uniformly: they produce related results on related\ninstantiations. In dependently-typed programming languages, such relations and\nuniformity proofs can be expressed internally, and generated as a program\ntranslation.\n  We present a new parametricity translation for a significant fragment of Coq.\nPrevious translations of parametrically polymorphic propositions allowed\nnon-uniformity. For example, on related instantiations, a function may return\npropositions that are logically inequivalent (e.g. True and False). We show\nthat uniformity of polymorphic propositions is not achievable in general.\nNevertheless, our translation produces proofs that the two propositions are\nlogically equivalent and also that any two proofs of those propositions are\nrelated. This is achieved at the cost of potentially requiring more assumptions\non the instantiations, requiring them to be isomorphic in the worst case.\n  Our translation augments the previous one for Coq by carrying and\ncompositionally building extra proofs about parametricity relations. It is made\neasier by a new method for translating inductive types and pattern matching.\nThe new method builds upon and generalizes previous such translations for\ndependently-typed programming languages.\n  Using reification and reflection, we have implemented our translation as Coq\nprograms. We obtain several stronger free theorems applicable to an ongoing\ncompiler-correctness project. Previously, proofs of some of these theorems took\nseveral hours to finish.", "journal": ""}
{"doi": "10.48550/arXiv.1301.6039", "date": "2013-01-25", "title": "Recycling Proof Patterns in Coq: Case Studies", "authors": "J\u00f3nathan Heras, Ekaterina Komendantskaya", "abstract": "Development of Interactive Theorem Provers has led to the creation of big\nlibraries and varied infrastructures for formal proofs. However, despite (or\nperhaps due to) their sophistication, the re-use of libraries by non-experts or\nacross domains is a challenge. In this paper, we provide detailed case studies\nand evaluate the machine-learning tool ML4PG built to interactively data-mine\nthe electronic libraries of proofs, and to provide user guidance on the basis\nof proof patterns found in the existing libraries.", "journal": ""}
{"doi": "10.48550/arXiv.2501.01534", "date": "2025-01-02", "title": "Transaction Level Hierarchy Guided and Functional Coverage Driven Deductive Formal Verification", "authors": "Tobias Strauch", "abstract": "We demonstrate how dynamic verification (e.g. simulation) can be replaced by\ndeductive formal verification and how to benefit from the advantages of\nsymbolic verification and the reuse of verification proofs. To do this, we swap\nthe well-known module-hierarchy based concept with a transaction-level (TL)\nbased alternative, which still allows us to describe the design as precisely as\non RTL. We enhance the aspect-oriented and TL oriented language PDVL to support\nthe definition of functional coverage (FC) and assertions at all levels of a\nTL-hierarchy. We then show how to use a deductive formal verification (DFV)\nflow which compiles PDVL code into Gallina code to be used by the Coq theorem\nprover. It can be argued that FC can be converted into proof obligations and\nthat proving them is equivalent to 100\\% coverage. We also demonstrate how\nlower-level proofs can be reused when verifying aspects at higher-levels of a\nTL-hierarchy. We argue that the traditional assertion-based verification (ABV)\nmethodology is still supported and SVA can be proven using DFV.", "journal": ""}
{"doi": "10.48550/arXiv.2112.06984", "date": "2021-12-13", "title": "Implementing a Category-Theoretic Framework for Typed Abstract Syntax", "authors": "Benedikt Ahrens, Ralph Matthes, Anders M\u00f6rtberg", "abstract": "In previous work (\"From signatures to monads in UniMath\"), we described a\ncategory-theoretic construction of abstract syntax from a signature, mechanized\nin the UniMath library based on the Coq proof assistant.\n  In the present work, we describe what was necessary to generalize that work\nto account for simply-typed languages. First, some definitions had to be\ngeneralized to account for the natural appearance of non-endofunctors in the\nsimply-typed case. As it turns out, in many cases our mechanized results\ncarried over to the generalized definitions without any code change. Second, an\nexisting mechanized library on $\\omega$-cocontinuous functors had to be\nextended by constructions and theorems necessary for constructing multi-sorted\nsyntax. Third, the theoretical framework for the semantical signatures had to\nbe generalized from a monoidal to a bicategorical setting, again to account for\nnon-endofunctors arising in the typed case. This uses actions of endofunctors\non functors with given source, and the corresponding notion of strong functors\nbetween actions, all formalized in UniMath using a recently developed library\nof bicategory theory. We explain what needed to be done to plug all of these\ningredients together, modularly.\n  The main result of our work is a general construction that, when fed with a\nsignature for a simply-typed language, returns an implementation of that\nlanguage together with suitable boilerplate code, in particular, a certified\nmonadic substitution operation.", "journal": "In Proceedings of the 11th ACM SIGPLAN International Conference on\n  Certified Programs and Proofs (CPP '22), 2022, ACM, New York, NY, USA"}
{"doi": "10.48550/arXiv.2209.09472", "date": "2022-09-20", "title": "Correctness of Broadcast via Multicast: Graphically and Formally", "authors": "Wolfgang Jeltsch, Javier D\u00edaz", "abstract": "Maintaining data consistency among multiple parties requires nodes to\nrepeatedly send data to all other nodes. For example, the nodes of a blockchain\nnetwork have to disseminate the blocks they create across the whole network.\nThe scientific literature typically takes the ideal perspective that such data\ndistribution is performed by broadcasting to all nodes directly, while in\npractice data is distributed by repeated multicast. Since correctness and\nsecurity of consistency maintenance protocols usually have been established for\nthe ideal setting only, it is vital to show that these properties carry over to\nreal-world implementations. Therefore, it is desirable to prove that the ideal\nand the real behavior are equivalent.\n  In the work described in this paper, we take an important step towards such a\nproof by proving a simpler variant of this equivalence statement. The\nsimplification is that we consider only a concrete pair of network topologies,\nwhich nevertheless illustrates important phenomena encountered with arbitrary\ntopologies. For describing systems that distribute data, we use a\ndomain-specific language of processes that corresponds to a class of Petri nets\nand is embedded in a general-purpose process calculus. This way, we can outline\nour proof using an intuitive graphical notation and leverage the rich theory of\nprocess calculi in the actual proof, which is machine-checked using the\nIsabelle proof assistant.", "journal": "EPTCS 369, 2022, pp. 37-50"}
{"doi": "10.48550/arXiv.1812.03624", "date": "2018-12-10", "title": "Formalization of Metatheory of the Quipper Quantum Programming Language in a Linear Logic", "authors": "Mohamed Yousri Mahmoud, Amy P. Felty", "abstract": "We develop a linear logical framework within the Hybrid system and use it to\nreason about the type system of a quantum lambda calculus. In particular, we\nconsider a practical version of the calculus called Proto-Quipper, which\ncontains the core of Quipper. Quipper is a new quantum programming language\nunder active development and recently has gained much popularity among the\nquantum computing communities. Hybrid is a system that is designed to support\nthe use of higher-order abstract syntax (HOAS) for representing and reasoning\nabout formal systems implemented in the Coq Proof Assistant. In this work, we\nextend the system with a linear specification logic (SL) in order to reason\nabout the linear type system of Quipper. To this end, we formalize the\nsemantics of Proto-Quipper by encoding the typing and evaluation rules in the\nSL, and prove type soundness.", "journal": ""}
{"doi": "10.48550/arXiv.1106.3448", "date": "2011-06-17", "title": "Type classes for efficient exact real arithmetic in Coq", "authors": "Robbert Krebbers, Bas Spitters", "abstract": "Floating point operations are fast, but require continuous effort on the part\nof the user in order to ensure that the results are correct. This burden can be\nshifted away from the user by providing a library of exact analysis in which\nthe computer handles the error estimates. Previously, we [Krebbers/Spitters\n2011] provided a fast implementation of the exact real numbers in the Coq proof\nassistant. Our implementation improved on an earlier implementation by O'Connor\nby using type classes to describe an abstract specification of the underlying\ndense set from which the real numbers are built. In particular, we used dyadic\nrationals built from Coq's machine integers to obtain a 100 times speed up of\nthe basic operations already. This article is a substantially expanded version\nof [Krebbers/Spitters 2011] in which the implementation is extended in the\nvarious ways. First, we implement and verify the sine and cosine function.\nSecondly, we create an additional implementation of the dense set based on\nCoq's fast rational numbers. Thirdly, we extend the hierarchy to capture order\non undecidable structures, while it was limited to decidable structures before.\nThis hierarchy, based on type classes, allows us to share theory on the\nnaturals, integers, rationals, dyadics, and reals in a convenient way. Finally,\nwe obtain another dramatic speed-up by avoiding evaluation of termination\nproofs at runtime.", "journal": "Logical Methods in Computer Science, Volume 9, Issue 1 (February\n  14, 2013) lmcs:958"}
{"doi": "10.48550/arXiv.1412.8091", "date": "2014-12-27", "title": "Conversion of HOL Light proofs into Metamath", "authors": "Mario Carneiro", "abstract": "We present an algorithm for converting proofs from the OpenTheory interchange\nformat, which can be translated to and from any of the HOL family of proof\nlanguages (HOL4, HOL Light, ProofPower, and Isabelle), into the ZFC-based\nMetamath language. This task is divided into two steps: the translation of an\nOpenTheory proof into a Metamath HOL formalization, $\\mathtt{\\text{hol.mm}}$,\nfollowed by the embedding of the HOL formalization into the main ZFC\nfoundations of the main Metamath library, $\\mathtt{\\text{set.mm}}$. This\nprocess provides a means to link the simplicity of the Metamath foundations to\nthe intense automation efforts which have borne fruit in HOL Light, allowing\nthe production of complete Metamath proofs of theorems in HOL Light, while also\nproving that HOL Light is consistent, relative to Metamath's ZFC\naxiomatization.", "journal": ""}
{"doi": "10.48550/arXiv.2202.10349", "date": "2022-02-21", "title": "Certified Verification of Relational Properties", "authors": "Lionel Blatter, Nikolai Kosmatov, Virgile Prevosto, Pascale Le Gall", "abstract": "The use of function contracts to specify the behavior of functions often\nremains limited to the scope of a single function call. Relational properties\nlink several function calls together within a single specification. They can\nexpress more advanced properties of a given function, such as non-interference,\ncontinuity, or monotonicity. They can also relate calls to different functions,\nfor instance, to show that an optimized implementation is equivalent to its\noriginal counterpart. However, relational properties cannot be expressed and\nverified directly in the traditional setting of modular deductive verification.\nSelf-composition has been proposed to overcome this limitation, but it requires\ncomplex transformations and additional separation hypotheses for real-life\nlanguages with pointers. We propose a novel approach that is not based on code\ntransformation and avoids those drawbacks. It directly applies a verification\ncondition generator to produce logical formulas that must be verified to ensure\na given relational property. The approach has been fully formalized and proved\nsound in the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2403.13310", "date": "2024-03-20", "title": "A Semantic Search Engine for Mathlib4", "authors": "Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong", "abstract": "The interactive theorem prover Lean enables the verification of formal\nmathematical proofs and is backed by an expanding community. Central to this\necosystem is its mathematical library, mathlib4, which lays the groundwork for\nthe formalization of an expanding range of mathematical theories. However,\nsearching for theorems in mathlib4 can be challenging. To successfully search\nin mathlib4, users often need to be familiar with its naming conventions or\ndocumentation strings. Therefore, creating a semantic search engine that can be\nused easily by individuals with varying familiarity with mathlib4 is very\nimportant. In this paper, we present a semantic search engine\n(https://leansearch.net/) for mathlib4 that accepts informal queries and finds\nthe relevant theorems. We also establish a benchmark for assessing the\nperformance of various search engines for mathlib4.", "journal": ""}
{"doi": "10.48550/arXiv.2009.12154", "date": "2020-09-25", "title": "Integration of Formal Proof into Unified Assurance Cases with Isabelle/SACM", "authors": "Simon Foster, Yakoub Nemouchi, Mario Gleirscher, Ran Wei, Tim Kelly", "abstract": "Assurance cases are often required to certify critical systems. The use of\nformal methods in assurance can improve automation, increase confidence, and\novercome errant reasoning. However, assurance cases can never be fully\nformalised, as the use of formal methods is contingent on models that are\nvalidated by informal processes. Consequently, assurance techniques should\nsupport both formal and informal artifacts, with explicated inferential links\nbetween them. In this paper, we contribute a formal machine-checked interactive\nlanguage, called Isabelle/SACM, supporting the computer-assisted construction\nof assurance cases compliant with the OMG Structured Assurance Case Meta-Model.\nThe use of Isabelle/SACM guarantees well-formedness, consistency, and\ntraceability of assurance cases, and allows a tight integration of formal and\ninformal evidence of various provenance. In particular, Isabelle brings a\ndiverse range of automated verification techniques that can provide evidence.\nTo validate our approach, we present a substantial case study based on the\nTokeneer secure entry system benchmark. We embed its functional specification\ninto Isabelle, verify its security requirements, and form a modular security\ncase in Isabelle/SACM that combines the heterogeneous artifacts. We thus show\nthat Isabelle is a suitable platform for critical systems assurance.", "journal": ""}
{"doi": "10.48550/arXiv.1303.7332", "date": "2013-03-29", "title": "A weak HOAS approach to the POPLmark Challenge", "authors": "Alberto Ciaffaglione, Ivan Scagnetto", "abstract": "Capitalizing on previous encodings and formal developments about nominal\ncalculi and type systems, we propose a weak Higher-Order Abstract Syntax\nformalization of the type language of pure System F<: within Coq, a proof\nassistant based on the Calculus of Inductive Constructions.\n  Our encoding allows us to accomplish the proof of the transitivity property\nof algorithmic subtyping, which is in fact the first of the three tasks stated\nby the POPLmark Challenge, a set of problems that capture the most critical\nissues in formalizing programming language metatheory.", "journal": "EPTCS 113, 2013, pp. 109-124"}
{"doi": "10.48550/arXiv.2001.10630", "date": "2020-01-28", "title": "First-Order Logic for Flow-Limited Authorization", "authors": "Andrew K. Hirsch, Pedro H. Azevedo de Amorim, Ethan Cecchetti, Ross Tate, Owen Arden", "abstract": "We present the Flow-Limited Authorization First-Order Logic (FLAFOL), a logic\nfor reasoning about authorization decisions in the presence of information-flow\npolicies. We formalize the FLAFOL proof system, characterize its\nproof-theoretic properties, and develop its security guarantees. In particular,\nFLAFOL is the first logic to provide a non-interference guarantee while\nsupporting all connectives of first-order logic. Furthermore, this guarantee is\nthe first to combine the notions of non-interference from both authorization\nlogic and information-flow systems. All theorems in this paper are proven in\nCoq.", "journal": ""}
{"doi": "10.48550/arXiv.2108.02995", "date": "2021-08-06", "title": "Extracting functional programs from Coq, in Coq", "authors": "Danil Annenkov, Mikkel Milo, Jakob Botsch Nielsen, Bas Spitters", "abstract": "We implement extraction of Coq programs to functional languages based on\nMetaCoq's certified erasure. We extend the MetaCoq erasure output language with\ntyping information and use it as an intermediate representation, which we call\n$\\lambda^T_\\square$. We complement the extraction functionality with a full\npipeline that includes several standard transformations (eta-expansion,\ninlining, etc) implemented in a proof-generating manner along with a verified\noptimisation pass removing unused arguments. We prove the pass correct wrt. a\nconventional call-by-value operational semantics of functional languages. From\nthe optimised $\\lambda^T_\\square$ representation, we obtain code in two\nfunctional smart contract languages (Liquidity and CameLIGO), the functional\nlanguage Elm, and a subset of the multi-paradigm language for systems\nprogramming Rust. Rust is currently gaining popularity as a language for smart\ncontracts, and we demonstrate how our extraction can be used to extract smart\ncontract code for the Concordium network. The development is done in the\ncontext of the ConCert framework that enables smart contract verification. We\ncontribute with two verified real-world smart contracts (boardroom voting and\nescrow), which we use, among other examples, to exemplify the applicability of\nthe pipeline. In addition, we develop a verified web application and extract it\nto fully functional Elm code. In total, this gives us a way to write\ndependently typed programs in Coq, verify, and then extract them to several\ntarget languages while retaining a small trusted computing base of only MetaCoq\nand the pretty-printers into these languages.", "journal": ""}
{"doi": "10.48550/arXiv.2501.14421", "date": "2025-01-24", "title": "Reasoning about Weak Isolation Levels in Separation Logic", "authors": "Anders Alnor Mathiasen, L\u00e9on Gondelman, L\u00e9on Ducruet, Amin Timany, Lars Birkedal", "abstract": "Isolation levels, consistency guarantees among concurrently execution\ntransactions in local- and distributed systems, have been formalized in a\nnumber of models. Thus far, no model can reason about executable\nimplementations of databases or local transaction libraries providing weak\nisolation levels. Weak isolation levels are characterized by being highly\nconcurrent and, unlike their stronger counterpart serializability, they are not\nequivalent to the consistency guarantees provided by a transaction library\nimplemented using a global lock. In this paper, we formalize three weak\nisolation levels in separation logic, namely read uncommitted, read committed,\nand snapshot isolation. We define modular separation logic specifications that\nare independent of the underlying transaction library implementation.\nHistorically, isolation levels have been specified using examples of executions\nbetween concurrent transactions that are not allowed to occur, and we\ndemonstrate that our specifications correctly prohibit such examples. To show\nthat our specifications are realizable, we formally verify that an executable\nimplementation of a key-value database running the multi-version concurrency\ncontrol algorithm from the original snapshot isolation paper satisfies our\nspecification of snapshot isolation. Moreover, we prove implications between\nthe specifications -- snapshot isolation implies read committed and read\ncommitted implies read uncommitted -- and thus the verification effort of the\ndatabase serves as proof that all of our specifications are realizable. All\nresults are mechanised in the Coq proof assistant on top of the Iris separation\nlogic framework.", "journal": ""}
{"doi": "10.48550/arXiv.2101.02602", "date": "2021-01-07", "title": "Schemes in Lean", "authors": "Kevin Buzzard, Chris Hughes, Kenny Lau, Amelia Livingston, Ramon Fern\u00e1ndez Mir, Scott Morrison", "abstract": "We tell the story of how schemes were formalised in three different ways in\nthe Lean theorem prover.", "journal": ""}
{"doi": "10.48550/arXiv.2208.13428", "date": "2022-08-29", "title": "Constructive Many-one Reduction from the Halting Problem to Semi-unification (Extended Version)", "authors": "Andrej Dudenhefner", "abstract": "Semi-unification is the combination of first-order unification and\nfirst-order matching. The undecidability of semi-unification has been proven by\nKfoury, Tiuryn, and Urzyczyn in the 1990s by Turing reduction from Turing\nmachine immortality (existence of a diverging configuration). The particular\nTuring reduction is intricate, uses non-computational principles, and involves\nvarious intermediate models of computation. The present work gives a\nconstructive many-one reduction from the Turing machine halting problem to\nsemi-unification. This establishes RE-completeness of semi-unification under\nmany-one reductions. Computability of the reduction function, constructivity of\nthe argument, and correctness of the argument is witnessed by an axiom-free\nmechanization in the Coq proof assistant. Arguably, this serves as\ncomprehensive, precise, and surveyable evidence for the result at hand. The\nmechanization is incorporated into the existing, well-maintained Coq library of\nundecidability proofs. Notably, a variant of Hooper's argument for the\nundecidability of Turing machine immortality is part of the mechanization.", "journal": "Logical Methods in Computer Science, Volume 19, Issue 4 (December\n  8, 2023) lmcs:9977"}
{"doi": "10.48550/arXiv.1606.05937", "date": "2016-06-20", "title": "Formalization of Phase Ordering", "authors": "Tiago Cogumbreiro, Jun Shirako, Vivek Sarkar", "abstract": "Phasers pose an interesting synchronization mechanism that generalizes many\ncollective synchronization patterns seen in parallel programming languages,\nincluding barriers, clocks, and point-to-point synchronization using latches or\nsemaphores. This work characterizes scheduling constraints on phaser\noperations, by relating the execution state of two tasks that operate on the\nsame phaser. We propose a formalization of Habanero phasers,\nMay-Happen-In-Parallel, and Happens-Before relations for phaser operations, and\nshow that these relations conform with the semantics. Our formalization and\nproofs are fully mechanized using the Coq proof assistant, and are available\nonline.", "journal": "EPTCS 211, 2016, pp. 13-24"}
{"doi": "10.48550/arXiv.2203.00145", "date": "2022-02-28", "title": "Getting There and Back Again", "authors": "Olivier Danvy", "abstract": "\"There and Back Again\" (TABA) is a programming pattern where the recursive\ncalls traverse one data structure and the subsequent returns traverse another.\nThis article presents new TABA examples, refines existing ones, and formalizes\nboth their control flow and their data flow using the Coq Proof Assistant. Each\nformalization mechanizes a pen-and-paper proof, thus making it easier to \"get\"\nTABA. In addition, this article identifies and illustrates a tail-recursive\nvariant of TABA, There and Forth Again (TAFA) that does not come back but goes\nforth instead with more tail calls.", "journal": "Fundamenta Informaticae, Volume 185, Issue 2 (May 6, 2022) fi:9159"}
{"doi": "10.48550/arXiv.1111.3109", "date": "2011-11-14", "title": "A coinductive semantics of the Unlimited Register Machine", "authors": "Alberto Ciaffaglione", "abstract": "We exploit (co)inductive specifications and proofs to approach the evaluation\nof low-level programs for the Unlimited Register Machine (URM) within the Coq\nsystem, a proof assistant based on the Calculus of (Co)Inductive Constructions\ntype theory. Our formalization allows us to certify the implementation of\npartial functions, thus it can be regarded as a first step towards the\ndevelopment of a workbench for the formal analysis and verification of both\nconverging and diverging computations.", "journal": "EPTCS 73, 2011, pp. 49-63"}
{"doi": "10.48550/arXiv.1310.5539", "date": "2013-10-21", "title": "Directed Cycle Double Cover Conjecture: Fork Graphs", "authors": "Andrea Jim\u00e9nez, Martin Loebl", "abstract": "We explore the well-known Jaeger's directed cycle double cover conjecture\nwhich is equivalent to the assertion that every cubic bridgeless graph has an\nembedding on a closed orientable surface with no dual loop. We associate each\ncubic graph G with a novel object H that we call a \"hexagon graph\"; perfect\nmatchings of H describe all embeddings of G on closed orientable surfaces. The\nstudy of hexagon graphs leads us to define a new class of graphs that we call\n\"lean fork-graphs\". Fork graphs are cubic bridgeless graphs obtained from a\ntriangle by sequentially connecting fork-type graphs and performing Y-Delta,\nDelta-Y transformations; lean fork-graphs are fork graphs fulfilling a\nconnectivity property. We prove that Jaeger's conjecture holds for the class of\nlean fork-graphs. The class of lean fork-graphs is rich; namely, for each cubic\nbridgeless graph G there is a lean fork-graph containing a subdivision of G as\nan induced subgraph. Our results establish for the first time, to the best of\nour knowledge, the validity of Jaeger's conjecture in a broad inductively\ndefined class of graphs.", "journal": ""}
{"doi": "10.48550/arXiv.2408.09766", "date": "2024-08-19", "title": "From a Natural to a Formal Language with DSL Assistant", "authors": "My M. Mosthaf, Andrzej W\u0105sowski", "abstract": "The development of domain-specific languages (DSLs) is a laborious and\niterative process that seems to naturally lean to the use of generative\nartificial intelligence. We design and prototype DSL Assistant, a tool that\nintegrates generative language models to support the development of DSLs. DSL\nAssistant uses OpenAI's assistant API with GPT-4o to generate DSL grammars and\nexample instances. To reflect real-world use, DSL Assistant supports several\ndifferent interaction modes for evolving a DSL design, and includes automatic\nerror repair. Our experiments show that DSL Assistant helps users to create and\nmodify DSLs. However, the quality of the generated DSLs depends on the specific\ndomain and the followed interaction patterns.", "journal": ""}
{"doi": "10.48550/arXiv.2302.14491", "date": "2023-02-28", "title": "Formalization of $p$-adic $L$-functions in Lean 3", "authors": "Ashvni Narayanan", "abstract": "The Euler--Riemann zeta function is a largely studied numbertheoretic object,\nand the birthplace of several conjectures, such as the Riemann Hypothesis.\nDifferent approaches are used to study it, including $p$-adic analysis :\nderiving information from $p$-adic zeta functions. A generalized version of\n$p$-adic zeta functions (Riemann zeta function) are $p$-adic $L$-functions\n(resp. Dirichlet $L$-functions). This paper describes formalization of $p$-adic\n$L$-functions in an interactive theorem prover Lean 3. Kubota--Leopoldt\n$p$-adic $L$-functions are meromorphic functions emerging from the special\nvalues they take at negative integers in terms of generalized Bernoulli\nnumbers. They also take twisted values of the Dirichlet $L$-function at\nnegative integers. This work has never been done before in any theorem prover.\nOur work is done with the support of \\lean{mathlib} 3, one of Lean's\nmathematical libraries. It required formalization of a lot of associated\ntopics, such as Dirichlet characters, Bernoulli polynomials etc. We formalize\nthese first, then the definition of a $p$-adic $L$-function in terms of an\nintegral with respect to the Bernoulli measure, proving that they take the\nrequired values at negative integers.", "journal": ""}
{"doi": "10.48550/arXiv.2308.16797", "date": "2023-08-31", "title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation", "authors": "John Mendon\u00e7a, Patr\u00edcia Pereira, Helena Moniz, Jo\u00e3o Paulo Carvalho, Alon Lavie, Isabel Trancoso", "abstract": "Despite significant research effort in the development of automatic dialogue\nevaluation metrics, little thought is given to evaluating dialogues other than\nin English. At the same time, ensuring metrics are invariant to semantically\nsimilar responses is also an overlooked topic. In order to achieve the desired\nproperties of robustness and multilinguality for dialogue evaluation metrics,\nwe propose a novel framework that takes advantage of the strengths of current\nevaluation models with the newly-established paradigm of prompting Large\nLanguage Models (LLMs). Empirical results show our framework achieves state of\nthe art results in terms of mean Spearman correlation scores across several\nbenchmarks and ranks first place on both the Robust and Multilingual tasks of\nthe DSTC11 Track 4 \"Automatic Evaluation Metrics for Open-Domain Dialogue\nSystems\", proving the evaluation capabilities of prompted LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2201.05860", "date": "2022-01-15", "title": "View-Based Owicki-Gries Reasoning for Persistent x86-TSO (Extended Version)", "authors": "Eleni Vafeiadi Bila, Brijesh Dongol, Ori Lahav, Azalea Raad, John Wickerson", "abstract": "The rise of persistent memory is disrupting computing to its core. Our work\naims to help programmers navigate this brave new world by providing a program\nlogic for reasoning about x86 code that uses low-level operations such as\nmemory accesses and fences, as well as persistency primitives such as flushes.\nOur logic, Pierogi, benefits from a simple underlying operational semantics\nbased on views, is able to handle optimised flush operations, and is mechanised\nin the Isabelle/HOL proof assistant. We detail the proof rules of Pierogi and\nprove them sound. We also show how Pierogi can be used to reason about a range\nof challenging single- and multi-threaded persistent programs.", "journal": ""}
{"doi": "10.48550/arXiv.2404.14223", "date": "2024-04-22", "title": "Error Credits: Resourceful Reasoning about Error Bounds for Higher-Order Probabilistic Programs", "authors": "Alejandro Aguirre, Philipp G. Haselwarter, Markus de Medeiros, Kwing Hei Li, Simon Oddershede Gregersen, Joseph Tassarotti, Lars Birkedal", "abstract": "Probabilistic programs often trade accuracy for efficiency, and thus may,\nwith a small probability, return an incorrect result. It is important to obtain\nprecise bounds for the probability of these errors, but existing verification\napproaches have limitations that lead to error probability bounds that are\nexcessively coarse, or only apply to first-order programs. In this paper we\npresent Eris, a higher-order separation logic for proving error probability\nbounds for probabilistic programs written in an expressive higher-order\nlanguage. Our key novelty is the introduction of error credits, a separation\nlogic resource that tracks an upper bound on the probability that a program\nreturns an erroneous result. By representing error bounds as a resource, we\nrecover the benefits of separation logic, including compositionality,\nmodularity, and dependency between errors and program terms, allowing for more\nprecise specifications. Moreover, we enable novel reasoning principles such as\nexpectation-preserving error composition, amortized error reasoning, and error\ninduction. We illustrate the advantages of our approach by proving amortized\nerror bounds on a range of examples, including collision probabilities in hash\nfunctions, which allow us to write more modular specifications for data\nstructures that use them as clients. We also use our logic to prove correctness\nand almost-sure termination of rejection sampling algorithms. All of our\nresults have been mechanized in the Coq proof assistant using the Iris\nseparation logic framework and the Coquelicot real analysis library.", "journal": ""}
{"doi": "10.48550/arXiv.1611.03857", "date": "2016-11-11", "title": "The Physics of Motorcycles and Fast Bicycles : Lean, Stability and Counter-steering", "authors": "B Shayak", "abstract": "In this work I will obtain the system of nonlinear equations that correctly\ndescribes the motion of motorcycles and fast bicycles (above 30 km/hr) for the\nfirst time in literature. I will use it to calculate the lean angle during a\nturn and prove that the motion of the vehicle is unconditionally stable at all\noperating speeds under consideration. I will then employ it to give a\nquantitative model of counter-steering - the phenomenon by which a turning\nmobike first goes the wrong way and then starts going the right way.", "journal": ""}
{"doi": "10.48550/arXiv.2001.10834", "date": "2020-01-27", "title": "Smart Induction for Isabelle/HOL (System Description)", "authors": "Yutaka Nagashima", "abstract": "Proof assistants offer tactics to facilitate inductive proofs. However, it\nstill requires human ingenuity to decide what arguments to pass to those\ninduction tactics. To automate this process, we present smart_induct for\nIsabelle/HOL. Given an inductive problem in any problem domain, smart_induct\nlists promising arguments for the induct tactic without relying on a search.\nOur evaluation demonstrated smart_induct produces valuable recommendations\nacross problem domains.", "journal": ""}
{"doi": "10.48550/arXiv.1104.1998", "date": "2011-04-11", "title": "Amortised Resource Analysis with Separation Logic", "authors": "Robert Atkey", "abstract": "Type-based amortised resource analysis following Hofmann and Jost---where\nresources are associated with individual elements of data structures and doled\nout to the programmer under a linear typing discipline---have been successful\nin providing concrete resource bounds for functional programs, with good\nsupport for inference. In this work we translate the idea of amortised resource\nanalysis to imperative pointer-manipulating languages by embedding a logic of\nresources, based on the affine intuitionistic Logic of Bunched Implications,\nwithin Separation Logic. The Separation Logic component allows us to assert the\npresence and shape of mutable data structures on the heap, while the resource\ncomponent allows us to state the consumable resources associated with each\nmember of the structure. We present the logic on a small imperative language,\nbased on Java bytecode, with procedures and mutable heap. We have formalised\nthe logic and its soundness property within the Coq proof assistant and\nextracted a certified verification condition generator. We also describe an\nproof search procedure that allows generated verification conditions to be\ndischarged while using linear programming to infer consumable resource\nannotations. We demonstrate the logic on some examples, including proving the\ntermination of in-place list reversal on lists with cyclic tails.", "journal": "Logical Methods in Computer Science, Volume 7, Issue 2 (June 23,\n  2011) lmcs:685"}
{"doi": "10.48550/arXiv.1510.04748", "date": "2015-10-16", "title": "Formalization of the pumping lemma for context-free languages", "authors": "Marcus V. M. Ramos, Ruy J. G. B. de Queiroz, Nelma Moreira, Jos\u00e9 Carlos Bacelar Almeida", "abstract": "Context-free languages (CFLs) are highly important in computer language\nprocessing technology as well as in formal language theory. The Pumping Lemma\nis a property that is valid for all context-free languages, and is used to show\nthe existence of non context-free languages. This paper presents a\nformalization, using the Coq proof assistant, of the Pumping Lemma for\ncontext-free languages.", "journal": ""}
{"doi": "10.48550/arXiv.2010.15030", "date": "2020-10-28", "title": "Actris 2.0: Asynchronous Session-Type Based Reasoning in Separation Logic", "authors": "Jonas Kastberg Hinrichsen, Jesper Bengtson, Robbert Krebbers", "abstract": "Message passing is a useful abstraction for implementing concurrent programs.\nFor real-world systems, however, it is often combined with other programming\nand concurrency paradigms, such as higher-order functions, mutable state,\nshared-memory concurrency, and locks. We present Actris: a logic for proving\nfunctional correctness of programs that use a combination of the aforementioned\nfeatures. Actris combines the power of modern concurrent separation logics with\na first-class protocol mechanism -- based on session types -- for reasoning\nabout message passing in the presence of other concurrency paradigms. We show\nthat Actris provides a suitable level of abstraction by proving functional\ncorrectness of a variety of examples, including a channel-based merge sort, a\nchannel-based load-balancing mapper, and a variant of the map-reduce model,\nusing concise specifications. While Actris was already presented in a\nconference paper (POPL'20), this paper expands the prior presentation\nsignificantly. Moreover, it extends Actris to Actris 2.0 with a notion of\nsubprotocols -- based on session-type subtyping -- that permits additional\nflexibility when composing channel endpoints, and that takes full advantage of\nthe asynchronous semantics of message passing in Actris. Soundness of Actris\n2.0 is proven using a model of its protocol mechanism in the Iris framework. We\nhave mechanised the theory of Actris, together with custom tactics, as well as\nall examples in the paper, in the Coq proof assistant.", "journal": "Logical Methods in Computer Science, Volume 18, Issue 2 (June 10,\n  2022) lmcs:6869"}
{"doi": "10.48550/arXiv.0810.2179", "date": "2008-10-13", "title": "Structural abstract interpretation, A formal study using Coq", "authors": "Yves Bertot", "abstract": "interpreters are tools to compute approximations for behaviors of a program.\nThese approximations can then be used for optimisation or for error detection.\nIn this paper, we show how to describe an abstract interpreter using the\ntype-theory based theorem prover Coq, using inductive types for syntax and\nstructural recursive programming for the abstract interpreter's kernel. The\nabstract interpreter can then be proved correct with respect to a Hoare logic\nfor the programming language.", "journal": "Dans LERNET Summer School (2008)"}
{"doi": "10.48550/arXiv.2001.10490", "date": "2020-01-28", "title": "Beyond Notations: Hygienic Macro Expansion for Theorem Proving Languages", "authors": "Sebastian Ullrich, Leonardo de Moura", "abstract": "In interactive theorem provers (ITPs), extensible syntax is not only crucial\nto lower the cognitive burden of manipulating complex mathematical objects, but\nplays a critical role in developing reusable abstractions in libraries. Most\nITPs support such extensions in the form of restrictive \"syntax sugar\"\nsubstitutions and other ad hoc mechanisms, which are too rudimentary to support\nmany desirable abstractions. As a result, libraries are littered with\nunnecessary redundancy. Tactic languages in these systems are plagued by a\nseemingly unrelated issue: accidental name capture, which often produces\nunexpected and counterintuitive behavior. We take ideas from the Scheme family\nof programming languages and solve these two problems simultaneously by\nproposing a novel hygienic macro system custom-built for ITPs. We further\ndescribe how our approach can be extended to cover type-directed macro\nexpansion resulting in a single, uniform system offering multiple abstraction\nlevels that range from supporting simplest syntax sugars to elaboration of\nformerly baked-in syntax. We have implemented our new macro system and\nintegrated it into the new version of the Lean theorem prover, Lean 4. Despite\nits expressivity, the macro system is simple enough that it can easily be\nintegrated into other systems.", "journal": "Logical Methods in Computer Science, Volume 18, Issue 2 (April 13,\n  2022) lmcs:7421"}
{"doi": "10.48550/arXiv.1510.09092", "date": "2015-10-30", "title": "Formalization of context-free language theory", "authors": "Marcus V. M. Ramos, Ruy J. G. B. de Queiroz, Nelma Moreira, Jos\u00e9 Carlos Bacelar Almeida", "abstract": "Context-free language theory is a subject of high importance in computer\nlanguage processing technology as well as in formal language theory. This paper\npresents a formalization, using the Coq proof assistant, of fundamental results\nrelated to context-free grammars and languages. These include closure\nproperties (union, concatenation and Kleene star), grammar simplification\n(elimination of useless symbols inaccessible symbols, empty rules and unit\nrules) and the existence of a Chomsky Normal Form for context-free grammars.", "journal": ""}
{"doi": "10.48550/arXiv.1310.2338", "date": "2013-10-09", "title": "Certified proofs in programs involving exceptions", "authors": "Jean-Guillaume Dumas, Dominique Duval, Burak Ekici, Jean-Claude Reynaud", "abstract": "Exception handling is provided by most modern programming languages. It\nallows to deal with anomalous or exceptional events which require special\nprocessing. In computer algebra, exception handling is an efficient way to\nimplement the dynamic evaluation paradigm: for instance, in linear algebra,\ndynamic evaluation can be used for applying programs which have been written\nfor matrices with coefficients in a field to matrices with coefficients in a\nring. Thus, a proof system for computer algebra should include a treatement of\nexceptions, which must rely on a careful description of a semantics of\nexceptions. The categorical notion of monad can be used for formalizing the\nraising of exceptions: this has been proposed by Moggi and implemented in\nHaskell. In this paper, we provide a proof system for exceptions which involves\nboth raising and handling, by extending Moggi's approach. Moreover, the core\npart of this proof system is dual to a proof system for side effects in\nimperative languages, which relies on the categorical notion of comonad. Both\nproof systems are implemented in the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2009.11403", "date": "2020-09-23", "title": "CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq", "authors": "Koundinya Vajjha, Avraham Shinnar, Vasily Pestun, Barry Trager, Nathan Fulton", "abstract": "Reinforcement learning algorithms solve sequential decision-making problems\nin probabilistic environments by optimizing for long-term reward. The desire to\nuse reinforcement learning in safety-critical settings inspires a recent line\nof work on formally constrained reinforcement learning; however, these methods\nplace the implementation of the learning algorithm in their Trusted Computing\nBase. The crucial correctness property of these implementations is a guarantee\nthat the learning algorithm converges to an optimal policy. This paper begins\nthe work of closing this gap by developing a Coq formalization of two canonical\nreinforcement learning algorithms: value and policy iteration for finite state\nMarkov decision processes. The central results are a formalization of Bellman's\noptimality principle and its proof, which uses a contraction property of\nBellman optimality operator to establish that a sequence converges in the\ninfinite horizon limit. The CertRL development exemplifies how the Giry monad\nand mechanized metric coinduction streamline optimality proofs for\nreinforcement learning algorithms. The CertRL library provides a general\nframework for proving properties about Markov decision processes and\nreinforcement learning algorithms, paving the way for further work on\nformalization of reinforcement learning algorithms.", "journal": ""}
{"doi": "10.48550/arXiv.9511103", "date": "2001-03-29", "title": "A Concrete Final Coalgebra Theorem for ZF Set Theory", "authors": "Lawrence C. Paulson", "abstract": "A special final coalgebra theorem, in the style of Aczel's, is proved within\nstandard Zermelo-Fraenkel set theory. Aczel's Anti-Foundation Axiom is replaced\nby a variant definition of function that admits non-well-founded constructions.\nVariant ordered pairs and tuples, of possibly infinite length, are special\ncases of variant functions. Analogues of Aczel's Solution and Substitution\nLemmas are proved in the style of Rutten and Turi. The approach is less general\nthan Aczel's, but the treatment of non-well-founded objects is simple and\nconcrete. The final coalgebra of a functor is its greatest fixedpoint. The\ntheory is intended for machine implementation and a simple case of it is\nalready implemented using the theorem prover Isabelle.", "journal": "published in P. Dybjer, B. Nordstrm and J. Smith (editors), Types\n  for Proofs and Programs '94 (Springer LNCS 996, published 1995), 120-139"}
{"doi": "10.48550/arXiv.2404.08494", "date": "2024-04-12", "title": "Almost-Sure Termination by Guarded Refinement", "authors": "Simon Oddershede Gregersen, Alejandro Aguirre, Philipp G. Haselwarter, Joseph Tassarotti, Lars Birkedal", "abstract": "Almost-sure termination is an important correctness property for\nprobabilistic programs, and a number of program logics have been developed for\nestablishing it. However, these logics have mostly been developed for\nfirst-order programs written in languages with specific syntactic patterns for\nlooping. In this paper, we consider almost-sure termination for higher-order\nprobabilistic programs with general references. This combination of features\nallows for recursion and looping to be encoded through a variety of patterns.\nTherefore, rather than developing proof rules for reasoning about particular\nrecursion patterns, we instead propose an approach based on proving refinement\nbetween a higher-order program and a simpler probabilistic model, in such a way\nthat the refinement preserves termination behavior. By proving a refinement,\nalmost-sure termination behavior of the program can then be established by\nanalyzing the simpler model. We present this approach in the form of Caliper, a\nhigher-order separation logic for proving termination-preserving refinements.\nCaliper uses probabilistic couplings to carry out relational reasoning between\na program and a model. To handle the range of recursion patterns found in\nhigher-order programs, Caliper uses guarded recursion, in particular the\nprinciple of L\\\"ob induction. A technical novelty is that Caliper does not\nrequire the use of transfinite step indexing or other technical restrictions\nfound in prior work on guarded recursion for termination-preservation\nrefinement. We demonstrate the flexibility of this approach by proving\nalmost-sure termination of several examples, including first-order loop\nconstructs, a random list generator, treaps, and a sampler for Galton-Watson\ntrees that uses higher-order store. All the results have been mechanized in the\nCoq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1509.02032", "date": "2015-09-07", "title": "Formalization of simplification for context-free grammars", "authors": "Marcus V. M. Ramos, Ruy J. G. B. de Queiroz", "abstract": "Context-free grammar simplification is a subject of high importance in\ncomputer language processing technology as well as in formal language theory.\nThis paper presents a formalization, using the Coq proof assistant, of the fact\nthat general context-free grammars generate languages that can be also\ngenerated by simpler and equivalent context-free grammars. Namely, useless\nsymbol elimination, inaccessible symbol elimination, unit rules elimination and\nempty rules elimination operations were described and proven correct with\nrespect to the preservation of the language generated by the original grammar.", "journal": ""}
{"doi": "10.48550/arXiv.2401.11904", "date": "2024-01-22", "title": "Towards an Independent Version of Tarski's System of Geometry", "authors": "Pierre Boutry, St\u00e9phane Kastenbaum, Cl\u00e9ment Saintier", "abstract": "In 1926-1927, Tarski designed a set of axioms for Euclidean geometry which\nreached its final form in a manuscript by Schwabh\\\"auser, Szmielew and Tarski\nin 1983. The differences amount to simplifications obtained by Tarski and\nGupta. Gupta presented an independent version of Tarski's system of geometry,\nthus establishing that his version could not be further simplified without\nmodifying the axioms. To obtain the independence of one of his axioms, namely\nPasch's axiom, he proved the independence of one of its consequences: the\npreviously eliminated symmetry of betweenness. However, an independence model\nfor the non-degenerate part of Pasch's axiom was provided by Szczerba for\nanother version of Tarski's system of geometry in which the symmetry of\nbetweenness holds. This independence proof cannot be directly used for Gupta's\nversion as the statements of the parallel postulate differ.\n  In this paper, we present our progress towards obtaining an independent\nversion of a variant of Gupta's system. Compared to Gupta's version, we split\nPasch's axiom into this previously eliminated axiom and its non-degenerate part\nand change the statement of the parallel postulate. We verified the\nindependence properties by mechanizing counter-models using the Coq\nproof-assistant.", "journal": "EPTCS 398, 2024, pp. 73-84"}
{"doi": "10.48550/arXiv.1905.06604", "date": "2019-05-16", "title": "Making Agile Development Processes fit for V-style Certification Procedures", "authors": "Sergio Bezzecchi, Paolo Crisafulli, Charlotte Pichot, Burkhart Wolff", "abstract": "We present a process for the development of safety and security critical\ncomponents in transportation systems targeting a high-level certification\n(CENELEC 50126/50128, DO 178, CC ISO/IEC 15408). The process adheres to the\nobjectives of an \"agile development\" in terms of evolutionary flexibility and\ncontinuous improvement. Yet, it enforces the overall coherence of the\ndevelopment artifacts (ranging from proofs over tests to code) by a particular\nenvironment (CVCE). In particular, the validation process is built around a\nformal development based on the interactive theorem proving system\nIsabelle/HOL, by linking the business logic of the application to the operating\nsystem model, down to code and concrete hardware models thanks to a series of\nrefinement proofs. We apply both the process and its support in CVCE to a\ncase-study that comprises a model of an odometric service in a railway-system\nwith its corresponding implementation integrated in seL4 (a secure kernel for\nwhich a comprehensive Isabelle development exists). Novel techniques\nimplemented in Isabelle enforce the coherence of semi-formal and formal\ndefinitions within specific certification processes in order to improve their\ncost-effectiveness . This paper has been published at ERTS2018.", "journal": ""}
{"doi": "10.48550/arXiv.2004.07390", "date": "2020-04-15", "title": "Trakhtenbrot's Theorem in Coq, A Constructive Approach to Finite Model Theory", "authors": "Dominik Kirst, Dominique Larchey-Wendling", "abstract": "We study finite first-order satisfiability (FSAT) in the constructive setting\nof dependent type theory. Employing synthetic accounts of enumerability and\ndecidability, we give a full classification of FSAT depending on the\nfirst-order signature of non-logical symbols. On the one hand, our development\nfocuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as\nthe signature contains an at least binary relation symbol. Our proof proceeds\nby a many-one reduction chain starting from the Post correspondence problem. On\nthe other hand, we establish the decidability of FSAT for monadic first-order\nlogic, i.e. where the signature only contains at most unary function and\nrelation symbols, as well as the enumerability of FSAT for arbitrary enumerable\nsignatures. All our results are mechanised in the framework of a growing Coq\nlibrary of synthetic undecidability proofs.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12615", "date": "2025-02-18", "title": "Generalized Hostadter functions G, H and beyond: numeration systems and discrepancy", "authors": "Pierre Letouzey", "abstract": "Hofstadter's G function is recursively defined via $G(0)=0$ and then\n$G(n)=n-G(G(n-1))$. Following Hofstadter, a family $(F_k)$ of similar functions\nis obtained by varying the number $k$ of nested recursive calls in this\nequation. We study here some Fibonacci-like sequences that are deeply connected\nwith these functions $F_k$. In particular, the Zeckendorf theorem can be\nadapted to provide digital expansions via sums of terms of these sequences. On\nthese digital expansions, the functions $F_k$ are acting as right shifts of the\ndigits. These Fibonacci-like sequences can be expressed in terms of zeros of\nthe polynomial $X^k{-}X^{k-1}{-}1$. Considering now the discrepancy of each\nfunction $F_k$, i.e., the maximal distance between $F_k$ and its linear\nequivalent, we retrieve the fact that this discrepancy is finite exactly when\n$k \\le 4$. Thanks to that, we solve two twenty-year-old OEIS conjectures\nstating how close the functions $F_3$ and $F_4$ are from the integer parts of\ntheir linear equivalents. Moreover we establish that $F_k$ can coincide exactly\nwith such an integer part only when $k\\le 2$, while $F_k$ is almost additive\nexactly when $k \\le 4$. Finally, a nice fractal shape a la Rauzy has been\nencountered when investigating the discrepancy of $F_3$. Almost all this\narticle has been formalized and verified in the Coq/Rocq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2212.03129", "date": "2022-12-06", "title": "Formally Verified Native Code Generation in an Effectful JIT -- or: Turning the CompCert Backend into a Formally Verified JIT Compiler", "authors": "Aur\u00e8le Barri\u00e8re, Sandrine Blazy, David Pichardie", "abstract": "Modern Just-in-Time compilers (or JITs) typically interleave several\nmechanisms to execute a program. For faster startup times and to observe the\ninitial behavior of an execution, interpretation can be initially used. But\nafter a while, JITs dynamically produce native code for parts of the program\nthey execute often. Although some time is spent compiling dynamically, this\nmechanism makes for much faster times for the remaining of the program\nexecution. Such compilers are complex pieces of software with various\ncomponents, and greatly rely on a precise interplay between the different\nlanguages being executed, including on-stack-replacement. Traditional static\ncompilers like CompCert have been mechanized in proof assistants, but JITs have\nbeen scarcely formalized so far, partly due to their impure nature and their\nnumerous components. This work presents a model JIT with dynamic generation of\nnative code, implemented and formally verified in Coq. Although some parts of a\nJIT cannot be written in Coq, we propose a proof methodology to delimit,\nspecify and reason on the impure effects of a JIT. We argue that the daunting\ntask of formally verifying a complete JIT should draw on existing proofs of\nnative code generation. To this end, our work successfully reuses CompCert and\nits correctness proofs during dynamic compilation. Finally, our prototype can\nbe extracted and executed.", "journal": ""}
{"doi": "10.48550/arXiv.1712.10213", "date": "2017-12-29", "title": "Unifying Theories of Time with Generalised Reactive Processes", "authors": "Simon Foster, Ana Cavalcanti, Jim Woodcock, Frank Zeyda", "abstract": "Hoare and He's theory of reactive processes provides a unifying foundation\nfor the formal semantics of concurrent and reactive languages. Though highly\napplicable, their theory is limited to models that can express event histories\nas discrete sequences. In this paper, we show how their theory can be\ngeneralised by using an abstract trace algebra. We show how the algebra,\nnotably, allows us to also consider continuous-time traces and thereby\nfacilitate models of hybrid systems. We then use this algebra to reconstruct\nthe theory of reactive processes in our generic setting, and prove\ncharacteristic laws for sequential and parallel processes, all of which have\nbeen mechanically verified in the Isabelle/HOL proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.0901.3619", "date": "2009-01-23", "title": "Mechanized semantics for the Clight subset of the C language", "authors": "Sandrine Blazy, Xavier Leroy", "abstract": "This article presents the formal semantics of a large subset of the C\nlanguage called Clight. Clight includes pointer arithmetic, \"struct\" and\n\"union\" types, C loops and structured \"switch\" statements. Clight is the source\nlanguage of the CompCert verified compiler. The formal semantics of Clight is a\nbig-step operational semantics that observes both terminating and diverging\nexecutions and produces traces of input/output events. The formal semantics of\nClight is mechanized using the Coq proof assistant. In addition to the\nsemantics of Clight, this article describes its integration in the CompCert\nverified compiler and several ways by which the semantics was validated.", "journal": "Journal of Automated Reasoning 43, 3 (2009) 263-288"}
{"doi": "10.48550/arXiv.1905.04444", "date": "2019-05-11", "title": "Partisan Lean of States: Electoral College and Popular Vote", "authors": "Andrey Sarantsev", "abstract": "We compare federal election results for each state versus the USA in every\nsecond year from 1992 to 2018, to model partisan lean of each state and its\ndependence on the nationwide popular vote. For each state, we model both its\ncurrent partisan lean and its rate of change, as well as sensitivity of state\nresults with respect to the nationwide popular vote, using Bayesian linear\nregression. We apply this to simulate the Electoral College outcome in 2020,\ngiven even (equal) nationwide popular vote, as well as 2016, 2008, and 2004\nnationwide popular vote. We backtest 2012 and 2016 elections given actual\npopular vote. Taking equal popular vote for two major parties, we prove that\nthe Electoral College is biased towards Republicans.", "journal": ""}
{"doi": "10.48550/arXiv.2410.10259", "date": "2024-10-14", "title": "Data Models of German Lute Tablature With TScore", "authors": "Markus Lepper, Baltasar Tranc\u00f3n Widemann", "abstract": "TScore is both an abstract formalism and its computer implementation to\nconstruct models of arbitrary kinds of time-related data. It is a research\nproject about the semantics of musical notation, applying the method of\ncomputer-aided re-modelling to diverse formalisms and semantics of time-related\ndata. Here we present the application to German tablature notation. While the\ncurrent implemention is merely a proof of concept, the lean architecture of\nTScore allows easy adaptation and extension.", "journal": ""}
{"doi": "10.48550/arXiv.2212.11396", "date": "2022-12-21", "title": "ABODE-Net: An Attention-based Deep Learning Model for Non-intrusive Building Occupancy Detection Using Smart Meter Data", "authors": "Zhirui Luo, Ruobin Qi, Qingqing Li, Jun Zheng, Sihua Shao", "abstract": "Occupancy information is useful for efficient energy management in the\nbuilding sector. The massive high-resolution electrical power consumption data\ncollected by smart meters in the advanced metering infrastructure (AMI) network\nmake it possible to infer buildings' occupancy status in a non-intrusive way.\nIn this paper, we propose a deep leaning model called ABODE-Net which employs a\nnovel Parallel Attention (PA) block for building occupancy detection using\nsmart meter data. The PA block combines the temporal, variable, and channel\nattention modules in a parallel way to signify important features for occupancy\ndetection. We adopt two smart meter datasets widely used for building occupancy\ndetection in our performance evaluation. A set of state-of-the-art shallow\nmachine learning and deep learning models are included for performance\ncomparison. The results show that ABODE-Net significantly outperforms other\nmodels in all experimental cases, which proves its validity as a solution for\nnon-intrusive building occupancy detection.", "journal": ""}
{"doi": "10.48550/arXiv.2006.13613", "date": "2020-06-24", "title": "Formalizing the Soundness of the Encoding Methods of SAT-based Model Checking", "authors": "Daisuke Ishii, Saito Fujii", "abstract": "One of the effective model checking methods is to utilize the efficient\ndecision procedure of SAT (or SMT) solvers. In a SAT-based model checking, a\nsystem and its property are encoded into a set of logic formulas and the safety\nis checked based on the satisfiability of the formulas. As the encoding methods\nare improved and crafted (e.g., k-induction and IC3/PDR), verifying their\ncorrectness becomes more important. This research aims at a formal verification\nof the SMC methods using the Coq proof assistant. Our contributions are\ntwofold: (1) We specify the basic encoding methods, k-induction and (a\nsimplified version of) IC3/PDR in Coq as a set of simple and modular encoding\npredicates. (2) We provide a formal proof of the soundness of the encoding\nmethods based on our formalized lemmas on state sequences and paths.", "journal": ""}
{"doi": "10.48550/arXiv.1309.5149", "date": "2013-09-20", "title": "Pretty-big-step-semantics-based Certified Abstract Interpretation (Preliminary version)", "authors": "Martin Bodin, Thomas Jensen, Alan Schmitt", "abstract": "We present a technique for deriving semantic program analyses from a natural\nsemantics specification of the programming language. The technique is based on\na particular kind of semantics called pretty-big-step semantics. We present a\npretty-big-step semantics of a language with simple objects called O'While and\nspecify a series of instrumentations of the semantics that explicitates the\nflows of values in a program. This leads to a semantics-based dependency\nanalysis, at the core, e.g., of tainting analysis in software security. The\nformalization has been realized with the Coq proof assistant.", "journal": "EPTCS 129, 2013, pp. 360-383"}
{"doi": "10.48550/arXiv.1103.0431", "date": "2011-03-02", "title": "Fast Convergence Rate of Multiple Kernel Learning with Elastic-net Regularization", "authors": "Taiji Suzuki, Ryota Tomioka, Masashi Sugiyama", "abstract": "We investigate the learning rate of multiple kernel leaning (MKL) with\nelastic-net regularization, which consists of an $\\ell_1$-regularizer for\ninducing the sparsity and an $\\ell_2$-regularizer for controlling the\nsmoothness. We focus on a sparse setting where the total number of kernels is\nlarge but the number of non-zero components of the ground truth is relatively\nsmall, and prove that elastic-net MKL achieves the minimax learning rate on the\n$\\ell_2$-mixed-norm ball. Our bound is sharper than the convergence rates ever\nshown, and has a property that the smoother the truth is, the faster the\nconvergence rate is.", "journal": ""}
{"doi": "10.48550/arXiv.1610.08685", "date": "2016-10-27", "title": "A Framework for Certified Self-Stabilization", "authors": "Karine Altisen, Pierre Corbineau, Stephane Devismes", "abstract": "We propose a general framework to build certified proofs of distributed\nself-stabilizing algorithms with the proof assistant Coq. We first define in\nCoq the locally shared memory model with composite atomicity, the most commonly\nused model in the self-stabilizing area. We then validate our framework by\ncertifying a non trivial part of an existing silent self-stabilizing algorithm\nwhich builds a $k$-clustering of the network. We also certify a quantitative\nproperty related to the output of this algorithm. Precisely, we show that the\ncomputed $k$-clustering contains at most $\\lfloor \\frac{n-1}{k+1} \\rfloor + 1$\nclusterheads, where $n$ is the number of nodes in the network. To obtain these\nresults, we also developed a library which contains general tools related to\npotential functions and cardinality of sets.", "journal": "Logical Methods in Computer Science, Volume 13, Issue 4 (November\n  28, 2017) lmcs:2183"}
{"doi": "10.48550/arXiv.1612.02353", "date": "2016-12-07", "title": "Efficient Certified RAT Verification", "authors": "Lu\u00eds Cruz-Filipe, Marijn Heule, Warren Hunt, Matt Kaufmann, Peter Schneider-Kamp", "abstract": "Clausal proofs have become a popular approach to validate the results of SAT\nsolvers. However, validating clausal proofs in the most widely supported format\n(DRAT) is expensive even in highly optimized implementations. We present a new\nformat, called LRAT, which extends the DRAT format with hints that facilitate a\nsimple and fast validation algorithm. Checking validity of LRAT proofs can be\nimplemented using trusted systems such as the languages supported by theorem\nprovers. We demonstrate this by implementing two certified LRAT checkers, one\nin Coq and one in ACL2.", "journal": ""}
{"doi": "10.48550/arXiv.2302.00744", "date": "2023-02-01", "title": "A Formal Algebraic Framework for DSL Composition", "authors": "Zachary Flores, Angelo Taranto, Eric Bond", "abstract": "We discuss a formal framework for using algebraic structures to model a\nmeta-language that can write, compose, and provide interoperability between\nabstractions of DSLs. The purpose of this formal framework is to provide a\nverification of compositional properties of the meta-language. Throughout our\npaper we discuss the construction of this formal framework, as well its\nrelation to our team's work on the DARPA V-SPELLS program via the pipeline we\nhave developed for completing our verification tasking on V-SPELLS. We aim to\ngive a broad overview of this verification pipeline in our paper. The pipeline\ncan be split into four main components: the first is providing a formal model\nof the meta-language in Coq; the second is to give a specification in Coq of\nour chosen algebraic structures; third, we need to implement specific instances\nof our algebraic structures in Coq, as well as give a proof in Coq that this\nimplementation is an algebraic structure according to our specification in the\nsecond step; and lastly, we need to give a proof in Coq that the formal model\nfor the meta-language in the first step is an instance of the implementation in\nthe third step.", "journal": ""}
{"doi": "10.48550/arXiv.2003.09993", "date": "2020-03-22", "title": "A Trustful Monad for Axiomatic Reasoning with Probability and Nondeterminism", "authors": "Reynald Affeldt, Jacques Garrigue, David Nowak, Takafumi Saikawa", "abstract": "The algebraic properties of the combination of probabilistic choice and\nnondeterministic choice have long been a research topic in program semantics.\nThis paper explains a formalization in the Coq proof assistant of a monad\nequipped with both choices: the geometrically convex monad. This formalization\nhas an immediate application: it provides a model for a monad that implements a\nnon-trivial interface which allows for proofs by equational reasoning using\nprobabilistic and nondeterministic effects. We explain the technical choices we\nmade to go from the literature to a complete Coq formalization, from which we\nidentify reusable theories about mathematical structures such as convex spaces\nand concrete categories, and that we integrate in a framework for monadic\nequational reasoning.", "journal": "Journal of Functional Programming, 31(E17), 2021"}
{"doi": "10.48550/arXiv.2403.11919", "date": "2024-03-18", "title": "A Coq Mechanization of JavaScript Regular Expression Semantics", "authors": "No\u00e9 De Santo, Aur\u00e8le Barri\u00e8re, Cl\u00e9ment Pit-Claudel", "abstract": "We present an executable, proven-safe, faithful, and future-proof Coq\nmechanization of JavaScript regular expression (regex) matching, as specified\nby the latest published edition of ECMA-262 section 22.2. This is, to our\nknowledge, the first time that an industrial-strength regex language has been\nfaithfully mechanized in an interactive theorem prover. We highlight\ninteresting challenges that arose in the process (including issues of encoding,\ncorner cases, and executability), and we document the steps that we took to\nensure that the result is straightforwardly auditable and that our\nunderstanding of the specification aligns with existing implementations.\n  We demonstrate the usability and versatility of the mechanization through a\nbroad collection of analyses, case studies, and experiments: we prove that\nJavaScript regex matching always terminates and is safe (no assertion\nfailures); we identify subtle corner cases that led to mistakes in previous\npublications; we verify an optimization extracted from a state-of-the-art regex\nengine; we show that some classic properties described in automata textbooks\nand used in derivatives-based matchers do not hold in JavaScript regexes; and\nwe demonstrate that the cost of updating the mechanization to account for\nchanges in the original specification is reasonably low.\n  Our mechanization can be extracted to OCaml and JavaScript and linked with\nUnicode libraries to produce an executable regex engine that passes the\nrelevant parts of the official Test262 conformance test suite.", "journal": ""}
{"doi": "10.48550/arXiv.2001.04559", "date": "2020-01-13", "title": "Boosting Deep Face Recognition via Disentangling Appearance and Geometry", "authors": "Ali Dabouei, Fariborz Taherkhani, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi", "abstract": "In this paper, we propose a framework for disentangling the appearance and\ngeometry representations in the face recognition task. To provide supervision\nfor this aim, we generate geometrically identical faces by incorporating\nspatial transformations. We demonstrate that the proposed approach enhances the\nperformance of deep face recognition models by assisting the training process\nin two ways. First, it enforces the early and intermediate convolutional layers\nto learn more representative features that satisfy the properties of\ndisentangled embeddings. Second, it augments the training set by altering faces\ngeometrically. Through extensive experiments, we demonstrate that integrating\nthe proposed approach into state-of-the-art face recognition methods\neffectively improves their performance on challenging datasets, such as LFW,\nYTF, and MegaFace. Both theoretical and practical aspects of the method are\nanalyzed rigorously by concerning ablation studies and knowledge transfer\ntasks. Furthermore, we show that the knowledge leaned by the proposed method\ncan favor other face-related tasks, such as attribute prediction.", "journal": ""}
{"doi": "10.48550/arXiv.2006.16711", "date": "2020-06-30", "title": "Binary intersection formalized", "authors": "\u0160t\u011bp\u00e1n Holub, \u0160t\u011bp\u00e1n Starosta", "abstract": "We provide a reformulation and a formalization of the classical result by\nJuhani Karhum\\\"aki characterizing intersections of two languages of the form\n$\\{x,y\\}^*\\cap \\{u,v\\}^*$. We use the terminology of morphisms which allows to\nformulate the result in a shorter and more transparent way, and we formalize\nthe result in the proof assistant Isabelle/HOL.", "journal": "Theoretical Computer Science 866 (2021) 14-24"}
{"doi": "10.48550/arXiv.2407.14107", "date": "2024-07-19", "title": "Approximate Relational Reasoning for Higher-Order Probabilistic Programs", "authors": "Philipp G. Haselwarter, Kwing Hei Li, Alejandro Aguirre, Simon Oddershede Gregersen, Joseph Tassarotti, Lars Birkedal", "abstract": "Properties such as provable security and correctness for randomized programs\nare naturally expressed relationally as approximate equivalences. As a result,\na number of relational program logics have been developed to reason about such\napproximate equivalences of probabilistic programs. However, existing\napproximate relational logics are mostly restricted to first-order programs\nwithout general state.\n  In this paper we develop Approxis, a higher-order approximate relational\nseparation logic for reasoning about approximate equivalence of programs\nwritten in an expressive ML-like language with discrete probabilistic sampling,\nhigher-order functions, and higher-order state. The Approxis logic recasts the\nconcept of error credits in the relational setting to reason about relational\napproximation, which allows for expressive notions of modularity and\ncomposition, a range of new approximate relational rules, and an\ninternalization of a standard limiting argument for showing exact probabilistic\nequivalences by approximation. We also use Approxis to develop a logical\nrelation model that quantifies over error credits, which can be used to prove\nexact contextual equivalence. We demonstrate the flexibility of our approach on\na range of examples, including the PRP/PRF switching lemma, IND\\$-CPA security\nof an encryption scheme, and a collection of rejection samplers. All of the\nresults have been mechanized in the Coq proof assistant and the Iris separation\nlogic framework.", "journal": "Proc. ACM Program. Lang. 9, POPL, Article 41 (January 2025)"}
{"doi": "10.48550/arXiv.1907.05818", "date": "2019-07-12", "title": "Verified Self-Explaining Computation", "authors": "Jan Stolarek, James Cheney", "abstract": "Common programming tools, like compilers, debuggers, and IDEs, crucially rely\non the ability to analyse program code to reason about its behaviour and\nproperties. There has been a great deal of work on verifying compilers and\nstatic analyses, but far less on verifying dynamic analyses such as program\nslicing. Recently, a new mathematical framework for slicing was introduced in\nwhich forward and backward slicing are dual in the sense that they constitute a\nGalois connection. This paper formalises forward and backward dynamic slicing\nalgorithms for a simple imperative programming language, and formally verifies\ntheir duality using the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2203.16211", "date": "2022-03-30", "title": "Lay-it-out: Interactive Design of Layout-Sensitive Grammars", "authors": "Fengmin Zhu, Jiangyi Liu, Fei He", "abstract": "Layout-sensitive grammars have been adopted in many modern programming\nlanguages. However, tool support for this kind of grammars still remains\nlimited and immature. In this paper, we present Lay-it-out, an interactive\nframework for layout-sensitive grammar design. Beginning with a user-defined\nambiguous grammar, our framework refines it by synthesizing layout constraints\nthrough user interaction. For ease of interaction, a shortest nonempty\nambiguous sentence (if exists) is automatically generated by our bounded\nambiguity checker via SMT solving. The soundness and completeness of our SMT\nencoding are mechanized in the Coq proof assistant. Case studies on real\ngrammars, including a full grammar, demonstrate the practicality and\nscalability of our approach.", "journal": ""}
{"doi": "10.48550/arXiv.1508.00455", "date": "2015-07-29", "title": "Equations for Hereditary Substitution in Leivant's Predicative System F: A Case Study", "authors": "Cyprien Mangin, Matthieu Sozeau", "abstract": "This paper presents a case study of formalizing a normalization proof for\nLeivant's Predicative System F using the Equations package. Leivant's\nPredicative System F is a stratified version of System F, where type\nquantification is annotated with kinds representing universe levels. A weaker\nvariant of this system was studied by Stump & Eades, employing the hereditary\nsubstitution method to show normalization. We improve on this result by showing\nnormalization for Leivant's original system using hereditary substitutions and\na novel multiset ordering on types. Our development is done in the Coq proof\nassistant using the Equations package, which provides an interface to define\ndependently-typed programs with well-founded recursion and full dependent\npattern- matching. Equations allows us to define explicitly the hereditary\nsubstitution function, clarifying its algorithmic behavior in presence of term\nand type substitutions. From this definition, consistency can easily be\nderived. The algorithmic nature of our development is crucial to reflect\nlanguages with type quantification, enlarging the class of languages on which\nreflection methods can be used in the proof assistant.", "journal": "EPTCS 185, 2015, pp. 71-86"}
{"doi": "10.48550/arXiv.2501.07920", "date": "2025-01-14", "title": "Coinductive Proofs for Temporal Hyperliveness", "authors": "Arthur Correnson, Bernd Finkbeiner", "abstract": "Temporal logics for hyperproperties have recently emerged as an expressive\nspecification technique for relational properties of reactive systems. While\nthe model checking problem for such logics has been widely studied, there is a\nscarcity of deductive proof systems for temporal hyperproperties. In\nparticular, hyperproperties with an alternation of universal and existential\nquantification over system executions are rarely supported. In this paper, we\nfocus on the difficult class of hyperproperties of the form\n$\\forall^*\\exists^*\\psi$, where $\\psi$ is a safety relation. We show that\nhyperproperties of this class -- which includes many hyperliveness properties\nof interest -- can always be approximated by coinductive relations. This\nenables intuitive proofs by coinduction. Based on this observation, we define\nHyCo (HYperproperties, COinductively), a mechanized framework to reason about\ntemporal hyperproperties within the Coq proof assistant. We detail the\nconstruction of HyCo, provide a proof of its soundness, and exemplify its use\nby applying it to the verification of reactive systems modeled as imperative\nprograms with nondeterminism and I/O.", "journal": ""}
{"doi": "10.48550/arXiv.1905.10289", "date": "2019-05-24", "title": "MatchZoo: A Learning, Practicing, and Developing System for Neural Text Matching", "authors": "Jiafeng Guo, Yixing Fan, Xiang Ji, Xueqi Cheng", "abstract": "Text matching is the core problem in many natural language processing (NLP)\ntasks, such as information retrieval, question answering, and conversation.\nRecently, deep leaning technology has been widely adopted for text matching,\nmaking neural text matching a new and active research domain. With a large\nnumber of neural matching models emerging rapidly, it becomes more and more\ndifficult for researchers, especially those newcomers, to learn and understand\nthese new models. Moreover, it is usually difficult to try these models due to\nthe tedious data pre-processing, complicated parameter configuration, and\nmassive optimization tricks, not to mention the unavailability of public codes\nsometimes. Finally, for researchers who want to develop new models, it is also\nnot an easy task to implement a neural text matching model from scratch, and to\ncompare with a bunch of existing models. In this paper, therefore, we present a\nnovel system, namely MatchZoo, to facilitate the learning, practicing and\ndesigning of neural text matching models. The system consists of a powerful\nmatching library and a user-friendly and interactive studio, which can help\nresearchers: 1) to learn state-of-the-art neural text matching models\nsystematically, 2) to train, test and apply these models with simple\nconfigurable steps; and 3) to develop their own models with rich APIs and\nassistance.", "journal": "Proceedings of the 42nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '19), July 21--25,\n  2019, Paris, France"}
{"doi": "10.48550/arXiv.2311.10965", "date": "2023-11-18", "title": "(Nearest) Neighbors You Can Rely On: Formally Verified k-d Tree Construction and Search in Coq", "authors": "Nadeem Abdul Hamid", "abstract": "The k-d tree is a classic binary space-partitioning tree used to organize\npoints in k-dimensional space. While used in computational geometry and\ngraphics, the data structure has a long history of application in nearest\nneighbor search. The objective of the nearest neighbor search problem is to\nefficiently find the closest point(s) to a given query point, and is the basis,\nin turn, of common machine learning techniques. We present in this paper a case\nstudy in the certified implementation, using the Coq proof assistant, of k-d\ntree construction from a set of data and the accompanying K-nearest neighbors\nsearch algorithm. Our experience demonstrates an intuitive method for\nspecifying properties of these algorithms using the notion of list\npermutations.", "journal": ""}
{"doi": "10.48550/arXiv.2208.00315", "date": "2022-07-30", "title": "Implementing and Verifying Release-Acquire Transactional Memory (Extended Version)", "authors": "Sadegh Dalvandi, Brijesh Dongol", "abstract": "Transactional memory (TM) is an intensively studied synchronisation paradigm\nwith many proposed implementations in software and hardware, and combinations\nthereof. However, TM under relaxed memory, e.g., C11 (the 2011 C/C++ standard)\nis still poorly understood, lacking rigorous foundations that support\nverifiable implementations. This paper addresses this gap by developing\nTMS2-RA, a relaxed operational TM specification. We integrate TMS2-RA with RC11\n(the repaired C11 memory model that disallows load-buffering) to provide a\nformal semantics for TM libraries and their clients. We develop a logic, TARO,\nfor verifying client programs that use TMS2-RA for synchronisation. We also\nshow how TMS2-RA can be implemented by a C11 library, TML-RA, that uses relaxed\nand release-acquire atomics, yet guarantees the synchronisation properties\nrequired by TMS2-RA. We benchmark TML-RA and show that it outperforms its\nsequentially consistent counterpart in the STAMP benchmarks. Finally, we use a\nsimulation-based verification technique to prove correctness of TML-RA. Our\nentire development is supported by the Isabelle/HOL proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1710.03912", "date": "2017-10-11", "title": "Consistency of the Predicative Calculus of Cumulative Inductive Constructions (pCuIC)", "authors": "Amin Timany, Matthieu Sozeau", "abstract": "In order to avoid well-know paradoxes associated with self-referential\ndefinitions, higher-order dependent type theories stratify the theory using a\ncountably infinite hierarchy of universes (also known as sorts), Type$_0$ :\nType$_1$ : $\\cdots$ . Such type systems are called cumulative if for any type\n$A$ we have that $A$ : Type$_{i}$ implies $A$ : Type$_{i+1}$. The predicative\ncalculus of inductive constructions (pCIC) which forms the basis of the Coq\nproof assistant, is one such system.\n  In this paper we present and establish the soundness of the predicative\ncalculus of cumulative inductive constructions (pCuIC) which extends the\ncumulativity relation to inductive types.", "journal": ""}
{"doi": "10.48550/arXiv.2410.15908", "date": "2024-10-21", "title": "Formalising CXL Cache Coherence", "authors": "Chengsong Tan, Alastair F. Donaldson, John Wickerson", "abstract": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.", "journal": ""}
{"doi": "10.48550/arXiv.2405.11932", "date": "2024-05-20", "title": "Nonequilbrium physics of generative diffusion models", "authors": "Zhendong Yu, Haiping Huang", "abstract": "Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work.", "journal": "Phys. Rev. E 111, 014111 (2025)"}
{"doi": "10.48550/arXiv.2208.06907", "date": "2022-08-14", "title": "Impossibility theorems involving weakenings of expansion consistency and resoluteness in voting", "authors": "Wesley H. Holliday, Chase Norman, Eric Pacuit, Saam Zahedian", "abstract": "A fundamental principle of individual rational choice is Sen's $\\gamma$\naxiom, also known as expansion consistency, stating that any alternative chosen\nfrom each of two menus must be chosen from the union of the menus. Expansion\nconsistency can also be formulated in the setting of social choice. In voting\ntheory, it states that any candidate chosen from two fields of candidates must\nbe chosen from the combined field of candidates. An important special case of\nthe axiom is binary expansion consistency, which states that any candidate\nchosen from an initial field of candidates and chosen in a head-to-head match\nwith a new candidate must also be chosen when the new candidate is added to the\nfield, thereby ruling out spoiler effects. In this paper, we study the tension\nbetween this weakening of expansion consistency and weakenings of resoluteness,\nan axiom demanding the choice of a single candidate in any election. As is well\nknown, resoluteness is inconsistent with basic fairness conditions on social\nchoice, namely anonymity and neutrality. Here we prove that even significant\nweakenings of resoluteness, which are consistent with anonymity and neutrality,\nare inconsistent with binary expansion consistency. The proofs make use of SAT\nsolving, with the correctness of a SAT encoding formally verified in the Lean\nTheorem Prover, as well as a strategy for generalizing impossibility theorems\nobtained for special types of voting methods (namely majoritarian and pairwise\nvoting methods) to impossibility theorems for arbitrary voting methods. This\nproof strategy may be of independent interest for its potential applicability\nto other impossibility theorems in social choice.", "journal": ""}
{"doi": "10.48550/arXiv.2210.07746", "date": "2022-10-14", "title": "Formalising the $h$-principle and sphere eversion", "authors": "Patrick Massot, Floris van Doorn, Oliver Nash", "abstract": "In differential topology and geometry, the h-principle is a property enjoyed\nby certain construction problems. Roughly speaking, it states that the only\nobstructions to the existence of a solution come from algebraic topology.\n  We describe a formalisation in Lean of the local h-principle for first-order,\nopen, ample partial differential relations. This is a significant result in\ndifferential topology, originally proven by Gromov in 1973 as part of his\nsweeping effort which greatly generalised many previous flexibility results in\ntopology and geometry. In particular it reproves Smale's celebrated sphere\neversion theorem, a visually striking and counter-intuitive construction. Our\nformalisation uses Theilli\\`ere's implementation of convex integration from\n2018.\n  This paper is the first part of the sphere eversion project, aiming to\nformalise the global version of the h-principle for open and ample first order\ndifferential relations, for maps between smooth manifolds. Our current local\nversion for vector spaces is the main ingredient of this proof, and is\nsufficient to prove the titular corollary of the project. From a broader\nperspective, the goal of this project is to show that one can formalise\nadvanced mathematics with a strongly geometric flavour and not only\nalgebraically-flavoured", "journal": ""}
{"doi": "10.48550/arXiv.2302.10455", "date": "2023-02-21", "title": "A Deforestation of Reducts: Refocusing", "authors": "Olivier Danvy", "abstract": "In a small-step semantics with a deterministic reduction strategy, refocusing\nis a transformation that connects a reduction-based normalization function\n(i.e., a normalization function that enumerates the successive terms in a\nreduction sequence -- the successive reducts) and a reduction-free\nnormalization function (i.e., a normalization function that does not construct\nany reduct because all the reducts are deforested). This transformation was\nintroduced by Nielsen and the author in the early 2000's with an informal\ncorrectness proof. Since then, it has been used in a variety of settings,\nstarting with Biernacka and the author's syntactic correspondence between\ncalculi and abstract machines, and several formal proofs of it have been put\nforward. This article presents a simple, if overdue, formal proof of refocusing\nthat uses the Coq Proof Assistant and is aligned with the simplicity of the\noriginal idea.", "journal": ""}
{"doi": "10.48550/arXiv.2201.00383", "date": "2022-01-02", "title": "Integrating Artificial Intelligence and Augmented Reality in Robotic Surgery: An Initial dVRK Study Using a Surgical Education Scenario", "authors": "Yonghao Long, Jianfeng Cao, Anton Deguet, Russell H. Taylor, Qi Dou", "abstract": "Robot-assisted surgery has become progressively more and more popular due to\nits clinical advantages. In the meanwhile, the artificial intelligence and\naugmented reality in robotic surgery are developing rapidly and receive lots of\nattention. However, current methods have not discussed the coherent integration\nof AI and AR in robotic surgery. In this paper, we develop a novel system by\nseamlessly merging artificial intelligence module and augmented reality\nvisualization to automatically generate the surgical guidance for robotic\nsurgery education. Specifically, we first leverage reinforcement leaning to\nlearn from expert demonstration and then generate 3D guidance trajectory,\nproviding prior context information of the surgical procedure. Along with other\ninformation such as text hint, the 3D trajectory is then overlaid in the stereo\nview of dVRK, where the user can perceive the 3D guidance and learn the\nprocedure. The proposed system is evaluated through a preliminary experiment on\nsurgical education task peg-transfer, which proves its feasibility and\npotential as the next generation of robot-assisted surgery education solution.", "journal": ""}
{"doi": "10.48550/arXiv.2112.06700", "date": "2021-12-13", "title": "Verified Compilation of Quantum Oracles", "authors": "Liyi Li, Finn Voichick, Kesha Hietala, Yuxiang Peng, Xiaodi Wu, Michael Hicks", "abstract": "Quantum algorithms often apply classical operations, such as arithmetic or\npredicate checks, over a quantum superposition of classical data; these\nso-called oracles are often the largest components of a quantum program. To\nease the construction of efficient, correct oracle functions, this paper\npresents VQO, a high-assurance framework implemented with the Coq proof\nassistant. The core of VQO is OQASM, the oracle quantum assembly language.\nOQASM operations move qubits between two different bases via the quantum\nFourier transform, thus admitting important optimizations, but without inducing\nentanglement and the exponential blowup that comes with it. OQASM's design\nenabled us to prove correct VQO's compilers -- from a simple imperative\nlanguage called OQIMP to OQASM, and from OQASM to SQIR, a general-purpose\nquantum assembly language -- and allowed us to efficiently test properties of\nOQASM programs using the QuickChick property-based testing framework. We have\nused VQO to implement a variety of arithmetic and geometric operators that are\nbuilding blocks for important oracles, including those used in Shor's and\nGrover's algorithms. We found that VQO's QFT-based arithmetic oracles require\nfewer qubits, sometimes substantially fewer, than those constructed using\n\"classical\" gates; VQO's versions of the latter were nevertheless on par with\nor better than (in terms of both qubit and gate counts) oracles produced by\nQuipper, a state-of-the-art but unverified quantum programming platform.", "journal": ""}
{"doi": "10.48550/arXiv.1411.7140", "date": "2014-11-26", "title": "Program certification with computational effects", "authors": "Jean-Guillaume Dumas, Dominique Duval, Burak Ekici, Damien Pous", "abstract": "Dynamic evaluation is a paradigm in computer algebra which was introduced for\ncomputing with algebraic numbers. In linear algebra, for instance, dynamic\nevaluation can be used to apply programs which have been written for matrices\nwith coefficients modulo some prime number to matrices with coefficients modulo\nsome composite number. A way to implement dynamic evaluation in modern\ncomputing languages is to use the exceptions mechanism provided by the\nlanguage. In this paper, we pesent a proof system for exceptions which involves\nboth raising and handling, by extending Moggi's approach based on monads.\nMoreover, the core part of this proof system is dual to a proof system for the\nstate effect in imperative languages, which relies on the categorical notion of\ncomonad. Both proof systems are implemented in the Coq proof assistant, and\nthey are combined in order to deal with both effects at the same time.", "journal": ""}
{"doi": "10.48550/arXiv.2405.19270", "date": "2024-05-29", "title": "Formalising the Local Compactness of the Adele Ring", "authors": "Salvatore Mercuri", "abstract": "The adele ring of a number field is a central object in modern number theory.\nIts status as a locally compact topological ring is one of the key reasons why.\nWe describe a formal proof that the adele ring of a number field is locally\ncompact in the Lean 4 theorem prover. Our work includes the formalisations of\nnew types, including the completion of a number field at an infinite place, the\ninfinite adele ring and the finite $S$-adele ring, as well as formal proofs\nthat completions of a number field are locally compact and their rings of\nintegers at finite places are compact.", "journal": ""}
{"doi": "10.48550/arXiv.1105.2576", "date": "2011-05-12", "title": "TRX: A Formally Verified Parser Interpreter", "authors": "Adam Koprowski, Henri Binsztok", "abstract": "Parsing is an important problem in computer science and yet surprisingly\nlittle attention has been devoted to its formal verification. In this paper, we\npresent TRX: a parser interpreter formally developed in the proof assistant\nCoq, capable of producing formally correct parsers. We are using parsing\nexpression grammars (PEGs), a formalism essentially representing recursive\ndescent parsing, which we consider an attractive alternative to context-free\ngrammars (CFGs). From this formalization we can extract a parser for an\narbitrary PEG grammar with the warranty of total correctness, i.e., the\nresulting parser is terminating and correct with respect to its grammar and the\nsemantics of PEGs; both properties formally proven in Coq.", "journal": "Logical Methods in Computer Science, Volume 7, Issue 2 (June 24,\n  2011) lmcs:686"}
{"doi": "10.48550/arXiv.2001.02659", "date": "2020-01-08", "title": "An Equational Theory for Weak Bisimulation via Generalized Parameterized Coinduction", "authors": "Yannick Zakowski, Paul He, Chung-Kil Hur, Steve Zdancewic", "abstract": "Coinductive reasoning about infinitary structures such as streams is widely\napplicable. However, practical frameworks for developing coinductive proofs and\nfinding reasoning principles that help structure such proofs remain a\nchallenge, especially in the context of machine-checked formalization.\n  This paper gives a novel presentation of an equational theory for reasoning\nabout structures up to weak bisimulation. The theory is both compositional,\nmaking it suitable for defining general-purpose lemmas, and also incremental,\nmeaning that the bisimulation can be created interactively. To prove the\ntheory's soundness, this paper also introduces generalized parameterized\ncoinduction, which addresses expressivity problems of earlier works and\nprovides a practical framework for coinductive reasoning. The paper presents\nthe resulting equational theory for streams, but the technique applies to other\nstructures too.\n  All of the results in this paper have been proved in Coq, and the generalized\nparameterized coinduction framework is available as a Coq library.", "journal": ""}
{"doi": "10.48550/arXiv.2003.11331", "date": "2020-03-25", "title": "A Formalization of SQL with Nulls", "authors": "Wilmer Ricciotti, James Cheney", "abstract": "SQL is the world's most popular declarative language, forming the basis of\nthe multi-billion-dollar database industry. Although SQL has been standardized,\nthe full standard is based on ambiguous natural language rather than formal\nspecification. Commercial SQL implementations interpret the standard in\ndifferent ways, so that, given the same input data, the same query can yield\ndifferent results depending on the SQL system it is run on. Even for a\nparticular system, mechanically checked formalization of all widely-used\nfeatures of SQL remains an open problem. The lack of a well-understood formal\nsemantics makes it very difficult to validate the soundness of database\nimplementations.\n  Although formal semantics for fragments of SQL were designed in the past,\nthey usually did not support set and bag operations, lateral joins, nested\nsubqueries, and, crucially, null values. Null values complicate SQL's semantics\nin profound ways analogous to null pointers or side-effects in other\nprogramming languages. Since certain SQL queries are equivalent in the absence\nof null values, but produce different results when applied to tables containing\nincomplete data, semantics which ignore null values are able to prove query\nequivalences that are unsound in realistic databases.\n  A formal semantics of SQL supporting all the aforementioned features was only\nproposed recently. In this paper, we report about our mechanization of SQL\nsemantics covering set/bag operations, lateral joins, nested subqueries, and\nnulls, written in the Coq proof assistant, and describe the validation of key\nmetatheoretic properties. Additionally, we are able to use the same framework\nto formalize the semantics of a flat relational calculus (with null values),\nand show a certified translation of its normal forms into SQL.", "journal": ""}
{"doi": "10.48550/arXiv.2012.09138", "date": "2020-12-16", "title": "Extracting Smart Contracts Tested and Verified in Coq", "authors": "Danil Annenkov, Mikkel Milo, Jakob Botsch Nielsen, Bas Spitters", "abstract": "We implement extraction of Coq programs to functional languages based on\nMetaCoq's certified erasure. As part of this, we implement an optimisation pass\nremoving unused arguments. We prove the pass correct wrt. a conventional\ncall-by-value operational semantics of functional languages. We apply this to\ntwo functional smart contract languages, Liquidity and Midlang, and to the\nfunctional language Elm. Our development is done in the context of the ConCert\nframework that enables smart contract verification. We contribute a verified\nboardroom voting smart contract featuring maximum voter privacy such that each\nvote is kept private except under collusion of all other parties. We also\nintegrate property-based testing into ConCert using QuickChick and our\ndevelopment is the first to support testing properties of interacting smart\ncontracts. We test several complex contracts such as a DAO-like contract, an\nescrow contract, an implementation of a Decentralized Finance (DeFi) contract\nwhich includes a custom token standard (Tezos FA2), and more. In total, this\ngives us a way to write dependent programs in Coq, test them\nsemi-automatically, verify, and then extract to functional smart contract\nlanguages, while retaining a small trusted computing base of only MetaCoq and\nthe pretty-printers into these languages.", "journal": "CPP'2021: Proceedings of the 10th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs, January 18--19, 2021, Virtual,\n  Denmark"}
{"doi": "10.48550/arXiv.1810.08380", "date": "2018-10-19", "title": "Formalizing computability theory via partial recursive functions", "authors": "Mario Carneiro", "abstract": "We present an extension to the $\\mathtt{mathlib}$ library of the Lean theorem\nprover formalizing the foundations of computability theory. We use primitive\nrecursive functions and partial recursive functions as the main objects of\nstudy, and we use a constructive encoding of partial functions such that they\nare executable when the programs in question provably halt. Main theorems\ninclude the construction of a universal partial recursive function and a proof\nof the undecidability of the halting problem. Type class inference provides a\ntransparent way to supply G\\\"{o}del numberings where needed and encapsulate the\nencoding details.", "journal": ""}
{"doi": "10.48550/arXiv.2203.16344", "date": "2022-03-06", "title": "Formalizing the Ring of Ad\u00e8les of a Global Field", "authors": "Mar\u00eda In\u00e9s de Frutos-Fern\u00e1ndez", "abstract": "The ring of ad\\`eles of a global field and its group of units, the group of\nid\\`eles, are fundamental objects in modern number theory. We discuss a\nformalization of their definitions in the Lean 3 theorem prover. As a\nprerequisite, we formalized adic valuations on Dedekind domains. We present\nsome applications, including the statement of the main theorem of global class\nfield theory and a proof that the ideal class group of a number field is\nisomorphic to an explicit quotient of its id\\`ele class group.", "journal": ""}
{"doi": "10.48550/arXiv.2405.08863", "date": "2024-05-14", "title": "HepLean: Digitalising high energy physics", "authors": "Joseph Tooby-Smith", "abstract": "We introduce HepLean, an open-source project to digitalise definitions,\ntheorems, proofs, and calculations in high energy physics using the interactive\ntheorem prover Lean 4. HepLean has the potential to benefit the high energy\nphysics community in four ways: making it easier to find existing results,\nallowing the creation of new results using artificial intelligence and\nautomated methods, allowing easy review of papers for mathematical correctness,\nand providing new ways to teach high energy physics. We will discuss these in\ndetail. We will also demonstrate the digitalisation of three areas of high\nenergy physics in HepLean: Cabibbo-Kobayashi-Maskawa matrices in flavour\nphysics, local anomaly cancellation, and Higgs physics.", "journal": ""}
{"doi": "10.48550/arXiv.2107.10988", "date": "2021-07-23", "title": "Formalizing Galois Theory", "authors": "Thomas Browning, Patrick Lutz", "abstract": "We describe a project to formalize Galois theory using the Lean theorem\nprover, which is part of a larger effort to formalize all of the standard\nundergraduate mathematics curriculum in Lean. We discuss some of the challenges\nwe faced and the decisions we made in the course of this project. The main\ntheorems we formalized are the primitive element theorem, the fundamental\ntheorem of Galois theory, and the equivalence of several characterizations of\nfinite degree Galois extensions.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17149", "date": "2025-02-24", "title": "Programming Really Is Simple Mathematics", "authors": "Bertrand Meyer, Reto Weber", "abstract": "A re-construction of the fundamentals of programming as a small mathematical\ntheory (PRISM) based on elementary set theory. Highlights:\n  $\\bullet$ Zero axioms. No properties are assumed, all are proved (from\nstandard set theory).\n  $\\bullet$ A single concept covers specifications and programs.\n  $\\bullet$ Its definition only involves one relation and one set.\n  $\\bullet$ Everything proceeds from three operations: choice, composition and\nrestriction.\n  $\\bullet$ These techniques suffice to derive the axioms of classic papers on\nthe \"laws of programming\" as consequences and prove them mechanically.\n  $\\bullet$ The ordinary subset operator suffices to define both the notion of\nprogram correctness and the concepts of specialization and refinement.\n  $\\bullet$ From this basis, the theory deduces dozens of theorems\ncharacterizing important properties of programs and programming.\n  $\\bullet$ All these theorems have been mechanically verified (using\nIsabelle/HOL); the proofs are available in a public repository. This paper is a\nconsiderable extension and rewrite of an earlier contribution\n[arXiv:1507.00723]", "journal": ""}
{"doi": "10.48550/arXiv.1609.00322", "date": "2016-09-01", "title": "Open Call-by-Value (Extended Version)", "authors": "Beniamino Accattoli, Giulio Guerrieri", "abstract": "The elegant theory of the call-by-value lambda-calculus relies on weak\nevaluation and closed terms, that are natural hypotheses in the study of\nprogramming languages. To model proof assistants, however, strong evaluation\nand open terms are required, and it is well known that the operational\nsemantics of call-by-value becomes problematic in this case. Here we study the\nintermediate setting -- that we call Open Call-by-Value -- of weak evaluation\nwith open terms, on top of which Gr\\'egoire and Leroy designed the abstract\nmachine of Coq. Various calculi for Open Call-by-Value already exist, each one\nwith its pros and cons. This paper presents a detailed comparative study of the\noperational semantics of four of them, coming from different areas such as the\nstudy of abstract machines, denotational semantics, linear logic proof nets,\nand sequent calculus. We show that these calculi are all equivalent from a\ntermination point of view, justifying the slogan Open Call-by-Value.", "journal": ""}
{"doi": "10.48550/arXiv.2407.13671", "date": "2024-07-18", "title": "Liquid Amortization: Proving Amortized Complexity with LiquidHaskell (Functional Pearl)", "authors": "Jan van Br\u00fcgge", "abstract": "Formal reasoning about the time complexity of algorithms and data structures\nis usually done in interactive theorem provers like Isabelle/HOL. This includes\nreasoning about amortized time complexity which looks at the worst case\nperformance over a series of operations. However, most programs are not written\nwithin a theorem prover and thus use the data structures of the production\nlanguage. To verify the correctness it is necessary to translate the data\nstructures from the production language into the language of the prover. Such a\ntranslation step could introduce errors, for example due to a mismatch in\nfeatures between the two languages. We show how to prove amortized complexity\nof data structures directly in Haskell using LiquidHaskell. Besides skipping\nthe translation step, our approach can also provide a didactic advantage.\nLearners do not have to learn an additional language for proofs and can focus\non the new concepts only. For this paper, we do not assume prior knowledge of\namortized complexity as we explain the concepts and apply them in our first\ncase study, a simple stack with multipop. Moving to more complicated (and\nuseful) data structures, we show that the same technique works for binomial\nheaps which can be used to implement a priority queue. We also prove amortized\ncomplexity bounds for Claessen's version of the finger tree, a sequence-like\ndata structure with constant-time cons/uncons on either end. Finally we discuss\nthe current limitations of LiquidHaskell that made certain versions of the data\nstructures not feasible.", "journal": ""}
{"doi": "10.48550/arXiv.1505.04324", "date": "2015-05-16", "title": "Elaboration in Dependent Type Theory", "authors": "Leonardo de Moura, Jeremy Avigad, Soonho Kong, Cody Roux", "abstract": "To be usable in practice, interactive theorem provers need to provide\nconvenient and efficient means of writing expressions, definitions, and proofs.\nThis involves inferring information that is often left implicit in an ordinary\nmathematical text, and resolving ambiguities in mathematical expressions. We\nrefer to the process of passing from a quasi-formal and partially-specified\nexpression to a completely precise formal one as elaboration. We describe an\nelaboration algorithm for dependent type theory that has been implemented in\nthe Lean theorem prover. Lean's elaborator supports higher-order unification,\ntype class inference, ad hoc overloading, insertion of coercions, the use of\ntactics, and the computational reduction of terms. The interactions between\nthese components are subtle and complex, and the elaboration algorithm has been\ncarefully designed to balance efficiency and usability. We describe the central\ndesign goals, and the means by which they are achieved.", "journal": ""}
{"doi": "10.48550/arXiv.1911.00385", "date": "2019-11-01", "title": "A Formal Proof of PAC Learnability for Decision Stumps", "authors": "Joseph Tassarotti, Koundinya Vajjha, Anindya Banerjee, Jean-Baptiste Tristan", "abstract": "We present a formal proof in Lean of probably approximately correct (PAC)\nlearnability of the concept class of decision stumps. This classic result in\nmachine learning theory derives a bound on error probabilities for a simple\ntype of classifier. Though such a proof appears simple on paper, analytic and\nmeasure-theoretic subtleties arise when carrying it out fully formally. Our\nproof is structured so as to separate reasoning about deterministic properties\nof a learning function from proofs of measurability and analysis of\nprobabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2412.08739", "date": "2024-12-11", "title": "VEL: A Formally Verified Reasoner for OWL2 EL Profile", "authors": "Atalay Mert Ileri, Nalen Rangarajan, Jack Cannell, Hande McGinty", "abstract": "Over the past two decades, the Web Ontology Language (OWL) has been\ninstrumental in advancing the development of ontologies and knowledge graphs,\nproviding a structured framework that enhances the semantic integration of\ndata. However, the reliability of deductive reasoning within these systems\nremains challenging, as evidenced by inconsistencies among popular reasoners in\nrecent competitions. This evidence underscores the limitations of current\ntesting-based methodologies, particularly in high-stakes domains such as\nhealthcare. To mitigate these issues, in this paper, we have developed VEL, a\nformally verified EL++ reasoner equipped with machine-checkable correctness\nproofs that ensure the validity of outputs across all possible inputs. This\nformalization, based on the algorithm of Baader et al., has been transformed\ninto executable OCaml code using the Coq proof assistant's extraction\ncapabilities. Our formalization revealed several errors in the original\ncompleteness proofs, which led to changes to the algorithm to ensure its\ncompleteness. Our work demonstrates the necessity of mechanization of reasoning\nalgorithms to ensure their correctness at theoretical and implementation\nlevels.", "journal": ""}
{"doi": "10.48550/arXiv.2304.10424", "date": "2023-04-20", "title": "Engel's theorem in Mathlib", "authors": "Oliver Nash", "abstract": "We discuss the theory of Lie algebras in Lean's Mathlib library. Using\nnilpotency as the theme, we outline a computer formalisation of Engel's theorem\nand an application to root space theory. We emphasise that all arguments work\nwith coefficients in any commutative ring.", "journal": ""}
{"doi": "10.48550/arXiv.2401.12061", "date": "2024-01-22", "title": "Scalable Automated Verification for Cyber-Physical Systems in Isabelle/HOL", "authors": "Jonathan Juli\u00e1n Huerta y Munive, Simon Foster, Mario Gleirscher, Georg Struth, Christian Pardillo Laursen, Thomas Hickman", "abstract": "We formally introduce IsaVODEs (Isabelle verification with Ordinary\nDifferential Equations), a framework for the verification of cyber-physical\nsystems. We describe the semantic foundations of the framework's formalisation\nin the Isabelle/HOL proof assistant. A user-friendly language specification\nbased on a robust state model makes our framework flexible and adaptable to\nvarious engineering workflows. New additions to the framework increase both its\nexpressivity and proof automation. Specifically, formalisations related to\nforward diamond correctness specifications, certification of unique solutions\nto ordinary differential equations (ODEs) as flows, and invariant reasoning for\nsystems of ODEs contribute to the framework's scalability and usability.\nVarious examples and an evaluation validate the effectiveness of our framework.", "journal": ""}
{"doi": "10.48550/arXiv.1610.01004", "date": "2016-10-04", "title": "Reducing Opacity to Linearizability: A Sound and Complete Method", "authors": "Alasdair Armstrong, Brijesh Dongol, Simon Doherty", "abstract": "Transactional memory is a mechanism that manages thread synchronisation on\nbehalf of a programmer so that blocks of code execute with an illusion of\natomicity. The main safety criterion for transactional memory is opacity, which\ndefines conditions for serialising concurrent transactions.\n  Proving opacity is complicated because it allows concurrent transactions to\nobserve distinct memory states, while TM implementations are typically based on\none single shared store. This paper presents a sound and complete method, based\non coarse-grained abstraction, for reducing proofs of opacity to the relatively\nsimpler correctness condition: linearizability. We use our methods to verify\nTML and NORec from the literature and show our techniques extend to relaxed\nmemory models by showing that both are opaque under TSO without requiring\nadditional fences. Our methods also elucidate TM designs at higher level of\nabstraction; as an application, we develop a variation of NORec with fast-path\nreads transactions. All our proofs have been mechanised, either in the Isabelle\ntheorem prover or the PAT model checker.", "journal": ""}
{"doi": "10.48550/arXiv.1808.04006", "date": "2018-08-12", "title": "Typed Closure Conversion for the Calculus of Constructions", "authors": "William J. Bowman, Amal Ahmed", "abstract": "Dependently typed languages such as Coq are used to specify and verify the\nfull functional correctness of source programs. Type-preserving compilation can\nbe used to preserve these specifications and proofs of correctness through\ncompilation into the generated target-language programs. Unfortunately,\ntype-preserving compilation of dependent types is hard. In essence, the problem\nis that dependent type systems are designed around high-level compositional\nabstractions to decide type checking, but compilation interferes with the\ntype-system rules for reasoning about run-time terms.\n  We develop a type-preserving closure-conversion translation from the Calculus\nof Constructions (CC) with strong dependent pairs ($\\Sigma$ types)---a subset\nof the core language of Coq---to a type-safe, dependently typed compiler\nintermediate language named CC-CC. The central challenge in this work is how to\ntranslate the source type-system rules for reasoning about functions into\ntarget type-system rules for reasoning about closures. To justify these rules,\nwe prove soundness of CC-CC by giving a model in CC. In addition to type\npreservation, we prove correctness of separate compilation.", "journal": ""}
{"doi": "10.48550/arXiv.2407.12840", "date": "2024-07-04", "title": "Categorical Foundations of Formalized Condensed Mathematics", "authors": "Dagur Asgeirsson, Riccardo Brasca, Nikolas Kuhn, Filippo Alberto Edoardo Nuccio Mortarino Majno di Capriglio, Adam Topaz", "abstract": "Condensed mathematics, developed by Clausen and Scholze over the last few\nyears, proposes a generalization of topology with better categorical\nproperties. It replaces the concept of a topological space by that of a\ncondensed set, which can be defined as a sheaf for the coherent topology on a\ncertain category of compact Hausdorff spaces. In this case, the sheaf condition\nhas a fairly simple explicit description, which arises from studying the\nrelationship between the coherent, regular and extensive topologies. In this\npaper, we establish this relationship under minimal assumptions on the\ncategory, going beyond the case of compact Hausdorff spaces. Along the way, we\nalso provide a characterization of sheaves and covering sieves for these\ncategories. All results in this paper have been fully formalized in the Lean\nproof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1903.00687", "date": "2019-03-02", "title": "A unifying representer theorem for inverse problems and machine learning", "authors": "Michael Unser", "abstract": "The standard approach for dealing with the ill-posedness of the training\nproblem in machine learning and/or the reconstruction of a signal from a\nlimited number of measurements is regularization. The method is applicable\nwhenever the problem is formulated as an optimization task. The standard\nstrategy consists in augmenting the original cost functional by an energy that\npenalizes solutions with undesirable behavior. The effect of regularization is\nvery well understood when the penalty involves a Hilbertian norm. Another\npopular configuration is the use of an $\\ell_1$-norm (or some variant thereof)\nthat favors sparse solutions. In this paper, we propose a higher-level\nformulation of regularization within the context of Banach spaces. We present a\ngeneral representer theorem that characterizes the solutions of a remarkably\nbroad class of optimization problems. We then use our theorem to retrieve a\nnumber of known results in the literature---e.g., the celebrated representer\ntheorem of machine leaning for RKHS, Tikhonov regularization, representer\ntheorems for sparsity promoting functionals, the recovery of spikes---as well\nas a few new ones.", "journal": ""}
{"doi": "10.48550/arXiv.2207.12742", "date": "2022-07-26", "title": "A formalization of the change of variables formula for integrals in mathlib", "authors": "S\u00e9bastien Gou\u00ebzel", "abstract": "We report on a formalization of the change of variables formula in integrals,\nin the mathlib library for Lean. Our version of this theorem is extremely\ngeneral, and builds on developments in linear algebra, analysis, measure theory\nand descriptive set theory. The interplay between these domains is transparent\nthanks to the highly integrated development model of mathlib.", "journal": ""}
{"doi": "10.48550/arXiv.2412.01671", "date": "2024-12-02", "title": "Verified Foundations for Differential Privacy", "authors": "Markus de Medeiros, Muhammad Naveed, Tancrede Lepoint, Temesghen Kahsai, Tristan Ravitch, Stefan Zetzsche, Anjali Joshi, Joseph Tassarotti, Aws Albarghouthi, Jean-Baptiste Tristan", "abstract": "Differential privacy (DP) has become the gold standard for privacy-preserving\ndata analysis, but implementing it correctly has proven challenging. Prior work\nhas focused on verifying DP at a high level, assuming the foundations are\ncorrect and a perfect source of randomness is available. However, the\nunderlying theory of differential privacy can be very complex and subtle. Flaws\nin basic mechanisms and random number generation have been a critical source of\nvulnerabilities in real-world DP systems.\n  In this paper, we present SampCert, the first comprehensive, mechanized\nfoundation for differential privacy. SampCert is written in Lean with over\n12,000 lines of proof. It offers a generic and extensible notion of DP, a\nframework for constructing and composing DP mechanisms, and formally verified\nimplementations of Laplace and Gaussian sampling algorithms. SampCert provides\n(1) a mechanized foundation for developing the next generation of\ndifferentially private algorithms, and (2) mechanically verified primitives\nthat can be deployed in production systems. Indeed, SampCert's verified\nalgorithms power the DP offerings of Amazon Web Services (AWS), demonstrating\nits real-world impact.\n  SampCert's key innovations include: (1) A generic DP foundation that can be\ninstantiated for various DP definitions (e.g., pure, concentrated, R\\'enyi DP);\n(2) formally verified discrete Laplace and Gaussian sampling algorithms that\navoid the pitfalls of floating-point implementations; and (3) a simple\nprobability monad and novel proof techniques that streamline the formalization.\nTo enable proving complex correctness properties of DP and random number\ngeneration, SampCert makes heavy use of Lean's extensive Mathlib library,\nleveraging theorems in Fourier analysis, measure and probability theory, number\ntheory, and topology.", "journal": ""}
{"doi": "10.48550/arXiv.2002.00620", "date": "2020-02-03", "title": "Validating Mathematical Structures", "authors": "Kazuhiko Sakaguchi", "abstract": "Sharing of notations and theories across an inheritance hierarchy of\nmathematical structures, e.g., groups and rings, is important for productivity\nwhen formalizing mathematics in proof assistants. The packed classes\nmethodology is a generic design pattern to define and combine mathematical\nstructures in a dependent type theory with records. When combined with\nmechanisms for implicit coercions and unification hints, packed classes enable\nautomated structure inference and subtyping in hierarchies, e.g., that a ring\ncan be used in place of a group. However, large hierarchies based on packed\nclasses are challenging to implement and maintain. We identify two hierarchy\ninvariants that ensure modularity of reasoning and predictability of inference\nwith packed classes, and propose algorithms to check these invariants. We\nimplement our algorithms as tools for the Coq proof assistant, and show that\nthey significantly improve the development process of Mathematical Components,\na library for formalized mathematics.", "journal": "IJCAR 2020, LNCS, Springer, vol. 12167, pp. 138--157"}
{"doi": "10.48550/arXiv.1701.08186", "date": "2017-01-27", "title": "Implementing Open Call-by-Value (Extended Version)", "authors": "Beniamino Accattoli, Giulio Guerrieri", "abstract": "The theory of the call-by-value lambda-calculus relies on weak evaluation and\nclosed terms, that are natural hypotheses in the study of programming\nlanguages. To model proof assistants, however, strong evaluation and open terms\nare required. Open call-by-value is the intermediate setting of weak evaluation\nwith open terms, on top of which Gr\\'egoire and Leroy designed the abstract\nmachine of Coq. This paper provides a theory of abstract machines for open\ncall-by-value. The literature contains machines that are either simple but\ninefficient, as they have an exponential overhead, or efficient but heavy, as\nthey rely on a labelling of environments and a technical optimization. We\nintroduce a machine that is simple and efficient: it does not use labels and it\nimplements open call-by-value within a bilinear overhead. Moreover, we provide\na new fine understanding of how different optimizations impact on the\ncomplexity of the overhead.", "journal": ""}
{"doi": "10.48550/arXiv.2004.10263", "date": "2020-04-21", "title": "The Imandra Automated Reasoning System (system description)", "authors": "Grant Olney Passmore, Simon Cruanes, Denis Ignatovich, Dave Aitken, Matt Bray, Elijah Kagan, Kostya Kanishev, Ewen Maclean, Nicola Mometto", "abstract": "We describe Imandra, a modern computational logic theorem prover designed to\nbridge the gap between decision procedures such as SMT, semi-automatic\ninductive provers of the Boyer-Moore family like ACL2, and interactive proof\nassistants for typed higher-order logics. Imandra's logic is computational,\nbased on a pure subset of OCaml in which all functions are terminating, with\nrestrictions on types and higher-order functions that allow conjectures to be\ntranslated into multi-sorted first-order logic with theories, including\narithmetic and datatypes. Imandra has novel features supporting large-scale\nindustrial applications, including a seamless integration of bounded and\nunbounded verification, first-class computable counterexamples, efficiently\nexecutable models and a cloud-native architecture supporting live multiuser\ncollaboration.\n  The core reasoning mechanisms of Imandra are (i) a semi-complete procedure\nfor finding models of formulas in the logic mentioned above, centered around\nthe lazy expansion of recursive functions, and (ii) an inductive waterfall and\nsimplifier which \"lifts\" many Boyer-Moore ideas to our typed higher-order\nsetting.\n  These mechanisms are tightly integrated and subject to many forms of user\ncontrol. Imandra's user interfaces include an interactive toplevel, Jupyter\nnotebooks and asynchronous document-based verification (in the spirit of\nIsabelle's Prover IDE) with VS Code.", "journal": ""}
{"doi": "10.48550/arXiv.2310.01916", "date": "2023-10-03", "title": "Verified completeness in Henkin-style for intuitionistic propositional logic", "authors": "Huayu Guo, Dongheng Chen, Bruno Bentzen", "abstract": "This paper presents a formalization of the classical proof of completeness in\nHenkin-style developed by Troelstra and van Dalen for intuitionistic logic with\nrespect to Kripke models. The completeness proof incorporates their insights in\na fresh and elegant manner that is better suited for mechanization. We discuss\ndetails of our implementation in the Lean theorem prover with emphasis on the\nprime extension lemma and construction of the canonical model. Our\nimplementation is restricted to a system of intuitionistic propositional logic\nwith implication, conjunction, disjunction, and falsity given in terms of a\nHilbert-style axiomatization. As far as we know, our implementation is the\nfirst verified Henkin-style proof of completeness for intuitionistic logic\nfollowing Troelstra and van Dalen's method in the literature. The full source\ncode can be found online at https://github.com/bbentzen/ipl.", "journal": "Joint proceedings of the Third International Workshop on Logics\n  for New-Generation Artificial Intelligence and the International Workshop on\n  Logic, AI and Law, pp.36-48, 2023"}
{"doi": "10.48550/arXiv.2006.13635", "date": "2020-06-24", "title": "ReLoC Reloaded: A Mechanized Relational Logic for Fine-Grained Concurrency and Logical Atomicity", "authors": "Dan Frumin, Robbert Krebbers, Lars Birkedal", "abstract": "We present a new version of ReLoC: a relational separation logic for proving\nrefinements of programs with higher-order state, fine-grained concurrency,\npolymorphism and recursive types. The core of ReLoC is its refinement judgment\n$e \\precsim e' : \\tau$, which states that a program $e$ refines a program $e'$\nat type $\\tau$. ReLoC provides type-directed structural rules and symbolic\nexecution rules in separation-logic style for manipulating the judgment,\nwhereas in prior work on refinements for languages with higher-order state and\nconcurrency, such proofs were carried out by unfolding the judgment into its\ndefinition in the model. ReLoC's abstract proof rules make it simpler to carry\nout refinement proofs, and enable us to generalize the notion of logically\natomic specifications to the relational case, which we call logically atomic\nrelational specifications.\n  We build ReLoC on top of the Iris framework for separation logic in Coq,\nallowing us to leverage features of Iris to prove soundness of ReLoC, and to\ncarry out refinement proofs in ReLoC. We implement tactics for interactive\nproofs in ReLoC, allowing us to mechanize several case studies in Coq, and\nthereby demonstrate the practicality of ReLoC.\n  ReLoC Reloaded extends ReLoC (LICS'18) with various technical improvements, a\nnew Coq mechanization, and support for Iris's prophecy variables. The latter\nallows us to carry out refinement proofs that involve reasoning about the\nprogram's future. We also expand ReLoC's notion of logically atomic relational\nspecifications with a new flavor based on the HOCAP pattern by Svendsen et al.", "journal": "Logical Methods in Computer Science, Volume 17, Issue 3 (July 21,\n  2021) lmcs:6598"}
{"doi": "10.48550/arXiv.2101.00127", "date": "2021-01-01", "title": "Formalizing Hall's Marriage Theorem in Lean", "authors": "Alena Gusakov, Bhavik Mehta, Kyle A. Miller", "abstract": "We formalize Hall's Marriage Theorem in the Lean theorem prover for inclusion\nin mathlib, which is a community-driven effort to build a unified mathematics\nlibrary for Lean. One goal of the mathlib project is to contain all of the\ntopics of a complete undergraduate mathematics education.\n  We provide three presentations of the main theorem statement: in terms of\nindexed families of finite sets, of relations on types, and of matchings in\nbipartite graphs. We also formalize a version of K\\H{o}nig's lemma (in terms of\ninverse limits) to boost the theorem to the case of countably infinite index\nsets. We give a description of the design of the recent mathlib library for\nsimple graphs, and we also give a necessary and sufficient condition for a\nsimple graph to carry a function.", "journal": ""}
{"doi": "10.48550/arXiv.2207.03880", "date": "2022-07-08", "title": "Constrained Training of Neural Networks via Theorem Proving", "authors": "Mark Chevallier, Matthew Whyte, Jacques D. Fleuriot", "abstract": "We introduce a theorem proving approach to the specification and generation\nof temporal logical constraints for training neural networks. We formalise a\ndeep embedding of linear temporal logic over finite traces (LTL$_f$) and an\nassociated evaluation function characterising its semantics within the\nhigher-order logic of the Isabelle theorem prover. We then proceed to formalise\na loss function $\\mathcal{L}$ that we formally prove to be sound, and\ndifferentiable to a function $d\\mathcal{L}$. We subsequently use Isabelle's\nautomatic code generation mechanism to produce OCaml versions of LTL$_f$,\n$\\mathcal{L}$ and $d\\mathcal{L}$ that we integrate with PyTorch via OCaml\nbindings for Python. We show that, when used for training in an existing deep\nlearning framework for dynamic movement, our approach produces expected results\nfor common movement specification patterns such as obstacle avoidance and\npatrolling. The distinctive benefit of our approach is the fully rigorous\nmethod for constrained training, eliminating many of the risks inherent to\nad-hoc implementations of logical aspects directly in an \"unsafe\" programming\nlanguage such as Python.", "journal": ""}
{"doi": "10.48550/arXiv.2311.13692", "date": "2023-11-22", "title": "Molly: A Verified Compiler for Cryptoprotocol Roles", "authors": "Daniel J. Dougherty, Joshua D. Guttman", "abstract": "Molly is a program that compiles cryptographic protocol roles written in a\nhigh-level notation into straight-line programs in an intermediate-level\nimperative language, suitable for implementation in a conventional programming\nlanguage. We define a denotational semantics for protocol roles based on an\naxiomatization of the runtime. A notable feature of our approach is that we\nassume that encryption is randomized. Thus, at the runtime level we treat\nencryption as a relation rather than a function. Molly is written in Coq, and\ngenerates a machine-checked proof that the procedure it constructs is correct\nwith respect to the runtime semantics. Using Coq's extraction mechanism, one\ncan build an efficient functional program for compilation.", "journal": ""}
{"doi": "10.48550/arXiv.1907.00713", "date": "2019-07-01", "title": "Verifying that a compiler preserves concurrent value-dependent information-flow security", "authors": "Robert Sison, Toby Murray", "abstract": "It is common to prove by reasoning over source code that programs do not leak\nsensitive data. But doing so leaves a gap between reasoning and reality that\ncan only be filled by accounting for the behaviour of the compiler. This task\nis complicated when programs enforce value-dependent information-flow security\nproperties (in which classification of locations can vary depending on values\nin other locations) and complicated further when programs exploit\nshared-variable concurrency.\n  Prior work has formally defined a notion of concurrency-aware refinement for\npreserving value-dependent security properties. However, that notion is\nconsiderably more complex than standard refinement definitions typically\napplied in the verification of semantics preservation by compilers. To date it\nremains unclear whether it can be applied to a realistic compiler, because\nthere exist no general decomposition principles for separating it into smaller,\nmore familiar, proof obligations.\n  In this work, we provide such a decomposition principle, which we show can\nalmost halve the complexity of proving secure refinement. Further, we\ndemonstrate its applicability to secure compilation, by proving in Isabelle/HOL\nthe preservation of value-dependent security by a proof-of-concept compiler\nfrom an imperative While language to a generic RISC-style assembly language,\nfor programs with shared-memory concurrency mediated by locking primitives.\nFinally, we execute our compiler in Isabelle on a While language model of the\nCross Domain Desktop Compositor, demonstrating to our knowledge the first use\nof a compiler verification result to carry an information-flow security\nproperty down to the assembly-level model of a non-trivial concurrent program.", "journal": ""}
{"doi": "10.48550/arXiv.2104.14445", "date": "2021-04-29", "title": "Trakhtenbrot's Theorem in Coq: Finite Model Theory through the Constructive Lens", "authors": "Dominik Kirst, Dominique Larchey-Wendling", "abstract": "We study finite first-order satisfiability (FSAT) in the constructive setting\nof dependent type theory. Employing synthetic accounts of enumerability and\ndecidability, we give a full classification of FSAT depending on the\nfirst-order signature of non-logical symbols. On the one hand, our development\nfocuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as\nthe signature contains an at least binary relation symbol. Our proof proceeds\nby a many-one reduction chain starting from the Post correspondence problem. On\nthe other hand, we establish the decidability of FSAT for monadic first-order\nlogic, i.e. where the signature only contains at most unary function and\nrelation symbols, as well as the enumerability of FSAT for arbitrary enumerable\nsignatures. To showcase an application of Trakhtenbrot's theorem, we continue\nour reduction chain with a many-one reduction from FSAT to separation logic.\nAll our results are mechanised in the framework of a growing Coq library of\nsynthetic undecidability proofs.", "journal": "Logical Methods in Computer Science, Volume 18, Issue 2 (June 14,\n  2022) lmcs:7422"}
{"doi": "10.48550/arXiv.2207.03913", "date": "2022-07-08", "title": "NOx emissions trends in hydrogen lean premixed flamelets at high strain rate", "authors": "Alessandro Porcarelli, Boris Kruljevic, Ivan Langella", "abstract": "NO$_{\\rm x}$ formation in lean premixed and highly-strained pure hydrogen-air\nflamelets is investigated numerically. Lean conditions are established at an\nequivalence ratio of 0.7. Detailed-chemistry, one-dimensional simulations are\nperformed on a reactants-to-products counter-flow configuration with an applied\nstrain rate ranging from $a=100 \\, {\\rm s}^{-1}$ to $a=10000 \\, {\\rm s}^{-1}$\nand the \\texttt{GRI3.0} mechanism. Following a similar setup, two-dimensional\ndirect numerical simulations are also conducted for representative strain rates\nof $2000 \\, {\\rm s}^{-1}$ and $5000 \\, {\\rm s}^{-1}$. Both solutions show a\ndecreasing NO$_{\\rm x}$ trend as the applied strain rate is increased. This\ndecreasing emission outcome is highlighted for the first time in this study for\nlean pure-hydrogen flamelets. A deep analysis of the 2D solution underlines\nthat there is no production of NO$_{\\rm x}$ in the second dimension, thus\nproving that the emission trend is not a result of a setup preconditioning, but\nis instead a direct physical effect of stretch on the flame. Furthermore, a\ndetailed analysis of the NO$_{\\rm x}$ formation pathways at $a=2000 \\, {\\rm\ns}^{-1}$ and $a=5000 \\, {\\rm s}^{-1}$ is performed. Thermal NO$_{\\rm x}$ and\nNNH pathways are shown to both contribute significantly to the total NO$_{\\rm\nx}$ production. While the NNH route contribution is roughly constant at\ndifferent strain rates, a significant decrease is observed along the thermal\nNO$_{\\rm x}$ route. Overall, results show that lean and highly-strained\nhydrogen flames experience a significant decrease of NO$_{\\rm x}$. This\nproperty is discussed and analysed in the paper.", "journal": ""}
{"doi": "10.48550/arXiv.2108.01883", "date": "2021-08-04", "title": "Reasoning about Iteration and Recursion Uniformly based on Big-step Semantics", "authors": "Ximeng Li, Qianying Zhang, Guohui Wang, Zhiping Shi, Yong Guan", "abstract": "A reliable technique for deductive program verification should be proven\nsound with respect to the semantics of the programming language. For each\ndifferent language, the construction of a separate soundness proof is often a\nlaborious undertaking. In language-independent program verification, common\naspects of computer programs are addressed to enable sound reasoning for all\nlanguages. In this work, we propose a solution for the sound reasoning about\niteration and recursion based on the big-step operational semantics of any\nprogramming language. We give inductive proofs on the soundness and relative\ncompleteness of our reasoning technique. We illustrate the technique at\nsimplified programming languages of the imperative and functional paradigms,\nwith diverse features. We also mechanism all formal results in the Coq proof\nassistant.", "journal": ""}
{"doi": "10.48550/arXiv.2106.05987", "date": "2021-06-10", "title": "Hybrid Systems Verification with Isabelle/HOL: Simpler Syntax, Better Models, Faster Proofs", "authors": "Simon Foster, Jonathan Juli\u00e1n Huerta y Munive, Mario Gleirscher, Georg Struth", "abstract": "We extend a semantic verification framework for hybrid systems with the\nIsabelle/HOL proof assistant by an algebraic model for hybrid program stores, a\nshallow expression model for hybrid programs and their correctness\nspecifications, and domain-specific deductive and calculational support. The\nnew store model yields clean separations and dynamic local views of variables,\ne.g. discrete/continuous, mutable/immutable, program/logical, and enhanced ways\nof manipulating them using combinators, projections and framing. This leads to\nmore local inference rules, procedures and tactics for reasoning with invariant\nsets, certifying solutions of hybrid specifications or calculating derivatives\nwith increased proof automation and scalability. The new expression model\nprovides more user-friendly syntax, better control of name spaces and\ninterfaces connecting the framework with real-world modelling languages.", "journal": ""}
{"doi": "10.48550/arXiv.1811.10819", "date": "2018-11-27", "title": "Isabelle/jEdit as IDE for Domain-specific Formal Languages and Informal Text Documents", "authors": "Makarius Wenzel", "abstract": "Isabelle/jEdit is the main application of the Prover IDE (PIDE) framework and\nthe default user-interface of Isabelle, but it is not limited to theorem\nproving. This paper explores possibilities to use it as a general IDE for\nformal languages that are defined in user-space, and embedded into informal\ntext documents. It covers overall document structure with auxiliary files and\ndocument antiquotations, formal text delimiters and markers for interpretation\n(via control symbols). The ultimate question behind this: How far can we\nstretch a plain text editor like jEdit in order to support semantic text\nprocessing, with support by the underlying PIDE framework?", "journal": "EPTCS 284, 2018, pp. 71-84"}
{"doi": "10.48550/arXiv.0904.1110", "date": "2009-04-07", "title": "On formal verification of arithmetic-based cryptographic primitives", "authors": "David Nowak", "abstract": "Cryptographic primitives are fundamental for information security: they are\nused as basic components for cryptographic protocols or public-key\ncryptosystems. In many cases, their security proofs consist in showing that\nthey are reducible to computationally hard problems. Those reductions can be\nsubtle and tedious, and thus not easily checkable. On top of the proof\nassistant Coq, we had implemented in previous work a toolbox for writing and\nchecking game-based security proofs of cryptographic primitives. In this paper\nwe describe its extension with number-theoretic capabilities so that it is now\npossible to write and check arithmetic-based cryptographic primitives in our\ntoolbox. We illustrate our work by machine checking the game-based proofs of\nunpredictability of the pseudo-random bit generator of Blum, Blum and Shub, and\nsemantic security of the public-key cryptographic scheme of Goldwasser and\nMicali.", "journal": "In Information Security and Cryptology - ICISC 2008, 11th\n  International Conference, Seoul, Korea, December 3-5, 2008, Proceedings,\n  volume 5461 of Lecture Notes in Computer Science, pages 368-382, Springer"}
{"doi": "10.48550/arXiv.2111.02498", "date": "2021-11-03", "title": "Interpolating between the Jaccard distance and an analogue of the normalized information distance", "authors": "Bj\u00f8rn Kjos-Hanssen", "abstract": "Jim\\'enez, Becerra, and Gelbukh (2013) defined a family of \"symmetric Tversky\nratio models\" $S_{\\alpha,\\beta}$, $0\\le\\alpha\\le 1$, $\\beta>0$. Each function\n$D_{\\alpha,\\beta}=1-S_{\\alpha,\\beta}$ is a semimetric on the powerset of a\ngiven finite set.\n  We show that $D_{\\alpha,\\beta}$ is a metric if and only if $0\\le\\alpha \\le\n\\frac12$ and $\\beta\\ge 1/(1-\\alpha)$. This result is formally verified in the\nLean proof assistant.\n  The extreme points of this parametrized space of metrics are $\\mathcal\nV_1=D_{1/2,2}$, the Jaccard distance, and $\\mathcal V_{\\infty}=D_{0,1}$, an\nanalogue of the normalized information distance of M. Li, Chen, X. Li, Ma, and\nVit\\'anyi (2004).\n  As a second interpolation, in general we also show that $\\mathcal V_p$ is a\nmetric, $1\\le p\\le\\infty$, where $$\\Delta_p(A,B)=(|B\\setminus A|^p+|A\\setminus\nB|^p)^{1/p},$$ $$\\mathcal V_p(A,B)=\\frac{\\Delta_p(A,B)}{|A\\cap B| +\n\\Delta_p(A,B)}.$$", "journal": "Journal of Logic and Computation 2023"}
{"doi": "10.48550/arXiv.1711.03028", "date": "2017-11-08", "title": "Simplicity: A New Language for Blockchains", "authors": "Russell O'Connor", "abstract": "Simplicity is a typed, combinator-based, functional language without loops\nand recursion, designed to be used for crypto-currencies and blockchain\napplications. It aims to improve upon existing crypto-currency languages, such\nas Bitcoin Script and Ethereum's EVM, while avoiding some of the problems they\nface. Simplicity comes with formal denotational semantics defined in Coq, a\npopular, general purpose software proof assistant. Simplicity also includes\noperational semantics that are defined with an abstract machine that we call\nthe Bit Machine. The Bit Machine is used as a tool for measuring the\ncomputational space and time resources needed to evaluate Simplicity programs.\nOwing to its Turing incompleteness, Simplicity is amenable to static analysis\nthat can be used to derive upper bounds on the computational resources needed,\nprior to execution. While Turing incomplete, Simplicity can express any\nfinitary function, which we believe is enough to build useful \"smart contracts\"\nfor blockchain applications.", "journal": "2017. Proceedings of the 2017 Workshop on Programming Languages\n  and Analysis for Security. ACM, New York, NY, USA"}
{"doi": "10.48550/arXiv.2010.16014", "date": "2020-10-30", "title": "Isabelle/HOL as a Meta-Language for Teaching Logic", "authors": "Asta Halkj\u00e6r From, J\u00f8rgen Villadsen, Patrick Blackburn", "abstract": "Proof assistants are important tools for teaching logic. We support this\nclaim by discussing three formalizations in Isabelle/HOL used in a recent\ncourse on automated reasoning. The first is a formalization of System W (a\nsystem of classical propositional logic with only two primitive symbols), the\nsecond is the Natural Deduction Assistant (NaDeA), and the third is a one-sided\nsequent calculus that uses our Sequent Calculus Verifier (SeCaV). We describe\neach formalization in turn, concentrating on how we used them in our teaching,\nand commenting on features that are interesting or useful from a logic\neducation perspective. In the conclusion, we reflect on the lessons learned and\nwhere they might lead us next.", "journal": "EPTCS 328, 2020, pp. 18-34"}
{"doi": "10.48550/arXiv.2412.00422", "date": "2024-11-30", "title": "IRS Aided Federated Learning: Multiple Access and Fundamental Tradeoff", "authors": "Guangji Chen, Jun Li, Qingqing Wu, Yiyang Ni", "abstract": "This paper investigates an intelligent reflecting surface (IRS) aided\nwireless federated learning (FL) system, where an access point (AP) coordinates\nmultiple edge devices to train a machine leaning model without sharing their\nown raw data. During the training process, we exploit the joint channel\nreconfiguration via IRS and resource allocation design to reduce the latency of\na FL task. Particularly, we propose three transmission protocols for assisting\nthe local model uploading from multiple devices to an AP, namely IRS aided time\ndivision multiple access (I-TDMA), IRS aided frequency division multiple access\n(I-FDMA), and IRS aided non-orthogonal multiple access (INOMA), to investigate\nthe impact of IRS on the multiple access for FL. Under the three protocols, we\nminimize the per-round latency subject to a given training loss by jointly\noptimizing the device scheduling, IRS phase-shifts, and\ncommunicationcomputation resource allocation. For the associated problem under\nI-TDMA, an efficient algorithm is proposed to solve it optimally by exploiting\nits intrinsic structure, whereas the highquality solutions of the problems\nunder I-FDMA and I-NOMA are obtained by invoking a successive convex\napproximation (SCA) based approach. Then, we further develop a theoretical\nframework for the performance comparison of the proposed three transmission\nprotocols. Sufficient conditions for ensuring that I-TDMA outperforms I-NOMA\nand those of its opposite are unveiled, which is fundamentally different from\nthat NOMA always outperforms TDMA in the system without IRS. Simulation results\nvalidate our theoretical findings and also demonstrate the usefulness of IRS\nfor enhancing the fundamental tradeoff between the learning latency and\nlearning accuracy.", "journal": ""}
{"doi": "10.48550/arXiv.1304.3596", "date": "2013-04-12", "title": "Formal Verification of a C Value Analysis Based on Abstract Interpretation", "authors": "Sandrine Blazy, Vincent Laporte, Andr\u00e9 Maroneze, David Pichardie", "abstract": "Static analyzers based on abstract interpretation are complex pieces of\nsoftware implementing delicate algorithms. Even if static analysis techniques\nare well understood, their implementation on real languages is still\nerror-prone. This paper presents a formal verification using the Coq proof\nassistant: a formalization of a value analysis (based on abstract\ninterpretation), and a soundness proof of the value analysis. The formalization\nrelies on generic interfaces. The mechanized proof is facilitated by a\ntranslation validation of a Bourdoncle fixpoint iterator. The work has been\nintegrated into the CompCert verified C-compiler. Our verified analysis\ndirectly operates over an intermediate language of the compiler having the same\nexpressiveness as C. The automatic extraction of our value analysis into OCaml\nyields a program with competitive results, obtained from experiments on a\nnumber of benchmarks and comparisons with the Frama-C tool.", "journal": "SAS - 20th Static Analysis Symposium Lecture Notes in Computer\n  Science (2013) 324-344"}
{"doi": "10.48550/arXiv.2310.02704", "date": "2023-10-04", "title": "Extending Isabelle/HOL's Code Generator with support for the Go programming language", "authors": "Terru St\u00fcbinger, Lars Hupel", "abstract": "The Isabelle proof assistant includes a small functional language, which\nallows users to write and reason about programs. So far, these programs could\nbe extracted into a number of functional languages: Standard ML, OCaml, Scala,\nand Haskell. This work adds support for Go as a fifth target language for the\nCode Generator. Unlike the previous targets, Go is not a functional language\nand encourages code in an imperative style, thus many of the features of\nIsabelle's language (particularly data types, pattern matching, and type\nclasses) have to be emulated using imperative language constructs in Go. The\ndeveloped Code Generation is provided as an add-on library that can be simply\nimported into existing theories.", "journal": "FM 2024: Formal Methods Volume 14934 of the series Lecture Notes\n  in Computer Science pp 3-19, Springer"}
{"doi": "10.48550/arXiv.2108.11279", "date": "2021-08-25", "title": "The number of primitive words of unbounded exponent in the language of an HD0L-system is finite", "authors": "Karel Klouda, \u0160t\u011bp\u00e1n Starosta", "abstract": "Let $H$ be an HD0L-system. We show that there are only finitely many\nprimitive words $v$ with the property that $v^k$, for all integers $k$, is an\nelement of the factorial language of $H$. In particular, this result applies to\nthe set of all factors of a morphic word. We provide a formalized proof in the\nproof assistant Isabelle/HOL as part of the Combinatorics on Words Formalized\nproject.", "journal": "Journal of Combinatorial Theory, Series A, 206, 105904, 2024"}
{"doi": "10.48550/arXiv.1508.02621", "date": "2015-08-11", "title": "On parametric multilevel q-Gevrey asymptotics for some linear Cauchy problem", "authors": "Alberto Lastra, St\u00e9phane Malek", "abstract": "We study a linear $q-$difference-differential Cauchy problem, under the\naction of a perturbation parameter $\\epsilon$. This work deals with a\n$q-$analog of the research made in a previoues work, giving rise to a\ngeneralization of a recent work by the second author. This generalization is\nrelated to the nature of the forcing term which suggests the use of a\n$q-$analog of an acceleration procedure.\n  The proof leans on a $q-$analog of the so-called Ramis-Sibuya theorem which\nentails two distinct $q-$Gevrey orders. The work concludes with an application\nof the main result when the forcing term solves a related problem.", "journal": ""}
{"doi": "10.48550/arXiv.1607.01986", "date": "2016-07-07", "title": "On multiscale Gevrey and q-Gevrey asymptotics for some linear q-difference differential initial value Cauchy problems", "authors": "Alberto Lastra, Stephane Malek", "abstract": "We study the asymptotic behavior of the solutions related to a singularly\nperturbed q-difference-differential problem in the complex domain. The analytic\nsolution can be splitted according to the nature of the equation and its\ngeometry so that both, Gevrey and q-Gevrey asymptotic phenomena are observed\nand can be distinguished, relating the analytic and the formal solution.\n  The proof leans on a two level novel version of Ramis-Sibuya theorem under\nGevrey and q-Gevrey orders.", "journal": ""}
{"doi": "10.48550/arXiv.2311.12649", "date": "2023-11-21", "title": "MathGloss: Building mathematical glossaries from text", "authors": "Lucy Horowitz, Valeria de Paiva", "abstract": "MathGloss is a project to create a knowledge graph (KG) for undergraduate\nmathematics from text, automatically, using modern natural language processing\n(NLP) tools and resources already available on the web. MathGloss is a linked\ndatabase of undergraduate concepts in mathematics. So far, it combines five\nresources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph\nhosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses\nat the University of Chicago, (iii) the syllabus of the French undergraduate\nmathematics curriculum which includes hyperlinks to the automated theorem\nprover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by\nmathematicians, and (v) the nLab, a wiki for category theory also curated by\nmathematicians. MathGloss's goal is to bring together resources for learning\nmathematics and to allow every mathematician to tailor their learning to their\nown preferences. Moreover, by organizing different resources for learning\nundergraduate mathematics alongside those for learning formal mathematics, we\nhope to make it easier for mathematicians and formal tools (theorem provers,\ncomputer algebra systems, etc) experts to \"understand\" each other and break\ndown some of the barriers to formal math.", "journal": ""}
{"doi": "10.48550/arXiv.2205.01981", "date": "2022-05-04", "title": "The Isabelle ENIGMA", "authors": "Zarathustra A. Goertzel, Jan Jakub\u016fv, Cezary Kaliszyk, Miroslav Ol\u0161\u00e1k, Jelle Piepenbrock, Josef Urban", "abstract": "We significantly improve the performance of the E automated theorem prover on\nthe Isabelle Sledgehammer problems by combining learning and theorem proving in\nseveral ways. In particular, we develop targeted versions of the ENIGMA\nguidance for the Isabelle problems, targeted versions of neural premise\nselection, and targeted strategies for E. The methods are trained in several\niterations over hundreds of thousands untyped and typed first-order problems\nextracted from Isabelle. Our final best single-strategy ENIGMA and premise\nselection system improves the best previous version of E by 25.3% in 15\nseconds, outperforming also all other previous ATP and SMT systems.", "journal": ""}
{"doi": "10.48550/arXiv.1906.00046", "date": "2019-05-31", "title": "Interaction Trees: Representing Recursive and Impure Programs in Coq", "authors": "Li-yao Xia, Yannick Zakowski, Paul He, Chung-Kil Hur, Gregory Malecha, Benjamin C. Pierce, Steve Zdancewic", "abstract": "\"Interaction trees\" (ITrees) are a general-purpose data structure for\nrepresenting the behaviors of recursive programs that interact with their\nenvironments. A coinductive variant of \"free monads,\" ITrees are built out of\nuninterpreted events and their continuations. They support compositional\nconstruction of interpreters from \"event handlers\", which give meaning to\nevents by defining their semantics as monadic actions. ITrees are expressive\nenough to represent impure and potentially nonterminating, mutually recursive\ncomputations, while admitting a rich equational theory of equivalence up to\nweak bisimulation. In contrast to other approaches such as relationally\nspecified operational semantics, ITrees are executable via code extraction,\nmaking them suitable for debugging, testing, and implementing software\nartifacts that are amenable to formal verification.\n  We have implemented ITrees and their associated theory as a Coq library,\nmechanizing classic domain- and category-theoretic results about program\nsemantics, iteration, monadic structures, and equational reasoning. Although\nthe internals of the library rely heavily on coinductive proofs, the interface\nhides these details so that clients can use and reason about ITrees without\nexplicit use of Coq's coinduction tactics.\n  To showcase the utility of our theory, we prove the termination-sensitive\ncorrectness of a compiler from a simple imperative source language to an\nassembly-like target whose meanings are given in an ITree-based denotational\nsemantics. Unlike previous results using operational techniques, our\nbisimulation proof follows straightforwardly by structural induction and\nelementary rewriting via an equational theory of combinators for control-flow\ngraphs.", "journal": ""}
{"doi": "10.48550/arXiv.1708.07226", "date": "2017-08-24", "title": "From Concurrent Programs to Simulating Sequential Programs: Correctness of a Transformation", "authors": "Allan Blanchard, Fr\u00e9d\u00e9ric Loulergue, Nikolai Kosmatov", "abstract": "Frama-C is a software analysis framework that provides a common\ninfrastructure and a common behavioral specification language to plugins that\nimplement various static and dynamic analyses of C programs. Most plugins do\nnot support concurrency. We have proposed Conc2Seq, a Frama-C plugin based on\nprogram transformation, capable to leverage the existing huge code base of\nplugins and to handle concurrent C programs.\n  In this paper we formalize and sketch the proof of correctness of the program\ntransformation principle behind Conc2Seq, and present an effort towards the\nfull mechanization of both the formalization and proofs with the proof\nassistant Coq.", "journal": "EPTCS 253, 2017, pp. 109-123"}
{"doi": "10.48550/arXiv.2311.03585", "date": "2023-11-06", "title": "OpenBSD formal driver verification with SeL4", "authors": "Adriana Nicolae, Paul Irofti, Ioana Leustean", "abstract": "The seL4 microkernel is currently the only kernel that has been fully\nformally verified. In general, the increased interest in ensuring the security\nof a kernel's code results from its important role in the entire operating\nsystem. One of the basic features of an operating system is that it abstracts\nthe handling of devices. This abstraction is represented by device drivers -\nthe software that manages the hardware. A proper verification of the software\ncomponent could ensure that the device would work properly unless there is a\nhardware failure.In this paper, we choose to model the behavior of a device\ndriver and build the proof that the code implementation matches the expected\nbehavior. The proof was written in Isabelle/HOL, the code translation from C to\nIsabelle was done automatically by the use of the C-to-Isabelle Parser and\nAutoCorres tools. We choose Isabelle theorem prover because its efficiency was\nalready shown through the verification of seL4 microkernel.", "journal": ""}
{"doi": "10.48550/arXiv.2202.04330", "date": "2022-02-09", "title": "Reflexive tactics for algebra, revisited", "authors": "Kazuhiko Sakaguchi", "abstract": "Computational reflection allows us to turn verified decision procedures into\nefficient automated reasoning tools in proof assistants. The typical\napplications of such methodology include mathematical structures that have\ndecidable theory fragments, e.g., equational theories of commutative rings and\nlattices. However, such existing tools are known not to cooperate with packed\nclasses, a methodology to define mathematical structures in dependent type\ntheory, that allows for the sharing of vocabulary across the inheritance\nhierarchy. Additionally, such tools do not support homomorphisms whose domain\nand codomain types may differ. This paper demonstrates how to implement\nreflexive tactics that support packed classes and homomorphisms. As\napplications of our methodology, we adapt the ring and field tactics of Coq to\nthe commutative ring and field structures of the Mathematical Components\nlibrary, and apply the resulting tactics to the formal proof of the\nirrationality of $\\zeta(3)$ by Chyzak, Mahboubi, and Sibut-Pinote, to bring\nmore proof automation.", "journal": ""}
{"doi": "10.48550/arXiv.1808.08511", "date": "2018-08-26", "title": "Optimization of Executable Formal Interpreters developed in Higher-order Theorem Proving Systems", "authors": "Zheng Yang, Hang Lei", "abstract": "In recent publications, we presented a novel formal symbolic process virtual\nmachine (FSPVM) framework that combined higher-order theorem proving and\nsymbolic execution for verifying the reliability and security of smart\ncontracts developed in the Ethereum blockchain system without suffering the\nstandard issues surrounding reusability, consistency, and automation. A\nspecific FSPVM, denoted as FSPVM-E, was developed in Coq based on a general,\nextensible, and reusable formal memory (GERM) framework, an extensible and\nuniversal formal intermediate programming language, denoted as Lolisa, which is\na large subset of the Solidity programming language that uses generalized\nalgebraic datatypes, and a corresponding formally verified interpreter for\nLolisa, denoted as FEther, which serves as a crucial component of FSPVM-E.\nHowever, our past work has demonstrated that the execution efficiency of the\nstandard development of FEther is extremely low. As a result, FSPVM-E fails to\nachieve its expected verification effect. The present work addresses this issue\nby first identifying three root causes of the low execution efficiency of\nformal interpreters. We then build abstract models of these causes, and present\nrespective optimization schemes for rectifying the identified conditions.\nFinally, we apply these optimization schemes to FEther, and demonstrate that\nits execution efficiency has been improved significantly.", "journal": "IEEE Access, 2018"}
{"doi": "10.48550/arXiv.2202.03159", "date": "2022-02-07", "title": "$L^2$-Betti numbers and computability of reals", "authors": "Clara Loeh, Matthias Uschold", "abstract": "We study the computability degree of real numbers arising as $L^2$-Betti\nnumbers or $L^2$-torsion of groups, parametrised over the Turing degree of the\nword problem.", "journal": ""}
{"doi": "10.48550/arXiv.1707.06901", "date": "2017-07-21", "title": "A Verified Compiler for Probability Density Functions", "authors": "Manuel Eberl, Johannes H\u00f6lzl, Tobias Nipkow", "abstract": "Bhat et al. developed an inductive compiler that computes density functions\nfor probability spaces described by programs in a simple probabilistic\nfunctional language. In this work, we implement such a compiler for a modified\nversion of this language within the theorem prover Isabelle and give a formal\nproof of its soundness w.r.t. the semantics of the source and target language.\nTogether with Isabelle's code generation for inductive predicates, this yields\na fully verified, executable density compiler. The proof is done in two steps,\nusing a standard refinement approach: first, an abstract compiler working with\nabstract functions modelled directly in the theorem prover's logic is defined\nand proven sound. Then, this compiler is refined to a concrete version that\nreturns a target-language expression.", "journal": ""}
{"doi": "10.48550/arXiv.1311.6107", "date": "2013-11-24", "title": "Off-policy reinforcement learning for $ H_\\infty $ control design", "authors": "Biao Luo, Huai-Ning Wu, Tingwen Huang", "abstract": "The $H_\\infty$ control design problem is considered for nonlinear systems\nwith unknown internal system model. It is known that the nonlinear $ H_\\infty $\ncontrol problem can be transformed into solving the so-called\nHamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial\ndifferential equation that is generally impossible to be solved analytically.\nEven worse, model-based approaches cannot be used for approximately solving HJI\nequation, when the accurate system model is unavailable or costly to obtain in\npractice. To overcome these difficulties, an off-policy reinforcement leaning\n(RL) method is introduced to learn the solution of HJI equation from real\nsystem data instead of mathematical system model, and its convergence is\nproved. In the off-policy RL method, the system data can be generated with\narbitrary policies rather than the evaluating policy, which is extremely\nimportant and promising for practical systems. For implementation purpose, a\nneural network (NN) based actor-critic structure is employed and a least-square\nNN weight update algorithm is derived based on the method of weighted\nresiduals. Finally, the developed NN-based off-policy RL method is tested on a\nlinear F16 aircraft plant, and further applied to a rotational/translational\nactuator system.", "journal": ""}
{"doi": "10.48550/arXiv.1601.04299", "date": "2016-01-17", "title": "Heterogeneous substitution systems revisited", "authors": "Benedikt Ahrens, Ralph Matthes", "abstract": "Matthes and Uustalu (TCS 327(1-2):155-174, 2004) presented a categorical\ndescription of substitution systems capable of capturing syntax involving\nbinding which is independent of whether the syntax is made up from least or\ngreatest fixed points. We extend this work in two directions: we continue the\nanalysis by creating more categorical structure, in particular by organizing\nsubstitution systems into a category and studying its properties, and we\ndevelop the proofs of the results of the cited paper and our new ones in\nUniMath, a recent library of univalent mathematics formalized in the Coq\ntheorem prover.", "journal": ""}
{"doi": "10.48550/arXiv.2012.10511", "date": "2020-12-18", "title": "An Infrastructure for Faithful Execution of Remote Attestation Protocols", "authors": "Adam Petz, Perry Alexander", "abstract": "Remote attestation is an emerging technology for establishing trust in a\nremote computing system. Copland is a domain-specific language for specifying\nlayered attestation protocols, characterizing attestation-relevant system\nevents, and describing evidence bundling. In this work we formally define and\nverify a Copland Compiler and Copland Virtual Machine for executing Copland\nprotocols. The compiler translates Copland into instructions that are executed\non the virtual machine. The compiler and virtual machine are implemented as\nmonadic, functional programs in the Coq proof assistant and verified with\nrespect to the Copland event and evidence semantics. In addition we introduce\nthe Attestation Manager Monad as an environment for managing Copland term\nexecution providing support for managing nonces, binding results of Copland\nprotocols to variables, and appraising evidence results.", "journal": ""}
{"doi": "10.48550/arXiv.1910.04492", "date": "2019-10-10", "title": "Infinitesimal ideal systems and the Atiyah class", "authors": "Madeleine Jotz Lean", "abstract": "This short note gives a geometric interpretation of the Atiyah class of a Lie\npair. It proves that it vanishes if the subalgebroid is the kernel of a\nfibration of Lie algebroids. In other words, the Atiyah class of a Lie pair\nvanishes if the subalgebroid is the fiber of an ideal system in the Lie\nalgebroid. In order to prove this, a new characterisation of the ideal\ncondition for an infinitesimal ideal system is found.", "journal": ""}
{"doi": "10.48550/arXiv.2004.02983", "date": "2020-04-06", "title": "Integrating Owicki-Gries for C11-Style Memory Models into Isabelle/HOL", "authors": "Sadegh Dalvandi, Brijesh Dongol, Simon Doherty", "abstract": "Weak memory presents a new challenge for program verification and has\nresulted in the development of a variety of specialised logics. For C11-style\nmemory models, our previous work has shown that it is possible to extend Hoare\nlogic and Owicki-Gries reasoning to verify correctness of weak memory programs.\nThe technique introduces a set of high-level assertions over C11 states\ntogether with a set of basic Hoare-style axioms over atomic weak memory\nstatements (e.g., reads/writes), but retains all other standard proof\nobligations for compound statements. This paper takes this line of work further\nby showing Nipkow and Nieto's encoding of Owicki-Gries in the Isabelle theorem\nprover can be extended to handle C11-style weak memory models in a\nstraightforward manner. We exemplify our techniques over several litmus tests\nfrom the literature and a non-trivial example: Peterson's algorithm adapted for\nC11. For the examples we consider, the proof outlines can be automatically\ndischarged using the existing Isabelle tactics developed by Nipkow and Nieto.\nThe benefit here is that programs can be written using a familiar pseudocode\nsyntax with assertions embedded directly into the program.", "journal": ""}
{"doi": "10.48550/arXiv.1910.13553", "date": "2019-10-29", "title": "Modelling and testing timed data-flow reactive systems in Coq from controlled natural-language requirements", "authors": "Gustavo Carvalho, Igor Meira", "abstract": "Data-flow reactive systems (DFRSs) are a class of embedded systems whose\ninputs and outputs are always available as signals. Input signals can be seen\nas data provided by sensors, whereas the output data are provided to system\nactuators. In previous works, verifying properties of DFRS models was\naccomplished in a programmatic way, with no formal guarantees, and test cases\nwere generated by translating theses models into other notations. Here, we use\nCoq as a single framework to specify and verify DFRS models. Moreover, the\nspecification of DFRSs in Coq is automatically derived from controlled\nnatural-language requirements. Property verification is defined in both logical\nand functional terms. The latter allows for easier proof construction. Tests\nare generated with the support of the QuickChick tool. Considering examples\nfrom the literature, but also from the aerospace industry (Embraer), our\ntesting strategy was evaluated in terms of performance and the ability to\ndetect defects generated by mutation; within 8 seconds, we achieved an average\nmutation score of 75.80%.", "journal": ""}
{"doi": "10.48550/arXiv.2411.12522", "date": "2024-11-19", "title": "Canonical insurance models: stochastic equations and comparison theorems", "authors": "Marcus C. Christiansen, Christian Furrer", "abstract": "Thiele's differential equation explains the change in prospective reserve and\nplays a fundamental role in safe-side calculations and other types of actuarial\nmodel comparisons. This paper presents a `model lean' version of Thiele's\nequation with the novel feature that it supports any canonical insurance model,\nirrespective of the model's intertemporal dependence structure. The basis for\nthis is a canonical and path-wise model construction that simultaneously\nhandles discrete and absolutely continuous modeling regimes. Comparison\ntheorems for differing canonical insurance models follow directly from the\nresulting stochastic backward equations. The elegance with which these\ncomparison theorems handle non-equivalence of probability measures is one of\ntheir major advantages over previous results.", "journal": ""}
{"doi": "10.48550/arXiv.1607.02226", "date": "2016-07-08", "title": "Renaming Global Variables in C Mechanically Proved Correct", "authors": "Julien Cohen", "abstract": "Most integrated development environments are shipped with refactoring tools.\nHowever, their refactoring operations are often known to be unreliable. As a\nconsequence, developers have to test their code after applying an automatic\nrefactoring. In this article, we consider a refactoring operation (renaming of\nglobal variables in C), and we prove that its core implementation preserves the\nset of possible behaviors of transformed programs. That proof of correctness\nrelies on the operational semantics of C provided by CompCert C in Coq.", "journal": "EPTCS 216, 2016, pp. 50-64"}
{"doi": "10.48550/arXiv.2303.05244", "date": "2023-03-09", "title": "Transport via Partial Galois Connections and Equivalences", "authors": "Kevin Kappelmann", "abstract": "Multiple types can represent the same concept. For example, lists and trees\ncan both represent sets. Unfortunately, this easily leads to incomplete\nlibraries: some set-operations may only be available on lists, others only on\ntrees. Similarly, subtypes and quotients are commonly used to construct new\ntype abstractions in formal verification. In such cases, one often wishes to\nreuse operations on the representation type for the new type abstraction, but\nto no avail: the types are not the same.\n  To address these problems, we present a new framework that transports\nprograms via equivalences. Existing transport frameworks are either designed\nfor dependently typed, constructive proof assistants, use univalence, or are\nrestricted to partial quotient types. Our framework (1) is designed for simple\ntype theory, (2) generalises previous approaches working on partial quotient\ntypes, and (3) is based on standard mathematical concepts, particularly Galois\nconnections and equivalences. We introduce the notion of partial Galois\nconnections and equivalences and prove their closure properties under\n(dependent) function relators, (co)datatypes, and compositions. We formalised\nthe framework in Isabelle/HOL and provide a prototype.\n  This is the extended version of \"Transport via Partial Galois Connections and\nEquivalences\", 21st Asian Symposium on Programming Languages and Systems, 2023.", "journal": "Asian Symposium on Programming Languages and Systems APLAS 2023:\n  Programming Languages and Systems, 225-245"}
{"doi": "10.48550/arXiv.2003.03785", "date": "2020-03-08", "title": "Dependently Typed Knowledge Graphs", "authors": "Zhangsheng Lai, Aik Beng Ng, Liang Ze Wong, Simon See, Shaowei Lin", "abstract": "Reasoning over knowledge graphs is traditionally built upon a hierarchy of\nlanguages in the Semantic Web Stack. Starting from the Resource Description\nFramework (RDF) for knowledge graphs, more advanced constructs have been\nintroduced through various syntax extensions to add reasoning capabilities to\nknowledge graphs. In this paper, we show how standardized semantic web\ntechnologies (RDF and its query language SPARQL) can be reproduced in a unified\nmanner with dependent type theory. In addition to providing the basic\nfunctionalities of knowledge graphs, dependent types add expressiveness in\nencoding both entities and queries, explainability in answers to queries\nthrough witnesses, and compositionality and automation in the construction of\nwitnesses. Using the Coq proof assistant, we demonstrate how to build and query\ndependently typed knowledge graphs as a proof of concept for future works in\nthis direction.", "journal": ""}
{"doi": "10.48550/arXiv.1509.03339", "date": "2015-09-10", "title": "A Formal C Memory Model for Separation Logic", "authors": "Robbert Krebbers", "abstract": "The core of a formal semantics of an imperative programming language is a\nmemory model that describes the behavior of operations on the memory. Defining\na memory model that matches the description of C in the C11 standard is\nchallenging because C allows both high-level (by means of typed expressions)\nand low-level (by means of bit manipulation) memory accesses. The C11 standard\nhas restricted the interaction between these two levels to make more effective\ncompiler optimizations possible, on the expense of making the memory model\ncomplicated.\n  We describe a formal memory model of the (non-concurrent part of the) C11\nstandard that incorporates these restrictions, and at the same time describes\nlow-level memory operations. This formal memory model includes a rich\npermission model to make it usable in separation logic and supports reasoning\nabout program transformations. The memory model and essential properties of it\nhave been fully formalized using the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2307.08514", "date": "2023-07-17", "title": "Modular Denotational Semantics for Effects with Guarded Interaction Trees", "authors": "Dan Frumin, Amin Timany, Lars Birkedal", "abstract": "We present guarded interaction trees -- a structure and a fully formalized\nframework for representing higher-order computations with higher-order effects\nin Coq, inspired by domain theory and the recently proposed interaction trees.\nWe also present an accompanying separation logic for reasoning about guarded\ninteraction trees. To demonstrate that guarded interaction trees provide a\nconvenient domain for interpreting higher-order languages with effects, we\ndefine an interpretation of a PCF-like language with effects and show that this\ninterpretation is sound and computationally adequate; we prove the latter using\na logical relation defined using the separation logic. Guarded interaction\ntrees also allow us to combine different effects and reason about them\nmodularly. To illustrate this point, we give a modular proof of type soundness\nof cross-language interactions for safe interoperability of different\nhigher-order languages with different effects. All results in the paper are\nformalized in Coq using the Iris logic over guarded type theory.", "journal": ""}
{"doi": "10.48550/arXiv.2205.03473", "date": "2022-05-06", "title": "Energy-efficient Connected Cruise Control with Lean Penetration of Connected Vehicles", "authors": "Minghao Shen, Chaozhe R. He, Tamas Molnar, A. Harvey Bell, Gabor Orosz", "abstract": "This paper focuses on energy-efficient longitudinal controller design for a\nconnected automated truck that travels in mixed traffic consisting of connected\nand non-connected vehicles. The truck has access to information about connected\nvehicles beyond line of sight using vehicle-to-vehicle (V2V) communication. A\nnovel connected cruise control design is proposed which incorporates additional\ndelays into the control law when responding to distant connected vehicles to\naccount for the finite propagation of traffic waves. The speeds of\nnon-connected vehicles are modeled as stochastic processes. A fundamental\ntheorem is proven which links the spectral properties of the motion signals to\nthe average energy consumption. This enables us to tune controller parameters\nand maximize energy efficiency. Simulations with synthetic data and real\ntraffic data are used to demonstrate the energy efficiency of the control\ndesign. It is demonstrated that even with lean penetration of connected\nvehicles, our controller can bring significant energy savings.", "journal": ""}
{"doi": "10.48550/arXiv.1610.03554", "date": "2015-10-25", "title": "Review of Inferring Latent Attributes from Twitter", "authors": "Surabhi Singh Ludu", "abstract": "This paper reviews literature from 2011 to 2013 on how Latent attributes like\ngender, political leaning etc. can be inferred from a person's twitter and\nneighborhood data. Prediction of demographic data can bring value to\nbusinesses, can prove instrumental in legal investigation. Moreover, political\nleanings can be inferred from the wide variety of user data available on-line.\nThe motive of this review is to understand how large data sets can be made from\navailable twitter data. The tweeting and re tweeting behavior of a user can be\nuser to infer attributes like, gender, age etc. We explore in this text how\nthis field can be expanded in future and possible avenues for future research.", "journal": ""}
{"doi": "10.48550/arXiv.2201.06692", "date": "2022-01-18", "title": "Explainable Decision Making with Lean and Argumentative Explanations", "authors": "Xiuyi Fan, Francesca Toni", "abstract": "It is widely acknowledged that transparency of automated decision making is\ncrucial for deployability of intelligent systems, and explaining the reasons\nwhy some decisions are \"good\" and some are not is a way to achieving this\ntransparency. We consider two variants of decision making, where \"good\"\ndecisions amount to alternatives (i) meeting \"most\" goals, and (ii) meeting\n\"most preferred\" goals. We then define, for each variant and notion of\n\"goodness\" (corresponding to a number of existing notions in the literature),\nexplanations in two formats, for justifying the selection of an alternative to\naudiences with differing needs and competences: lean explanations, in terms of\ngoals satisfied and, for some notions of \"goodness\", alternative decisions, and\nargumentative explanations, reflecting the decision process leading to the\nselection, while corresponding to the lean explanations. To define\nargumentative explanations, we use assumption-based argumentation (ABA), a\nwell-known form of structured argumentation. Specifically, we define ABA\nframeworks such that \"good\" decisions are admissible ABA arguments and draw\nargumentative explanations from dispute trees sanctioning this admissibility.\nFinally, we instantiate our overall framework for explainable decision-making\nto accommodate connections between goals and decisions in terms of decision\ngraphs incorporating defeasible and non-defeasible information.", "journal": ""}
{"doi": "10.48550/arXiv.1202.6472", "date": "2012-02-29", "title": "First steps towards the certification of an ARM simulator using Compcert", "authors": "Xiaomu Shi, Jean-Fran\u00e7ois Monin, Frederic Tuong, Fr\u00e9d\u00e9ric Blanqui", "abstract": "The simulation of Systems-on-Chip (SoC) is nowadays a hot topic because,\nbeyond providing many debugging facilities, it allows the development of\ndedicated software before the hardware is available. Low-consumption CPUs such\nas ARM play a central role in SoC. However, the effectiveness of simulation\ndepends on the faithfulness of the simulator. To this effect, we propose here\nto prove significant parts of such a simulator, SimSoC. Basically, on one hand,\nwe develop a Coq formal model of the ARM architecture while on the other hand,\nwe consider a version of the simulator including components written in\nCompcert-C. Then we prove that the simulation of ARM operations, according to\nCompcert-C formal semantics, conforms to the expected formal model of ARM. Size\nissues are partly dealt with using automatic generation of significant parts of\nthe Coq model and of SimSoC from the official textual definition of ARM.\nHowever, this is still a long-term project. We report here the current stage of\nour efforts and discuss in particular the use of Compcert-C in this framework.", "journal": ""}
{"doi": "10.48550/arXiv.2306.09074", "date": "2023-06-15", "title": "Category Theory in Isabelle/HOL as a Basis for Meta-logical Investigation", "authors": "Jonas Bayer, Aleksey Gonus, Christoph Benzm\u00fcller, Dana S. Scott", "abstract": "This paper presents meta-logical investigations based on category theory\nusing the proof assistant Isabelle/HOL. We demonstrate the potential of a free\nlogic based shallow semantic embedding of category theory by providing a\nformalization of the notion of elementary topoi. Additionally, we formalize\nsymmetrical monoidal closed categories expressing the denotational semantic\nmodel of intuitionistic multiplicative linear logic. Next to these\nmeta-logical-investigations, we contribute to building an Isabelle category\ntheory library, with a focus on ease of use in the formalization beyond\ncategory theory itself. This work paves the way for future formalizations based\non category theory and demonstrates the power of automated reasoning in\ninvestigating meta-logical questions.", "journal": "Intelligent Computer Mathematics (CICM 2023). Lecture Notes in\n  Computer Science, vol 14101, pp. 69-83. Springer, Cham"}
{"doi": "10.48550/arXiv.1904.10570", "date": "2019-04-23", "title": "A formalization of forcing and the unprovability of the continuum hypothesis", "authors": "Jesse Michael Han, Floris van Doorn", "abstract": "We describe a formalization of forcing using Boolean-valued models in the\nLean 3 theorem prover, including the fundamental theorem of forcing and a deep\nembedding of first-order logic with a Boolean-valued soundness theorem. As an\napplication of our framework, we specialize our construction to the Boolean\nalgebra of regular opens of the Cantor space $2^{\\omega_2 \\times \\omega}$ and\nformally verify the failure of the continuum hypothesis in the resulting model.", "journal": ""}
{"doi": "10.48550/arXiv.2303.14544", "date": "2023-03-25", "title": "Privacy-Enhancing Technologies in Federated Learning for the Internet of Healthcare Things: A Survey", "authors": "Fatemeh Mosaiyebzadeh, Seyedamin Pouriyeh, Reza M. Parizi, Quan Z. Sheng, Meng Han, Liang Zhao, Giovanna Sannino, Daniel Mac\u00eado Batista", "abstract": "Advancements in wearable medical devices in IoT technology are shaping the\nmodern healthcare system. With the emergence of the Internet of Healthcare\nThings (IoHT), we are witnessing how efficient healthcare services are provided\nto patients and how healthcare professionals are effectively used AI-based\nmodels to analyze the data collected from IoHT devices for the treatment of\nvarious diseases. To avoid privacy breaches, these data must be processed and\nanalyzed in compliance with the legal rules and regulations such as HIPAA and\nGDPR. Federated learning is a machine leaning based approach that allows\nmultiple entities to collaboratively train a ML model without sharing their\ndata. This is particularly useful in the healthcare domain where data privacy\nand security are big concerns. Even though FL addresses some privacy concerns,\nthere is still no formal proof of privacy guarantees for IoHT data. Privacy\nEnhancing Technologies (PETs) are a set of tools and techniques that are\ndesigned to enhance the privacy and security of online communications and data\nsharing. PETs provide a range of features that help protect users' personal\ninformation and sensitive data from unauthorized access and tracking. This\npaper reviews PETs in detail and comprehensively in relation to FL in the IoHT\nsetting and identifies several key challenges for future research.", "journal": ""}
{"doi": "10.48550/arXiv.1206.0136", "date": "2012-06-01", "title": "General Bindings and Alpha-Equivalence in Nominal Isabelle", "authors": "Christian Urban, Cezary Kaliszyk", "abstract": "Nominal Isabelle is a definitional extension of the Isabelle/HOL theorem\nprover. It provides a proving infrastructure for reasoning about programming\nlanguage calculi involving named bound variables (as opposed to de-Bruijn\nindices). In this paper we present an extension of Nominal Isabelle for dealing\nwith general bindings, that means term constructors where multiple variables\nare bound at once. Such general bindings are ubiquitous in programming language\nresearch and only very poorly supported with single binders, such as\nlambda-abstractions. Our extension includes new definitions of\nalpha-equivalence and establishes automatically the reasoning infrastructure\nfor alpha-equated terms. We also prove strong induction principles that have\nthe usual variable convention already built in.", "journal": "Logical Methods in Computer Science, Volume 8, Issue 2 (June 20,\n  2012) lmcs:813"}
{"doi": "10.48550/arXiv.1507.03634", "date": "2015-07-13", "title": "Idempotents in intensional type theory", "authors": "Michael Shulman", "abstract": "We study idempotents in intensional Martin-L\\\"of type theory, and in\nparticular the question of when and whether they split. We show that in the\npresence of propositional truncation and Voevodsky's univalence axiom, there\nexist idempotents that do not split; thus in plain MLTT not all idempotents can\nbe proven to split. On the other hand, assuming only function extensionality,\nan idempotent can be split if and only if its witness of idempotency satisfies\none extra coherence condition. Both proofs are inspired by parallel results of\nLurie in higher category theory, showing that ideas from higher category theory\nand homotopy theory can have applications even in ordinary MLTT.\n  Finally, we show that although the witness of idempotency can be recovered\nfrom a splitting, the one extra coherence condition cannot in general; and we\nconstruct \"the type of fully coherent idempotents\", by splitting an idempotent\non the type of partially coherent ones. Our results have been formally verified\nin the proof assistant Coq.", "journal": "Logical Methods in Computer Science, Volume 12, Issue 3 (April 27,\n  2017) lmcs:2027"}
{"doi": "10.48550/arXiv.2308.04741", "date": "2023-08-09", "title": "Local Reasoning about Probabilistic Behaviour for Classical-Quantum Programs", "authors": "Huiling Wu, Yuxin Deng, Ming Xu", "abstract": "Verifying the functional correctness of programs with both classical and\nquantum constructs is a challenging task. The presence of probabilistic\nbehaviour entailed by quantum measurements and unbounded while loops complicate\nthe verification task greatly. We propose a new quantum Hoare logic for local\nreasoning about probabilistic behaviour by introducing distribution formulas to\nspecify probabilistic properties. We show that the proof rules in the logic are\nsound with respect to a denotational semantics. To demonstrate the\neffectiveness of the logic, we formally verify the correctness of non-trivial\nquantum algorithms including the HHL and Shor's algorithms. Moreover, we embed\nour logic into the proof assistant Coq. The resulting logical framework, called\nCoqQLR, can facilitate semi-automated reasoning about classical--quantum\nprograms.", "journal": ""}
{"doi": "10.48550/arXiv.2310.14929", "date": "2023-10-23", "title": "Definitional Functoriality for Dependent (Sub)Types -- Extended version", "authors": "Th\u00e9o Laurent, Meven Lennon-Bertrand, Kenji Maillard", "abstract": "Dependently typed proof assistant rely crucially on definitional equality,\nwhich relates types and terms that are automatically identified in the\nunderlying type theory. This paper extends type theory with definitional\nfunctor laws, equations satisfied propositionally by a large class of\ncontainer-like type constructors $F : \\mathrm{Type} \\to \\mathrm{Type}$,\nequipped with a $\\mathrm{map}_{F} : (A \\to B) \\to F\\,A \\to F\\,B$, such as lists\nor trees. Promoting these equations to definitional ones strengthens the\ntheory, enabling slicker proofs and more automation for functorial type\nconstructors. This extension is used to modularly justify a structural form of\ncoercive subtyping, propagating subtyping through type formers in a map-like\nfashion. We show that the resulting notion of coercive subtyping, thanks to the\nextra definitional equations, is equivalent to a natural and implicit form of\nsubsumptive subtyping. The key result of decidability of type-checking in a\ndependent type system with functor laws for lists has been entirely mechanized\nin Coq. This is the extended version of the work with the same name published\nat ESOP'24.", "journal": ""}
{"doi": "10.48550/arXiv.1310.0794", "date": "2013-10-02", "title": "Formal verification in Coq of program properties involving the global state effect", "authors": "Jean-Guillaume Dumas, Dominique Duval, Burak Ekici, Damien Pous", "abstract": "The syntax of an imperative language does not mention explicitly the state,\nwhile its denotational semantics has to mention it. In this paper we present a\nframework for the verification in Coq of properties of programs manipulating\nthe global state effect. These properties are expressed in a proof system which\nis close to the syntax, as in effect systems, in the sense that the state does\nnot appear explicitly in the type of expressions which manipulate it. Rather,\nthe state appears via decorations added to terms and to equations. In this\nsystem, proofs of programs thus present two aspects: properties can be verified\n{\\em up to effects} or the effects can be taken into account. The design of our\nCoq library consequently reflects these two aspects: our framework is centered\naround the construction of two inductive and dependent types, one for terms up\nto effects and one for the manipulation of decorations.", "journal": ""}
{"doi": "10.48550/arXiv.1703.01033", "date": "2017-03-03", "title": "Existence of travelling waves and high activation energy limits for a onedimensional thermo-diffusive lean spray flame model", "authors": "Pierre Berthonnaud, Komla Domelevo", "abstract": "We provide a mathematical analysis of a thermo-diffusive combustion model of\nlean spray flames, for which we prove the existence of travelling waves. In the\nhigh activation energy singular limit we show the existence of two distinct\ncombustion regimes with a sharp transition -- the diffusion limited regime and\nthe vaporisation controlled regime. The latter is specific to spray flames with\nslow enough vaporisation. We give a complete characterisation of these regimes,\nincluding explicit velocities, profiles, and upper estimate of the size of the\ninternal combustion layer. Our model is on the one hand simple enough to allow\nfor explicit asymptotic limits and on the other hand rich enough to capture\nsome particular aspects of spray combustion. Finally, we briefly discuss the\ncases where the vaporisation is infinitely fast, or where the spray is\npolydisperse.", "journal": ""}
{"doi": "10.48550/arXiv.2208.00758", "date": "2022-08-01", "title": "Finding smart contract vulnerabilities with ConCert's property-based testing framework", "authors": "Mikkel Milo, Eske Hoy Nielsen, Danil Annenkov, Bas Spitters", "abstract": "We provide three detailed case studies of vulnerabilities in smart contracts,\nand show how property-based testing would have found them:\n  1. the Dexter1 token exchange;\n  2. the iToken;\n  3. the ICO of Brave's BAT token.\n  The last example is, in fact, new, and was missed in the auditing process. We\nhave implemented this testing in ConCert, a general executable\nmodel/specification of smart contract execution in the Coq proof assistant.\nConCert contracts can be used to generate verified smart contracts in Tezos'\nLIGO and Concordium's rust language. We thus show the effectiveness of\ncombining formal verification and property-based testing of smart contracts.", "journal": "FMBC: Formal Methods for Blockchains, 2022"}
{"doi": "10.48550/arXiv.2311.11795", "date": "2023-11-20", "title": "Effects and Coeffects in Call-By-Push-Value (Extended Version)", "authors": "Cassia Torczon, Emmanuel Su\u00e1rez Acevedo, Shubh Agrawal, Joey Velez-Ginorio, Stephanie Weirich", "abstract": "Effect and coeffect tracking integrate many types of compile-time analysis,\nsuch as cost, liveness, or dataflow, directly into a language's type system. In\nthis paper, we investigate the addition of effect and coeffect tracking to the\ntype system of call-by-push-value (CBPV), a computational model useful in\ncompilation for its isolation of effects and for its ability to cleanly express\nboth call-by-name and call-by-value computations. Our main result is\neffect-and-coeffect soundness, which asserts that the type system accurately\nbounds the effects that the program may trigger during execution and accurately\ntracks the demands that the program may make on its environment. This result\nholds for two different dynamic semantics: a generic one that can be adapted\nfor different coeffects and one that is adapted for reasoning about resource\nusage. In particular, the second semantics discards the evaluation of unused\nvalues and pure computations while ensuring that effectful computations are\nalways evaluated, even if their results are not required. Our results have been\nmechanized using the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1911.04732", "date": "2019-11-12", "title": "Smart Contract Interactions in Coq", "authors": "Jakob Botsch Nielsen, Bas Spitters", "abstract": "We present a model/executable specification of smart contract execution in\nCoq. Our formalization allows for inter-contract communication and generalizes\nexisting work by allowing modelling of both depth-first execution blockchains\n(like Ethereum) and breadth-first execution blockchains (like Tezos). We\nrepresent smart contracts programs in Coq's functional language Gallina,\nenabling easier reasoning about functional correctness of concrete contracts\nthan other approaches. In particular we develop a Congress contract in this\nstyle. This contract -- a simplified version of the infamous DAO -- is\ninteresting because of its very dynamic communication pattern with other\ncontracts. We give a high-level partial specification of the Congress's\nbehavior, related to reentrancy, and prove that the Congress satisfies it for\nall possible smart contract execution orders.", "journal": "1st Workshop on Formal Methods for Blockchains, 3rd Formal Methods\n  World Congress on October 11, 2019 in Porto, Portugal"}
{"doi": "10.48550/arXiv.2008.12433", "date": "2020-08-28", "title": "2-adjoint equivalences in homotopy type theory", "authors": "Daniel Carranza, Jonathan Chang, Chris Kapulkin, Ryan Sandford", "abstract": "We introduce the notion of (half) 2-adjoint equivalences in Homotopy Type\nTheory and prove their expected properties. We formalized these results in the\nLean Theorem Prover.", "journal": "Logical Methods in Computer Science, Volume 17, Issue 1 (January\n  22, 2021) lmcs:6778"}
{"doi": "10.48550/arXiv.2409.08119", "date": "2024-09-12", "title": "Duality theory in linear optimization and its extensions -- formally verified", "authors": "Martin Dvorak, Vladimir Kolmogorov", "abstract": "Farkas established that a system of linear inequalities has a solution if and\nonly if we cannot obtain a contradiction by taking a linear combination of the\ninequalities. We state and formally prove several Farkas-like theorems over\nlinearly ordered fields in Lean 4. Furthermore, we extend duality theory to the\ncase when some coefficients are allowed to take ``infinite values''.", "journal": ""}
{"doi": "10.48550/arXiv.2501.05222", "date": "2025-01-09", "title": "ParaRev: Building a dataset for Scientific Paragraph Revision annotated with revision instruction", "authors": "L\u00e9ane Jourdan, Nicolas Hernandez, Richard Dufour, Florian Boudin, Akiko Aizawa", "abstract": "Revision is a crucial step in scientific writing, where authors refine their\nwork to improve clarity, structure, and academic quality. Existing approaches\nto automated writing assistance often focus on sentence-level revisions, which\nfail to capture the broader context needed for effective modification. In this\npaper, we explore the impact of shifting from sentence-level to paragraph-level\nscope for the task of scientific text revision. The paragraph level definition\nof the task allows for more meaningful changes, and is guided by detailed\nrevision instructions rather than general ones. To support this task, we\nintroduce ParaRev, the first dataset of revised scientific paragraphs with an\nevaluation subset manually annotated with revision instructions. Our\nexperiments demonstrate that using detailed instructions significantly improves\nthe quality of automated revisions compared to general approaches, no matter\nthe model or the metric considered.", "journal": "https://aclanthology.org/2025.wraicogs-1/"}
{"doi": "10.48550/arXiv.2112.05996", "date": "2021-12-11", "title": "Formalising the Foundations of Discrete Reinforcement Learning in Isabelle/HOL", "authors": "Mark Chevallier, Jacques Fleuriot", "abstract": "We present a formalisation of finite Markov decision processes with rewards\nin the Isabelle theorem prover. We focus on the foundations required for\ndynamic programming and the use of reinforcement learning agents over such\nprocesses. In particular, we derive the Bellman equation from first principles\n(in both scalar and vector form), derive a vector calculation that produces the\nexpected value of any policy p, and go on to prove the existence of a\nuniversally optimal policy where there is a discounting factor less than one.\nLastly, we prove that the value iteration and the policy iteration algorithms\nwork in finite time, producing an epsilon-optimal and a fully optimal policy\nrespectively.", "journal": ""}
{"doi": "10.48550/arXiv.2007.07571", "date": "2020-07-15", "title": "Computational Logic for Biomedicine and Neurosciences", "authors": "Elisabetta de Maria, Joelle Despeyroux, Amy Felty, Pietro Li\u00f2, Carlos Olarte, Abdorrahim Bahrami", "abstract": "We advocate here the use of computational logic for systems biology, as a\n\\emph{unified and safe} framework well suited for both modeling the dynamic\nbehaviour of biological systems, expressing properties of them, and verifying\nthese properties. The potential candidate logics should have a traditional\nproof theoretic pedigree (including either induction, or a sequent calculus\npresentation enjoying cut-elimination and focusing), and should come with\ncertified proof tools. Beyond providing a reliable framework, this allows the\ncorrect encodings of our biological systems. % For systems biology in general\nand biomedicine in particular, we have so far, for the modeling part, three\ncandidate logics: all based on linear logic. The studied properties and their\nproofs are formalized in a very expressive (non linear) inductive logic: the\nCalculus of Inductive Constructions (CIC). The examples we have considered so\nfar are relatively simple ones; however, all coming with formal semi-automatic\nproofs in the Coq system, which implements CIC. In neuroscience, we are\ndirectly using CIC and Coq, to model neurons and some simple neuronal circuits\nand prove some of their dynamic properties. % In biomedicine, the study of\nmulti omic pathway interactions, together with clinical and electronic health\nrecord data should help in drug discovery and disease diagnosis. Future work\nincludes using more automatic provers. This should enable us to specify and\nstudy more realistic examples, and in the long term to provide a system for\ndisease diagnosis and therapy prognosis.", "journal": ""}
{"doi": "10.48550/arXiv.1301.4779", "date": "2013-01-21", "title": "Formal Verification of Hardware Synthesis", "authors": "Thomas Braibant, Adam Chlipala", "abstract": "We report on the implementation of a certified compiler for a high-level\nhardware description language (HDL) called Fe-Si (FEatherweight SynthesIs).\nFe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded\natomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq.\nThe target language of the compiler corresponds to a synthesisable subset of\nVerilog or VHDL. A key aspect of our approach is that input programs to the\ncompiler can be defined and proved correct inside Coq. Then, we use extraction\nand a Verilog back-end (written in OCaml) to get a certified version of a\nhardware design.", "journal": "Computer Aided Verification, Saint Petersburg : Russie,\n  F\\'ed\\'eration De (2013)"}
{"doi": "10.48550/arXiv.1801.01665", "date": "2018-01-05", "title": "Political Discourse on Social Media: Echo Chambers, Gatekeepers, and the Price of Bipartisanship", "authors": "Kiran Garimella, Gianmarco De Francisci Morales, Aristides Gionis, Michael Mathioudakis", "abstract": "Echo chambers, i.e., situations where one is exposed only to opinions that\nagree with their own, are an increasing concern for the political discourse in\nmany democratic countries. This paper studies the phenomenon of political echo\nchambers on social media. We identify the two components in the phenomenon: the\nopinion that is shared ('echo'), and the place that allows its exposure\n('chamber' --- the social network), and examine closely at how these two\ncomponents interact. We define a production and consumption measure for\nsocial-media users, which captures the political leaning of the content shared\nand received by them. By comparing the two, we find that Twitter users are, to\na large degree, exposed to political opinions that agree with their own. We\nalso find that users who try to bridge the echo chambers, by sharing content\nwith diverse leaning, have to pay a 'price of bipartisanship' in terms of their\nnetwork centrality and content appreciation. In addition, we study the role of\n'gatekeepers', users who consume content with diverse leaning but produce\npartisan content (with a single-sided leaning), in the formation of echo\nchambers. Finally, we apply these findings to the task of predicting partisans\nand gatekeepers from social and content features. While partisan users turn out\nrelatively easy to identify, gatekeepers prove to be more challenging.", "journal": ""}
{"doi": "10.48550/arXiv.2005.01624", "date": "2020-05-04", "title": "Continuous and monotone machines", "authors": "Michal Kone\u010dn\u00fd, Florian Steinberg, Holger Thies", "abstract": "We investigate a variant of the fuel-based approach to modeling diverging\ncomputation in type theories and use it to abstractly capture the essence of\noracle Turing machines. The resulting objects we call continuous machines. We\nprove that it is possible to translate back and forth between such machines and\nnames in the standard function encoding used in computable analysis. Put\ndifferently, among the operators on Baire space, exactly the partial continuous\nones are implementable by continuous machines and the data that such a machine\nprovides is a description of the operator as a sequentially realizable\nfunctional.\n  Continuous machines are naturally formulated in type theories and we have\nformalized our findings in Coq. Continuous machines, their equivalence to the\nstandard encoding and correctness of basic operations are now part of Incone, a\nCoq library for computable analysis. While the correctness proofs use a\nclassical meta-theory with countable choice, the translations and algorithms\nthat are proven correct are all fully executable. Along the way we formally\nprove some known results such as existence of a self-modulating moduli of\ncontinuity for partial continuous operators on Baire space.\n  To illustrate their versatility we use continuous machines to specify some\nalgorithms that operate on objects that cannot be fully described by finite\nmeans, such as real numbers and functions. We present particularly simple\nalgorithms for finding the multiplicative inverse of a real number and for\ncomposition of partial continuous operators on Baire space. Some of the\nsimplicity is achieved by utilizing the fact that continuous machines are\ncompatible with multivalued semantics. We also connect continuous machines to\nthe construction of precompletions and completions of represented spaces,\ntopics that have recently caught the attention of the computable analysis\ncommunity.", "journal": ""}
{"doi": "10.48550/arXiv.2004.00055", "date": "2020-03-31", "title": "Epistemic Phase Transitions in Mathematical Proofs", "authors": "Scott Viteri, Simon DeDeo", "abstract": "Mathematical proofs are both paradigms of certainty and some of the most\nexplicitly-justified arguments that we have in the cultural record. Their very\nexplicitness, however, leads to a paradox, because the probability of error\ngrows exponentially as the argument expands. When a mathematician encounters a\nproof, how does she come to believe it? Here we show that, under a\ncognitively-plausible belief formation mechanism combining deductive and\nabductive reasoning, belief in mathematical arguments can undergo what we call\nan epistemic phase transition: a dramatic and rapidly-propagating jump from\nuncertainty to near-complete confidence at reasonable levels of claim-to-claim\nerror rates. To show this, we analyze an unusual dataset of forty-eight\nmachine-aided proofs from the formalized reasoning system Coq, including major\ntheorems ranging from ancient to 21st Century mathematics, along with five\nhand-constructed cases including Euclid, Apollonius, Hernstein's Topics in\nAlgebra, and Andrew Wiles's proof of Fermat's Last Theorem. Our results bear\nboth on recent work in the history and philosophy of mathematics on how we\nunderstand proofs, and on a question, basic to cognitive science, of how we\njustify complex beliefs.", "journal": "Cognition, 225, 105120 (2022)"}
{"doi": "10.48550/arXiv.2402.03488", "date": "2024-02-05", "title": "Redex -> Coq: towards a theory of decidability of Redex's reduction semantics", "authors": "Mallku Soldevila, Rodrigo Ribeiro, Beta Ziliani", "abstract": "We propose the first steps in the development of a tool to automate the\ntranslation of Redex models into a (hopefully) semantically equivalent model in\nCoq, and to provide tactics to help in the certification of fundamental\nproperties of such models. The work is heavily based on a model of Redex's\nsemantics developed by Klein et al. By means of a simple generalization of the\nmatching problem in Redex, we obtain an algorithm suitable for its\nmechanization in Coq, for which we prove its soundness properties and its\ncorrespondence with the original solution proposed by Klein et al. In the\nprocess, we also adequate some parts of our mechanization to better prepare it\nfor the future inclusion of Redex features absent in the present model, like\nits Kleene-star operator. Finally, we discuss future avenues of development\nthat are enabled by this work.", "journal": ""}
{"doi": "10.48550/arXiv.2305.04755", "date": "2023-05-08", "title": "If it's Provably Secure, It Probably Isn't: Why Learning from Proof Failure is Hard", "authors": "Ross Anderson, Nicholas Boucher", "abstract": "In this paper we're going to explore the ways in which security proofs can\nfail, and their broader lessons for security engineering. To mention just one\nexample, Larry Paulson proved the security of SSL/TLS using his theorem prover\nIsabelle in 1999, yet it's sprung multiple leaks since then, from timing\nattacks to Heartbleed. We will go through a number of other examples in the\nhope of elucidating general principles. Proofs can be irrelevant, they can be\nopaque, they can be misleading and they can even be wrong. So we can look to\nthe philosophy of mathematics for illumination. But the problem is more\ngeneral. What happens, for example, when we have a choice between relying on\nmathematics and on physics? The security proofs claimed for quantum\ncryptosystems based on entanglement raise some pointed questions and may engage\nthe philosophy of physics. And then there's the other varieties of assurance;\nwe will recall the reliance placed on FIPS-140 evaluations, which API attacks\nsuggested may have been overblown. Where the defenders focus their assurance\neffort on a subsystem or a model that cannot capture the whole attack surface\nthey may just tell the attacker where to focus their effort. However, we think\nit's deeper and broader than that. The models of proof and assurance on which\nwe try to rely have a social aspect, which we can try to understand from other\nperspectives ranging from the philosophy or sociology of science to the\npsychology of shared attention. These perspectives suggest, in various ways,\nhow the management of errors and exceptions may be particularly poor. They do\nnot merely relate to failure modes that the designers failed to consider\nproperly or at all; they also relate to failure modes that the designers (or\nperhaps the verifiers) did not want to consider for institutional and cultural\nreasons.", "journal": ""}
{"doi": "10.48550/arXiv.2004.03673", "date": "2020-04-07", "title": "Maintaining a Library of Formal Mathematics", "authors": "Floris van Doorn, Gabriel Ebner, Robert Y. Lewis", "abstract": "The Lean mathematical library mathlib is developed by a community of users\nwith very different backgrounds and levels of experience. To lower the barrier\nof entry for contributors and to lessen the burden of reviewing contributions,\nwe have developed a number of tools for the library which check proof\ndevelopments for subtle mistakes in the code and generate documentation suited\nfor our varied audience.", "journal": ""}
{"doi": "10.48550/arXiv.2406.19723", "date": "2024-06-28", "title": "LIPO+: Frugal Global Optimization for Lipschitz Functions", "authors": "Ga\u00ebtan Serr\u00e9, Perceval Beja-Battais, Sophia Chirrane, Argyris Kalogeratos, Nicolas Vayatis", "abstract": "In this paper, we propose simple yet effective empirical improvements to the\nalgorithms of the LIPO family, introduced in [Malherbe2017], that we call LIPO+\nand AdaLIPO+. We compare our methods to the vanilla versions of the algorithms\nover standard benchmark functions and show that they converge significantly\nfaster. Finally, we show that the LIPO family is very prone to the curse of\ndimensionality and tends quickly to Pure Random Search when the dimension\nincreases. We give a proof for this, which is also formalized in Lean\nmathematical language. Source codes and a demo are provided online.", "journal": ""}
{"doi": "10.48550/arXiv.1005.2672", "date": "2010-05-15", "title": "Proviola: A Tool for Proof Re-animation", "authors": "Carst Tankink, Herman Geuvers, James McKinna, Freek Wiedijk", "abstract": "To improve on existing models of interaction with a proof assistant (PA), in\nparticular for storage and replay of proofs, we in- troduce three related\nconcepts, those of: a proof movie, consisting of frames which record both user\ninput and the corresponding PA response; a camera, which films a user's\ninteractive session with a PA as a movie; and a proviola, which replays a movie\nframe-by-frame to a third party. In this paper we describe the movie data\nstructure and we discuss a proto- type implementation of the camera and\nproviola based on the ProofWeb system. ProofWeb uncouples the interaction with\na PA via a web- interface (the client) from the actual PA that resides on the\nserver. Our camera films a movie by \"listening\" to the ProofWeb communication.\nThe first reason for developing movies is to uncouple the reviewing of a formal\nproof from the PA used to develop it: the movie concept enables users to\ndiscuss small code fragments without the need to install the PA or to load a\nwhole library into it. Other advantages include the possibility to develop a\nseparate com- mentary track to discuss or explain the PA interaction. We assert\nthat a combined camera+proviola provides a generic layer between a client\n(user) and a server (PA). Finally we claim that movies are the right type of\ndata to be stored in an encyclopedia of formalized mathematics, based on our\nexperience in filming the Coq standard library.", "journal": ""}
{"doi": "10.48550/arXiv.1501.05425", "date": "2015-01-22", "title": "Foundational Extensible Corecursion", "authors": "Jasmin Christian Blanchette, Andrei Popescu, Dmitriy Traytel", "abstract": "This paper presents a formalized framework for defining corecursive functions\nsafely in a total setting, based on corecursion up-to and relational\nparametricity. The end product is a general corecursor that allows corecursive\n(and even recursive) calls under well-behaved operations, including\nconstructors. Corecursive functions that are well behaved can be registered as\nsuch, thereby increasing the corecursor's expressiveness. The metatheory is\nformalized in the Isabelle proof assistant and forms the core of a prototype\ntool. The corecursor is derived from first principles, without requiring new\naxioms or extensions of the logic.", "journal": ""}
{"doi": "10.48550/arXiv.2203.11341", "date": "2022-03-21", "title": "Binary codes that do not preserve primitivity", "authors": "\u0160t\u011bp\u00e1n Holub, Martin Ra\u0161ka, \u0160t\u011bp\u00e1n Starosta", "abstract": "A code $X$ is not primitivity preserving if there is a primitive list\n${\\mathbf w} \\in {\\tt lists} X$ whose concatenation is imprimitive. We\nformalize a full characterization of such codes in the binary case in the proof\nassistant Isabelle/HOL. Part of the formalization, interesting on its own, is a\ndescription of $\\{x,y\\}$-interpretations of the square $xx$ if $|y| \\leq |x|$.\nWe also provide a formalized parametric solution of the related equation\n$x^jy^k = z^\\ell$.", "journal": ""}
{"doi": "10.48550/arXiv.1408.3317", "date": "2014-08-14", "title": "Maximally Permissive Controlled System Synthesis for Modal Logic", "authors": "Allan van Hulst, Michel Reniers, Wan Fokkink", "abstract": "We propose a new method for controlled system synthesis on non-deterministic\nautomata, which includes the synthesis for deadlock-freeness, as well as\ninvariant and reachability expressions. Our technique restricts the behavior of\na Kripke-structure with labeled transitions, representing the uncontrolled\nsystem, such that it adheres to a given requirement specification in an\nexpressive modal logic. while all non-invalidating behavior is retained. This\ninduces maximal permissiveness in the context of supervisory control. Research\npresented in this paper allows a system model to be constrained according to a\nbroad set of liveness, safety and fairness specifications of desired behavior,\nand embraces most concepts from Ramadge-Wonham supervisory control, including\ncontrollability and marker-state reachability. Synthesis is defined in this\npaper as a formal construction, which allowed a careful validation of its\ncorrectness using the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1807.07892", "date": "2018-07-20", "title": "Bridging the Gap between Programming Languages and Hardware Weak Memory Models", "authors": "Anton Podkopaev, Ori Lahav, Viktor Vafeiadis", "abstract": "We develop a new intermediate weak memory model, IMM, as a way of\nmodularizing the proofs of correctness of compilation from concurrent\nprogramming languages with weak memory consistency semantics to mainstream\nmulti-core architectures, such as POWER and ARM. We use IMM to prove the\ncorrectness of compilation from the promising semantics of Kang et al. to POWER\n(thereby correcting and improving their result) and ARMv7, as well as to the\nrecently revised ARMv8 model. Our results are mechanized in Coq, and to the\nbest of our knowledge, these are the first machine-verified compilation\ncorrectness results for models that are weaker than x86-TSO.", "journal": ""}
{"doi": "10.48550/arXiv.2210.15609", "date": "2022-10-27", "title": "The formal verification of the ctm approach to forcing", "authors": "Emmanuel Gunther, Miguel Pagano, Pedro S\u00e1nchez Terraf, Mat\u00edas Steinberg", "abstract": "We discuss some highlights of our computer-verified proof of the\nconstruction, given a countable transitive set-model $M$ of $\\mathit{ZFC}$, of\ngeneric extensions satisfying $\\mathit{ZFC}+\\neg\\mathit{CH}$ and\n$\\mathit{ZFC}+\\mathit{CH}$. Moreover, let $\\mathcal{R}$ be the set of instances\nof the Axiom of Replacement. We isolated a 21-element subset\n$\\Omega\\subseteq\\mathcal{R}$ and defined\n$\\mathcal{F}:\\mathcal{R}\\to\\mathcal{R}$ such that for every\n$\\Phi\\subseteq\\mathcal{R}$ and $M$-generic $G$, $M\\models \\mathit{ZC} \\cup\n\\mathcal{F}\\text{``}\\Phi \\cup \\Omega$ implies $M[G]\\models \\mathit{ZC} \\cup\n\\Phi \\cup \\{ \\neg \\mathit{CH} \\}$, where $\\mathit{ZC}$ is Zermelo set theory\nwith Choice.\n  To achieve this, we worked in the proof assistant Isabelle, basing our\ndevelopment on the Isabelle/ZF library by L. Paulson and others.", "journal": ""}
{"doi": "10.48550/arXiv.1701.05888", "date": "2017-01-20", "title": "A Higher-Order Logic for Concurrent Termination-Preserving Refinement", "authors": "Joseph Tassarotti, Ralf Jung, Robert Harper", "abstract": "Compiler correctness proofs for higher-order concurrent languages are\ndifficult: they involve establishing a termination-preserving refinement\nbetween a concurrent high-level source language and an implementation that uses\nlow-level shared memory primitives. However, existing logics for proving\nconcurrent refinement either neglect properties such as termination, or only\nhandle first-order state. In this paper, we address these limitations by\nextending Iris, a recent higher-order concurrent separation logic, with support\nfor reasoning about termination-preserving refinements. To demonstrate the\npower of these extensions, we prove the correctness of an efficient\nimplementation of a higher-order, session-typed language. To our knowledge,\nthis is the first program logic capable of giving a compiler correctness proof\nfor such a language. The soundness of our extensions and our compiler\ncorrectness proof have been mechanized in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1803.06494", "date": "2018-03-17", "title": "Attack Trees in Isabelle", "authors": "Florian Kamm\u00fcller", "abstract": "In this paper, we present a proof theory for attack trees. Attack trees are a\nwell established and useful model for the construction of attacks on systems\nsince they allow a stepwise exploration of high level attacks in application\nscenarios. Using the expressiveness of Higher Order Logic in Isabelle, we\nsucceed in developing a generic theory of attack trees with a state-based\nsemantics based on Kripke structures and CTL. The resulting framework allows\nmechanically supported logic analysis of the meta-theory of the proof calculus\nof attack trees and at the same time the developed proof theory enables\napplication to case studies. A central correctness and completeness result\nproved in Isabelle establishes a connection between the notion of attack tree\nvalidity and CTL. The application is illustrated on the example of a healthcare\nIoT system and GDPR compliance verification.", "journal": ""}
{"doi": "10.48550/arXiv.2404.14919", "date": "2024-04-23", "title": "Stalnaker's Epistemic Logic in Isabelle/HOL", "authors": "Laura P. Gamboa Guzman, Kristin Y. Rozier", "abstract": "The foundations of formal models for epistemic and doxastic logics often rely\non certain logical aspects of modal logics such as S4 and S4.2 and their\nsemantics; however, the corresponding mathematical results are often stated in\npapers or books without including a detailed proof, or a reference to it, that\nallows the reader to convince themselves about them. We reinforce the\nfoundations of the epistemic logic S4.2 for countably many agents by\nformalizing its soundness and completeness results for the class of all\nweakly-directed pre-orders in the proof assistant Isabelle/HOL. This logic\ncorresponds to the knowledge fragment, i.e., the logic for formulas that may\nonly include knowledge modalities in Stalnaker's system for knowledge and\nbelief. Additionally, we formalize the equivalence between two axiomatizations\nfor S4, which are used depending on the type of semantics given to the modal\noperators, as one is commonly used for the relational semantics, and the other\none arises naturally from the topological semantics.", "journal": "EPTCS 402, 2024, pp. 4-17"}
{"doi": "10.48550/arXiv.2308.05485", "date": "2023-08-10", "title": "Substitution for Non-Wellfounded Syntax with Binders through Monoidal Categories", "authors": "Ralph Matthes, Kobe Wullaert, Benedikt Ahrens", "abstract": "We describe a generic construction of non-wellfounded syntax involving\nvariable binding and its monadic substitution operation. Our construction of\nthe syntax and its substitution takes place in category theory, notably by\nusing monoidal categories and strong functors between them. A language is\nspecified by a multi-sorted binding signature, say {\\Sigma}. First, we provide\nsufficient criteria for {\\Sigma} to generate a language of possibly infinite\nterms, through {\\omega}-continuity. Second, we construct a monadic substitution\noperation for the language generated by {\\Sigma}. A cornerstone in this\nconstruction is a mild generalization of the notion of heterogeneous\nsubstitution systems developed by Matthes and Uustalu; such a system\nencapsulates the necessary corecursion scheme for implementing substitution.\nThe results are formalized in the Coq proof assistant, through the UniMath\nlibrary of univalent mathematics.", "journal": ""}
{"doi": "10.48550/arXiv.1502.01292", "date": "2015-02-04", "title": "Machine-Checked Proofs For Realizability Checking Algorithms", "authors": "Andreas Katis, Andrew Gacek, Michael W. Whalen", "abstract": "Virtual integration techniques focus on building architectural models of\nsystems that can be analyzed early in the design cycle to try to lower cost,\nreduce risk, and improve quality of complex embedded systems. Given appropriate\narchitectural descriptions, assume/guarantee contracts, and compositional\nreasoning rules, these techniques can be used to prove important safety\nproperties about the architecture prior to system construction. For these\nproofs to be meaningful, each leaf-level component contract must be realizable;\ni.e., it is possible to construct a component such that for any input allowed\nby the contract assumptions, there is some output value that the component can\nproduce that satisfies the contract guarantees. We have recently proposed (in\n[1]) a contract-based realizability checking algorithm for assume/guarantee\ncontracts over infinite theories supported by SMT solvers such as linear\ninteger/real arithmetic and uninterpreted functions. In that work, we used an\nSMT solver and an algorithm similar to k-induction to establish the\nrealizability of a contract, and justified our approach via a hand proof. Given\nthe central importance of realizability to our virtual integration approach, we\nwanted additional confidence that our approach was sound. This paper describes\na complete formalization of the approach in the Coq proof and specification\nlanguage. During formalization, we found several small mistakes and missing\nassumptions in our reasoning. Although these did not compromise the correctness\nof the algorithm used in the checking tools, they point to the value of\nmachine-checked formalization. In addition, we believe this is the first\nmachine-checked formalization for a realizability algorithm.", "journal": ""}
{"doi": "10.48550/arXiv.1510.05216", "date": "2015-10-18", "title": "From F to DOT: Type Soundness Proofs with Definitional Interpreters", "authors": "Tiark Rompf, Nada Amin", "abstract": "Scala's type system unifies ML modules, object-oriented, and functional\nprogramming. The Dependent Object Types (DOT) family of calculi has been\nproposed as a new foundation for Scala and similar languages. Unfortunately, it\nis not clear how DOT relates to any well-known type systems, and type soundness\nhas only been established for very restricted subsets. In fact, important Scala\nfeatures are known to break at least one key metatheoretic property such as\nenvironment narrowing or subtyping transitivity, which are usually required for\na type soundness proof.\n  First, and, perhaps surprisingly, we show how rich DOT calculi can still be\nproved sound. The key insight is that narrowing and subtyping transitivity only\nneed to hold for runtime objects, but not for code that is never executed.\nAlas, the dominant method of proving type soundness, Wright and Felleisen's\nsyntactic approach, is based on term rewriting, which does not a priori make a\ndistinction between runtime and type assignment time.\n  Second, we demonstrate how type soundness can be proved for advanced,\npolymorphic, type systems with respect to high-level, definitional\ninterpreters, implemented in Coq. We present the first mechanized soundness\nproof in this style for System F<: and several extensions, including mutable\nreferences. Our proofs use only simple induction: another surprising result, as\nthe combination of big-step semantics, mutable references, and polymorphism is\ncommonly believed to require co-inductive proof techniques.\n  Third, we show how DOT-like calculi emerge as generalizations of F<:,\nexposing a rich design space of calculi with path-dependent types which we\ncollectively call System D. Armed with insights from the definitional\ninterpreter semantics, we also show how equivalent small-step semantics and\nsoundness proofs in Wright-Felleisen-style can be derived for these systems.", "journal": ""}
{"doi": "10.48550/arXiv.1210.5658", "date": "2012-10-20", "title": "Homotopy type theory and Voevodsky's univalent foundations", "authors": "\u00c1lvaro Pelayo, Michael A. Warren", "abstract": "Recent discoveries have been made connecting abstract homotopy theory and the\nfield of type theory from logic and theoretical computer science. This has\ngiven rise to a new field, which has been christened \"homotopy type theory\". In\nthis direction, Vladimir Voevodsky observed that it is possible to model type\ntheory using simplicial sets and that this model satisfies an additional\nproperty, called the Univalence Axiom, which has a number of striking\nconsequences. He has subsequently advocated a program, which he calls univalent\nfoundations, of developing mathematics in the setting of type theory with the\nUnivalence Axiom and possibly other additional axioms motivated by the\nsimplicial set model. Because type theory possesses good computational\nproperties, this program can be carried out in a computer proof assistant. In\nthis paper we give an introduction to homotopy type theory in Voevodsky's\nsetting, paying attention to both theoretical and practical issues. In\nparticular, the paper serves as an introduction to both the general ideas of\nhomotopy type theory as well as to some of the concrete details of Voevodsky's\nwork using the well-known proof assistant Coq. The paper is written for a\ngeneral audience of mathematicians with basic knowledge of algebraic topology;\nthe paper does not assume any preliminary knowledge of type theory, logic, or\ncomputer science.", "journal": ""}
{"doi": "10.48550/arXiv.1911.06567", "date": "2019-11-15", "title": "Reconciling Event Structures with Modern Multiprocessors", "authors": "Evgenii Moiseenko, Anton Podkopaev, Ori Lahav, Orestis Melkonian, Viktor Vafeiadis", "abstract": "Weakestmo is a recently proposed memory consistency model that uses event\nstructures to resolve the infamous \"out-of-thin-air\" problem. Although it has\nbeen shown to have important benefits over other memory models, its established\ncompilation schemes are suboptimal in that they add more fences than necessary.\nIn this paper, we prove the correctness in Coq of the intended compilation\nschemes for Weakestmo to a range of hardware memory models (x86, POWER, ARMv7,\nARMv8, RISC-V). Our proof is the first that establishes correctness of\ncompilation of an event-structure-based model that forbids \"thin-air\"\nbehaviors, as well as the first mechanized compilation proof of a weak memory\nmodel supporting sequentially consistent accesses to such a range of hardware\nplatforms. Our compilation proof goes via the recent Intermediate Memory Model\n(IMM), which we suitably extend with sequentially consistent accesses.", "journal": ""}
{"doi": "10.48550/arXiv.2409.18030", "date": "2024-09-26", "title": "Certifying rings of integers in number fields", "authors": "Anne Baanen, Alain Chavarri Villarello, Sander R. Dahmen", "abstract": "Number fields and their rings of integers, which generalize the rational\nnumbers and the integers, are foundational objects in number theory. There are\nseveral computer algebra systems and databases concerned with the computational\naspects of these. In particular, computing the ring of integers of a given\nnumber field is one of the main tasks of computational algebraic number theory.\nIn this paper, we describe a formalization in Lean 4 for certifying such\ncomputations. In order to accomplish this, we developed several data types\namenable to computation. Moreover, many other underlying mathematical concepts\nand results had to be formalized, most of which are also of independent\ninterest. These include resultants and discriminants, as well as methods for\nproving irreducibility of univariate polynomials over finite fields and over\nthe rational numbers. To illustrate the feasibility of our strategy, we\nformally verified entries from the $\\textit{Number fields}$ section of the\n$\\textit{L-functions and modular forms database}$ (LMFDB). These concern, for\nseveral number fields, the explicitly given $\\textit{integral basis}$ of the\nring of integers and the $\\textit{discriminant}$. To accomplish this, we wrote\nSageMath code that computes the corresponding certificates and outputs a Lean\nproof of the statement to be verified.", "journal": "CPP 2025: Proceedings of the 14th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs, pp. 50-66"}
{"doi": "10.48550/arXiv.1212.3870", "date": "2012-12-17", "title": "Interactive verification of Markov chains: Two distributed protocol case studies", "authors": "Johannes H\u00f6lzl, Tobias Nipkow", "abstract": "Probabilistic model checkers like PRISM only check probabilistic systems of a\nfixed size. To guarantee the desired properties for an arbitrary size,\nmathematical analysis is necessary. We show for two case studies how this can\nbe done in the interactive proof assistant Isabelle/HOL. The first case study\nis a detailed description of how we verified properties of the ZeroConf\nprotocol, a decentral address allocation protocol. The second case study shows\nthe more involved verification of anonymity properties of the Crowds protocol,\nan anonymizing protocol.", "journal": "EPTCS 103, 2012, pp. 17-31"}
{"doi": "10.48550/arXiv.2010.10296", "date": "2020-10-19", "title": "Definitional Quantifiers Realise Semantic Reasoning for Proof by Induction", "authors": "Yutaka Nagashima", "abstract": "Proof assistants offer tactics to apply proof by induction, but these tactics\nrely on inputs given by human engineers. To automate this laborious process, we\ndeveloped SeLFiE, a boolean query language to represent experienced users'\nknowledge on how to apply the induct tactic in Isabelle/HOL: when we apply an\ninduction heuristic written in SeLFiE to an inductive problem and arguments to\nthe induct tactic, the SeLFiE interpreter judges whether the arguments are\nplausible for that problem according to the heuristic by examining both the\nsyntactic structure of the problem and definitions of the relevant constants.\nTo examine the intricate interaction between syntactic analysis and analysis of\nconstant definitions, we introduce definitional quantifiers. For evaluation we\nbuild an automatic induction prover using SeLFiE. Our evaluation based on 347\ninductive problems shows that our new prover achieves 1.4 x 10^3% improvement\nover the corresponding baseline prover for 1.0 second of timeout and the median\nvalue of speedup is 4.48x.", "journal": ""}
{"doi": "10.48550/arXiv.2211.07959", "date": "2022-11-15", "title": "The Lean Data Scientist: Recent Advances towards Overcoming the Data Bottleneck", "authors": "Chen Shani, Jonathan Zarecki, Dafna Shahaf", "abstract": "Machine learning (ML) is revolutionizing the world, affecting almost every\nfield of science and industry. Recent algorithms (in particular, deep networks)\nare increasingly data-hungry, requiring large datasets for training. Thus, the\ndominant paradigm in ML today involves constructing large, task-specific\ndatasets.\n  However, obtaining quality datasets of such magnitude proves to be a\ndifficult challenge. A variety of methods have been proposed to address this\ndata bottleneck problem, but they are scattered across different areas, and it\nis hard for a practitioner to keep up with the latest developments. In this\nwork, we propose a taxonomy of these methods. Our goal is twofold: (1) We wish\nto raise the community's awareness of the methods that already exist and\nencourage more efficient use of resources, and (2) we hope that such a taxonomy\nwill contribute to our understanding of the problem, inspiring novel ideas and\nstrategies to replace current annotation-heavy approaches.", "journal": ""}
{"doi": "10.48550/arXiv.0809.3960", "date": "2008-09-23", "title": "Formalising the pi-calculus using nominal logic", "authors": "Jesper Bengtson, Joachim Parrow", "abstract": "We formalise the pi-calculus using the nominal datatype package, based on\nideas from the nominal logic by Pitts et al., and demonstrate an implementation\nin Isabelle/HOL. The purpose is to derive powerful induction rules for the\nsemantics in order to conduct machine checkable proofs, closely following the\nintuitive arguments found in manual proofs. In this way we have covered many of\nthe standard theorems of bisimulation equivalence and congruence, both late and\nearly, and both strong and weak in a uniform manner. We thus provide one of the\nmost extensive formalisations of a process calculus ever done inside a theorem\nprover.\n  A significant gain in our formulation is that agents are identified up to\nalpha-equivalence, thereby greatly reducing the arguments about bound names.\nThis is a normal strategy for manual proofs about the pi-calculus, but that\nkind of hand waving has previously been difficult to incorporate smoothly in an\ninteractive theorem prover. We show how the nominal logic formalism and its\nsupport in Isabelle accomplishes this and thus significantly reduces the tedium\nof conducting completely formal proofs. This improves on previous work using\nweak higher order abstract syntax since we do not need extra assumptions to\nfilter out exotic terms and can keep all arguments within a familiar\nfirst-order logic.", "journal": "Logical Methods in Computer Science, Volume 5, Issue 2 (June 30,\n  2009) lmcs:832"}
{"doi": "10.48550/arXiv.1404.7792", "date": "2014-04-30", "title": "Towards Verification of Constituent Systems through Automated Proof", "authors": "Luis Diogo Couto, Simon Foster, Richard Payne", "abstract": "This paper explores verification of constituent systems within the context of\nthe Symphony tool platform for Systems of Systems (SoS). Our SoS modelling\nlanguage, CML, supports various contractual specification elements, such as\nstate invariants and operation preconditions, which can be used to specify\ncontractual obligations on the constituent systems of a SoS. To support\nverification of these obligations we have developed a proof obligation\ngenerator and theorem prover plugin for Symphony. The latter uses the\nIsabelle/HOL theorem prover to automatically discharge the proof obligations\narising from a CML model. Our hope is that the resulting proofs can then be\nused to formally verify the conformance of each constituent system, which is\nturn would result in a dependable SoS.", "journal": ""}
{"doi": "10.48550/arXiv.1008.2112", "date": "2010-08-12", "title": "Resumptions, Weak Bisimilarity and Big-Step Semantics for While with Interactive I/O: An Exercise in Mixed Induction-Coinduction", "authors": "Keiko Nakata, Tarmo Uustalu", "abstract": "We look at the operational semantics of languages with interactive I/O\nthrough the glasses of constructive type theory. Following on from our earlier\nwork on coinductive trace-based semantics for While, we define several big-step\nsemantics for While with interactive I/O, based on resumptions and\ntermination-sensitive weak bisimilarity. These require nesting inductive\ndefinitions in coinductive definitions, which is interesting both\nmathematically and from the point-of-view of implementation in a proof\nassistant.\n  After first defining a basic semantics of statements in terms of resumptions\nwith explicit internal actions (delays), we introduce a semantics in terms of\ndelay-free resumptions that essentially removes finite sequences of delays on\nthe fly from those resumptions that are responsive. Finally, we also look at a\nsemantics in terms of delay-free resumptions supplemented with a silent\ndivergence option. This semantics hinges on decisions between convergence and\ndivergence and is only equivalent to the basic one classically.\n  We have fully formalized our development in Coq.", "journal": "EPTCS 32, 2010, pp. 57-75"}
{"doi": "10.48550/arXiv.2002.08334", "date": "2020-02-19", "title": "Holistic Specifications for Robust Programs", "authors": "Sophia Drossopoulou, James Noble, Julian Mackay, Susan Eisenbach", "abstract": "Functional specifications describe what program components do: the sufficient\nconditions to invoke a component's operations. They allow us to reason about\nthe use of components in the closed world setting, where the component\ninteracts with known client code, and where the client code must establish the\nappropriate pre-conditions before calling into the component.\n  Sufficient conditions are not enough to reason about the use of components in\nthe open world setting, where the component interacts with external code,\npossibly of unknown provenance, and where the component itself may evolve over\ntime. In this open world setting, we must also consider the necessary}\nconditions, i.e, what are the conditions without which an effect will not\nhappen.\n  In this paper we propose the language Chainmail for writing holistic\nspecifications that focus on necessary conditions (as well as sufficient\nconditions). We give a formal semantics for \\Chainmail. The core of Chainmail\nhas been mechanised in the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2109.04258", "date": "2021-09-09", "title": "A Derivative-based Parser Generator for Visibly Pushdown Grammars", "authors": "Xiaodong Jia, Ashish Kumar, Gang Tan", "abstract": "In this paper, we present a derivative-based, functional recognizer and\nparser generator for visibly pushdown grammars. The generated parser accepts\nambiguous grammars and produces a parse forest containing all valid parse trees\nfor an input string in linear time. Each parse tree in the forest can then be\nextracted also in linear time. Besides the parser generator, to allow more\nflexible forms of the visibly pushdown grammars, we also present a translator\nthat converts a tagged CFG to a visibly pushdown grammar in a sound way, and\nthe parse trees of the tagged CFG are further produced by running the semantic\nactions embedded in the parse trees of the translated visibly pushdown grammar.\nThe performance of the parser is compared with a popular parsing tool ANTLR and\nother popular hand-crafted parsers. The correctness of the core parsing\nalgorithm is formally verified in the proof assistant Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2301.10061", "date": "2023-01-24", "title": "Asynchronous Probabilistic Couplings in Higher-Order Separation Logic", "authors": "Simon Oddershede Gregersen, Alejandro Aguirre, Philipp G. Haselwarter, Joseph Tassarotti, Lars Birkedal", "abstract": "Probabilistic couplings are the foundation for many probabilistic relational\nprogram logics and arise when relating random sampling statements across two\nprograms. In relational program logics, this manifests as dedicated coupling\nrules that, e.g., say we may reason as if two sampling statements return the\nsame value. However, this approach fundamentally requires aligning or\n\"synchronizing\" the sampling statements of the two programs which is not always\npossible.\n  In this paper, we develop Clutch, a higher-order probabilistic relational\nseparation logic that addresses this issue by supporting asynchronous\nprobabilistic couplings. We use Clutch to develop a logical step-indexed\nlogical relational to reason about contextual refinement and equivalence of\nhigher-order programs written in a rich language with higher-order local state\nand impredicative polymorphism. Finally, we demonstrate the usefulness of our\napproach on a number of case studies.\n  All the results that appear in the paper have been formalized in the Coq\nproof assistant using the Coquelicot library and the Iris separation logic\nframework.", "journal": ""}
{"doi": "10.48550/arXiv.2303.16726", "date": "2023-03-29", "title": "Text revision in Scientific Writing Assistance: An Overview", "authors": "L\u00e9ane Jourdan, Florian Boudin, Richard Dufour, Nicolas Hernandez", "abstract": "Writing a scientific article is a challenging task as it is a highly codified\ngenre. Good writing skills are essential to properly convey ideas and results\nof research work. Since the majority of scientific articles are currently\nwritten in English, this exercise is all the more difficult for non-native\nEnglish speakers as they additionally have to face language issues. This\narticle aims to provide an overview of text revision in writing assistance in\nthe scientific domain.\n  We will examine the specificities of scientific writing, including the format\nand conventions commonly used in research articles.\n  Additionally, this overview will explore the various types of writing\nassistance tools available for text revision. Despite the evolution of the\ntechnology behind these tools through the years, from rule-based approaches to\ndeep neural-based ones, challenges still exist (tools' accessibility, limited\nconsideration of the context, inexplicit use of discursive information, etc.)", "journal": "ceur-ws.Vol-3617(2023)22-36"}
{"doi": "10.48550/arXiv.2401.02948", "date": "2024-01-05", "title": "Hashing Modulo Context-Sensitive $\u03b1$-Equivalence", "authors": "Lasse Blaauwbroek, Miroslav Ol\u0161\u00e1k, Herman Geuvers", "abstract": "The notion of $\\alpha$-equivalence between $\\lambda$-terms is commonly used\nto identify terms that are considered equal. However, due to the primitive\ntreatment of free variables, this notion falls short when comparing subterms\noccurring within a larger context. Depending on the usage of the Barendregt\nconvention (choosing different variable names for all involved binders), it\nwill equate either too few or too many subterms. We introduce a formal notion\nof context-sensitive $\\alpha$-equivalence, where two open terms can be compared\nwithin a context that resolves their free variables. We show that this\nequivalence coincides exactly with the notion of bisimulation equivalence.\nFurthermore, we present an efficient $O(n\\log n)$ runtime hashing scheme that\nidentifies $\\lambda$-terms modulo context-sensitive $\\alpha$-equivalence,\ngeneralizing over traditional bisimulation partitioning algorithms and\nimproving upon a previously established $O(n\\log^2 n)$ bound for a hashing\nmodulo ordinary $\\alpha$-equivalence by Maziarz et al. Hashing $\\lambda$-terms\nis useful in many applications that require common subterm elimination and\nstructure sharing. We have employed the algorithm to obtain a large-scale,\ndensely packed, interconnected graph of mathematical knowledge from the Coq\nproof assistant for machine learning purposes.", "journal": ""}
{"doi": "10.48550/arXiv.2104.00762", "date": "2021-04-01", "title": "Flexible Instruction-Set Semantics via Type Classes", "authors": "Thomas Bourgeat, Ian Clester, Andres Erbsen, Samuel Gruetter, Pratap Singh, Andrew Wright, Adam Chlipala", "abstract": "Instruction sets, from families like x86 and ARM, are at the center of many\nambitious formal-methods projects. Many verification, synthesis, programming,\nand debugging tools rely on formal semantics of instruction sets, but different\ntools can use semantics in rather different ways. As a result, a central\nchallenge for that community is how semantics should be written and what\ntechniques should be used to connect them to new use cases. The best-known work\napplying single semantics across quite-different tools relies on\ndomain-specific languages like Sail, where the language and its translation\ntools are specialized to the realm of instruction sets. We decided to explore a\ndifferent approach, with semantics written in a carefully chosen subset of\nHaskell. This style does not depend on any new language translators, relying\ninstead on parameterization of semantics over type-class instances. As a\nresult, a semantics can be a first-class object within a logic, and application\nof a semantics for a new kind of tool can be a first-class operation in the\nlogic, allowing sharing of theorems across applications. Our case study is for\nthe open RISC-V instruction-set family, and we have used a single core\nsemantics to support testing, interactive proof, and model checking of both\nsoftware and hardware. We especially highlight an application of a first-class\nsemantics within Coq that can be instantiated in different ways within one\nproof: simulation between variants where multiplication is implemented in\nhardware or in the machine code of a particular software trap handler.", "journal": ""}
{"doi": "10.48550/arXiv.1211.6468", "date": "2012-11-27", "title": "Using Isabelle to verify special relativity, with application to hypercomputation theory", "authors": "Mike Stannett, Istv\u00e1n N\u00e9meti", "abstract": "Logicians at the R\\'enyi Mathematical Institute in Budapest have spent\nseveral years developing versions of relativity theory (special, general, and\nother variants) based wholly on first order logic, and have argued in favour of\nthe physical decidability, via exploitation of cosmological phenomena, of\nformally undecidable questions such as the Halting Problem and the consistency\nof set theory.\n  The Hungarian theories are very extensive, and their associated proofs are\nintuitively very satisfying, but this brings its own risks since intuition can\nsometimes be misleading. As part of a joint project, researchers at Sheffield\nhave recently started generating rigorous machine-verified versions of the\nHungarian proofs, so as to demonstrate the soundness of their work. In this\npaper, we explain the background to the project and demonstrate an Isabelle\nproof of the theorem \"No inertial observer can travel faster than light\".\n  This approach to physical theories and physical computability has several\npay-offs: (a) we can be certain our intuition hasn't led us astray (or if it\nhas, we can identify where this has happened); (b) we can identify which axioms\nare specifically required in the proof of each theorem and to what extent those\naxioms can be weakened (the fewer assumptions we make up-front, the stronger\nthe results); and (c) we can identify whether new formal proof techniques and\ntactics are needed when tackling physical as opposed to mathematical theories.", "journal": "Journal of Automated Reasoning, 52,4 (2014), 361-378"}
{"doi": "10.48550/arXiv.2303.09692", "date": "2023-03-16", "title": "Probabilistic unifying relations for modelling epistemic and aleatoric uncertainty: semantics and automated reasoning with theorem proving", "authors": "Kangfeng Ye, Jim Woodcock, Simon Foster", "abstract": "Probabilistic programming combines general computer programming, statistical\ninference, and formal semantics to help systems make decisions when facing\nuncertainty. Probabilistic programs are ubiquitous, including having a\nsignificant impact on machine intelligence. While many probabilistic algorithms\nhave been used in practice in different domains, their automated verification\nbased on formal semantics is still a relatively new research area. In the last\ntwo decades, it has attracted much interest. Many challenges, however, remain.\nThe work presented in this paper, probabilistic unifying relations (ProbURel),\ntakes a step towards our vision to tackle these challenges.\n  Our work is based on Hehner's predicative probabilistic programming, but\nthere are several obstacles to the broader adoption of his work. Our\ncontributions here include (1) the formalisation of its syntax and semantics by\nintroducing an Iverson bracket notation to separate relations from arithmetic;\n(2) the formalisation of relations using Unifying Theories of Programming (UTP)\nand probabilities outside the brackets using summation over the topological\nspace of the real numbers; (3) the constructive semantics for probabilistic\nloops using Kleene's fixed-point theorem; (4) the enrichment of its semantics\nfrom distributions to subdistributions and superdistributions to deal with the\nconstructive semantics; (5) the unique fixed-point theorem to simplify the\nreasoning about probabilistic loops; and (6) the mechanisation of our theory in\nIsabelle/UTP, an implementation of UTP in Isabelle/HOL, for automated reasoning\nusing theorem proving.\n  We demonstrate our work with six examples, including problems in robot\nlocalisation, classification in machine learning, and the termination of\nprobabilistic loops.", "journal": "Theoretical Computer Science Volume 1021, 21 December 2024, 114876"}
{"doi": "10.48550/arXiv.1802.02229", "date": "2018-02-06", "title": "Axiomatic Foundations and Algorithms for Deciding Semantic Equivalences of SQL Queries", "authors": "Shumo Chu, Brendan Murphy, Jared Roesch, Alvin Cheung, Dan Suciu", "abstract": "Deciding the equivalence of SQL queries is a fundamental problem in data\nmanagement. As prior work has mainly focused on studying the theoretical\nlimitations of the problem, very few implementations for checking such\nequivalences exist. In this paper, we present a new formalism and\nimplementation for reasoning about the equivalences of SQL queries. Our\nformalism, U-semiring, extends SQL's semiring semantics with unbounded\nsummation and duplicate elimination. U-semiring is defined using only very few\naxioms and can thus be easily implemented using proof assistants such as Coq\nfor automated query reasoning. Yet, they are sufficient enough to enable us\nreason about sophisticated SQL queries that are evaluated over bags and sets,\nalong with various integrity constraints. To evaluate the effectiveness of\nU-semiring, we have used it to formally verify 39 query rewrite rules from both\nclassical data management research papers and real-world SQL engines, where\nmany of them have never been proven correct before.", "journal": ""}
{"doi": "10.48550/arXiv.1803.04870", "date": "2018-03-13", "title": "Narcissus: Deriving Correct-By-Construction Decoders and Encoders from Binary Formats", "authors": "Benjamin Delaware, Sorawit Suriyakarn, Cl\u00e9ment Pit--Claudel, Qianchuan Ye, Adam Chlipala", "abstract": "It is a neat result from functional programming that libraries of parser\ncombinators can support rapid construction of decoders for quite a range of\nformats. With a little more work, the same combinator program can denote both a\ndecoder and an encoder. Unfortunately, the real world is full of gnarly\nformats, as with the packet formats that make up the standard Internet protocol\nstack. Most past parser-combinator approaches cannot handle these formats, and\nthe few exceptions require redundancy -- one part of the natural grammar needs\nto be hand-translated into hints in multiple parts of a parser program. We show\nhow to recover very natural and nonredundant format specifications, covering\nall popular network packet formats and generating both decoders and encoders\nautomatically. The catch is that we use the Coq proof assistant to derive both\nkinds of artifacts using tactics, automatically, in a way that guarantees that\nthey form inverses of each other. We used our approach to reimplement packet\nprocessing for a full Internet protocol stack, inserting our replacement into\nthe OCaml-based MirageOS unikernel, resulting in minimal performance\ndegradation.", "journal": ""}
{"doi": "10.48550/arXiv.2403.14649", "date": "2024-02-23", "title": "The economic value of scientific software", "authors": "Nicolas Jullien", "abstract": "Academic institutions and their staff use, adapt and create software. We're\nthinking of business tools used to carry out their mission: teaching management\n(Moodle) or subject teaching support (such as Maxima for formal calculus), for\nexample. We're talking about software resulting from research work, designed by\na researcher or a team as part of a research project (funded by ANR, Europe,\netc. or not) or as a research service for a third party. These projects can\nlast for decades (such as the Coq program proof assistant project, or the GPAC\nmultimedia content distribution platform).We discuss why this software is\nproduced, with what resources, the interest that institutions derive from it,\nwhat we call the ''valorization'' of software resulting from scientific\nresearch. The latter is multifaceted, as are the missions of scientific\ninstitutions: social value (contribution to the world heritage of knowledge),\nfinancial value (contracts), economic value (business creation), scientific\nvalue (publication), image value (visibility of the institution among target\naudiences: students, researchers, companies, prescribers).", "journal": "Revue Lamy Droit de l'immat{\\'e}riel, 2024, 210"}
{"doi": "10.48550/arXiv.2501.05616", "date": "2025-01-09", "title": "Validate Quantum State Preparation Programs", "authors": "Liyi Li, Anshu Sharma, Zoukarneini Difaizi Tagba, Sean Frett, Alex Potanin", "abstract": "One of the key steps in quantum algorithms is to prepare an initial quantum\nsuperposition state with different kinds of features. These so-called state\npreparation algorithms are essential to the behavior of quantum algorithms, and\ncomplicated state preparation algorithms are difficult to develop correctly and\neffectively. This paper presents Pqasm: a high-assurance framework implemented\nwith the Coq proof assistant, allowing us to certify our Pqasm tool to\ncorrectly reflect quantum program behaviors. The key in the framework is to\nreduce the program correctness assurance of a program containing a quantum\nsuperposition state to the program correctness assurance for the program state\nwithout superposition. The reduction allows the development of an effective\ntesting framework for testing quantum state preparation algorithm\nimplementations on a classical computer - considered to be a hard problem with\nno clear solution until this point. We utilize the QuickChick property-based\ntesting framework to test state preparation programs. We evaluated the\neffectiveness of our approach over 5 case studies implemented using Pqasm; such\ncases are not even simulatable in the current quantum simulators.", "journal": ""}
{"doi": "10.48550/arXiv.2106.03606", "date": "2021-06-07", "title": "2-Cartesian fibrations I: A model for $\\infty$-bicategories fibred in $\\infty$-bicategories", "authors": "Fernando Abell\u00e1n Garc\u00eda, Walker H. Stern", "abstract": "In this paper, we provide a notion of $\\infty$-bicategories fibred in\n$\\infty$-bicategories which we call 2-Cartesian fibrations. Our definition is\nformulated using the language of marked biscaled simplicial sets: Those are\nscaled simplicial sets equipped with an additional collection of triangles\ncontaining the scaled 2-simplices, which we call lean triangles, in addition to\na collection of edges containing all degenerate 1-simplices. We prove the\nexistence of a left proper combinatorial simplicial model category whose\nfibrant objects are precisely the 2-Cartesian fibrations over a chosen scaled\nsimplicial set $S$. Over the terminal scaled simplicial set, this provides a\nnew model structure modeling $\\infty$-bicategories, which we show is Quillen\nequivalent to Lurie's scaled simplicial set model. We conclude by providing a\ncharacterization of 2-Cartesian fibrations over an $\\infty$-bicategory. This\ncharacterization then allows us to identify those 2-Cartesian fibrations\narising as the coherent nerve of a fibration of\n$\\operatorname{Set}^+_{\\Delta}$-enriched categories, thus showing that our\ndefinition recovers the preexisting notions of fibred 2-categories.", "journal": ""}
{"doi": "10.48550/arXiv.2205.08762", "date": "2022-05-18", "title": "Leapfrog: Certified Equivalence for Protocol Parsers", "authors": "Ryan Doenges, Tobias Kapp\u00e9, John Sarracino, Nate Foster, Greg Morrisett", "abstract": "We present Leapfrog, a Coq-based framework for verifying equivalence of\nnetwork protocol parsers. Our approach is based on an automata model of P4\nparsers, and an algorithm for symbolically computing a compact representation\nof a bisimulation, using \"leaps.\" Proofs are powered by a certified compilation\nchain from first-order entailments to low-level bitvector verification\nconditions, which are discharged using off-the-shelf SMT solvers. As a result,\nparser equivalence proofs in Leapfrog are fully automatic and push-button.\n  We mechanically prove the core metatheory that underpins our approach,\nincluding the key transformations and several optimizations. We evaluate\nLeapfrog on a range of practical case studies, all of which require minimal\nconfiguration and no manual proof. Our largest case study uses Leapfrog to\nperform translation validation for a third-party compiler from automata to\nhardware pipelines. Overall, Leapfrog represents a step towards a world where\nall parsers for critical network infrastructure are verified. It also suggests\ndirections for follow-on efforts, such as verifying relational properties\ninvolving security.", "journal": "Proc. PLDI 2022, 950-965"}
{"doi": "10.48550/arXiv.1206.3523", "date": "2012-06-15", "title": "A static cost analysis for a higher-order language", "authors": "N. Danner, J. Paykin, J. S. Royer", "abstract": "We develop a static complexity analysis for a higher-order functional\nlanguage with structural list recursion. The complexity of an expression is a\npair consisting of a cost and a potential. The former is defined to be the size\nof the expression's evaluation derivation in a standard big-step operational\nsemantics. The latter is a measure of the \"future\" cost of using the value of\nthat expression. A translation function tr maps target expressions to\ncomplexities. Our main result is the following Soundness Theorem: If t is a\nterm in the target language, then the cost component of tr(t) is an upper bound\non the cost of evaluating t. The proof of the Soundness Theorem is formalized\nin Coq, providing certified upper bounds on the cost of any expression in the\ntarget language.", "journal": "M. Might and D. V. Horn (eds.), Proceedings of the 7th workshop on\n  Programming languages meets program verification, pages 25-34. ACM Press,\n  2013"}
{"doi": "10.48550/arXiv.2408.03077", "date": "2024-08-06", "title": "Model-free optimal controller for discrete-time Markovian jump linear systems: A Q-learning approach", "authors": "Ehsan Badfar, Babak Tavassoli", "abstract": "This research paper introduces a model-free optimal controller for\ndiscrete-time Markovian jump linear systems (MJLSs), employing principles from\nthe methodology of reinforcement learning (RL). While Q-learning methods have\ndemonstrated efficacy in determining optimal controller gains for deterministic\nsystems, their application to systems with Markovian switching remains\nunexplored. To address this research gap, we propose a Q-function involving the\nMarkovian mode. Subsequently, a Q-learning algorithm is proposed to learn the\nunknown kernel matrix using raw input-state information from the system.\nNotably, the study proves the convergence of the proposed Q-learning optimal\ncontroller gains to the model-based optimal controller gains after proving the\nconvergence of a value iteration algorithm as the first step. Addition of\nexcitation noise to input which is required to ensure the leaning performance\ndoes not lead to any bias. Unlike the conventional optimal controller, the\nproposed method does not require any knowledge on system dynamics and\neliminates the need for solving coupled algebraic Riccati equations arising in\noptimal control of MJLSs. Finally, the efficiency of the proposed method is\ndemonstrated through a simulation study.", "journal": ""}
{"doi": "10.48550/arXiv.2307.00724", "date": "2023-07-03", "title": "LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion", "authors": "Weiyi Xiong, Jianan Liu, Tao Huang, Qing-Long Han, Yuxuan Xia, Bing Zhu", "abstract": "As an emerging technology and a relatively affordable device, the 4D imaging\nradar has already been confirmed effective in performing 3D object detection in\nautonomous driving. Nevertheless, the sparsity and noisiness of 4D radar point\nclouds hinder further performance improvement, and in-depth studies about its\nfusion with other modalities are lacking. On the other hand, as a new image\nview transformation strategy, \"sampling\" has been applied in a few image-based\ndetectors and shown to outperform the widely applied \"depth-based splatting\"\nproposed in Lift-Splat-Shoot (LSS), even without image depth prediction.\nHowever, the potential of \"sampling\" is not fully unleashed. This paper\ninvestigates the \"sampling\" view transformation strategy on the camera and 4D\nimaging radar fusion-based 3D object detection. LiDAR Excluded Lean (LXL)\nmodel, predicted image depth distribution maps and radar 3D occupancy grids are\ngenerated from image perspective view (PV) features and radar bird's eye view\n(BEV) features, respectively. They are sent to the core of LXL, called \"radar\noccupancy-assisted depth-based sampling\", to aid image view transformation. We\ndemonstrated that more accurate view transformation can be performed by\nintroducing image depths and radar information to enhance the \"sampling\"\nstrategy. Experiments on VoD and TJ4DRadSet datasets show that the proposed\nmethod outperforms the state-of-the-art 3D object detection methods by a\nsignificant margin without bells and whistles. Ablation studies demonstrate\nthat our method performs the best among different enhancement settings.", "journal": ""}
{"doi": "10.48550/arXiv.0411028", "date": "2004-11-10", "title": "Hopf Bifurcation and Chaos in Tabu Learning Neuron Models", "authors": "Chunguang Li, Guanrong Chen, Xiaofeng Liao, Juebang Yu", "abstract": "In this paper, we consider the nonlinear dynamical behaviors of some tabu\nleaning neuron models. We first consider a tabu learning single neuron model.\nBy choosing the memory decay rate as a bifurcation parameter, we prove that\nHopf bifurcation occurs in the neuron. The stability of the bifurcating\nperiodic solutions and the direction of the Hopf bifurcation are determined by\napplying the normal form theory. We give a numerical example to verify the\ntheoretical analysis. Then, we demonstrate the chaotic behavior in such a\nneuron with sinusoidal external input, via computer simulations. Finally, we\nstudy the chaotic behaviors in tabu learning two-neuron models, with linear and\nquadratic proximity functions respectively.", "journal": ""}
{"doi": "10.48550/arXiv.0302103", "date": "2003-02-10", "title": "Weak convergence results for inhomogeneous rotating fluid equations", "authors": "Isabelle Gallagher, Laure Saint-Raymond", "abstract": "We consider the equations governing incompressible, viscous fluids in three\nspace dimensions, rotating around an inhomogeneous vector B(x): this is a\ngeneralization of the usual rotating fluid model (where B is constant). We\nprove the weak convergence of Leray--type solutions towards a vector field\nwhich satisfies the usual 2D Navier--Stokes equation in the regions of space\nwhere B is constant, with Dirichlet boundary conditions, and a heat--type\nequation elsewhere. The method of proof uses weak compactness arguments.", "journal": ""}
{"doi": "10.48550/arXiv.2109.05599", "date": "2021-09-12", "title": "DELP: Dynamic Epistemic Logic for Security Protocols", "authors": "Ioana Leustean, Bogdan Macovei", "abstract": "The formal analysis of security protocols is a challenging field, with\nvarious approaches being studied nowadays. The famous Burrows-Abadi-Needham\nLogic was the first logical system aiming to validate security protocols.\nCombining ideas from previous approaches, in this paper we define a complete\nsystem of dynamic epistemic logic for modeling security protocols. Our logic is\nimplemented, and few of its properties are verifyied, using the theorem prover\nLean.", "journal": ""}
{"doi": "10.48550/arXiv.2411.03065", "date": "2024-11-05", "title": "Growing conditioned BGW trees with log-concave offspring distributions", "authors": "William Fleurat", "abstract": "We show that given a log-concave offspring distribution, the corresponding\nsequence of Bienaym\\'e-Galton-Watson trees conditioned to have $n\\geq 1$\nvertices admits a realization as a Markov process $(T_n)_{n\\geq1}$ which adds a\nnew ``right-leaning'' leaf at each step. This applies for instance to offspring\ndistributions which are Poisson, binomial, geometric, or any convolution of\nthose. By a negative result of Janson, the log-concavity condition is optimal\nin the restricted case of offspring distributions supported in $\\{0,1,2\\}$. We\nthen prove a generalization to the case of an offspring distribution supported\non an arithmetic progression, if we assume log-concavity along that\nprogression.\n  As an application, we deduce the existence of increasing couplings in an\ninhomogeneous model of random subtrees of the Ulam--Harris tree.\n  These results generalize a construction of Luczak and Winkler which applies\nto uniformly sampled subtrees with $n$ vertices of the infinite complete\n$d$-ary trees. Our proofs are elementary and we tried to make them as\nself-contained as possible.", "journal": ""}
{"doi": "10.48550/arXiv.2003.09340", "date": "2020-03-20", "title": "Ordered Functional Decision Diagrams: A Functional Semantics For Binary Decision Diagrams", "authors": "Joan Thibault, Khalil Ghorbal", "abstract": "We introduce a novel framework, termed $\\lambda$DD, that revisits Binary\nDecision Diagrams from a purely functional point of view. The framework allows\nto classify the already existing variants, including the most recent ones like\nChain-DD and ESRBDD, as implementations of a special class of ordered models.\nWe enumerate, in a principled way, all the models of this class and isolate its\nmost expressive model. This new model, termed $\\lambda$DD-O-NUCX, is suitable\nfor both dense and sparse Boolean functions, and is moreover invariant by\nnegation. The canonicity of $\\lambda$DD-O-NUCX is formally verified using the\nCoq proof assistant. We furthermore give bounds on the size of the different\ndiagrams: the potential gain achieved by more expressive models can be at most\nlinear in the number of variables n.", "journal": ""}
{"doi": "10.48550/arXiv.1310.5504", "date": "2013-10-21", "title": "Barnette's Conjecture", "authors": "Lean Arts, Meike Hopman, Veerle Timmermans", "abstract": "This report provides an overview of theorems and statements related to a\nconjecture stated by D.W. Barnette in 1969 (which is an open problem in graph\ntheory): Every cubic, bipartite, polyhedral graph contains a Hamilton cycle.", "journal": ""}
{"doi": "10.48550/arXiv.2403.11716", "date": "2024-03-18", "title": "Models for Storage in Database Backends", "authors": "Edgard Schiebelbein, Saalik Hatia, Annette Bieniusa, Gustavo Petri, Carla Ferreira, Marc Shapiro", "abstract": "This paper describes ongoing work on developing a formal specification of a\ndatabase backend. We present the formalisation of the expected behaviour of a\nbasic transactional system that calls into a simple store API, and instantiate\nin two semantic models. The first one is a map-based, classical versioned\nkey-value store; the second one, journal-based, appends individual transaction\neffects to a journal. We formalise a significant part of the specification in\nthe Coq proof assistant. This work will form the basis for a formalisation of a\nfull-fledged backend store with features such as caching or write-ahead\nlogging, as variations on maps and journals.", "journal": ""}
{"doi": "10.48550/arXiv.1902.07366", "date": "2019-02-20", "title": "A constructive Knaster-Tarski proof of the uncountability of the reals", "authors": "Ingo Blechschmidt, Matthias Hutzler", "abstract": "We give an uncountability proof of the reals which relies on their order\ncompleteness instead of their sequential completeness. We use neither a form of\nthe axiom of choice nor the law of excluded middle, therefore the proof applies\nto the MacNeille reals in any flavor of constructive mathematics. The proof\nleans heavily on Levy's unusual proof of the uncountability of the reals.", "journal": ""}
{"doi": "10.48550/arXiv.1908.05294", "date": "2019-08-14", "title": "Undecidability of $D_{<:}$ and Its Decidable Fragments", "authors": "Jason Hu, Ond\u0159ej Lhot\u00e1k", "abstract": "Dependent Object Types (DOT) is a calculus with path dependent types,\nintersection types, and object self-references, which serves as the core\ncalculus of Scala 3. Although the calculus has been proven sound, it remains\nopen whether type checking in DOT is decidable. In this paper, we establish\nundecidability proofs of type checking and subtyping of $D_{<:}$, a syntactic\nsubset of DOT. It turns out that even for $D_{<:}$, undecidability is\nsurprisingly difficult to show, as evidenced by counterexamples for past\nattempts. To prove undecidability, we discover an equivalent definition of the\n$D_{<:}$ subtyping rules in normal form. Besides being easier to reason about,\nthis definition makes the phenomenon of bad bounds explicit as a single\ninference rule. After removing this rule, we discover two decidable fragments\nof $D_{<:}$ subtyping and identify algorithms to decide them. We prove\nsoundness and completeness of the algorithms with respect to the fragments, and\nwe prove that the algorithms terminate. Our proofs are mechanized in a\ncombination of Coq and Agda.", "journal": ""}
{"doi": "10.48550/arXiv.2110.05063", "date": "2021-10-11", "title": "Efficient Extensional Binary Tries", "authors": "Andrew W Appel, Xavier Leroy", "abstract": "Lookup tables (finite maps) are a ubiquitous data structure. In pure\nfunctional languages they are best represented using trees instead of hash\ntables. In pure functional languages within constructive logic, without a\nprimitive integer type, they are well represented using binary tries instead of\nsearch trees. In this work, we introduce canonical binary tries, an improved\nbinary-trie data structure that enjoys a natural extensionality property, quite\nuseful in proofs, and supports sparseness more efficiently. We provide full\nproofs of correctness in Coq. We provide microbenchmark measurements of\ncanonical binary tries versus several other data structures for finite maps, in\na variety of application contexts; as well as measurement of canonical versus\noriginal tries in two big, real systems. The application context of data\nstructures contained in theorem statements imposes unusual requirements for\nwhich canonical tries are particularly well suited.", "journal": "Journal of Automated Reasoning, 2023, 67, pp.Article number 8"}
{"doi": "10.48550/arXiv.2501.13002", "date": "2025-01-22", "title": "Constructive characterisations of the must-preorder for asynchrony", "authors": "Giovanni Bernardi, Ilaria Castellani, Paul Laforgue, L\u00e9o Stefanesco", "abstract": "De Nicola and Hennessy's must-preorder is a contextual refinement which\nstates that a server q refines a server p if all clients satisfied by p are\nalso satisfied by q. Owing to the universal quantification over clients, this\ndefinition does not yield a practical proof method for the must-preorder, and\nalternative characterisations are necessary to reason over it. Finding these\ncharacterisations for asynchronous semantics, i.e. where outputs are\nnon-blocking, has thus far proven to be a challenge, usually tackled via ad-hoc\ndefinitions. We show that the standard characterisations of the must-preorder\ncarry over as they stand to asynchronous communication, if servers are enhanced\nto act as forwarders, i.e. they can input any message as long as they store it\nback into the shared buffer. Our development is constructive, is completely\nmechanised in Coq, and is independent of any calculus: our results pertain to\nSelinger output-buffered agents with feedback. This is a class of Labelled\nTransition Systems that captures programs that communicate via a shared\nunordered buffer, as in asynchronous CCS or the asynchronous pi-calculus. We\nshow that the standard coinductive characterisation lets us prove in Coq that\nconcrete programs are related by the must-preorder. Finally, our proofs show\nthat Brouwer's bar induction principle is a useful technique to reason on\nliveness preserving program transformations.", "journal": ""}
{"doi": "10.48550/arXiv.2311.06840", "date": "2023-11-12", "title": "Omitted Labels in Causality: A Study of Paradoxes", "authors": "Bijan Mazaheri, Siddharth Jain, Matthew Cook, Jehoshua Bruck", "abstract": "We explore what we call ``omitted label contexts,'' in which training data is\nlimited to a subset of the possible labels. This setting is common among\nspecialized human experts or specific focused studies. We lean on well-studied\nparadoxes (Simpson's and Condorcet) to illustrate the more general difficulties\nof causal inference in omitted label contexts. Contrary to the fundamental\nprinciples on which much of causal inference is built, we show that ``correct''\nadjustments sometimes require non-exchangeable treatment and control groups.\nThese pitfalls lead us to the study networks of conclusions drawn from\ndifferent contexts and the structures the form, proving an interesting\nconnection between these networks and social choice theory.", "journal": ""}
{"doi": "10.48550/arXiv.1108.3125", "date": "2011-08-16", "title": "Formal Component-Based Semantics", "authors": "Ken Madlener, Sjaak Smetsers, Marko van Eekelen", "abstract": "One of the proposed solutions for improving the scalability of semantics of\nprogramming languages is Component-Based Semantics, introduced by Peter D.\nMosses. It is expected that this framework can also be used effectively for\nmodular meta theoretic reasoning. This paper presents a formalization of\nComponent-Based Semantics in the theorem prover Coq. It is based on Modular\nSOS, a variant of SOS, and makes essential use of dependent types, while\nprofiting from type classes. This formalization constitutes a contribution\ntowards modular meta theoretic formalizations in theorem provers. As a small\nexample, a modular proof of determinism of a mini-language is developed.", "journal": "EPTCS 62, 2011, pp. 17-29"}
{"doi": "10.48550/arXiv.1612.05494", "date": "2016-12-16", "title": "Type Inference of Simulink Hierarchical Block Diagrams in Isabelle", "authors": "Viorel Preoteasa, Iulia Dragomir, Stavros Tripakis", "abstract": "Simulink is a de-facto industrial standard for the design of embedded\nsystems. In previous work, we developed a compositional analysis framework for\nSimulink models in Isabelle -- the Refinement Calculus of Reactive Systems\n(RCRS), which allows checking compatibility and substitutability of components.\nHowever, standard type checking was not considered in that work. In this paper\nwe present a method for the type inference of hierarchical block diagrams using\nthe Isabelle theorem prover. A Simulink diagram is translated into an (RCRS)\nIsabelle theory. Then the Isabelle's powerful type inference mechanism is used\nto infer the types of the diagram based on the types of the basic blocks. One\nof the aims is to handle formally as many diagrams as possible. In particular,\nwe want to be able to handle even those diagrams that may have typing\nambiguities, provided that they are accepted by Simulink. This method is\nimplemented in our toolset that translates Simulink diagrams into Isabelle\ntheories and simplifies them. We evaluate our technique on several case\nstudies, most notably, an automotive fuel control system benchmark provided by\nToyota.", "journal": ""}
{"doi": "10.48550/arXiv.2304.15006", "date": "2023-04-01", "title": "Topologically sorting VDM-SL definitions for Isabelle/HOL translation", "authors": "Leo Freitas", "abstract": "There is an ecosystem of VDM libraries and extensions that includes a\ntranslation and proof environment for VDM in Isabelle. Translation works for a\nlarge subset of VDM-SL and further constructs are being added on demand. A key\nimpediment for novice users is that Isabelle/HOL requires all definitions to be\ndeclared before they are used, where (mutually) recursive definitions must be\ndefined in tandem. In this paper, we describe a solution to this problem, which\nwill enable wider access to the translator plugin for novice users as well as\nreal models.", "journal": ""}
{"doi": "10.48550/arXiv.2202.05360", "date": "2022-02-10", "title": "Formalized functional analysis with semilinear maps", "authors": "Fr\u00e9d\u00e9ric Dupuis, Robert Y. Lewis, Heather Macbeth", "abstract": "Semilinear maps are a generalization of linear maps between vector spaces\nwhere we allow the scalar action to be twisted by a ring homomorphism such as\ncomplex conjugation. In particular, this generalization unifies the concepts of\nlinear and conjugate-linear maps. We implement this generalization in Lean's\n\\textsf{mathlib} library, along with a number of important results in\nfunctional analysis which previously were impossible to formalize properly.\nSpecifically, we prove the Fr\\'echet--Riesz representation theorem and the\nspectral theorem for compact self-adjoint operators generically over real and\ncomplex Hilbert spaces. We also show that semilinear maps have applications\nbeyond functional analysis by formalizing the one-dimensional case of a theorem\nof Dieudonn\\'e and Manin that classifies the isocrystals over an algebraically\nclosed field with positive characteristic.", "journal": ""}
{"doi": "10.48550/arXiv.2108.10868", "date": "2021-08-08", "title": "Towards Formalising Schutz' Axioms for Minkowski Spacetime in Isabelle/HOL", "authors": "Richard Schmoetten, Jake E. Palmer, Jacques D. Fleuriot", "abstract": "Special Relativity is a cornerstone of modern physical theory. While a\nstandard coordinate model is well-known and widely taught today, several\nalternative systems of axioms exist. This paper reports on the formalisation of\none such system which is closer in spirit to Hilbert's axiomatic approach to\nEuclidean geometry than to the vector space approach employed by Minkowski. We\npresent a mechanisation in Isabelle/HOL of the system of axioms as well as\ntheorems relating to temporal order. Proofs and excerpts of Isabelle/Isar\nscripts are discussed, particularly where the formal work required additional\nsteps, alternative approaches, or corrections to Schutz' prose.", "journal": ""}
{"doi": "10.48550/arXiv.1807.09873", "date": "2018-07-25", "title": "Formalizing the Cox-Ross-Rubinstein pricing of European derivatives in Isabelle/HOL", "authors": "Mnacho Echenim, Herv\u00e9 Guiol, Nicolas Peltier", "abstract": "We formalize in the proof assistant Isabelle essential basic notions and\nresults in financial mathematics. We provide generic formal definitions of\nconcepts such as markets, portfolios, derivative products, arbitrages or fair\nprices, and we show that, under the usual no-arbitrage condition, the existence\nof a replicating portfolio for a derivative implies that the latter admits a\nunique fair price. Then, we provide a formalization of the Cox-Rubinstein model\nand we show that the market is complete in this model, i.e., that every\nderivative product admits a replicating portfolio. This entails that in this\nmodel, every derivative product admits a unique fair price.", "journal": ""}
{"doi": "10.48550/arXiv.1007.3133", "date": "2010-07-19", "title": "Enforcing Secure Object Initialization in Java", "authors": "Laurent Hubert, Thomas Jensen, Vincent Monfort, David Pichardie", "abstract": "Sun and the CERT recommend for secure Java development to not allow partially\ninitialized objects to be accessed. The CERT considers the severity of the\nrisks taken by not following this recommendation as high. The solution\ncurrently used to enforce object initialization is to implement a coding\npattern proposed by Sun, which is not formally checked. We propose a modular\ntype system to formally specify the initialization policy of libraries or\nprograms and a type checker to statically check at load time that all loaded\nclasses respect the policy. This allows to prove the absence of bugs which have\nallowed some famous privilege escalations in Java. Our experimental results\nshow that our safe default policy allows to prove 91% of classes of java.lang,\njava.security and javax.security safe without any annotation and by adding 57\nsimple annotations we proved all classes but four safe. The type system and its\nsoundness theorem have been formalized and machine checked using Coq.", "journal": "15th European Symposium on Research in Computer Security (ESORICS)\n  6345 (2010) 101-115"}
{"doi": "10.48550/arXiv.1512.07304", "date": "2015-12-22", "title": "Mechanizing a Process Algebra for Network Protocols", "authors": "Timothy Bourke, Robert J. van Glabbeek, Peter H\u00f6fner", "abstract": "This paper presents the mechanization of a process algebra for Mobile Ad hoc\nNetworks and Wireless Mesh Networks, and the development of a compositional\nframework for proving invariant properties. Mechanizing the core process\nalgebra in Isabelle/HOL is relatively standard, but its layered structure\nnecessitates special treatment. The control states of reactive processes, such\nas nodes in a network, are modelled by terms of the process algebra. We propose\na technique based on these terms to streamline proofs of inductive invariance.\nThis is not sufficient, however, to state and prove invariants that relate\nstates across multiple processes (entire networks). To this end, we propose a\nnovel compositional technique for lifting global invariants stated at the level\nof individual nodes to networks of nodes.", "journal": ""}
{"doi": "10.48550/arXiv.1302.6421", "date": "2013-02-26", "title": "ML4PG in Computer Algebra verification", "authors": "J\u00f3nathan Heras, Ekaterina Komendantskaya", "abstract": "ML4PG is a machine-learning extension that provides statistical proof hints\nduring the process of Coq/SSReflect proof development. In this paper, we use\nML4PG to find proof patterns in the CoqEAL library -- a library that was\ndevised to verify the correctness of Computer Algebra algorithms. In\nparticular, we use ML4PG to help us in the formalisation of an efficient\nalgorithm to compute the inverse of triangular matrices.", "journal": ""}
{"doi": "10.48550/arXiv.1609.01493", "date": "2016-09-06", "title": "Axiomatizing Category Theory in Free Logic", "authors": "Christoph Benzm\u00fcller, Dana S. Scott", "abstract": "Starting from a generalization of the standard axioms for a monoid we present\na stepwise development of various, mutually equivalent foundational axiom\nsystems for category theory. Our axiom sets have been formalized in the\nIsabelle/HOL interactive proof assistant, and this formalization utilizes a\nsemantically correct embedding of free logic in classical higher-order logic.\nThe modeling and formal analysis of our axiom sets has been significantly\nsupported by series of experiments with automated reasoning tools integrated\nwith Isabelle/HOL. We also address the relation of our axiom systems to\nalternative proposals from the literature, including an axiom set proposed by\nFreyd and Scedrov for which we reveal a technical issue (when encoded in free\nlogic where free variables range over defined and undefined objects): either\nall operations, e.g. morphism composition, are total or their axiom system is\ninconsistent. The repair for this problem is quite straightforward, however.", "journal": ""}
{"doi": "10.48550/arXiv.2103.00513", "date": "2021-02-28", "title": "Accelerated design of linear-superelastic Ti-Nb nanocomposite alloys with ultralow modulus via high-throughput phase-field simulations and machine learning", "authors": "Yuquan Zhu, Tao Xu, Qinghua Wei, Hongxin Yang, Takahiro Shimada, Takayuki Kitamura, Tong-Yi Zhang", "abstract": "The controlled design of martensitic transformation (MT) to achieve specific\nproperties is crucial for the innovative application of shape memory alloys\n(SMAs) in advanced technologies. Herein, we explore and design the MT behaviors\nand the mechanical properties of Ti-Nb nanocomposites by combining\nhigh-throughput phase-field simulations and machine learning (ML) approaches.\nBased on the systematic phase-field simulations, we obtain data sets of the\nmechanical properties for various nanocomposites constructed by four\nmacroscopic degrees of freedom, which can be employed to design and optimize\nthe microstructures for different applications. To accelerate the phase-field\nscreening of the desired metallic biomaterials, a ML assisted strategy is\nadopted to perform multi-objective optimization of the mechanical properties,\nthrough which promising nanocomposite configurations are pre-screened for the\nnext set of phase-field simulations. With the ML guided simulations, an\noptimized candidate composed of Nb-rich matrix and Nb-lean nanofillers that\nexhibits a combination of unprecedented mechanical properties, including\nultra-low modulus, linear super-elasticity, and near-hysteresis-free is\ndesigned. The exceptional mechanical properties in the nanocomposite originate\nfrom optimized continuous MT rather than a sharp first-order transition, which\nis common in typical SMAs. This work provides a new computational approach and\ndesign concept for developing novel functional materials with extraordinary\nproperties.", "journal": ""}
{"doi": "10.48550/arXiv.1412.0542", "date": "2014-11-07", "title": "Budget Imbalance Criteria for Auctions: A Formalized Theorem", "authors": "Marco B. Caminati, Manfred Kerber, Colin Rowat", "abstract": "We present an original theorem in auction theory: it specifies general\nconditions under which the sum of the payments of all bidders is necessarily\nnot identically zero, and more generally not constant. Moreover, it explicitly\nsupplies a construction for a finite minimal set of possible bids on which such\na sum is not constant. In particular, this theorem applies to the important\ncase of a second-price Vickrey auction, where it reduces to a basic result of\nwhich a novel proof is given. To enhance the confidence in this new theorem, it\nhas been formalized in Isabelle/HOL: the main results and definitions of the\nformal proof are re- produced here in common mathematical language, and are\naccompanied by an informal discussion about the underlying ideas.", "journal": ""}
{"doi": "10.48550/arXiv.1806.10015", "date": "2018-06-26", "title": "An a priori analysis of a DNS database of turbulent lean premixed methane flames for LES with finite-rate chemistry", "authors": "A J Aspden, N Zettervall, C Fureby", "abstract": "An a priori analysis of a DNS database of turbulent lean premixed methane\nflames is presented considering the relative effects of turbulence and LES\nfiltering, along with a potential modelling approach for LES with finite-rate\nchemistry. The leading-order effect was found to be due to the filter\noperation; flame response to turbulence was a secondary effect, and manifested\nprimarily as an increase in standard deviations about conditional means. It was\nfound that the radicals O, H and OH were less impacted by the filter than other\nhigh-temperature radicals, which were significantly reduced in magnitude by the\nfilter. By considering reaction path diagrams, key reactions have been\nidentified that are responsible for disparities between the desired filtered\nreaction rates and the reaction rates evaluated using quantities available in\nLES calculations (i.e. the filtered species and temperature). More\nspecifically, the hydrogen abstraction reactions that take CH4 to CH3 (by O, H\nand OH) were found to have particularly enhanced reaction rates, and dominate\nthe reaction path. Under the conditions presented, reaction paths were found to\nbe independent of turbulence intensity. In general, the filtered reaction rates\nfrom the DNS were found to align more closely with the filtered laminar profile\nthan the reaction rate of filtered species and temperature, (but disparities\nwere found to decrease with increasing Karlovitz number). A simple model for\nscaling reaction rates is considered based on filtered laminar flame profiles,\nand the resulting reaction paths demonstrate proof-of-concept of a simple\napproach for formulating a reaction rate model for LES with finite-rate\nchemistry.", "journal": ""}
{"doi": "10.48550/arXiv.2203.01173", "date": "2022-03-02", "title": "Characteristics of de Bruijn's early proof checker Automath", "authors": "Herman Geuvers, Rob Nederpelt", "abstract": "The `mathematical language' Automath, conceived by N.G. de Bruijn in 1968,\nwas the first theorem prover actually working and was used for checking many\nspecimina of mathematical content. Its goals and syntactic ideas inspired Th.\nCoquand and G. Huet to develop the calculus of constructions, CC, which was one\nof the first widely used interactive theorem provers and forms the basis for\nthe widely used Coq system. The original syntax of Automath is not easy to\ngrasp. Yet, it is essentially based on a derivation system that is similar to\nthe Calculus of Constructions (`CC'). The relation between the Automath syntax\nand CC has not yet been sufficiently described, although there are many\nreferences in the type theory community to Automath. In this paper we focus on\nthe backgrounds and on some uncommon aspects of the syntax of Automath. We\nexpose the fundamental aspects of a `generic' Automath system, encapsulating\nthe most common versions of Automath. We present this generic Automath system\nin a modern syntactic frame. The obtained system makes use of {\\lambda}D, a\ndirect extension of CC with definitions.", "journal": "Fundamenta Informaticae, Volume 185, Issue 4 (July 7, 2022)\n  fi:9343"}
{"doi": "10.48550/arXiv.2008.01241", "date": "2020-08-03", "title": "Pricing Options Under Rough Volatility with Backward SPDEs", "authors": "Christian Bayer, Jinniao Qiu, Yao Yao", "abstract": "In this paper, we study the option pricing problems for rough volatility\nmodels. As the framework is non-Markovian, the value function for a European\noption is not deterministic; rather, it is random and satisfies a backward\nstochastic partial differential equation (BSPDE). The existence and uniqueness\nof weak solution is proved for general nonlinear BSPDEs with unbounded random\nleading coefficients whose connections with certain forward-backward stochastic\ndifferential equations are derived as well. These BSPDEs are then used to\napproximate American option prices. A deep leaning-based method is also\ninvestigated for the numerical approximations to such BSPDEs and associated\nnon-Markovian pricing problems. Finally, the examples of rough Bergomi type are\nnumerically computed for both European and American options.", "journal": ""}
{"doi": "10.48550/arXiv.1703.07342", "date": "2017-03-21", "title": "LaraDB: A Minimalist Kernel for Linear and Relational Algebra Computation", "authors": "Dylan Hutchison, Bill Howe, Dan Suciu", "abstract": "Analytics tasks manipulate structured data with variants of relational\nalgebra (RA) and quantitative data with variants of linear algebra (LA). The\ntwo computational models have overlapping expressiveness, motivating a common\nprogramming model that affords unified reasoning and algorithm design. At the\nlogical level we propose Lara, a lean algebra of three operators, that\nexpresses RA and LA as well as relevant optimization rules. We show a series of\nproofs that position Lara %formal and informal at just the right level of\nexpressiveness for a middleware algebra: more explicit than MapReduce but more\ngeneral than RA or LA. At the physical level we find that the Lara operators\nafford efficient implementations using a single primitive that is available in\na variety of backend engines: range scans over partitioned sorted maps.\n  To evaluate these ideas, we implemented the Lara operators as range iterators\nin Apache Accumulo, a popular implementation of Google's BigTable. First we\nshow how Lara expresses a sensor quality control task, and we measure the\nperformance impact of optimizations Lara admits on this task. Second we show\nthat the LaraDB implementation outperforms Accumulo's native MapReduce\nintegration on a core task involving join and aggregation in the form of matrix\nmultiply, especially at smaller scales that are typically a poor fit for\nscale-out approaches. We find that LaraDB offers a conceptually lean framework\nfor optimizing mixed-abstraction analytics tasks, without giving up fast\nrecord-level updates and scans.", "journal": ""}
{"doi": "10.48550/arXiv.2209.13894", "date": "2022-09-28", "title": "The Isabelle Community Benchmark", "authors": "Fabian Huch, Vincent Bode", "abstract": "Choosing hardware for theorem proving is no simple task: automated provers\nare highly complex and optimized programs, often utilizing a parallel\ncomputation model, and there is little prior research on the hardware impact on\nprover performance. To alleviate the problem for Isabelle, we initiated a\ncommunity benchmark where the build time of HOL-Analysis is measured. On $54$\ndistinct CPUs, a total of $669$ runs with different Isabelle configurations\nwere reported by Isabelle users. Results range from $107$s to over $11$h. We\nfound that current consumer CPUs performed best, with an optimal number of $8$\nto $16$ threads, largely independent of heap memory. As for hardware\nparameters, CPU base clock affected multi-threaded execution most with a linear\ncorrelation of $0.37$, whereas boost frequency was the most influential\nparameter for single-threaded runs (correlation coefficient $0.55$); cache size\nplayed no significant role. When comparing our benchmark scores with popular\nhigh-performance computing benchmarks, we found a strong linear relationship\nwith Dolfyn ($R^2 = 0.79$) in the single-threaded scenario. Using data from the\n3DMark CPU Profile consumer benchmark, we created a linear model for optimal\n(multi-threaded) Isabelle performance. When validating, the model has an\naverage $R^2$-score of $0.87$; the mean absolute error in the final model\ncorresponds to a wall-clock time of $46.6$s. With a dataset of true median\nvalues for the 3DMark, the error improves to $37.1$s.", "journal": "Proceedings of the Workshop on Practical Aspects of Automated\n  Reasoning Vol-3201 (2022)"}
{"doi": "10.48550/arXiv.1001.4898", "date": "2010-01-27", "title": "Formal Proof of a Wave Equation Resolution Scheme: the Method Error", "authors": "Sylvie Boldo, Fran\u00e7ois Cl\u00e9ment, Jean-Christophe Filli\u00e2tre, Micaela Mayero, Guillaume Melquiond, Pierre Weis", "abstract": "Popular finite difference numerical schemes for the resolution of the\none-dimensional acoustic wave equation are well-known to be convergent. We\npresent a comprehensive formalization of the simplest one and formally prove\nits convergence in Coq. The main difficulties lie in the proper definition of\nasymptotic behaviors and the implicit way they are handled in the mathematical\npen-and-paper proofs. To our knowledge, this is the first time such kind of\nmathematical proof is machine-checked.", "journal": ""}
{"doi": "10.48550/arXiv.1203.6412", "date": "2012-03-29", "title": "Barriers in Concurrent Separation Logic: Now With Tool Support!", "authors": "Aquinas Hobor, Cristian Gherghina", "abstract": "We develop and prove sound a concurrent separation logic for Pthreads-style\nbarriers. Although Pthreads barriers are widely used in systems, and separation\nlogic is widely used for verification, there has not been any effort to combine\nthe two. Unlike locks and critical sections, Pthreads barriers enable\nsimultaneous resource redistribution between multiple threads and are\ninherently stateful, leading to significant complications in the design of the\nlogic and its soundness proof. We show how our logic can be applied to a\nspecific example program in a modular way. Our proofs are machine-checked in\nCoq. We showcase a program verification toolset that automatically applies the\nlogic rules and discharges the associated proof obligations.", "journal": "Logical Methods in Computer Science, Volume 8, Issue 2 (April 20,\n  2012) lmcs:800"}
{"doi": "10.48550/arXiv.2212.05578", "date": "2022-12-11", "title": "A Formalization of Doob's Martingale Convergence Theorems in mathlib", "authors": "Kexing Ying, R\u00e9my Degenne", "abstract": "We present the formalization of Doob's martingale convergence theorems in the\nmathlib library for the Lean theorem prover. These theorems give conditions\nunder which (sub)martingales converge, almost everywhere or in $L^1$. In order\nto formalize those results, we build a definition of the conditional\nexpectation in Banach spaces and develop the theory of stochastic processes,\nstopping times and martingales. As an application of the convergence theorems,\nwe also present the formalization of L\\'evy's generalized Borel-Cantelli lemma.\nThis work on martingale theory is one of the first developments of probability\ntheory in mathlib, and it builds upon diverse parts of that library such as\ntopology, analysis and most importantly measure theory.", "journal": ""}
{"doi": "10.48550/arXiv.1910.00905", "date": "2019-10-02", "title": "Compositional Non-Interference for Fine-Grained Concurrent Programs", "authors": "Dan Frumin, Robbert Krebbers, Lars Birkedal", "abstract": "Non-interference is a program property that ensures the absence of\ninformation leaks. In the context of programming languages, there exist two\ncommon approaches for establishing non-interference: type systems and program\nlogics. Type systems provide strong automation (by means of type checking), but\nthey are inherently restrictive in the kind of programs they support. Program\nlogics support challenging programs, but they typically require significant\nhuman assistance, and cannot handle modules or higher-order programs.\n  To connect these two approaches, we present SeLoC---a separation logic for\nnon-interference, on top of which we build a type system using the technique of\nlogical relations. By building a type system on top of separation logic, we can\ncompositionally verify programs that consist of typed and untyped parts. The\nformer parts are verified through type checking, while the latter parts are\nverified through manual proof.\n  The core technical contribution of SeLoC is a relational form of weakest\npreconditions that can track information flow using separation logic resources.\nSeLoC is fully machine-checked, and built on top of the Iris framework for\nconcurrent separation logic in Coq. The integration with Iris provides seamless\nsupport for fine-grained concurrency, which was beyond the reach of prior type\nsystems and program logics for non-interference.", "journal": ""}
{"doi": "10.48550/arXiv.2009.08174", "date": "2020-09-17", "title": "Higher-Order Nonemptiness Step by Step", "authors": "Pawe\u0142 Parys", "abstract": "We show a new simple algorithm that checks whether a given higher-order\ngrammar generates a nonempty language of trees. The algorithm amounts to a\nprocedure that transforms a grammar of order n to a grammar of order n-1,\npreserving nonemptiness, and increasing the size only exponentially. After\nrepeating the procedure n times, we obtain a grammar of order 0, whose\nnonemptiness can be easily checked. Since the size grows exponentially at each\nstep, the overall complexity is n-EXPTIME, which is known to be optimal. More\nprecisely, the transformation (and hence the whole algorithm) is linear in the\nsize of the grammar, assuming that the arity of employed nonterminals is\nbounded by a constant. The same algorithm allows to check whether an infinite\ntree generated by a higher-order recursion scheme is accepted by an alternating\nsafety (or reachability) automaton, because this question can be reduced to the\nnonemptiness problem by taking a product of the recursion scheme with the\nautomaton.\n  A proof of correctness of the algorithm is formalised in the proof assistant\nCoq. Our transformation is motivated by a similar transformation of Asada and\nKobayashi (2020) changing a word grammar of order n to a tree grammar of order\nn-1. The step-by-step approach can be opposed to previous algorithms solving\nthe nonemptiness problem \"in one step\", being compulsorily more complicated.", "journal": ""}
{"doi": "10.48550/arXiv.2111.04082", "date": "2021-11-07", "title": "Pattern-based Subterm Selection in Isabelle", "authors": "Lars Noschinski, Christoph Traut", "abstract": "This article presents a pattern-based language designed to select (a set of)\nsubterms of a given term in a concise and robust way. Building on this\nlanguage, we implement a single-step rewriting tactic in the Isabelle theorem\nprover, which removes the need for obscure \"occurrence numbers\" for subterm\nselection.\n  The language was inspired by the \\emph{language of patterns} of Gonthier and\nTassi, but provides an elegant way of handling bound variables and a modular\nsemantics.", "journal": ""}
{"doi": "10.48550/arXiv.9809120", "date": "1998-09-29", "title": "A Natural Deduction style proof system for propositional $\u03bc$-calculus and its formalization in inductive type theories", "authors": "Marino Miculan", "abstract": "In this paper, we present a formalization of Kozen's propositional modal\n$\\mu$-calculus, in the Calculus of Inductive Constructions. We address several\nproblematic issues, such as the use of higher-order abstract syntax in\ninductive sets in presence of recursive constructors, the encoding of modal\n(``proof'') rules and of context sensitive grammars. The encoding can be used\nin the \\Coq system, providing an experimental computer-aided proof environment\nfor the interactive development of error-free proofs in the $\\mu$-calculus. The\ntechniques we adopted can be readily ported to other languages and proof\nsystems featuring similar problematic issues.", "journal": ""}
{"doi": "10.48550/arXiv.1905.04224", "date": "2019-05-10", "title": "Supervised machine learning based multi-task artificial intelligence classification of retinopathies", "authors": "Minhaj Alam, David Le, Jennifer I. Lim, R. V. P. Chan, Xincheng Yao", "abstract": "Artificial intelligence (AI) classification holds promise as a novel and\naffordable screening tool for clinical management of ocular diseases. Rural and\nunderserved areas, which suffer from lack of access to experienced\nophthalmologists may particularly benefit from this technology. Quantitative\noptical coherence tomography angiography (OCTA) imaging provides excellent\ncapability to identify subtle vascular distortions, which are useful for\nclassifying retinovascular diseases. However, application of AI for\ndifferentiation and classification of multiple eye diseases is not yet\nestablished. In this study, we demonstrate supervised machine learning based\nmulti-task OCTA classification. We sought 1) to differentiate normal from\ndiseased ocular conditions, 2) to differentiate different ocular disease\nconditions from each other, and 3) to stage the severity of each ocular\ncondition. Quantitative OCTA features, including blood vessel tortuosity (BVT),\nblood vascular caliber (BVC), vessel perimeter index (VPI), blood vessel\ndensity (BVD), foveal avascular zone (FAZ) area (FAZ-A), and FAZ contour\nirregularity (FAZ-CI) were fully automatically extracted from the OCTA images.\nA stepwise backward elimination approach was employed to identify sensitive\nOCTA features and optimal-feature-combinations for the multi-task\nclassification. For proof-of-concept demonstration, diabetic retinopathy (DR)\nand sickle cell retinopathy (SCR) were used to validate the supervised machine\nleaning classifier. The presented AI classification methodology is applicable\nand can be readily extended to other ocular diseases, holding promise to enable\na mass-screening platform for clinical deployment and telemedicine.", "journal": "https://www.mdpi.com/2077-0383/8/6/872"}
{"doi": "10.48550/arXiv.1905.10237", "date": "2019-05-24", "title": "Obstructions to representations up to homotopy and ideals", "authors": "Madeleine Jotz Lean", "abstract": "This paper considers the Pontryagin characters of graded vector bundles of\nfinite rank, in the cohomology vector spaces of a Lie algebroid over the same\nbase. These Pontryagin characters vanish if the graded vector bundle carries a\nrepresentation up to homotopy of the Lie algebroid. As a consequence, this\ngives a strong obstruction to the existence of a representation up to homotopy\non a graded vector bundle of finite rank. In particular, if a graded vector\nbundle $E[0]\\oplus F[1]\\to M$ carries a $2$-term representation up to homotopy\nof a Lie algebroid $A\\to M$, then all the (classical) $A$-Pontryagin classes of\n$E$ and $F$ must coincide. This paper generalises as well Bott's vanishing\ntheorem to the setting of Lie algebroid representations (up to homotopy) on\narbitrary vector bundles. As an application, the main theorems induce new\nobstructions to the existence of infinitesimal ideal systems in a given Lie\nalgebroid.", "journal": ""}
{"doi": "10.48550/arXiv.2011.07653", "date": "2020-11-15", "title": "Coming to Terms with Your Choices: An Existential Take on Dependent Types", "authors": "Georg Stefan Schmid, Olivier Blanvillain, Jad Hamza, Viktor Kun\u010dak", "abstract": "Type-level programming is an increasingly popular way to obtain additional\ntype safety. Unfortunately, it remains a second-class citizen in the majority\nof industrially-used programming languages. We propose a new dependently-typed\nsystem with subtyping and singleton types whose goal is to enable type-level\nprogramming in an accessible style. At the heart of our system lies a\nnon-deterministic choice operator. We argue that embracing non-determinism is\ncrucial for bringing dependent types to a broader audience of programmers,\nsince real-world programs will inevitably interact with imprecisely-typed, or\neven impure code. Furthermore, we show that singleton types combined with the\nchoice operator can serve as a replacement for many type functions of interest\nin practice. We establish the soundness of our approach using the Coq proof\nassistant. Our soundness approach models non-determinism using additional\nfunction arguments to represent choices. We represent type-level computation\nusing singleton types and existential types that quantify over choice\narguments. To demonstrate the practicality of our type system, we present an\nimplementation as a modification of the Scala compiler. We provide a case study\nin which we develop a strongly-typed wrapper for Spark datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2402.17223", "date": "2024-02-27", "title": "Time-Restricted Double-Spending Attack on PoW-based Blockchains", "authors": "Yiming Jiang, Jiangfan Zhang", "abstract": "Numerous blockchain applications are designed with tasks that naturally have\nfinite durations, and hence, a double-spending attack (DSA) on such blockchain\napplications leans towards being conducted within a finite timeframe,\nspecifically before the completion of their tasks. Furthermore, existing\nresearch suggests that practical attackers typically favor executing a DSA\nwithin a finite timeframe due to their limited computational resources. These\nobservations serve as the impetus for this paper to investigate a\ntime-restricted DSA (TR-DSA) model on Proof-of-Work based blockchains. In this\nTR-DSA model, an attacker only mines its branch within a finite timeframe, and\nthe TR-DSA is considered unsuccessful if the attacker's branch fails to surpass\nthe honest miners' branch when the honest miners' branch has grown by a\nspecific number of blocks. First, we developed a general closed-form expression\nfor the success probability of a TR-DSA. This developed probability not only\ncan assist in evaluating the risk of a DSA on blockchain applications with\ntimely tasks, but also can enable practical attackers with limited\ncomputational resources to assess the feasibility and expected reward of\nlaunching a TR-DSA. In addition, we provide rigorous proof that the success\nprobability of a TR-DSA is no greater than that of a time-unrestricted DSA\nwhere the attacker indefinitely mines its branch. This result implies that\nblockchain applications with timely tasks are less vulnerable to DSAs than\nblockchain applications that provide attackers with an unlimited timeframe for\ntheir attacks. Furthermore, we show that the success probability of a TR-DSA is\nalways smaller than one even though the attacker controls more than half of the\nhash rate in the network. This result alerts attackers that there is still a\nrisk of failure in launching a TR-DSA even if they amass a majority of the hash\nrate in the network.", "journal": ""}
{"doi": "10.48550/arXiv.1312.0694", "date": "2013-12-03", "title": "Monotonic References for Gradual Typing", "authors": "Jeremy G. Siek, Michael M. Vitousek", "abstract": "We describe an alternative approach to handling mutable references (aka.\npointers) within a gradually typed language that has different efficiency\ncharacteristics than the prior approach of Herman et al. [2010]. In particular,\nwe reduce the costs of reading and writing through references in statically\ntyped regions of code. We reduce the costs to be the same as they would in a\nstatically typed language, that is, simply the cost of a load or store\ninstruction (for primitive data types). This reduction in cost is especially\nimportant for programmers who would like to use gradual typing to facilitate\ntransitioning from a dynamically-typed prototype of an algorithm to a\nstatically-typed, high-performance implementation. The programmers we have in\nmind are scientists and engineers who currently prototype in Matlab and then\nmanually translate their algorithms into Fortran. We present the static and\ndynamic semantics for mutable references and a mechanized proof of type safety\nusing the Isabelle proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.2311.13995", "date": "2023-11-23", "title": "Explicit Refinement Types", "authors": "Jad Elkhaleq Ghalayini, Neel Krishnaswami", "abstract": "We present {\\lambda}ert, a type theory supporting refinement types with\nexplicit proofs. Instead of solving refinement constraints with an SMT solver\nlike DML and Liquid Haskell, our system requires and permits programmers to\nembed proofs of properties within the program text, letting us support a rich\nlogic of properties including quantifiers and induction. We show that the type\nsystem is sound by showing that every refined program erases to a simply-typed\nprogram, and by means of a denotational semantics, we show that every erased\nprogram has all of the properties demanded by its refined type. All of our\nproofs are formalised in Lean 4.", "journal": "Proceedings of the ACM on Programming Languages, 2023, Volume 7,\n  Issue ICFP, Article Number 195, pages 187-214"}
{"doi": "10.48550/arXiv.2501.13712", "date": "2025-01-23", "title": "Formally Verified Neurosymbolic Trajectory Learning via Tensor-based Linear Temporal Logic on Finite Traces", "authors": "Mark Chevallier, Filip Smola, Richard Schmoetten, Jacques D. Fleuriot", "abstract": "We present a novel formalisation of tensor semantics for linear temporal\nlogic on finite traces (LTLf), with formal proofs of correctness carried out in\nthe theorem prover Isabelle/HOL. We demonstrate that this formalisation can be\nintegrated into a neurosymbolic learning process by defining and verifying a\ndifferentiable loss function for the LTLf constraints, and automatically\ngenerating an implementation that integrates with PyTorch. We show that, by\nusing this loss, the process learns to satisfy pre-specified logical\nconstraints. Our approach offers a fully rigorous framework for constrained\ntraining, eliminating many of the inherent risks of ad-hoc, manual\nimplementations of logical aspects directly in an \"unsafe\" programming language\nsuch as Python, while retaining efficiency in implementation.", "journal": ""}
{"doi": "10.48550/arXiv.1905.05500", "date": "2019-05-14", "title": "Unifying Semantic Foundations for Automated Verification Tools in Isabelle/UTP", "authors": "Simon Foster, James Baxter, Ana Cavalcanti, Jim Woodcock, Frank Zeyda", "abstract": "The growing complexity and diversity of models used in the engineering of\ndependable systems implies that a variety of formal methods, across differing\nabstractions, paradigms, and presentations, must be integrated. Such an\nintegration relies on unified semantic foundations for the various notations,\nand co-ordination of a variety of automated verification tools. The\ncontribution of this paper is Isabelle/UTP, an implementation of Hoare and He's\nUnifying Theories of Programming, a framework for unification of formal\nsemantics. Isabelle/UTP permits the mechanisation of computational theories for\ndiverse paradigms, and their use in constructing formalised semantic models.\nThese can be further applied in the development of verification tools,\nharnessing Isabelle's proof automation facilities. Several layers of\nmathematical foundations are developed, including lenses to model variables and\nstate spaces as algebraic objects, alphabetised predicates and relations to\nmodel programs, including algebraic and axiomatic semantics, proof tools for\nHoare logic and refinement calculus, and UTP theories to encode computational\nparadigms.", "journal": ""}
{"doi": "10.48550/arXiv.1611.09606", "date": "2016-11-29", "title": "An Inductive Proof Method for Simulation-based Compiler Correctness", "authors": "Sigurd Schneider, Gert Smolka, Sebastian Hack", "abstract": "We study induction on the program structure as a proof method for\nbisimulation-based compiler correctness. We consider a first-order language\nwith mutually recursive function definitions, system calls, and an environment\nsemantics. The proof method relies on a generalization of compatibility of\nfunction definition with the bisimulation. We use the inductive method to show\ncorrectness of a form of dead code elimination. This is an interesting case\nstudy because the transformation removes function, variable, and parameter\ndefinitions from the program. While such transformations require modification\nof the simulation in a coinductive proof, the inductive method deals with them\nnaturally. All our results are formalized in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1710.03979", "date": "2017-10-11", "title": "The Refinement Calculus of Reactive Systems", "authors": "Viorel Preoteasa, Iulia Dragomir, Stavros Tripakis", "abstract": "The Refinement Calculus of Reactive Systems (RCRS) is a compositional formal\nframework for modeling and reasoning about reactive systems. RCRS provides a\nlanguage which allows to describe atomic components as symbolic transition\nsystems or QLTL formulas, and composite components formed using three primitive\ncomposition operators: serial, parallel, and feedback. The semantics of the\nlanguage is given in terms of monotonic property transformers, an extension to\nreactive systems of monotonic predicate transformers, which have been used to\ngive compositional semantics to sequential programs. RCRS allows to specify\nboth safety and liveness properties. It also allows to model input-output\nsystems which are both non-deterministic and non-input-receptive (i.e., which\nmay reject some inputs at some points in time), and can thus be seen as a\nbehavioral type system. RCRS provides a set of techniques for symbolic\ncomputer-aided reasoning, including compositional static analysis and\nverification. RCRS comes with a publicly available implementation which\nincludes a complete formalization of the RCRS theory in the Isabelle proof\nassistant.", "journal": ""}
{"doi": "10.48550/arXiv.2110.07127", "date": "2021-10-14", "title": "Monitoring the Mental State of Cooperativeness for Guiding an Elderly Person in Sit-to-Stand Assistance", "authors": "John Bell, H. Harry Asada", "abstract": "In providing physical assistance to elderly people, ensuring cooperative\nbehavior from the elderly persons is a critical requirement. In sit-to-stand\nassistance, for example, an older adult must lean forward, so that the body\nmass can shift towards the feet before a caregiver starts lifting the body. An\nexperienced caregiver guides the older adult through verbal communications and\nphysical interactions, so that the older adult may be cooperative throughout\nthe process. This guidance is of paramount importance and is a major challenge\nin introducing a robotic aid to the eldercare environment. The wide-scope goal\nof the current work is to develop an intelligent eldercare robot that can a)\nmonitor the mental state of an older adult, and b) guide the older adult\nthrough an assisting procedure so that he/she can be cooperative in being\nassisted. The current work presents a basic modeling framework for describing a\nhuman's physical behaviors reflecting an internal mental state, and an\nalgorithm for estimating the mental state through interactive observations. The\nsit-to-stand assistance problem is considered for the initial study. A simple\nKalman Filter is constructed for estimating the level of cooperativeness in\nresponse to applied cues, with a thresholding scheme being used to make\njudgments on the cooperativeness state.", "journal": ""}
{"doi": "10.48550/arXiv.1802.08872", "date": "2018-02-24", "title": "Deep learning for conifer/deciduous classification of airborne LiDAR 3D point clouds representing individual trees", "authors": "Hamid Hamraz, Nathan B. Jacobs, Marco A. Contreras, Chase H. Clark", "abstract": "The purpose of this study was to investigate the use of deep learning for\nconiferous/deciduous classification of individual trees from airborne LiDAR\ndata. To enable efficient processing by a deep convolutional neural network\n(CNN), we designed two discrete representations using leaf-off and leaf-on\nLiDAR data: a digital surface model with four channels (DSMx4) and a set of\nfour 2D views (4x2D). A training dataset of labeled tree crowns was generated\nvia segmentation of tree crowns, followed by co-registration with field data.\nPotential mislabels due to GPS error or tree leaning were corrected using a\nstatistical ensemble filtering procedure. Because the training data was heavily\nunbalanced (~8% conifers), we trained an ensemble of CNNs on random balanced\nsub-samples of augmented data (180 rotational variations per instance). The\n4x2D representation yielded similar classification accuracies to the DSMx4\nrepresentation (~82% coniferous and ~90% deciduous) while converging faster.\nThe data augmentation improved the classification accuracies, but more real\ntraining instances (especially coniferous) likely results in much stronger\nimprovements. Leaf-off LiDAR data were the primary source of useful\ninformation, which is likely due to the perennial nature of coniferous foliage.\nLiDAR intensity values also proved to be useful, but normalization yielded no\nsignificant improvements. Lastly, the classification accuracies of overstory\ntrees (~90%) were more balanced than those of understory trees (~90% deciduous\nand ~65% coniferous), which is likely due to the incomplete capture of\nunderstory tree crowns via airborne LiDAR. Automatic derivation of optimal\nfeatures via deep learning provide the opportunity for remarkable improvements\nin prediction tasks where captured data are not friendly to human visual system\n- likely yielding sub-optimal human-designed features.", "journal": ""}
{"doi": "10.48550/arXiv.1501.03951", "date": "2015-01-16", "title": "On parametric multisummable formal solutions to some nonlinear initial value Cauchy problems", "authors": "Alberto Lastra, Stephane Malek", "abstract": "We study a nonlinear initial value Cauchy problem depending upon a complex\nperturbation parameter $\\epsilon$ whose coefficients depend holomorphically on\n$(\\epsilon,t)$ near the origin in $\\mathbb{C}^{2}$ and are bounded holomorphic\non some horizontal strip in $\\mathbb{C}$ w.r.t the space variable. We consider\na family of forcing terms that are holomorphic on a common sector in time $t$\nand on sectors w.r.t the parameter $\\epsilon$ whose union form a covering of\nsome neighborhood of 0 in $\\mathbb{C}^{\\ast}$, which are asked to share a\ncommon formal power series asymptotic expansion of some Gevrey order as\n$\\epsilon$ tends to 0. The proof leans on a version of the so-called\nRamis-Sibuya theorem which entails two distinct Gevrey orders. Finally, we give\nan application to the study of parametric multi-level Gevrey solutions for some\nnonlinear initial value Cauchy problems with holomorphic coefficients and\nforcing term in $(\\epsilon,t)$ near 0 and bounded holomorphic on a strip in the\ncomplex space variable.", "journal": ""}
{"doi": "10.48550/arXiv.2110.08738", "date": "2021-10-17", "title": "The Game of Arrows on 3-Legged Spider Graphs", "authors": "Bryant G. Mathews", "abstract": "The Game of Cycles is a combinatorial game introduced by Francis Su in 2020\nin which players take turns marking arrows on the edges of a simple plane\ngraph, avoiding the creation of sinks and sources and seeking to complete a\n\"cycle cell.\" Su and his collaborators (2021) found winning strategies on\ngraphs with certain types of symmetry using reverse mirroring.\n  In this paper, we for the first time determine the winning player in the Game\nof Cycles on an infinite family of graphs lacking symmetry. In particular, we\nuse the Sprague-Grundy Theorem to show that player two has a winning strategy\nfor the Game of Cycles on any 3-legged spider graph with legs of odd length.\nBecause the cycle cell victory condition is extraneous for tree graphs\n(including spiders), we drop it from the rules and call the result the Game of\nArrows. Our proof leans heavily on a notion of state isomorphism that allows us\nto decompose a game state into states of smaller pieces of a graph, leading to\nnim-sum calculations with Grundy values.", "journal": ""}
{"doi": "10.48550/arXiv.2308.02606", "date": "2023-08-04", "title": "Improving Human-Object Interaction Detection via Virtual Image Learning", "authors": "Shuman Fang, Shuai Liu, Jie Li, Guannan Jiang, Xianming Lin, Rongrong Ji", "abstract": "Human-Object Interaction (HOI) detection aims to understand the interactions\nbetween humans and objects, which plays a curtail role in high-level semantic\nunderstanding tasks. However, most works pursue designing better architectures\nto learn overall features more efficiently, while ignoring the long-tail nature\nof interaction-object pair categories. In this paper, we propose to alleviate\nthe impact of such an unbalanced distribution via Virtual Image Leaning (VIL).\nFirstly, a novel label-to-image approach, Multiple Steps Image Creation\n(MUSIC), is proposed to create a high-quality dataset that has a consistent\ndistribution with real images. In this stage, virtual images are generated\nbased on prompts with specific characterizations and selected by\nmulti-filtering processes. Secondly, we use both virtual and real images to\ntrain the model with the teacher-student framework. Considering the initial\nlabels of some virtual images are inaccurate and inadequate, we devise an\nAdaptive Matching-and-Filtering (AMF) module to construct pseudo-labels. Our\nmethod is independent of the internal structure of HOI detectors, so it can be\ncombined with off-the-shelf methods by training merely 10 additional epochs.\nWith the assistance of our method, multiple methods obtain significant\nimprovements, and new state-of-the-art results are achieved on two benchmarks.", "journal": ""}
{"doi": "10.48550/arXiv.2203.08913", "date": "2022-03-16", "title": "Memorizing Transformers", "authors": "Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, Christian Szegedy", "abstract": "Language models typically need to be trained or finetuned in order to acquire\nnew knowledge, which involves updating their weights. We instead envision\nlanguage models that can simply read and memorize new data at inference time,\nthus acquiring new knowledge immediately. In this work, we extend language\nmodels with the ability to memorize the internal representations of past\ninputs. We demonstrate that an approximate kNN lookup into a non-differentiable\nmemory of recent (key, value) pairs improves language modeling across various\nbenchmarks and tasks, including generic webtext (C4), math papers (arXiv),\nbooks (PG-19), code (Github), as well as formal theorems (Isabelle). We show\nthat the performance steadily improves when we increase the size of memory up\nto 262K tokens. On benchmarks including code and mathematics, we find that the\nmodel is capable of making use of newly defined functions and theorems during\ntest time.", "journal": ""}
{"doi": "10.48550/arXiv.2501.10127", "date": "2025-01-17", "title": "A Formally Verified IEEE 754 Floating-Point Implementation of Interval Iteration for MDPs", "authors": "Bram Kohlen, Maximilian Sch\u00e4ffeler, Mohammad Abdulaziz, Arnd Hartmanns, Peter Lammich", "abstract": "Reasoning about quantitative properties of Markov Decision Processes (MDPs)\ninevitably requires computations on real or rational numbers. On modern\nhardware, these are usually efficiently implemented by floating-point numbers.\nHowever, due to their finite precision, many floating-point operations lead to\nsmall imprecisions. Probabilistic model checkers claim trustworthiness on the\nground of a solid theoretical basis, yet prior work has uncovered discrepancies\nbetween the claimed and actual accuracy of these systems. How can we trust\nimplementations of model checkers? Our answer is an efficiently executable,\nformally verified implementation of interval iteration for MDPs. Our\ncorrectness proofs span the entire development from the high-level abstract\nsemantics of MDPs to the low-level implementation in LLVM that uses\nfloating-point arithmetic. We use the Isabelle/HOL proof assistant to verify\nthe abstract definition of interval iteration. Next, we employ step-wise\nrefinement to derive an efficient implementation in LLVM code. To that end, we\nextend the Isabelle Refinement Framework with support for reasoning about\nfloating point arithmetic and directed rounding modes. We experimentally\nevaluate our implementation on a set of benchmark MDPs. Our results show that\nthe verified implementation is competitive with state-of-the-art tools for\nMDPs, while providing formal guarantees on the correctness of the results.", "journal": ""}
{"doi": "10.48550/arXiv.2406.11414", "date": "2024-06-17", "title": "Formally Certified Approximate Model Counting", "authors": "Yong Kiam Tan, Jiong Yang, Mate Soos, Magnus O. Myreen, Kuldeep S. Meel", "abstract": "Approximate model counting is the task of approximating the number of\nsolutions to an input Boolean formula. The state-of-the-art approximate model\ncounter for formulas in conjunctive normal form (CNF), ApproxMC, provides a\nscalable means of obtaining model counts with probably approximately correct\n(PAC)-style guarantees. Nevertheless, the validity of ApproxMC's approximation\nrelies on a careful theoretical analysis of its randomized algorithm and the\ncorrectness of its highly optimized implementation, especially the latter's\nstateful interactions with an incremental CNF satisfiability solver capable of\nnatively handling parity (XOR) constraints.\n  We present the first certification framework for approximate model counting\nwith formally verified guarantees on the quality of its output approximation.\nOur approach combines: (i) a static, once-off, formal proof of the algorithm's\nPAC guarantee in the Isabelle/HOL proof assistant; and (ii) dynamic, per-run,\nverification of ApproxMC's calls to an external CNF-XOR solver using proof\ncertificates. We detail our general approach to establish a rigorous connection\nbetween these two parts of the verification, including our blueprint for\nturning the formalized, randomized algorithm into a verified proof checker, and\nour design of proof certificates for both ApproxMC and its internal CNF-XOR\nsolving steps. Experimentally, we show that certificate generation adds little\noverhead to an approximate counter implementation, and that our certificate\nchecker is able to fully certify $84.7\\%$ of instances with generated\ncertificates when given the same time and memory limits as the counter.", "journal": ""}
{"doi": "10.48550/arXiv.2410.14718", "date": "2024-10-13", "title": "Brownian Motion in Isabelle/HOL", "authors": "Christian Pardillo Laursen, Simon Foster, Mark Post", "abstract": "In order to formally verify robotic controllers, we must tackle the inherent\nuncertainty of sensing and actuation in a physical environment. We can model\nuncertainty using stochastic hybrid systems, which combine discrete jumps with\ncontinuous, stochastic behaviour. The verification of these systems is\nintractable for state-exploration based approaches, so we instead propose a\ndeductive verification approach. As a first step towards a deductive\nverification tool, we present a mechanisation of Brownian motion within\nIsabelle/HOL. For this, we mechanise stochastic kernels and Markov semigroups,\nwhich allow us to specify a range of processes with stationary, independent\nincrements. Further, we prove the Kolmogorov-Chentsov theorem, which allows us\nto construct H\\\"older continuous modifications of processes that satisfy\ncertain bounds on their expectation. This paves the way for modelling and\nverifying stochastic hybrid systems in Isabelle/HOL.", "journal": ""}
{"doi": "10.48550/arXiv.1212.3618", "date": "2012-12-14", "title": "Machine Learning in Proof General: Interfacing Interfaces", "authors": "Ekaterina Komendantskaya, J\u00f3nathan Heras, Gudmund Grov", "abstract": "We present ML4PG - a machine learning extension for Proof General. It allows\nusers to gather proof statistics related to shapes of goals, sequences of\napplied tactics, and proof tree structures from the libraries of interactive\nhigher-order proofs written in Coq and SSReflect. The gathered data is\nclustered using the state-of-the-art machine learning algorithms available in\nMATLAB and Weka. ML4PG provides automated interfacing between Proof General and\nMATLAB/Weka. The results of clustering are used by ML4PG to provide proof hints\nin the process of interactive proof development.", "journal": "EPTCS 118, 2013, pp. 15-41"}
{"doi": "10.48550/arXiv.2101.04337", "date": "2021-01-12", "title": "Blind Modulation Classification via Combined Machine Learning and Signal Feature Extraction", "authors": "Jafar Norolahi, Paeiz Azmi", "abstract": "In this study, an algorithm to blind and automatic modulation classification\nhas been proposed. It well benefits combined machine leaning and signal feature\nextraction to recognize diverse range of modulation in low signal power to\nnoise ratio (SNR). The presented algorithm contains four. First, it advantages\nspectrum analyzing to branching modulated signal based on regular and irregular\nspectrum character. Seconds, a nonlinear soft margin support vector (NS SVM)\nproblem is applied to received signal, and its symbols are classified to\ncorrect and incorrect (support vectors) symbols. The NS SVM employment leads to\ndiscounting in physical layer noise effect on modulated signal. After that, a\nk-center clustering can find center of each class. finally, in correlation\nfunction estimation of scatter diagram is correlated with pre-saved ideal\nscatter diagram of modulations. The correlation outcome is classification\nresult. For more evaluation, success rate, performance, and complexity in\ncompare to many published methods are provided. The simulation prove that the\nproposed algorithm can classified the modulated signal in less SNR. For\nexample, it can recognize 4-QAM in SNR=-4.2 dB, and 4-FSK in SNR=2.1 dB with\n%99 success rate. Moreover, due to using of kernel function in dual problem of\nNS SVM and feature base function, the proposed algorithm has low complexity and\nsimple implementation in practical issues.", "journal": ""}
{"doi": "10.48550/arXiv.1904.08430", "date": "2019-04-17", "title": "Dark Matter Strikes Back at the Galactic Center", "authors": "Rebecca K. Leane, Tracy R. Slatyer", "abstract": "Statistical evidence has previously suggested that the Galactic Center GeV\nExcess (GCE) originates largely from point sources, and not from annihilating\ndark matter. We examine the impact of unmodeled source populations on\nidentifying the true origin of the GCE using non-Poissonian template fitting\n(NPTF) methods. In a proof-of-principle example with simulated data, we\ndiscover that unmodeled sources in the Fermi Bubbles can lead to a dark matter\nsignal being misattributed to point sources by the NPTF. We discover striking\nbehavior consistent with a mismodeling effect in the real Fermi data, finding\nthat large artificial injected dark matter signals are completely misattributed\nto point sources. Consequently, we conclude that dark matter may provide a\ndominant contribution to the GCE after all.", "journal": "Phys. Rev. Lett. 123, 241101 (2019)"}
{"doi": "10.48550/arXiv.2207.11965", "date": "2022-07-25", "title": "Machine-checked executable semantics of Stateflow", "authors": "Shicheng Yi, Shuling Wang, Bohua Zhan, Naijun Zhan", "abstract": "Simulink is a widely used model-based development environment for embedded\nsystems. Stateflow is a component of Simulink for modeling event-driven control\nvia hierarchical state machines and flow charts. However, Stateflow lacks an\nofficial formal semantics, making it difficult to formally prove properties of\nits models in safety-critical applications. In this paper, we define a formal\nsemantics for a large subset of Stateflow, covering complex features such as\nhierarchical states and transitions, event broadcasts, early return, temporal\noperators, and so on. The semantics is formalized in Isabelle/HOL and proved to\nbe deterministic. We implement a tactic for automatic execution of the\nsemantics in Isabelle, as well as a translator in Python transforming Stateflow\nmodels to the syntax in Isabelle. Using these tools, we validate the semantics\nagainst a collection of examples illustrating the features we cover.", "journal": ""}
{"doi": "10.48550/arXiv.2112.04570", "date": "2021-12-08", "title": "Formalising Lie algebras", "authors": "Oliver Nash", "abstract": "Lie algebras are an important class of algebras which arise throughout\nmathematics and physics. We report on the formalisation of Lie algebras in\nLean's Mathlib library. Although basic knowledge of Lie theory will benefit the\nreader, none is assumed; the intention is that the overall themes will be\naccessible even to readers unfamiliar with Lie theory.\n  Particular attention is paid to the construction of the classical and\nexceptional Lie algebras. Thanks to these constructions, it is possible to\nstate the classification theorem for finite-dimensional semisimple Lie algebras\nover an algebraically closed field of characteristic zero.\n  In addition to the focus on Lie theory, we also aim to highlight the unity of\nMathlib. To this end, we include examples of achievements made possible only by\nleaning on several branches of the library simultaneously.", "journal": ""}
{"doi": "10.48550/arXiv.1806.03205", "date": "2018-06-08", "title": "Formal Small-step Verification of a Call-by-value Lambda Calculus Machine", "authors": "Fabian Kunze, Gert Smolka, Yannick Forster", "abstract": "We formally verify an abstract machine for a call-by-value lambda-calculus\nwith de Bruijn terms, simple substitution, and small-step semantics. We follow\na stepwise refinement approach starting with a naive stack machine with\nsubstitution. We then refine to a machine with closures, and finally to a\nmachine with a heap providing structure sharing for closures. We prove the\ncorrectness of the three refinement steps with compositional small-step\nbottom-up simulations. There is an accompanying Coq development verifying all\nresults.", "journal": "APLAS 2018, LNCS 11275, pp. 264-283"}
{"doi": "10.48550/arXiv.1811.11911", "date": "2018-11-29", "title": "From C to Interaction Trees: Specifying, Verifying, and Testing a Networked Server", "authors": "Nicolas Koh, Yao Li, Yishuai Li, Li-yao Xia, Lennart Beringer, Wolf Honor\u00e9, William Mansky, Benjamin C. Pierce, Steve Zdancewic", "abstract": "We present the first formal verification of a networked server implemented in\nC. Interaction trees, a general structure for representing reactive\ncomputations, are used to tie together disparate verification and testing tools\n(Coq, VST, and QuickChick) and to axiomatize the behavior of the operating\nsystem on which the server runs (CertiKOS). The main theorem connects a\nspecification of acceptable server behaviors, written in a straightforward \"one\nclient at a time\" style, with the CompCert semantics of the C program. The\nvariability introduced by low-level buffering of messages and interleaving of\nmultiple TCP connections is captured using network refinement, a variant of\nobservational refinement.", "journal": "Proceedings of the 8th ACM SIGPLAN International Conference on\n  Certified Programs and Proofs (CPP '19), January 14--15, 2019, Cascais,\n  Portugal"}
{"doi": "10.48550/arXiv.0409033", "date": "2004-09-06", "title": "Quantum Computation Based Probability Density Function Estimation", "authors": "Ferenc Bal\u00e1zs, S\u00e1ndor Imre", "abstract": "Signal processing techniques will lean on blind methods in the near future,\nwhere no redundant, resource allocating information will be transmitted through\nthe channel. To achieve a proper decision, however, it is essential to know at\nleast the probability density function (pdf), which to estimate is classically\na time consumption and/or less accurate hard task, that may make decisions to\nfail. This paper describes the design of a quantum assisted pdf estimation\nmethod also by an example, which promises to achieve the exact pdf by proper\nsetting of parameters in a very fast way.", "journal": ""}
{"doi": "10.48550/arXiv.1312.6532", "date": "2013-12-23", "title": "Guiding a General-Purpose C Verifier to Prove Cryptographic Protocols", "authors": "Fran\u00e7ois Dupressoir, Andrew D. Gordon, Jan J\u00fcrjens, David A. Naumann", "abstract": "We describe how to verify security properties of C code for cryptographic\nprotocols by using a general-purpose verifier. We prove security theorems in\nthe symbolic model of cryptography. Our techniques include: use of ghost state\nto attach formal algebraic terms to concrete byte arrays and to detect\ncollisions when two distinct terms map to the same byte array; decoration of a\ncrypto API with contracts based on symbolic terms; and expression of the\nattacker model in terms of C programs. We rely on the general-purpose verifier\nVCC; we guide VCC to prove security simply by writing suitable header files and\nannotations in implementation files, rather than by changing VCC itself. We\nformalize the symbolic model in Coq in order to justify the addition of axioms\nto VCC.", "journal": ""}
{"doi": "10.48550/arXiv.1503.00244", "date": "2015-03-01", "title": "23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and Management", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "abstract": "The global influence of Big Data is not only growing but seemingly endless.\nThe trend is leaning towards knowledge that is attained easily and quickly from\nmassive pools of Big Data. Today we are living in the technological world that\nDr. Usama Fayyad and his distinguished research fellows discussed in the\nintroductory explanations of Knowledge Discovery in Databases (KDD) predicted\nnearly two decades ago. Indeed, they were precise in their outlook on Big Data\nanalytics. In fact, the continued improvement of the interoperability of\nmachine learning, statistics, database building and querying fused to create\nthis increasingly popular science- Data Mining and Knowledge Discovery. The\nnext generation computational theories are geared towards helping to extract\ninsightful knowledge from even larger volumes of data at higher rates of speed.\nAs the trend increases in popularity, the need for a highly adaptive solution\nfor knowledge discovery will be necessary. In this research paper, we are\nintroducing the investigation and development of 23 bit-questions for a\nMetaknowledge template for Big Data Processing and clustering purposes. This\nresearch aims to demonstrate the construction of this methodology and proves\nthe validity and the beneficial utilization that brings Knowledge Discovery\nfrom Big Data.", "journal": ""}
{"doi": "10.48550/arXiv.2407.04116", "date": "2024-07-04", "title": "Ultraproducts in abstract categorical logic", "authors": "Marc Aiguier, Isabelle Bloch, Romain Pascual", "abstract": "In a previous publication, we introduced an abstract logic via an abstract\nnotion of quantifier. Drawing upon concepts from categorical logic, this\nabstract logic interprets formulas from context as subobjects in a specific\ncategory, e.g., Cartesian, regular, or coherent categories, Grothendieck, or\nelementary toposes. We proposed an entailment system formulated as a sequent\ncalculus which we proved complete. Building on this foundation, our current\nwork explores model theory within abstract logic. More precisely, we generalize\none of the most important and powerful classical model theory methods, namely\nthe ultraproduct method, and show its fundamental theorem, i.e., Los's theorem.\nThe result is shown as independently as possible of a given quantifier.", "journal": ""}
{"doi": "10.48550/arXiv.1703.09988", "date": "2017-03-29", "title": "Modular, Fully-abstract Compilation by Approximate Back-translation", "authors": "Dominique Devriese, Marco Patrignani, Frank Piessens, Steven Keuchel", "abstract": "A compiler is fully-abstract if the compilation from source language programs\nto target language programs reflects and preserves behavioural equivalence.\nSuch compilers have important security benefits, as they limit the power of an\nattacker interacting with the program in the target language to that of an\nattacker interacting with the program in the source language. Proving compiler\nfull-abstraction is, however, rather complicated. A common proof technique is\nbased on the back-translation of target-level program contexts to\nbehaviourally-equivalent source-level contexts. However, constructing such a\nback- translation is problematic when the source language is not strong enough\nto embed an encoding of the target language. For instance, when compiling from\nSTLC to ULC, the lack of recursive types in the former prevents such a\nback-translation.\n  We propose a general and elegant solution for this problem. The key insight\nis that it suffices to construct an approximate back-translation. The\napproximation is only accurate up to a certain number of steps and conservative\nbeyond that, in the sense that the context generated by the back-translation\nmay diverge when the original would not, but not vice versa. Based on this\ninsight, we describe a general technique for proving compiler full-abstraction\nand demonstrate it on a compiler from STLC to ULC. The proof uses asymmetric\ncross-language logical relations and makes innovative use of step-indexing to\nexpress the relation between a context and its approximate back-translation.\nThe proof extends easily to common compiler patterns such as modular\ncompilation and it, to the best of our knowledge, it is the first compiler full\nabstraction proof to have been fully mechanised in Coq. We believe this proof\ntechnique can scale to challenging settings and enable simpler, more scalable\nproofs of compiler full-abstraction.", "journal": "Logical Methods in Computer Science, Volume 13, Issue 4 (October\n  25, 2017) lmcs:3230"}
{"doi": "10.48550/arXiv.1911.05374", "date": "2019-11-13", "title": "Utilization of e-Nose Sensory Modality as Add-On Feature for Advanced Driver Assistance System", "authors": "Ira C. Valenzuela, Lean Karlo S. Tolentino, Ronnie O. Serfa Juan", "abstract": "The frequent usage of sensory modalities for Advanced Driver Assistance\nSystem (ADAS) and in some In-Vehicles Information System (IVIS) are only for\nvisuals and hearing applications. Yet, the air quality inside the car is not\nusually monitored. Not to mention, the existing breath analyzers are used only\nby road traffic police enforcer randomly in examining the alcohol content in\nmotorist's breath to avoid drunk driving related accidents. In this study, we\nproposed the development and utilization of an e-nose sensory module as an\nadded feature for Controller Area Network (CAN) technology application. An\nelectronic nose is characterized by its capability to intelligently sensed the\ngases present in the surrounding using an array of gas sensors with a pattern\nreorganization component. This e-nose comprises of a sensor array of five\ncommercially available sensors (TG 2602, TG 822, TG 825, TG 813 and TG 880), a\ndata acquisition interface, and a microprocessor. A software from the CAN\nmodule verified the system functionality. Experimental results indicate that\nthe proposed system is able to identify the five different types of gases\nnamely methane, ethanol, propone, isobutane, and hydrogen with high efficiency.\nThus, it can be used as an added safety feature for vehicles.", "journal": "International Journal of Advanced Trends in Computer Science and\n  Engineering (2019) 1783-1788"}
{"doi": "10.48550/arXiv.2301.04523", "date": "2023-01-11", "title": "Deep learning-assisted active metamaterials with heat-enhanced thermal transport", "authors": "Peng Jin, Liujun Xu, Guoqiang Xu, Jiaxin Li, Cheng-Wei Qiu, Jiping Huang", "abstract": "Heat management is crucial for state-of-the-art applications such as passive\nradiative cooling, thermally adjustable wearables, and camouflage systems.\nTheir adaptive versions, to cater to varied requirements, lean on the potential\nof adaptive metamaterials. Existing efforts, however, feature with highly\nanisotropic parameters, narrow working-temperature ranges, and the need for\nmanual intervention, which remain long-term and tricky obstacles for the most\nadvanced self-adaptive metamaterials. To surmount these barriers, we introduce\nheat-enhanced thermal diffusion metamaterials powered by deep learning. Such\nactive metamaterials can automatically sense ambient temperatures and swiftly,\nas well as continuously, adjust their thermal functions with a high degree of\ntunability. They maintain robust thermal performance even when external thermal\nfields change direction, and both simulations and experiments demonstrate\nexceptional results. Furthermore, we design two metadevices with on-demand\nadaptability, performing distinctive features with isotropic materials, wide\nworking temperatures, and spontaneous response. This work offers a framework\nfor the design of intelligent thermal diffusion metamaterials and can be\nexpanded to other diffusion fields, adapting to increasingly complex and\ndynamic environments.", "journal": ""}
{"doi": "10.48550/arXiv.2305.02521", "date": "2023-05-04", "title": "Towards a Scalable Proof Engine: A Performant Prototype Rewriting Primitive for Coq", "authors": "Jason Gross, Andres Erbsen, Jade Philipoom, Rajashree Agrawal, Adam Chlipala", "abstract": "We address the challenges of scaling verification efforts to match the\nincreasing complexity and size of systems. We propose a research agenda aimed\nat building a performant proof engine by studying the asymptotic performance of\nproof engines and redesigning their building blocks. As a case study, we\nexplore equational rewriting and introduce a novel prototype proof engine\nbuilding block for rewriting in Coq, utilizing proof by reflection for enhanced\nperformance.\n  Our prototype implementation can significantly improve the development of\nverified compilers, as demonstrated in a case study with the Fiat Cryptography\ntoolchain. The resulting extracted command-line compiler is about 1000$\\times$\nfaster while featuring simpler compiler-specific proofs. This work lays some\nfoundation for scaling verification efforts and contributes to the broader goal\nof developing a proof engine with good asymptotic performance, ultimately aimed\nat enabling the verification of larger and more complex systems.", "journal": "J Autom Reasoning 68, 19 (2024)"}
{"doi": "10.48550/arXiv.2310.01530", "date": "2023-10-02", "title": "A Pretty Expressive Printer (with Appendices)", "authors": "Sorawee Porncharoenwase, Justin Pombrio, Emina Torlak", "abstract": "Pretty printers make trade-offs between the expressiveness of their pretty\nprinting language, the optimality objective that they minimize when choosing\nbetween different ways to lay out a document, and the performance of their\nalgorithm. This paper presents a new pretty printer, $\\Pi_e$, that is strictly\nmore expressive than all pretty printers in the literature and provably\nminimizes an optimality objective. Furthermore, the time complexity of $\\Pi_e$\nis better than many existing pretty printers. When choosing among different\nways to lay out a document, $\\Pi_e$ consults a user-supplied cost factory,\nwhich determines the optimality objective, giving $\\Pi_e$ a unique degree of\nflexibility. We use the Lean theorem prover to verify the correctness (validity\nand optimality) of $\\Pi_e$, and implement $\\Pi_e$ concretely as a pretty\nprinter that we call PrettyExpressive. To evaluate our pretty printer against\nothers, we develop a formal framework for reasoning about the expressiveness of\npretty printing languages, and survey pretty printers in the literature,\ncomparing their expressiveness, optimality, worst-case time complexity, and\npractical running time. Our evaluation shows that PrettyExpressive is efficient\nand effective at producing optimal layouts. PrettyExpressive has also seen\nreal-world adoption: it serves as a foundation of a code formatter for Racket.", "journal": ""}
{"doi": "10.48550/arXiv.2407.19211", "date": "2024-07-27", "title": "A Construction of the Lie Algebra of a Lie Group in Isabelle/HOL", "authors": "Richard Schmoetten, Jacques D. Fleuriot", "abstract": "This paper describes a formal theory of smooth vector fields, Lie groups and\nthe Lie algebra of a Lie group in the theorem prover Isabelle. Lie groups are\nabstract structures that are composable, invertible and differentiable. They\nare pervasive as models of continuous transformations and symmetries in areas\nfrom theoretical particle physics, where they underpin gauge theories such as\nthe Standard Model, to the study of differential equations and robotics.\nFormalisation of mathematics in an interactive theorem prover, such as\nIsabelle, provides strong correctness guarantees by expressing definitions and\ntheorems in a logic that can be checked by a computer. Many libraries of\nformalised mathematics lack significant development of textbook material beyond\nundergraduate level, and this contribution to mathematics in Isabelle aims to\nreduce that gap, particularly in differential geometry. We comment on\nrepresentational choices and challenges faced when integrating complex\nformalisations, such as smoothness of vector fields, with the restrictions of\nthe simple type theory of HOL. This contribution paves the way for extensions\nboth in advanced mathematics, and in formalisations in natural science.", "journal": ""}
{"doi": "10.48550/arXiv.2005.01018", "date": "2020-05-03", "title": "Certified Semantics for Relational Programming", "authors": "Dmitry Rozplokhas, Andrey Vyatkin, Dmitry Boulytchev", "abstract": "We present a formal study of semantics for the relational programming\nlanguage miniKanren. First, we formulate a denotational semantics which\ncorresponds to the minimal Herbrand model for definite logic programs. Second,\nwe present operational semantics which models interleaving, the distinctive\nfeature of miniKanren implementation, and prove its soundness and completeness\nw.r.t. the denotational semantics. Our development is supported by a Coq\nspecification, from which a reference interpreter can be extracted. We also\nderive from our main result a certified semantics (and a reference interpreter)\nfor SLD resolution with cut and prove its soundness.", "journal": ""}
{"doi": "10.48550/arXiv.1904.02809", "date": "2019-04-04", "title": "Proving tree algorithms for succinct data structures", "authors": "Reynald Affeldt, Jacques Garrigue, Xuanrui Qi, Kazunari Tanaka", "abstract": "Succinct data structures give space-efficient representations of large\namounts of data without sacrificing performance. They rely one cleverly\ndesigned data representations and algorithms. We present here the formalization\nin Coq/SSReflect of two different tree-based succinct representations and their\naccompanying algorithms. One is the Level-Order Unary Degree Sequence, which\nencodes the structure of a tree in breadth-first order as a sequence of bits,\nwhere access operations can be defined in terms of Rank and Select, which work\nin constant time for static bit sequences. The other represents dynamic bit\nsequences as binary balanced trees, where Rank and Select present a low\nlogarithmic overhead compared to their static versions, and with efficient\ninsertion and deletion. The two can be stacked to provide a dynamic\nrepresentation of dictionaries for instance. While both representations are\nwell-known, we believe this to be their first formalization and a needed step\ntowards provably-safe implementations of big data.", "journal": ""}
{"doi": "10.48550/arXiv.1810.07855", "date": "2018-10-18", "title": "An Event-based Compositional Reasoning Approach for Concurrent Reactive Systems", "authors": "Yongwang Zhao, David Sanan, Fuyuan Zhang, Yang Liu", "abstract": "Reactive systems are composed of a well defined set of input events that the\nsystem reacts with by executing an associated handler to each event. In\nconcurrent environments, event handlers can interact with the execution of\nother programs such as hardware interruptions in preemptive systems, or other\ninstances of the reactive system in multicore architectures. State of the art\nrely-guarantee based verification frameworks only focus on imperative programs,\nbeing difficult to capture in the rely and guarantee relations interactions\nwith possible infinite sequences of event handlers, and the input arguments to\nevent handlers. In this paper, we propose the formalisation in Isabelle/HOL of\nan event-based rely-guarantee approach for concurrent reactive systems. We\ndevelop a Pi-Core language which incorporates a concurrent imperative and\nsystem specification language by `events', and we build a rely-guarantee proof\nsystem for Pi-Core and prove its soundness. Our approach can deal with\nmulticore and interruptible concurrency. We use two case studies to show this:\nan interruptible controller for stepper motors and an ARINC 653 multicore\nkernel, and prove the functional correctness and preservation of invariants of\nthem in Isabelle/HOL.", "journal": ""}
{"doi": "10.48550/arXiv.2210.11240", "date": "2022-10-07", "title": "Strong Normalization for the Calculus of Constructions", "authors": "Chris Casinghino", "abstract": "The calculus of constructions (CC) is a core theory for dependently typed\nprogramming and higher-order constructive logic. Originally introduced in\nCoquand's 1985 thesis, CC has inspired 25 years of research in programming\nlanguages and type theory. Today, extensions of CC form the basis of languages\nlike Coq and Agda. This survey reviews three proofs of CC's strong\nnormalization property (the fact that there are no infinite reduction sequences\nfrom well-typed expressions). It highlights the similarities in the structure\nof the proofs while showing how their differences are motivated by the varying\ngoals of their authors.", "journal": ""}
{"doi": "10.48550/arXiv.2306.15539", "date": "2023-06-27", "title": "Numerical study of nitrogen oxides chemistry during plasma assisted combustion in a sequential combustor", "authors": "Quentin Mal\u00e9, Nicolas Barl\u00e9on, Sergey Shcherbanev, Bayu Dharmaputra, Nicolas Noiray", "abstract": "Plasma Assisted Combustion (PAC) is a promising technology to enhance the\ncombustion of lean mixtures prone to instabilities and flame blow-off. Although\nmany PAC experiments demonstrated combustion enhancement, several studies\nreport an increase in NOx emissions. The aim of this study is to determine the\nkinetic pathways leading to NOx formation in the second stage of a sequential\ncombustor assisted by Nanosecond Repetitively Pulsed Discharges (NRPDs). For\nthis purpose, Large Eddy Simulation (LES) associated with an accurate\ndescription of the combustion/NOx chemistry and a phenomenological model of the\nplasma kinetics is used. Detailed kinetics 0-Dimensional reactors complement\nthe study. First, the LES setup is validated by comparison with experiments.\nThen, the NOx chemistry is analyzed. For the conditions of operation studied,\nit is shown that the production of atomic nitrogen in the plasma by direct\nelectron impact on nitrogen molecules increases the formation of NO. Then, the\nNO molecules are transported through the turbulent flame without being strongly\naffected. This study illustrates the need to limit the diatomic nitrogen\ndissociation process in order to mitigate harmful emissions. More generally,\nthe very good agreement with experimental measurements demonstrates the\ncapability of LES combined with accurate models to predict the NRPD effects on\nboth turbulent combustion and NOx emissions.", "journal": "Numerical study of nitrogen oxides chemistry during plasma\n  assisted combustion in a sequential combustor, Combustion and Flame Volume\n  260, February 2024, 113206"}
{"doi": "10.48550/arXiv.1610.05986", "date": "2016-10-19", "title": "Natural lifts of Dorfman brackets", "authors": "Madeleine Jotz Lean, Charlotte Kirchhoff-Lukat", "abstract": "In this note we prove that, for a vector bundle $E$ over a manifold $M$, a\nDorfman bracket on $TM\\oplus E^*$ anchored by $\\operatorname{pr}_{TM}$ and with\n$E$ a vector bundle over $M$, is equivalent to a lift from $\\Gamma(TM\\oplus\nE^*)$ to linear sections of $TE\\oplus T^*E\\to E$, that intertwines the given\nDorfman bracket with the Courant-Dorfman bracket on sections of $TE\\oplus\nT^*E$. This shows a universality of the Courant-Dorfman bracket, and allows us\nto caracterise twistings and symmetries of transitive Dorfman brackets via the\ncorresponding lifts.", "journal": ""}
{"doi": "10.48550/arXiv.1712.07035", "date": "2017-12-19", "title": "Lie 2-algebroids and matched pairs of 2-representations - a geometric approach", "authors": "Madeleine Jotz Lean", "abstract": "Li-Bland's correspondence between linear Courant algebroids and Lie\n$2$-algebroids is explained and shown to be an equivalence of categories.\nDecomposed VB-Courant algebroids are shown to be equivalent to split Lie\n2-algebroids in the same manner as decomposed VB-algebroids are equivalent to\n2-term representations up to homotopy (Gracia-Saz and Mehta). Several classes\nof examples are discussed, yielding new examples of split Lie 2-algebroids. We\nprove that the bicrossproduct of a matched pair of $2$-representations is a\nsplit Lie $2$-algebroid and we explain this result geometrically, as a\nconsequence of the equivalence of VB-Courant algebroids and Lie $2$-algebroids.\nThis explains in particular how the two notions of double\" of a matched pair of\nrepresentations are geometrically related. In the same manner, we explain the\ngeometric link between the two notions of double of a Lie bialgebroid.", "journal": "Pacific J. Math. 301 (2019) 143-188"}
{"doi": "10.48550/arXiv.1808.00863", "date": "2018-08-02", "title": "A Menger-like property of tree-cut width", "authors": "Archontia C. Giannopoulou, O-joung Kwon, Jean-Florent Raymond, Dimitrios M. Thilikos", "abstract": "In 1990, Thomas proved that every graph admits a tree decomposition of\nminimum width that additionally satisfies a certain vertex-connectivity\ncondition called leanness [A Menger-like property of tree-width: The finite\ncase. Journal of Combinatorial Theory, Series B, 48(1):67-76, 1990]. This\nresult had many uses and has been extended to several other decompositions.\n  In this paper, we consider tree-cut decompositions, that have been introduced\nby Wollan as a possible edge-version of tree decompositions [The structure of\ngraphs not admitting a fixed immersion. Journal of Combinatorial Theory, Series\nB, 110:47-66, 2015]. We show that every graph admits a tree-cut decomposition\nof minimum width that additionally satisfies an edge-connectivity condition\nanalogous to Thomas' leanness.", "journal": ""}
{"doi": "10.48550/arXiv.1811.04842", "date": "2018-11-12", "title": "On LA-Courant algebroids and Poisson Lie 2-algebroids", "authors": "Madeleine Jotz Lean", "abstract": "This paper provides an alternative, much simpler, definition for Li-Bland's\nLA-Courant algebroids, or Poisson Lie 2-algebroids, in terms of split Lie\n2-algebroids and self-dual 2-representations. This definition generalises in a\nprecise sense the characterisation of (decomposed) double Lie algebroids via\nmatched pairs of 2-representations. We use the known geometric examples of\nLA-Courant algebroids in order to provide new examples of Poisson Lie\n2-algebroids, and we explain in this general context Roytenberg's equivalence\nof Courant algebroids with symplectic Lie 2-algebroids. We study further the\ncore of an LA-Courant algebroid and we prove that it carries an induced\ndegenerate Courant algebroid structure. In the nondegenerate case, this gives a\nnew construction of a Courant algebroid from the corresponding symplectic Lie\n2-algebroid. Finally we completely characterise VB-Dirac and LA-Dirac\nstructures via simpler objects, that we compare to Li-Bland's pseudo-Dirac\nstructures.", "journal": ""}
{"doi": "10.48550/arXiv.2103.13189", "date": "2021-03-24", "title": "Transitive double Lie algebroids via core diagrams", "authors": "Madeleine Jotz Lean, Kirill Mackenzie", "abstract": "The core diagram of a double Lie algebroid consists in the core of the double\nLie algebroid, together with the two core-anchor maps to the sides of the\ndouble Lie algebroid. If these two core anchors are surjective, then the double\nLie algebroid and its core diagram are called transitive. This paper\nestablishes an equivalence between transitive double Lie algebroids, and\ntransitive core diagrams over a fixed base manifold. In other words, it proves\nthat a transitive double Lie algebroid is completely determined by its core\ndiagram. The comma double Lie algebroid associated to a morphism of Lie\nalgebroids is defined. If the latter morphism is one of the core-anchors of a\ntransitive core diagram, then the comma double algebroid can be quotiented out\nby the second core-anchor, yielding a transitive double Lie algebroid, which is\nthe one that is equivalent to the transitive core diagram. Brown's and\nMackenzie's equivalence of transitive core diagrams (of Lie groupoids) with\ntransitive double Lie groupoids is then used in order to show that a transitive\ndouble Lie algebroid with integrable sides and core is automatically integrable\nto a transitive double Lie groupoid. This research was a joint project with the\nsadly deceased second author. This paper is dedicated to his memory.", "journal": ""}
{"doi": "10.48550/arXiv.1912.07339", "date": "2019-12-16", "title": "Synthetic topology in Homotopy Type Theory for probabilistic programming", "authors": "Martin E. Bidlingmaier, Florian Faissole, Bas Spitters", "abstract": "The ALEA Coq library formalizes measure theory based on a variant of the Giry\nmonad on the category of sets. This enables the interpretation of a\nprobabilistic programming language with primitives for sampling from discrete\ndistributions. However, continuous distributions have to be discretized because\nthe corresponding measures cannot be defined on all subsets of their carriers.\n  This paper proposes the use of synthetic topology to model continuous\ndistributions for probabilistic computations in type theory. We study the\ninitial $\\sigma$-frame and the corresponding induced topology on arbitrary\nsets. Based on these intrinsic topologies we define valuations and lower\nintegrals on sets, and prove versions of the Riesz and Fubini theorems. We then\nshow how the Lebesgue valuation, and hence continuous distributions, can be\nconstructed.", "journal": "Mathematical Structures in Computer Science, 1-29, 2021"}
{"doi": "10.48550/arXiv.1007.2885", "date": "2010-07-16", "title": "Multi-Level Languages are Generalized Arrows", "authors": "Adam Megacz", "abstract": "Multi-level languages and Arrows both facilitate metaprogramming, the act of\nwriting a program which generates a program. The arr function required of all\nArrows turns arbitrary host language expressions into guest language\nexpressions; because of this, Arrows may be used for metaprogramming only when\nthe guest language is a superset of the host language. This restriction is also\npresent in multi-level languages which offer unlimited cross-level persistence.\n<p> This paper introduces generalized arrows and proves that they generalize\nArrows in the following sense: every Arrow in a programming language arises\nfrom a generalized arrow with that language's term category as its codomain.\nGeneralized arrows impose no containment relationship between the guest\nlanguage and host language; they facilitate heterogeneous metaprogramming. The\ncategory having all generalized arrows as its morphisms and the category having\nall multi-level languages as its morphisms are isomorphic categories. This is\nproven formally in Coq, and the proof is offered as justification for the\nassertion that multi-level languages are generalized arrows. <p> Combined with\nthe existence of a particular kind of retraction in the host language, this\nproof can be used to define an invertible translation from two-level terms to\none-level terms parameterized by a generalized arrow instance. This is\nergonomically significant: it lets guest language providers write generalized\narrow instances while the users of those guest languages write multi-level\nterms. This is beneficial because implementing a generalized arrow instance is\neasier than modifying a compiler, whereas writing two-level terms is easier\nthan manipulating generalized arrow terms.", "journal": ""}
{"doi": "10.48550/arXiv.2010.02600", "date": "2020-10-06", "title": "Converting the Point of View of Messages Spoken to Virtual Assistants", "authors": "Isabelle G. Lee, Vera Zu, Sai Srujana Buddi, Dennis Liang, Purva Kulkarni, Jack G. M. Fitzgerald", "abstract": "Virtual Assistants can be quite literal at times. If the user says \"tell Bob\nI love him,\" most virtual assistants will extract the message \"I love him\" and\nsend it to the user's contact named Bob, rather than properly converting the\nmessage to \"I love you.\" We designed a system to allow virtual assistants to\ntake a voice message from one user, convert the point of view of the message,\nand then deliver the result to its target user. We developed a rule-based\nmodel, which integrates a linear text classification model, part-of-speech\ntagging, and constituency parsing with rule-based transformation methods. We\nalso investigated Neural Machine Translation (NMT) approaches, including LSTMs,\nCopyNet, and T5. We explored 5 metrics to gauge both naturalness and\nfaithfulness automatically, and we chose to use BLEU plus METEOR for\nfaithfulness and relative perplexity using a separately trained language model\n(GPT) for naturalness. Transformer-Copynet and T5 performed similarly on\nfaithfulness metrics, with T5 achieving slight edge, a BLEU score of 63.8 and a\nMETEOR score of 83.0. CopyNet was the most natural, with a relative perplexity\nof 1.59. CopyNet also has 37 times fewer parameters than T5. We have publicly\nreleased our dataset, which is composed of 46,565 crowd-sourced samples.", "journal": ""}
{"doi": "10.48550/arXiv.2201.04919", "date": "2022-01-13", "title": "Translation Certification for Smart Contracts", "authors": "Jacco O. G. Krijnen, Manuel M. T. Chakravarty, Gabriele Keller, Wouter Swierstra", "abstract": "Compiler correctness is an old problem, but with the emergence of smart\ncontracts on blockchains that problem presents itself in a new light. Smart\ncontracts are self-contained pieces of software that control assets, which are\noften of high financial value, in an adversarial environment and, once\ncommitted to the blockchain, they cannot be changed anymore. Smart contracts\nare typically developed in a high-level contract language and compiled to\nlow-level virtual machine code before being committed to the blockchain. For a\nsmart contract user to trust a given piece of low-level code on the blockchain,\nthey must convince themselves that (a) they are in possession of the matching\nsource code and (b) that the compiler faithfully translated the source code's\nsemantics.\n  Classic approaches to compiler correctness tackle the second point. We argue\nthat translation certification also addresses the first. We describe the proof\narchitecture of a novel translation certification framework, implemented in\nCoq, for a functional smart contract language. We demonstrate that we can model\nthe compilation pipeline as a sequence of translation relations that facilitate\na modular proof approach and are robust in the face of an evolving compiler\nimplementation.", "journal": ""}
{"doi": "10.48550/arXiv.1503.05496", "date": "2015-03-18", "title": "IMP with exceptions over decorated logic", "authors": "Burak Ekici", "abstract": "In this paper, we facilitate the reasoning about impure programming\nlanguages, by annotating terms with `decorations' that describe what\ncomputational (side) effect evaluation of a term may involve. In a point-free\ncategorical language,called the `decorated logic', we formalize the mutable\nstate and the exception effects first separately, exploiting anice duality\nbetween them, and then combined. The combined decorated logic is used as the\ntarget language forthe denotational semantics of the IMP+Exc imperative\nprogramming language, and allows us to prove equivalencesbetween programs\nwritten in IMP+Exc. The combined logic is encoded in Coq, and this encoding is\nused to certifysome program equivalence proofs.", "journal": ""}
{"doi": "10.48550/arXiv.2209.08205", "date": "2022-09-17", "title": "Necessity Specifications for Robustness", "authors": "Julian Mackay, Sophia Drossopoulou, James Noble, Susan Eisenbach", "abstract": "Robust modules guarantee to do only what they are supposed to do - even in\nthe presence of untrusted, malicious clients, and considering not just the\ndirect behaviour of individual methods, but also the emergent behaviour from\ncalls to more than one method. Necessity is a language for specifying\nrobustness, based on novel necessity operators capturing temporal implication,\nand a proof logic that derives explicit robustness specifications from\nfunctional specifications. Soundness and an exemplar proof are mechanised in\nCoq.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12848", "date": "2025-02-18", "title": "Strands Rocq: Why is a Security Protocol Correct, Mechanically?", "authors": "Matteo Busi, Riccardo Focardi, Flaminia L. Luccio", "abstract": "Strand spaces are a formal framework for symbolic protocol verification that\nallows for pen-and-paper proofs of security. While extremely insightful,\npen-and-paper proofs are error-prone, and it is hard to gain confidence on\ntheir correctness. To overcome this problem, we developed StrandsRocq, a full\nmechanization of the strand spaces in Coq (soon to be renamed Rocq). The\nmechanization was designed to be faithful to the original pen-and-paper\ndevelopment, and it was engineered to be modular and extensible. StrandsRocq\nincorporates new original proof techniques, a novel notion of maximal\npenetrator that enables protocol compositionality, and a set of Coq tactics\ntailored to the domain, facilitating proof automation and reuse, and\nsimplifying the work of protocol analysts. To demonstrate the versatility of\nour approach, we modelled and analyzed a family of authentication protocols,\ndrawing inspiration from ISO/IEC 9798-2 two-pass authentication, the classical\nNeedham-Schroeder-Lowe protocol, as well as a recently-proposed static analysis\nfor a key management API. The analyses in StrandsRocq confirmed the high degree\nof proof reuse, and enabled us to distill the minimal requirements for protocol\nsecurity. Through mechanization, we identified and addressed several issues in\nthe original proofs and we were able to significantly improve the precision of\nthe static analysis for the key management API. Moreover, we were able to\nleverage the novel notion of maximal penetrator to provide a compositional\nproof of security for two simple authentication protocols.", "journal": ""}
{"doi": "10.48550/arXiv.2009.10664", "date": "2020-09-22", "title": "A Formally Verified Protocol for Log Replication with Byzantine Fault Tolerance", "authors": "Joel Wanner, Laurent Chuat, Adrian Perrig", "abstract": "Byzantine fault tolerant protocols enable state replication in the presence\nof crashed, malfunctioning, or actively malicious processes. Designing such\nprotocols without the assistance of verification tools, however, is remarkably\nerror-prone. In an adversarial environment, performance and flexibility come at\nthe cost of complexity, making the verification of existing protocols extremely\ndifficult. We take a different approach and propose a formally verified\nconsensus protocol designed for a specific use case: secure logging. Our\nprotocol allows each node to propose entries in a parallel subroutine, and\nguarantees that correct nodes agree on the set of all proposed entries, without\nleader election. It is simple yet practical, as it can accommodate the workload\nof a logging system such as Certificate Transparency. We show that it is\noptimal in terms of both required rounds and tolerable faults. Using\nIsabelle/HOL, we provide a fully machine-checked security proof based upon the\nHeard-Of model, which we extend to support signatures. We also present and\nevaluate a prototype implementation.", "journal": ""}
{"doi": "10.48550/arXiv.2502.14503", "date": "2025-02-20", "title": "LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of 4D Radar and Camera", "authors": "Weiyi Xiong, Zean Zou, Qiuchi Zhao, Fengchun He, Bing Zhu", "abstract": "As the previous state-of-the-art 4D radar-camera fusion-based 3D object\ndetection method, LXL utilizes the predicted image depth distribution maps and\nradar 3D occupancy grids to assist the sampling-based image view\ntransformation. However, the depth prediction lacks accuracy and consistency,\nand the concatenation-based fusion in LXL impedes the model robustness. In this\nwork, we propose LXLv2, where modifications are made to overcome the\nlimitations and improve the performance. Specifically, considering the position\nerror in radar measurements, we devise a one-to-many depth supervision strategy\nvia radar points, where the radar cross section (RCS) value is further\nexploited to adjust the supervision area for object-level depth consistency.\nAdditionally, a channel and spatial attention-based fusion module named\nCSAFusion is introduced to improve feature adaptiveness. Experimental results\non the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can\noutperform LXL in detection accuracy, inference speed and robustness,\ndemonstrating the effectiveness of the model.", "journal": ""}
{"doi": "10.48550/arXiv.2202.05587", "date": "2022-02-11", "title": "Formalization of Asymptotic Convergence for Stationary Iterative Methods", "authors": "Mohit Tekriwal, Joshua Miller, Jean-Baptiste Jeannin", "abstract": "Solutions to differential equations, which are used to model physical\nsystems, are computed numerically by solving a set of discretized equations.\nThis set of discretized equations is reduced to a large linear system, whose\nsolution is typically found using an iterative solver. We start with an initial\nguess, $x_0$, and iterate the algorithm to obtain a sequence of solution\nvectors, $x_k$, which are approximations to the exact solution of the linear\nsystem, $x$. The iterative algorithm is said to converge to $x$, in the field\nof reals, if and only if $x_k$ converges to $x$ in the limit of $k \\to \\infty$.\n  In this paper, we formally prove the asymptotic convergence of a particular\nclass of iterative methods called the stationary iterative methods, in the Coq\ntheorem prover. We formalize the necessary and sufficient conditions required\nfor the iterative convergence, and extend this result to two classical\niterative methods: the Gauss--Seidel method and the Jacobi method. For the\nGauss--Seidel method, we also formalize a set of easily testable conditions for\niterative convergence, called the Reich theorem, for a particular matrix\nstructure, and apply this on a model problem of the one-dimensional heat\nequation. We also apply the main theorem of iterative convergence to prove\nconvergence of the Jacobi method on the model problem.", "journal": ""}
{"doi": "10.48550/arXiv.2405.06753", "date": "2024-05-10", "title": "Linked tree-decompositions into finite parts", "authors": "Sandra Albrechtsen, Raphael W. Jacobs, Paul Knappe, Max Pitz", "abstract": "We prove that every graph which admits a tree-decomposition into finite parts\nhas a rooted tree-decomposition into finite parts that is linked, tight and\ncomponental.\n  As an application, we obtain that every graph without half-grid minor has a\nlean tree-decomposition into finite parts, strengthening the corresponding\nresult by Kriz and Thomas for graphs of finitely bounded tree-width. In\nparticular, it follows that every graph without half-grid minor has a\ntree-decomposition which efficiently distinguishes all ends and critical vertex\nsets, strengthening results by Carmesin and by Elm and Kurkofka for this graph\nclass.\n  As a second application of our main result, it follows that every graph which\nadmits a tree-decomposition into finite parts has a tree-decomposition into\nfinite parts that displays all the ends of $G$ and their combined degrees,\nresolving a question of Halin from 1977. This latter tree-decomposition yields\nshort, unified proofs of the characterisations due to Robertson, Seymour and\nThomas of graphs without half-grid minor, and of graphs without binary tree\nsubdivision.", "journal": ""}
{"doi": "10.48550/arXiv.2501.15060", "date": "2025-01-25", "title": "Weak solutions to a compressible viscous non-resistive MHD equations with general boundary data", "authors": "Yang Li, Young-Sam Kwon, Yongzhong Sun", "abstract": "This paper is concerned with a compressible MHD equations describing the\nevolution of viscous non-resistive fluids in piecewise regular bounded\nLipschitz domains. Under the general inflow-outflow boundary conditions, we\nprove existence of global-in-time weak solutions with finite energy initial\ndata. The present result extends considerably the previous work by Li and Sun\n[\\emph{J. Differential Equations.}, 267 (2019), pp. 3827-3851], where the\nhomogeneous Dirichlet boundary condition for velocity field is treated. The\nproof leans on the specific mathematical structure of equations and the\nrecently developed theory of open fluid systems. Furthermore, we establish the\nweak-strong uniqueness principle, namely a weak solution coincides with the\nstrong solution on the lifespan of the latter provided they emanate from the\nsame initial and boundary data. This basic property is expected to be useful in\nthe study of convergence of numerical solutions.", "journal": ""}
{"doi": "10.48550/arXiv.2408.12420", "date": "2024-08-22", "title": "Dataset | Mindset = Explainable AI | Interpretable AI", "authors": "Caesar Wu, Rajkumar Buyya, Yuan Fang Li, Pascal Bouvry", "abstract": "We often use \"explainable\" Artificial Intelligence (XAI)\" and \"interpretable\nAI (IAI)\" interchangeably when we apply various XAI tools for a given dataset\nto explain the reasons that underpin machine learning (ML) outputs. However,\nthese notions can sometimes be confusing because interpretation often has a\nsubjective connotation, while explanations lean towards objective facts. We\nargue that XAI is a subset of IAI. The concept of IAI is beyond the sphere of a\ndataset. It includes the domain of a mindset. At the core of this ambiguity is\nthe duality of reasons, in which we can reason either outwards or inwards. When\ndirected outwards, we want the reasons to make sense through the laws of\nnature. When turned inwards, we want the reasons to be happy, guided by the\nlaws of the heart. While XAI and IAI share reason as the common notion for the\ngoal of transparency, clarity, fairness, reliability, and accountability in the\ncontext of ethical AI and trustworthy AI (TAI), their differences lie in that\nXAI emphasizes the post-hoc analysis of a dataset, and IAI requires a priori\nmindset of abstraction. This hypothesis can be proved by empirical experiments\nbased on an open dataset and harnessed by High-Performance Computing (HPC). The\ndemarcation of XAI and IAI is indispensable because it would be impossible to\ndetermine regulatory policies for many AI applications, especially in\nhealthcare, human resources, banking, and finance. We aim to clarify these\nnotions and lay the foundation of XAI, IAI, EAI, and TAI for many practitioners\nand policymakers in future AI applications and research.", "journal": ""}
{"doi": "10.48550/arXiv.2204.03884", "date": "2022-04-08", "title": "SeCaV: A Sequent Calculus Verifier in Isabelle/HOL", "authors": "Asta Halkj\u00e6r From, Frederik Krogsdal Jacobsen, J\u00f8rgen Villadsen", "abstract": "We describe SeCaV, a sequent calculus verifier for first-order logic in\nIsabelle/HOL, and the SeCaV Unshortener, an online tool that expands succinct\nderivations into the full SeCaV syntax. We leverage the power of Isabelle/HOL\nas a proof checker for our SeCaV derivations. The interactive features of\nIsabelle/HOL make our system transparent. For instance, the user can simply\nclick on a side condition to see its exact definition. Our formalized soundness\nand completeness proofs pertain exactly to the calculus as exposed to the user\nand not just to some model of our tool. Users can also write their derivations\nin the SeCaV Unshortener, which provides a lighter syntax, and expand them for\nlater verification. We have used both tools in our teaching.", "journal": "EPTCS 357, 2022, pp. 38-55"}
{"doi": "10.48550/arXiv.2102.02600", "date": "2021-02-04", "title": "A formalization of Dedekind domains and class groups of global fields", "authors": "Anne Baanen, Sander R. Dahmen, Ashvni Narayanan, Filippo A. E. Nuccio", "abstract": "Dedekind domains and their class groups are notions in commutative algebra\nthat are essential in algebraic number theory. We formalized these structures\nand several fundamental properties, including number theoretic finiteness\nresults for class groups, in the Lean prover as part of the mathlib\nmathematical library. This paper describes the formalization process, noting\nthe idioms we found useful in our development and mathlib's decentralized\ncollaboration processes involved in this project.", "journal": ""}
{"doi": "10.48550/arXiv.1808.05342", "date": "2018-08-16", "title": "Formalisation of a frame stack semantics for a Java-like language", "authors": "Aleksy Schubert, Jacek Chrz\u0105szcz", "abstract": "We present a Coq formalisation of the small-step operational semantics of\nJafun, a small Java-like language with classes. This format of semantics makes\nit possible to naturally specify and prove invariants that should hold at each\ncomputation step. In contrast to the Featherweight Java approach the semantics\nexplicitly manipulates frame stack of method calls. Thanks to that one can\nexpress properties of computation that depend on execution of particular\nmethods.\n  On the basis of the semantics, we developed a type system that makes it\npossible to delineate a notion of a compound value and classify certain methods\nas extensional functions operating on them. In our formalisation we make a\nmechanised proof that the operational semantics for the untyped version of the\nsemantics agrees with the one for the typed one. We discuss different methods\nto make such formalisation effort and provide experiments that substantiate it.", "journal": ""}
{"doi": "10.48550/arXiv.2404.06477", "date": "2024-04-09", "title": "Mechanised Hypersafety Proofs about Structured Data: Extended Version", "authors": "Vladimir Gladshtein, Qiyuan Zhao, Willow Ahrens, Saman Amarasinghe, Ilya Sergey", "abstract": "Arrays are a fundamental abstraction to represent collections of data. It is\noften possible to exploit structural properties of the data stored in an array\n(e.g., repetition or sparsity) to develop a specialised representation\noptimised for space efficiency. Formally reasoning about correctness of\nmanipulations with such structured data is challenging, as they are often\ncomposed of multiple loops with non-trivial invariants.\n  In this work, we observe that specifications for structured data\nmanipulations can be phrased as hypersafety properties, i.e., predicates that\nrelate traces of $k$ programs. To turn this observation into an effective\nverification methodology, we developed the Logic for Graceful Tensor\nManipulation (LGTM), a new Hoare-style relational separation logic for\nspecifying and verifying computations over structured data. The key enabling\nidea of LGTM is that of parametrised hypersafety specifications that allow the\nnumber $k$ of the program components to depend on the program variables. We\nimplemented LGTM as a foundational embedding into Coq, mechanising its rules,\nmeta-theory, and the proof of soundness. Furthermore, we developed a library of\ndomain-specific tactics that automate computer-aided hypersafety reasoning,\nresulting in pleasantly short proof scripts that enjoy a high degree of reuse.\nWe argue for the effectiveness of relational reasoning about structured data in\nLGTM by specifying and mechanically proving correctness of 13 case studies\nincluding computations on compressed arrays and efficient operations over\nmultiple kinds of sparse tensors.", "journal": ""}
{"doi": "10.48550/arXiv.2110.01439", "date": "2021-10-04", "title": "SecurePtrs: Proving Secure Compilation with Data-Flow Back-Translation and Turn-Taking Simulation", "authors": "Akram El-Korashy, Roberto Blanco, J\u00e9r\u00e9my Thibault, Adrien Durier, Deepak Garg, Catalin Hritcu", "abstract": "Proving secure compilation of partial programs typically requires\nback-translating an attack against the compiled program to an attack against\nthe source program. To prove back-translation, one can syntactically translate\nthe target attacker to a source one -- i.e., syntax-directed back-translation\n-- or show that the interaction traces of the target attacker can also be\nemitted by source attackers -- i.e., trace-directed back-translation.\n  Syntax-directed back-translation is not suitable when the target attacker may\nuse unstructured control flow that the source language cannot directly\nrepresent. Trace-directed back-translation works with such syntactic\ndissimilarity because only the external interactions of the target attacker\nhave to be mimicked in the source, not its internal control flow. Revealing\nonly external interactions is, however, inconvenient when sharing memory via\nunforgeable pointers, since information about shared pointers stashed in\nprivate memory is not present on the trace. This made prior proofs\nunnecessarily complex, since the generated attacker had to instead stash all\nreachable pointers.\n  In this work, we introduce more informative *data-flow traces*, combining the\nbest of syntax- and trace-directed back-translation in a simpler technique that\nhandles both syntactic dissimilarity and memory sharing well, and that is\nproved correct in Coq. Additionally, we develop a novel *turn-taking\nsimulation* relation and use it to prove a recomposition lemma, which is key to\nreusing compiler correctness in such secure compilation proofs. We are the\nfirst to mechanize such a recomposition lemma in the presence of memory\nsharing.\n  We use these two innovations in a secure compilation proof for a code\ngeneration compiler pass between a source language with structured control flow\nand a target language with unstructured control flow, both with safe pointers\nand components.", "journal": ""}
{"doi": "10.48550/arXiv.0104043", "date": "2001-04-03", "title": "Asymptotic behavior of a stationary silo with absorbing walls", "authors": "S. R. M. Barros, P. A. Ferrari, N. L. Garcia, S. Martinez", "abstract": "We study the nearest neighbors one dimensional uniform q-model of force\nfluctuations in bead packs [Coppersmith et al (1996)], a stochastic model to\nsimulate the stress of granular media in two dimensional silos. The vertical\ncoordinate plays the role of time, and the horizontal coordinate the role of\nspace. The process is a discrete time Markov process with state space\n$\\R^{\\{1,...,N\\}}$. At each layer (time), the weight supported by each grain is\na random variable of mean one (its own weight) plus the sum of random fractions\nof the weights supported by the nearest neighboring grains at the previous\nlayer. The fraction of the weight given to the right neighbor of the successive\nlayer is a uniform random variable in $[0,1]$ independent of everything. The\nremaining weight is given to the left neighbor. In the boundaries, a uniform\nfraction of the weight leans on the wall of the silo. This corresponds to\n\\emph{absorbing boundary conditions}. For this model we show that there exists\na unique invariant measure. The mean weight at site $i$ under the invariant\nmeasure is $i(N+1-i)$; we prove that its variance is $\\frac12(i(N+1-i))^2 +\nO(N^3)$ and the covariances between grains $i\\neq j$ are of order $O(N^3)$.\nMoreover, as $N\\to\\infty$, the law under the invariant measure of the weights\ndivided by $N^2$ around site (integer part of) $rN$, $r\\in (0,1)$, converges to\na product of gamma distributions with parameters 2 and $2(r(1-r))^{-1}$ (sum of\ntwo exponentials of mean $r(1-r)/2$). Liu {\\it et al} (1995) proved that for a\nsilo with infinitely many weightless grains, any product of gamma distributions\nwith parameters 2 and $2/\\rho$ with $\\rho\\in [0,\\infty)$ are invariant. Our\nresult shows that as the silo grows, the model selects exactly one of these\nGamma's at each macroscopic place.", "journal": "J. Statist. Phys. 106 (2002), no. 3-4, 521--546"}
{"doi": "10.48550/arXiv.2212.02833", "date": "2022-12-06", "title": "A substructural logic for quantum measurements", "authors": "Daniel Lehmann", "abstract": "This paper presents a substructural logic of sequents with very restricted\nexchange and weakening rules. It is sound with respect to sequences of\nmeasurements of a quantic system. A sound and complete semantics is provided.\nThe semantic structures include a binary relation that expresses orthogonality\nbetween elements and enables the definition of an operation that generalizes\nthe projection operation in Hilbert spaces. The language has a unitary\nconnective, a sort of negation, and two dual binary connectives that are\nneither commutative nor associative, sorts of conjunction and disjunction. This\nprovides a logic for quantum measurements whose proof theory is aesthetically\npleasing.", "journal": ""}
{"doi": "10.48550/arXiv.1303.4193", "date": "2013-03-18", "title": "A Qualitative Comparison of the Suitability of Four Theorem Provers for Basic Auction Theory", "authors": "Christoph Lange, Marco B. Caminati, Manfred Kerber, Till Mossakowski, Colin Rowat, Makarius Wenzel, Wolfgang Windsteiger", "abstract": "Novel auction schemes are constantly being designed. Their design has\nsignificant consequences for the allocation of goods and the revenues\ngenerated. But how to tell whether a new design has the desired properties,\nsuch as efficiency, i.e. allocating goods to those bidders who value them most?\nWe say: by formal, machine-checked proofs. We investigated the suitability of\nthe Isabelle, Theorema, Mizar, and Hets/CASL/TPTP theorem provers for\nreproducing a key result of auction theory: Vickrey's 1961 theorem on the\nproperties of second-price auctions. Based on our formalisation experience,\ntaking an auction designer's perspective, we give recommendations on what\nsystem to use for formalising auctions, and outline further steps towards a\ncomplete auction theory toolbox.", "journal": ""}
{"doi": "10.48550/arXiv.2408.15817", "date": "2024-08-28", "title": "Unifying Model Execution and Deductive Verification with Interaction Trees in Isabelle/HOL", "authors": "Simon Foster, Chung-Kil Hur, Jim Woodcock", "abstract": "Model execution allows us to prototype and analyse software engineering\nmodels by stepping through their possible behaviours, using techniques like\nanimation and simulation. On the other hand, deductive verification allows us\nto construct formal proofs demonstrating satisfaction of certain critical\nproperties in support of high-assurance software engineering. To ensure\ncoherent results between execution and proof, we need unifying semantics and\nautomation. In this paper, we mechanise Interaction Trees (ITrees) in\nIsabelle/HOL to produce an execution and verification framework. ITrees are\ncoinductive structures that allow us to encode infinite labelled transition\nsystems, yet they are inherently executable. We use ITrees to create\nverification tools for stateful imperative programs, concurrent programs with\nmessage passing in the form of the CSP and \\Circus languages, and abstract\nsystem models in the style of the Z and B methods. We demonstrate how ITrees\ncan account for diverse semantic presentations, such as structural operational\nsemantics, a relational program model, and CSP's failures-divergences trace\nmodel. Finally, we demonstrate how ITrees can be executed using the Isabelle\ncode generator to support the animation of models.", "journal": ""}
{"doi": "10.48550/arXiv.2102.06513", "date": "2021-02-12", "title": "Complete Bidirectional Typing for the Calculus of Inductive Constructions", "authors": "Meven Lennon-Bertrand", "abstract": "This article presents a bidirectional type system for the Calculus of\nInductive Constructions (CIC). It introduces a new judgement intermediate\nbetween the usual inference and checking, dubbed constrained inference, to\nhandle the presence of computation in types. The key property of the system is\nits completeness with respect to the usual undirected one, which has been\nformally proven in Coq as a part of the MetaCoq project. Although it plays an\nimportant role in an ongoing completeness proof for a realistic typing\nalgorithm, the interest of bidirectionality is wider, as it gives insights and\nstructure when trying to prove properties on CIC or design variations and\nextensions. In particular, we put forward constrained inference, an\nintermediate between the usual inference and checking judgements, to handle the\npresence of computation in types.", "journal": ""}
{"doi": "10.48550/arXiv.0507064", "date": "2005-07-26", "title": "Termination of rewriting strategies: a generic approach", "authors": "Isabelle Gnaedig, Helene Kirchner", "abstract": "We propose a generic termination proof method for rewriting under strategies,\nbased on an explicit induction on the termination property. Rewriting trees on\nground terms are modeled by proof trees, generated by alternatively applying\nnarrowing and abstracting steps. The induction principle is applied through the\nabstraction mechanism, where terms are replaced by variables representing any\nof their normal forms. The induction ordering is not given a priori, but\ndefined with ordering constraints, incrementally set during the proof.\nAbstraction constraints can be used to control the narrowing mechanism, well\nknown to easily diverge. The generic method is then instantiated for the\ninnermost, outermost and local strategies.", "journal": ""}
{"doi": "10.48550/arXiv.0903.5282", "date": "2009-03-30", "title": "Multi-agent Q-Learning of Channel Selection in Multi-user Cognitive Radio Systems: A Two by Two Case", "authors": "Husheng Li", "abstract": "Resource allocation is an important issue in cognitive radio systems. It can\nbe done by carrying out negotiation among secondary users. However, significant\noverhead may be incurred by the negotiation since the negotiation needs to be\ndone frequently due to the rapid change of primary users' activity. In this\npaper, a channel selection scheme without negotiation is considered for\nmulti-user and multi-channel cognitive radio systems. To avoid collision\nincurred by non-coordination, each user secondary learns how to select channels\naccording to its experience. Multi-agent reinforcement leaning (MARL) is\napplied in the framework of Q-learning by considering the opponent secondary\nusers as a part of the environment. The dynamics of the Q-learning are\nillustrated using Metrick-Polak plot. A rigorous proof of the convergence of\nQ-learning is provided via the similarity between the Q-learning and\nRobinson-Monro algorithm, as well as the analysis of convergence of the\ncorresponding ordinary differential equation (via Lyapunov function). Examples\nare illustrated and the performance of learning is evaluated by numerical\nsimulations.", "journal": ""}
{"doi": "10.48550/arXiv.1311.2958", "date": "2013-11-12", "title": "The entirely coupled region of supercritical contact processes", "authors": "Achillefs Tzioufas", "abstract": "We consider translation-invariant, finite range, supercritical contact\nprocesses. We show the existence of unbounded space-time cones within which the\ndescendancy of the process from full occupancy may with positive probability be\nidentical to that of the process from the single site at its apex. The proof\ncomprises an argument that leans upon refinements of a successful coupling\namong these two processes, and is valid in $d$-dimensions.", "journal": ""}
{"doi": "10.48550/arXiv.1710.07676", "date": "2017-10-20", "title": "Partial inversion of the 2D attenuated $X$-ray transform with data on an arc", "authors": "Hiroshi Fujiwara, Kamran Sadiq, Alexandru Tamasan", "abstract": "In two dimensions, we consider the problem of inversion of the attenuated\n$X$-ray transform of a compactly supported function from data restricted to\nlines leaning on a given arc. We provide a method to reconstruct the function\non the convex hull of this arc. The attenuation is assumed known. The method of\nproof uses the Hilbert transform associated with $A$-analytic functions in the\nsense of Bukhgeim.", "journal": ""}
{"doi": "10.48550/arXiv.1407.2008", "date": "2014-07-08", "title": "Multi-level Gevrey solutions of singularly perturbed linear partial differential equations", "authors": "Alberto Lastra, St\u00e9phane Malek", "abstract": "We study the asymptotic behavior of the solutions related to a family of\nsingularly perturbed linear partial differential equations in the complex\ndomain. The analytic solutions obtained by means of a Borel-Laplace summation\nprocedure are represented by a formal power series in the perturbation\nparameter. Indeed, the geometry of the problem gives rise to a decomposition of\nthe formal and analytic solutions so that a multi-level Gevrey order phenomenon\nappears. This result leans on a Malgrange-Sibuya theorem in several Gevrey\nlevels.", "journal": ""}
{"doi": "10.48550/arXiv.1502.07222", "date": "2015-02-25", "title": "Strongly regular multi-level solutions of singularly perturbed linear partial differential equations", "authors": "Alberto Lastra, St\u00e9phane Malek, Javier Sanz", "abstract": "We study the asymptotic behavior of the solutions related to a family of\nsingularly perturbed partial differential equations in the complex domain. The\nanalytic solutions are asymptotically represented by a formal power series in\nthe perturbation parameter. The geometry of the problem and the nature of the\nelements involved in it give rise to different asymptotic levels related to the\nso-called strongly regular sequences. The result leans on a novel version of a\nmulti-level Ramis-Sibuya theorem.", "journal": ""}
{"doi": "10.48550/arXiv.1806.07387", "date": "2018-06-19", "title": "On parametric Gevrey asymptotics for some initial value problems in two asymmetric complex time variables", "authors": "Alberto Lastra, St\u00e9phane Malek", "abstract": "We study a family of nonlinear initial value partial differential equations\nin the complex domain under the action of two asymmetric time variables.\nDifferent Gevrey bounds and multisummability results are obtain depending on\neach element of the family, providing a more complete picture on the asymptotic\nbehavior of the solutions of PDEs in the complex domain in several complex\nvariables.\n  The main results lean on a fixed point argument in certain Banach space in\nthe Borel plane, together with a Borel summability procedure and the action of\ndifferent Ramis-Sibuya type theorems.", "journal": ""}
{"doi": "10.48550/arXiv.1904.04886", "date": "2019-04-09", "title": "Boundary layer expansions for initial value problems with two complex time variables", "authors": "Alberto Lastra, St\u00e9phane Malek", "abstract": "We study a family of partial differential equations in the complex domain,\nunder the action of a complex perturbation parameter $\\epsilon$. We construct\ninner and outer solutions of the problem and relate them to asymptotic\nrepresentations via Gevrey asymptotic expansions with respect to $\\epsilon$, in\nadequate domains. The construction of such analytic solutions is closely\nrelated to the procedure of summation with respect to an analytic germ, put\nforward in[J. Mozo-Fern\\'andez, R. Sch\\\"afke, Asymptotic expansions and\nsummability with respect to an analytic germ, Publ. Math. 63 (2019), no. 1,\n3--79.], whilst the asymptotic representation leans on the cohomological\napproach determined by Ramis-Sibuya Theorem.", "journal": ""}
{"doi": "10.48550/arXiv.2305.18017", "date": "2023-05-29", "title": "Trace models of concurrent valuation algebras", "authors": "Naso Evangelou-Oost, Larissa Meinicke, Callum Bannister, Ian J. Hayes", "abstract": "This paper introduces Concurrent Valuation Algebras (CVAs), a novel extension\nof ordered valuation algebras (OVAs). CVAs include two combine operators\nrepresenting parallel and sequential products, adhering to a weak exchange law.\nThis development offers theoretical and practical benefits for the\nspecification and modelling of concurrent and distributed systems. As a\npresheaf on a space of domains, CVAs enable localised specifications,\nsupporting modularity, compositionality, and the ability to represent large and\ncomplex systems. Furthermore, CVAs align with lattice-based refinement\nreasoning and are compatible with established methodologies such as Hoare and\nRely-Guarantee logics. The flexibility of CVAs is explored through three trace\nmodels, illustrating distinct paradigms of concurrent/distributed computing,\ninterrelated by morphisms. The paper also highlights the potential to\nincorporate a powerful local computation framework from valuation algebras for\nmodel checking in concurrent and distributed systems. The foundational results\npresented have been verified with the proof assistant Isabelle/HOL.", "journal": "Formal Methods and Software Engineering. ICFEM 2023. Lecture Notes\n  in Computer Science, vol 14308. Springer, Singapore"}
{"doi": "10.48550/arXiv.1811.08128", "date": "2018-11-20", "title": "Formal FocusST Specification of CAN", "authors": "Maria Spichkova", "abstract": "This paper presents a formal specification of the Controller Area Network\n(CAN) protocol using FocusST framework. We formally describe core components of\nthe protocol, which provides a basis for further formal analysis using the\nIsabelle/HOL theorem prover.", "journal": ""}
{"doi": "10.48550/arXiv.1408.0629", "date": "2014-08-04", "title": "Bounds for variables with few occurrences in conjunctive normal forms", "authors": "Oliver Kullmann, Xishun Zhao", "abstract": "We investigate connections between SAT (the propositional satisfiability\nproblem) and combinatorics, around the minimum degree (number of occurrences)\nof variables in various forms of redundancy-free boolean conjunctive normal\nforms (clause-sets).\n  Lean clause-sets do not have non-trivial autarkies, that is, it is not\npossible to satisfy some clauses and leave the other clauses untouched. The\ndeficiency of a clause-set is the difference of the number of clauses and the\nnumber of variables. We prove a precise upper bound on the minimum variable\ndegree of lean clause-sets in dependency on the deficiency. If a clause-set\ndoes not fulfil this upper bound, then it must have a non-trivial autarky; we\nshow that the autarky-reduction (elimination of affected clauses) can be done\nin polynomial time, while it is open to find the autarky itself in polynomial\ntime.\n  Then we investigate this upper bound for the special case of minimally\nunsatisfiable clause-sets. We show that the bound can be improved here,\nintroducing a general method to improve the underlying recurrence.\n  We consider precise relations, and thus the investigations have a\nnumber-theoretical flavour. We try to build a bridge from logic to\ncombinatorics (especially to hypergraph colouring), and we discuss thoroughly\nthe background and open problems, and provide many examples and explanations.", "journal": ""}
{"doi": "10.48550/arXiv.1511.04179", "date": "2015-11-13", "title": "Realisability semantics of abstract focussing, formalised", "authors": "St\u00e9phane Graham-Lengrand", "abstract": "We present a sequent calculus for abstract focussing, equipped with\nproof-terms: in the tradition of Zeilberger's work, logical connectives and\ntheir introduction rules are left as a parameter of the system, which collapses\nthe synchronous and asynchronous phases of focussing as macro rules. We go\nfurther by leaving as a parameter the operation that extends a context of\nhypotheses with new ones, which allows us to capture both classical and\nintuitionistic focussed sequent calculi. We then define the realisability\nsemantics of (the proofs of) the system, on the basis of Munch-Maccagnoni's\northogonality models for the classical focussed sequent calculus, but now\noperating at the higher level of abstraction mentioned above. We prove, at that\nlevel, the Adequacy Lemma, namely that if a term is of type A, then in the\nmodel its denotation is in the (set-theoretic) interpretation of A. This\nexhibits the fact that the universal quantification involved when taking the\northogonal of a set, reflects in the semantics Zeilberger's universal\nquantification in the macro rule for the asynchronous phase. The system and its\nsemantics are all formalised in Coq.", "journal": "EPTCS 197, 2015, pp. 15-28"}
{"doi": "10.48550/arXiv.2303.04895", "date": "2023-03-08", "title": "Morpho-logic from a Topos Perspective: Application to symbolic AI", "authors": "Marc Aiguier, Isabelle Bloch, Salim Nibouche, Ramon Pino Perez", "abstract": "Modal logics have proved useful for many reasoning tasks in symbolic\nartificial intelligence (AI), such as belief revision, spatial reasoning, among\nothers. On the other hand, mathematical morphology (MM) is a theory for\nnon-linear analysis of structures, that was widely developed and applied in\nimage analysis. Its mathematical bases rely on algebra, complete lattices,\ntopology. Strong links have been established between MM and mathematical\nlogics, mostly modal logics. In this paper, we propose to further develop and\ngeneralize this link between mathematical morphology and modal logic from a\ntopos perspective, i.e. categorial structures generalizing space, and\nconnecting logics, sets and topology. Furthermore, we rely on the internal\nlanguage and logic of topos. We define structuring elements, dilations and\nerosions as morphisms. Then we introduce the notion of structuring\nneighborhoods, and show that the dilations and erosions based on them lead to a\nconstructive modal logic, for which a sound and complete proof system is\nproposed. We then show that the modal logic thus defined (called morpho-logic\nhere), is well adapted to define concrete and efficient operators for revision,\nmerging, and abduction of new knowledge, or even spatial reasoning.", "journal": ""}
{"doi": "10.48550/arXiv.2207.03994", "date": "2022-07-02", "title": "LibNDT: Towards a Formal Library on Spreadable Properties over Linked Nested Datatypes", "authors": "Mathieu Montin, Am\u00e9lie Ledein, Catherine Dubois", "abstract": "Nested datatypes have been widely studied in the past 25 years, both\ntheoretically using category theory, and practically in programming languages\nsuch as Haskell. They consist in recursive polymorphic datatypes where the type\nparameter changes throughout the recursion. They have a variety of applications\nsuch as modelling memory or modelling constraints over regular datatypes\nwithout relying on dependent types. In this work, we focus on a specific subset\nof nested datatypes which we call Linked Nested DataTypes (LNDT). We show that\nsome usual datatypes such has List and Maybe, as well as some well-known nested\ndatatypes such as Nest and even Bush can be built as various instances of LNDT.\nWe proceed by presenting LibNDT, a library, developed both in Agda and Coq,\nwhich focuses on the set of constructs that can be spread directly from the\nparameter on which a LNDT is built, to the LNDT itself. These spreadable\nelements are of two kinds, functions, such as folds and map, and properties,\nsuch as the congruence of map or the satisfaction of a given predicate for at\nleast one, or all, elements of the structure. We make use of the dependent type\nsystem of both Coq and Agda to model the latter. This paper ends with a\ndiscussion about various interesting topics that were raised throughout our\ndevelopment such as the issue of termination, the comparison of our tools and\nthe proof effort required to extend LibNDT with additional elements.", "journal": "EPTCS 360, 2022, pp. 27-44"}
{"doi": "10.48550/arXiv.1706.05851", "date": "2017-06-19", "title": "Generic Approach to Certified Static Checking of Module-like Constructs", "authors": "Julia Belyakova", "abstract": "In this paper we consider the problem of certified static checking of\nmodule-like constructs of programming languages. We argue that there are\nalgorithms and properties related to modules that can be defined and proven in\nan abstract way. We advocate the design of a generic Coq library, which is\naimed to provide three building blocks for each checking mechanism:\npropositional, computable, and correctness proofs. Implemented part of the\nlibrary is justified by applying it to a certified static checker of an\nextension of STLC.", "journal": ""}
{"doi": "10.48550/arXiv.2306.12411", "date": "2023-06-21", "title": "Coqlex: Generating Formally Verified Lexers", "authors": "Wendlasida Ouedraogo, Gabriel Scherer, Lutz Strassburger", "abstract": "A compiler consists of a sequence of phases going from lexical analysis to\ncode generation. Ideally, the formal verification of a compiler should include\nthe formal verification of each component of the tool-chain. An example is the\nCompCert project, a formally verified C compiler, that comes with associated\ntools and proofs that allow to formally verify most of those components.\nHowever, some components, in particular the lexer, remain unverified. In fact,\nthe lexer of Compcert is generated using OCamllex, a lex-like OCaml lexer\ngenerator that produces lexers from a set of regular expressions with\nassociated semantic actions. Even though there exist various approaches, like\nCakeML or Verbatim++, to write verified lexers, they all have only limited\npractical applicability. In order to contribute to the end-to-end verification\nof compilers, we implemented a generator of verified lexers whose usage is\nsimilar to OCamllex. Our software, called Coqlex, reads a lexer specification\nand generates a lexer equipped with a Coq proof of its correctness. It provides\na formally verified implementation of most features of standard, unverified\nlexer generators.\n  The conclusions of our work are two-fold: Firstly, verified lexers gain to\nfollow a user experience similar to lex/flex or OCamllex, with a\ndomain-specific syntax to write lexers comfortably. This introduces a small gap\nbetween the written artifact and the verified lexer, but our design minimizes\nthis gap and makes it practical to review the generated lexer. The user remains\nable to prove further properties of their lexer. Secondly, it is possible to\ncombine simplicity and decent performance. Our implementation approach that\nuses Brzozowski derivatives is noticeably simpler than the previous work in\nVerbatim++ that tries to generate a deterministic finite automaton (DFA) ahead\nof time, and it is also noticeably faster thanks to careful design choices.\n  We wrote several example lexers that suggest that the convenience of using\nCoqlex is close to that of standard verified generators, in particular,\nOCamllex. We used Coqlex in an industrial project to implement a verified lexer\nof Ada. This lexer is part of a tool to optimize safety-critical programs, some\nof which are very large. This experience confirmed that Coqlex is usable in\npractice, and in particular that its performance is good enough. Finally, we\nperformed detailed performance comparisons between Coqlex, OCamllex, and\nVerbatim++. Verbatim++ is the state-of-the-art tool for verified lexers in Coq,\nand the performance of its lexer was carefully optimized in previous work by\nEgolf and al. (2022). Our results suggest that Coqlex is two orders of\nmagnitude slower than OCamllex, but two orders of magnitude faster than\nVerbatim++. Verified compilers and other language-processing tools are becoming\nimportant tools for safety-critical or security-critical applications. They\nprovide trust and replace more costly approaches to certification, such as\nmanually reading the generated code. Verified lexers are a missing piece in\nseveral Coq-based verified compilers today. Coqlex comes with safety\nguarantees, and thus shows that it is possible to build formally verified\nfront-ends.", "journal": "The Art, Science, and Engineering of Programming, 2024, Vol. 8,\n  Issue 1, Article 3"}
{"doi": "10.48550/arXiv.1507.06943", "date": "2015-07-24", "title": "Clues on Software Engineers Learning Styles", "authors": "Luiz Fernando Capretz", "abstract": "The Myers-Briggs Type Indicator (MBTI) has proved to be a useful instrument\nfor understanding student learning preferences and has enable comparisons of\nthe learning preferences for various personality types. Regarding learning\nstyles, there is no one best combination of characteristics, since each\npreference has its own advantages and disadvantages. Therefore, it is a fallacy\nto think that professors can devise a single teaching technique that would\nalways appeal to all students at the same time. The ideas presented in this\npaper have been taken into account in two 4th year courses, named Software\nRequirements and Software Design in which the students develop their capstone\nprojects. The results of this investigation may help college instructors to\nunderstanding the preferred leaning style of software engineers.", "journal": "International Journal of Computing & Information Sciences,\n  4(1):46-49, 2006"}
{"doi": "10.48550/arXiv.1404.6607", "date": "2014-04-26", "title": "FoCaLiZe: Inside an F-IDE", "authors": "Fran\u00e7ois Pessaux", "abstract": "For years, Integrated Development Environments have demonstrated their\nusefulness in order to ease the development of software. High-level security or\nsafety systems require proofs of compliance to standards, based on analyses\nsuch as code review and, increasingly nowadays, formal proofs of conformance to\nspecifications. This implies mixing computational and logical aspects all along\nthe development, which naturally raises the need for a notion of Formal IDE.\nThis paper examines the FoCaLiZe environment and explores the implementation\nissues raised by the decision to provide a single language to express\nspecification properties, source code and machine-checked proofs while allowing\nincremental development and code reusability. Such features create strong\ndependencies between functions, properties and proofs, and impose an particular\ncompilation scheme, which is described here. The compilation results are\nrunnable OCaml code and a checkable Coq term. All these points are illustrated\nthrough a running example.", "journal": "EPTCS 149, 2014, pp. 64-78"}
{"doi": "10.48550/arXiv.2503.02975", "date": "2025-03-04", "title": "Proof-Producing Translation of Functional Programs into a Time \\& Space Reasonable Model", "authors": "Kevin Kappelmann, Fabian Huch, Lukas Stevens, Mohammad Abdulaziz", "abstract": "We present a semi-automated framework to construct and reason about programs\nin a deeply-embedded while-language. The while-language we consider is a simple\ncomputation model that can simulate (and be simulated by) Turing machines with\na linear time and constant space blow-up. Our framework derives while-programs\nfrom functional programs written in a subset of Isabelle/HOL, namely\ntail-recursive functions with first-order arguments and algebraic datatypes. As\nfar as we are aware, it is the first framework targeting a computation model\nthat is reasonable in time and space from a complexity-theoretic perspective.", "journal": ""}
{"doi": "10.48550/arXiv.1102.0947", "date": "2011-02-04", "title": "Splicing systems and the Chomsky hierarchy", "authors": "Jean Berstel, Luc Boasson, Isabelle Fagnot", "abstract": "In this paper, we prove decidability properties and new results on the\nposition of the family of languages generated by (circular) splicing systems\nwithin the Chomsky hierarchy. The two main results of the paper are the\nfollowing. First, we show that it is decidable, given a circular splicing\nlanguage and a regular language, whether they are equal. Second, we prove the\nlanguage generated by an alphabetic splicing system is context-free. Alphabetic\nsplicing systems are a generalization of simple and semi-simple splicin systems\nalready considered in the literature.", "journal": ""}
{"doi": "10.48550/arXiv.1003.5954", "date": "2010-03-31", "title": "Multi-Stage Programs are Generalized Arrows", "authors": "Adam Megacz", "abstract": "The lambda calculus, subject to typing restrictions, provides a syntax for\nthe internal language of cartesian closed categories. This paper establishes a\nparallel result: staging annotations, subject to named level restrictions,\nprovide a syntax for the internal language of Freyd categories, which are known\nto be in bijective correspondence with Arrows. The connection is made by\ninterpreting multi-stage type systems as indexed functors from polynomial\ncategories to their reindexings. This result applies only to multi-stage\nlanguages which are (1) homogeneous, (2) allow cross-stage persistence and (3)\nplace no restrictions on the use of structural rules in typing derivations.\nRemoving these restrictions and repeating the construction yields generalized\narrows, of which Arrows are a particular case. A translation from well-typed\nmulti-stage programs to single-stage GArrow terms is provided. The translation\nis defined by induction on the structure of the proof that the multi-stage\nprogram is well-typed, relying on information encoded in the proof's use of\nstructural rules. Metalanguage designers can now factor out the syntactic\nmachinery of metaprogramming by providing a single translation from staging\nsyntax into expressions of generalized arrow type. Object language providers\nneed only implement the functions of the generalized arrow type class in\npoint-free style. Object language users may write metaprograms over these\nobject languages in a point-ful style, using the same binding, scoping,\nabstraction, and application mechanisms in both the object language and\nmetalanguage. This paper's principal contributions are the GArrow definition of\nFigures 2 and 3, the translation in Figure 5 and the category-theoretic\nsemantics of Definition 16. An accompanying Coq proof formalizes the type\nsystem, translation procedure, and key theorems.", "journal": ""}
{"doi": "10.48550/arXiv.2211.10665", "date": "2022-11-19", "title": "CryptOpt: Verified Compilation with Randomized Program Search for Cryptographic Primitives (full version)", "authors": "Joel Kuepper, Andres Erbsen, Jason Gross, Owen Conoly, Chuyue Sun, Samuel Tian, David Wu, Adam Chlipala, Chitchanok Chuengsatiansup, Daniel Genkin, Markus Wagner, Yuval Yarom", "abstract": "Most software domains rely on compilers to translate high-level code to\nmultiple different machine languages, with performance not too much worse than\nwhat developers would have the patience to write directly in assembly language.\nHowever, cryptography has been an exception, where many performance-critical\nroutines have been written directly in assembly (sometimes through\nmetaprogramming layers). Some past work has shown how to do formal verification\nof that assembly, and other work has shown how to generate C code automatically\nalong with formal proof, but with consequent performance penalties vs. the\nbest-known assembly. We present CryptOpt, the first compilation pipeline that\nspecializes high-level cryptographic functional programs into assembly code\nsignificantly faster than what GCC or Clang produce, with mechanized proof (in\nCoq) whose final theorem statement mentions little beyond the input functional\nprogram and the operational semantics of x86-64 assembly. On the optimization\nside, we apply randomized search through the space of assembly programs, with\nrepeated automatic benchmarking on target CPUs. On the formal-verification\nside, we connect to the Fiat Cryptography framework (which translates\nfunctional programs into C-like IR code) and extend it with a new formally\nverified program-equivalence checker, incorporating a modest subset of known\nfeatures of SMT solvers and symbolic-execution engines. The overall prototype\nis quite practical, e.g. producing new fastest-known implementations of\nfinite-field arithmetic for both Curve25519 (part of the TLS standard) and the\nBitcoin elliptic curve secp256k1 for the Intel $12^{th}$ and $13^{th}$\ngenerations.", "journal": ""}
{"doi": "10.48550/arXiv.2102.02616", "date": "2021-02-04", "title": "Optimal control of a quasilinear parabolic equation and its time discretization", "authors": "Luise Blank, Johannes Meisinger", "abstract": "In this paper we discuss the optimal control of a quasilinear parabolic state\nequation. Its form is leaned on the kind of problems arising for example when\ncontrolling the anisotropic Allen-Cahn equation as a model for crystal growth.\nMotivated by this application we consider the state equation as a result of a\ngradient flow of an energy functional. The quasilinear term is strongly\nmonotone and obeys a certain growth condition and the lower order term is\nnon-monotone. The state equation is discretized implicitly in time with\npiecewise constant functions. The existence of the control-to-state operator\nand its Lipschitz-continuity is shown for the time discretized as well as for\nthe time continuous problem. Latter is based on the convergence proof of the\ndiscretized solutions. Finally we present for both the existence of global\nminimizers. Also convergence of a subsequence of time discrete optimal controls\nto a global minimizer of the time continuous problem can be shown. Our results\nhold in arbitrary space dimensions.", "journal": ""}
{"doi": "10.48550/arXiv.1901.10541", "date": "2019-01-29", "title": "Abstract I/O Specification", "authors": "Willem Penninckx, Amin Timany, Bart Jacobs", "abstract": "We recently proposed an approach for the specification and modular formal\nverification of the interactive (I/O) behavior of programs, based on an\nembedding of Petri nets into separation logic. While this approach is scalable\nand modular in terms of the I/O APIs available to a program, enables composing\nlow-level I/O actions into high-level ones, and enables a convenient\nverification experience, it does not support high-level I/O actions that\ninvolve memory manipulation as well as low-level I/O (such as buffered I/O), or\nthat are in fact \"virtual I/O\" actions that are implemented purely through\nmemory manipulation. Furthermore, it does not allow rewriting an I/O\nspecification into an equivalent one.\n  In this paper, we propose a refined approach that does have these properties.\nThe essential insight is to fix the set of places of the Petri net to be the\nset of separation logic assertions, thus making available the full power of\nseparation logic for abstractly stating an arbitrary operation's specification\nin Petri net form, for composing operations into an I/O specification, and for\nequivalence reasoning on I/O specifications. Our refinement resolves the issue\nof the justification of the choice of Petri nets over other formalisms such as\ngeneral state transition systems, in that it \"refines them away\" into the more\nessential constructs of separating conjunction and abstract nested triples. To\nenable a convenient treatment of input operations, we propose the use of\nprophecy variables to eliminate their non-determinism.\n  We illustrate the approach through a number of example programs, including\none where subroutines specified and verified using I/O specifications run as\nthreads communicating through shared memory. The theory and examples of the\npaper have been machine-checked using the Iris library for program verification\nin the Coq proof assistant.", "journal": ""}
{"doi": "10.48550/arXiv.1805.05400", "date": "2018-05-14", "title": "Structural Operational Semantics for Control Flow Graph Machines", "authors": "Dmitri Garbuzov, William Mansky, Christine Rizkallah, Steve Zdancewic", "abstract": "Compilers use control flow graph (CFG) representations of low-level programs\nbecause they are suited to program analysis and optimizations. However,\nformalizing the behavior and metatheory of CFG programs is non-trivial: CFG\nprograms don't compose well, their semantics depends on auxiliary state, and,\nas a consequence, they do not enjoy a simple equational theory that can be used\nfor reasoning about the correctness of program transformations.\nLambda-calculus-based intermediate representations, in contrast, have\nwell-understood operational semantics and metatheory, including rich equational\ntheories, all of which makes them amenable to formal verification.\n  This paper establishes a tight equivalence between (a variant of) Levy's\ncall-by-push-value (CBPV) calculus and a control flow graph machine whose\ninstructions are in static single assignment (SSA) form. The correspondence is\nmade precise via a series of abstract machines that align the transitions of\nthe structural operational semantics of the CBPV language with the computation\nsteps of the SSA form.\n  The target machine, which is derived from the CBPV language, accurately\ncaptures the execution model of control flow graphs, including direct jumps,\nmutually recursive code blocks, and multi-argument function calls, and the\nclosure-free subset is similar to the SSA intermediate representations found in\nmodern compilers such as LLVM and GCC. The definitions of all the\nlanguage/abstract machine semantics and the theorems relating them are fully\nverified in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1811.03479", "date": "2018-11-08", "title": "A Program Logic for First-Order Encapsulated WebAssembly", "authors": "Conrad Watt, Petar Maksimovi\u0107, Neelakantan R. Krishnaswami, Philippa Gardner", "abstract": "We introduce Wasm Logic, a sound program logic for first-order, encapsulated\nWebAssembly. We design a novel assertion syntax, tailored to WebAssembly's\nstack-based semantics and the strong guarantees given by WebAssembly's type\nsystem, and show how to adapt the standard separation logic triple and proof\nrules in a principled way to capture WebAssembly's uncommon structured control\nflow. Using Wasm Logic, we specify and verify a simple WebAssembly B-tree\nlibrary, giving abstract specifications independent of the underlying\nimplementation. We mechanise Wasm Logic and its soundness proof in full in\nIsabelle/HOL. As part of the soundness proof, we formalise and fully mechanise\na novel, big-step semantics of WebAssembly, which we prove equivalent, up to\ntransitive closure, to the original WebAssembly small-step semantics. Wasm\nLogic is the first program logic for WebAssembly, and represents a first step\ntowards the creation of static analysis tools for WebAssembly.", "journal": ""}
{"doi": "10.48550/arXiv.2112.05770", "date": "2021-12-10", "title": "Magic Zeroes and Hidden Symmetries", "authors": "Nathaniel Craig, Isabel Garcia Garcia, Arkady Vainshtein, Zhengkang Zhang", "abstract": "Selection rules arising from accidental or broken symmetries may be\nsufficiently obscure that their agency is hidden, leading to the appearance of\n\"magic zeroes\" -- quantities that are suppressed without apparent recourse to a\nsymmetry explanation. Magic zeroes and their corresponding hidden symmetries\nmay shed new light on parametric hierarchies in the Standard Model and beyond.\nWe identify the hidden symmetry responsible for a recently-discovered magic\nzero, the vanishing of the putative leading contribution to the anomalous\ndipole moments of the muon upon integrating out weak doublet and singlet\nvector-like fermions. Some of the tools involved -- spurion analysis leveraging\ndiscrete symmetries of the free theory, field redefinitions, spectator fields,\nand non-supersymmetric non-renormalization theorems -- may prove useful in the\nhunt for new magic zeroes and their hidden symmetries.", "journal": ""}
{"doi": "10.48550/arXiv.1805.12482", "date": "2018-05-31", "title": "How to Simulate It in Isabelle: Towards Formal Proof for Secure Multi-Party Computation", "authors": "David Butler, David Aspinall, Adria Gascon", "abstract": "In cryptography, secure Multi-Party Computation (MPC) protocols allow\nparticipants to compute a function jointly while keeping their inputs private.\nRecent breakthroughs are bringing MPC into practice, solving fundamental\nchallenges for secure distributed computation. Just as with classic protocols\nfor encryption and key exchange, precise guarantees are needed for MPC designs\nand implementations; any flaw will give attackers a chance to break privacy or\ncorrectness. In this paper we present the first (as far as we know)\nformalisation of some MPC security proofs. These proofs provide probabilistic\nguarantees in the computational model of security, but have a different\ncharacter to machine proofs and proof tools implemented so far --- MPC proofs\nuse a \\emph{simulation} approach, in which security is established by showing\nindistinguishability between execution traces in the actual protocol execution\nand an ideal world where security is guaranteed by definition. We show that\nexisting machinery for reasoning about probabilistic programs adapted to this\nsetting, paving the way to precisely check a new class of cryptography\narguments. We implement our proofs using the CryptHOL framework inside\nIsabelle/HOL.", "journal": ""}
{"doi": "10.48550/arXiv.1504.00880", "date": "2015-04-03", "title": "N-manifolds of degree 2 and metric double vector bundles", "authors": "M. Jotz Lean", "abstract": "This paper shows the equivalence of the categories of $N$-manifolds of degree\n$2$ with the category of double vector bundles endowed with a linear metric.\nSplit Poisson $N$-manifolds of degree $2$ are shown to be equivalent to\nself-dual representations up to homotopy. As a consequence, the equivalence\nabove induces an equivalence between so called metric VB-algebroids and Poisson\n$N$-manifolds of degree $2$.\n  Then a new description of split Lie $2$-algebroids is given, as well as their\n\"duals\", the Dorfman $2$-representations. We show that Dorfman\n$2$-representations are equivalent in a simple manner to Lagrangian splittings\nof VB-Courant algebroids. This yields the equivalence of the categories of Lie\n$2$-algebroids and of VB-Courant algebroids. We give several natural classes of\nexamples of split Lie $2$-algebroids and of the corresponding VB-Courant\nalgebroids.\n  We then show that a split Poisson Lie $2$-algebroid is equivalent to the\n\"matched pair\" of a Dorfman $2$-representation with a self-dual representation\nup to homotopy. We deduce a new proof of the equivalence of categories of\nLA-Courant algebroids and Poisson Lie $2$-algebroids. We show that the core of\nan LA-Courant algebroid inherits naturally the structure of a degenerate\nCourant algebroid. This yields a new formula to retrieve in a direct manner the\nCourant algebroid found by Roytenberg to correspond to a symplectic Lie\n$2$-algebroid.\n  Finally we study VB- and LA-Dirac structures in VB- and LA-Courant\nalgebroids. As an application, we extend Li-Bland's results on pseudo-Dirac\nstructures and we construct a Manin pair associated to an LA-Dirac structure.", "journal": ""}
{"doi": "10.48550/arXiv.2206.12825", "date": "2022-06-26", "title": "Hardness of Interval Scheduling on Unrelated Machines", "authors": "Danny Hermelin, Yuval Itzhaki, Hendrik Molter, Dvir Shabtay", "abstract": "We provide new (parameterized) computational hardness results for Interval\nScheduling on Unrelated Machines. It is a classical scheduling problem\nmotivated from just-in-time or lean manufacturing, where the goal is to\ncomplete jobs exactly at their deadline. We are given $n$ jobs and $m$\nmachines. Each job has a deadline, a weight, and a processing time that may be\ndifferent on each machine. The goal is find a schedule that maximized the total\nweight of jobs completed exactly at their deadline. Note that this uniquely\ndefines a processing time interval for each job on each machine.\n  Interval Scheduling on Unrelated Machines is closely related to coloring\ninterval graphs and has been thoroughly studied for several decades. However,\nas pointed out by Mnich and van Bevern [Computers \\& Operations Research,\n2018], the parameterized complexity for the number $m$ of machines as a\nparameter remained open. We resolve this by showing that Interval Scheduling on\nUnrelated Machines is W[1]-hard when parameterized by the number $m$ of\nmachines. To this end, we prove W[1]-hardness with respect to $m$ of the\nspecial case where we have parallel machines with eligible machine sets for\njobs. This answers Open Problem 8 of Mnich and van Bevern's list of 15 open\nproblems in the parameterized complexity of scheduling [Computers \\& Operations\nResearch, 2018].\n  Furthermore, we resolve the computational complexity status of the unweighted\nversion of Interval Scheduling on Unrelated Machines by proving that it is\nNP-complete. This answers an open question by Sung and Vlach [Journal of\nScheduling, 2005].", "journal": ""}
{"doi": "10.48550/arXiv.2310.01998", "date": "2023-10-03", "title": "A Formalization of Complete Discrete Valuation Rings and Local Fields", "authors": "Mar\u00eda In\u00e9s de Frutos-Fern\u00e1ndez, Filippo Alberto Edoardo Nuccio Mortarino Majno Di Capriglio", "abstract": "Local fields, and fields complete with respect to a discrete valuation, are\nessential objects in commutative algebra, with applications to number theory\nand algebraic geometry. We formalize in Lean the basic theory of discretely\nvalued fields. In particular, we prove that the unit ball with respect to a\ndiscrete valuation on a field is a discrete valuation ring and, conversely,\nthat the adic valuation on the field of fractions of a discrete valuation ring\nis discrete. We define finite extensions of valuations and of discrete\nvaluation rings, and prove some global-to-local results. Building on this\ngeneral theory, we formalize the abstract definition and some fundamental\nproperties of local fields. As an application, we show that finite extensions\nof the field $\\mathbb{Q}_p$ of $p$-adic numbers and of the field\n$\\mathbb{F}_p(\\!(X)\\!)$ of Laurent series over $\\mathbb{F}_p$ are local fields.", "journal": ""}
{"doi": "10.48550/arXiv.2311.14347", "date": "2023-11-24", "title": "Typed compositional quantum computation with lenses", "authors": "Jacques Garrigue, Takafumi Saikawa", "abstract": "We propose a type-theoretic framework for describing and proving properties\nof quantum computations, in particular those presented as quantum circuits. Our\nproposal is based on an observation that, in the polymorphic type system of\nCoq, currying on quantum states allows us to apply quantum gates directly\ninside a complex circuit. By introducing a discrete notion of lens to control\nthis currying, we are further able to separate the combinatorics of the circuit\nstructure from the computational content of gates. We apply our development to\ndefine quantum circuits recursively from the bottom up, and prove their\ncorrectness compositionally.", "journal": ""}
{"doi": "10.48550/arXiv.1210.0755", "date": "2012-10-02", "title": "On fractional Schr\u00f6dinger equations in (\\mathbb{R}^N) without the Ambrosetti-Rabinowitz condition", "authors": "Simone Secchi", "abstract": "In this note we prove the existence of radially symmetric solutions for a\nclass of fractional Schr\\\"odinger equation in (\\mathbb{R}^N) of the form\n{equation*}\n  \\slap u + V(x) u = g(u), {equation*} where the nonlinearity $g$ does not\nsatisfy the usual Ambrosetti-Rabinowitz condition. Our approach is variational\nin nature, and leans on a Pohozaev identity for the fractional laplacian.", "journal": ""}
{"doi": "10.48550/arXiv.1809.01484", "date": "2018-09-05", "title": "Multiple vector bundles: cores, splittings and decompositions", "authors": "Malte Heuer, Madeleine Jotz Lean", "abstract": "This paper introduces $\\infty$- and $n$-fold vector bundles as special\nfunctors from the $\\infty$- and $n$-cube categories to the category of smooth\nmanifolds. We study the cores and \"n-pullbacks\" of $n$-fold vector bundles and\nwe prove that any $n$-fold vector bundle admits a non-canonical isomorphism to\na decomposed $n$-fold vector bundle. A colimit argument then shows that\n$\\infty$-fold vector bundles admit as well non-canonical decompositions. For\nthe convenience of the reader, the case of triple vector bundles is discussed\nin detail.", "journal": ""}
{"doi": "10.48550/arXiv.2106.12391", "date": "2021-06-23", "title": "On the Moore-Gibson-Thompson equation with memory with nonconvex kernels", "authors": "Monica Conti, Lorenzo Liverani, Vittorino Pata", "abstract": "We consider the MGT equation with memory $$\\partial_{ttt} u + \\alpha\n\\partial_{tt} u - \\beta \\Delta \\partial_{t} u - \\gamma\\Delta u +\n\\int_{0}^{t}g(s) \\Delta u(t-s) ds = 0.$$ We prove an existence and uniqueness\nresult removing the convexity assumption on the convolution kernel $g$, usually\nadopted in the literature. In the subcritical case $\\alpha\\beta>\\gamma$, we\nestablish the exponential decay of the energy, without leaning on the classical\ndifferential inequality involving $g$ and its derivative $g'$, namely,\n$$g'+\\delta g\\leq 0,\\quad\\delta>0,$$ but only asking that $g$ vanishes\nexponentially fast.", "journal": ""}
{"doi": "10.48550/arXiv.2305.19851", "date": "2023-05-31", "title": "A geometrisation of $\\mathbb N$-manifolds", "authors": "Malte Heuer, Madeleine Jotz", "abstract": "This paper proposes a geometrisation of $\\mathbb N$-manifolds of degree $n$\nas $n$-fold vector bundles equipped with a (signed) $S_n$-symmetry.\n  More precisely, it proves an equivalence between the categories of\n$[n]$-manifolds and the category of symmetric $n$-fold vector bundles, by\nfinding that symmetric $n$-fold vector bundle cocycles and $[n]$-manifold\ncocycles are identical.\n  This extends the already known equivalences of $[1]$-manifolds with vector\nbundles, and of $[2]$-manifolds with involutive double vector bundles, where\nthe involution is understood as an $S_2$-action.", "journal": ""}
{"doi": "10.48550/arXiv.2402.18585", "date": "2024-01-15", "title": "The algebraic entropies of the Leavitt path algebra and the graph algebras agree", "authors": "Wolfgang Bock, Crist\u00f3bal Gil Canto, Dolores Mart\u00edn Barquero, C\u00e1ndido Mart\u00edn Gonz\u00e1lez, Iv\u00e1n Ruiz Campos, Alfilgen Sebandal", "abstract": "In this note we prove that the algebras $L_K(E)$ and $KE$ have the same\nentropy. Entropy is always referred to the standard filtrations in the\ncorresponding kind of algebra. The main argument leans on (1) the holomorphic\nfunctional calculus; (2) the relation of entropy with suitable norm of the\nadjacency matrix; and (3) the Cohn path algebras which yield suitable bounds\nfor the algebraic entropies.", "journal": ""}
{"doi": "10.48550/arXiv.2304.07648", "date": "2023-04-15", "title": "Certifying Zero-Knowledge Circuits with Refinement Types", "authors": "Junrui Liu, Ian Kretz, Hanzhi Liu, Bryan Tan, Jonathan Wang, Yi Sun, Luke Pearson, Anders Miltner, I\u015f\u0131l Dillig, Yu Feng", "abstract": "Zero-knowledge (ZK) proof systems have emerged as a promising solution for\nbuilding security-sensitive applications. However, bugs in ZK applications are\nextremely difficult to detect and can allow a malicious party to silently\nexploit the system without leaving any observable trace. This paper presents\nCoda, a novel statically-typed language for building zero-knowledge\napplications. Critically, Coda makes it possible to formally specify and\nstatically check properties of a ZK application through a rich refinement type\nsystem. One of the key challenges in formally verifying ZK applications is that\nthey require reasoning about polynomial equations over large prime fields that\ngo beyond the capabilities of automated theorem provers. Coda mitigates this\nchallenge by generating a set of Coq lemmas that can be proven in an\ninteractive manner with the help of a tactic library. We have used Coda to\nre-implement 79 arithmetic circuits from widely-used Circom libraries and\napplications. Our evaluation shows that Coda makes it possible to specify\nimportant and formally verify correctness properties of these circuits. Our\nevaluation also revealed 6 previously-unknown vulnerabilities in the original\nCircom projects.", "journal": ""}
{"doi": "10.48550/arXiv.2503.00144", "date": "2025-02-28", "title": "Learner and Instructor Needs in AI-Supported Programming Learning Tools: Design Implications for Features and Adaptive Control", "authors": "Zihan Wu, Yicheng Tang, Barbara Ericson", "abstract": "AI-supported tools can help learners overcome challenges in programming\neducation by providing adaptive assistance. However, existing research often\nfocuses on individual tools rather than deriving broader design\nrecommendations. A key challenge in designing these systems is balancing\nlearner control with system-driven guidance. To explore user preferences for\nAI-supported programming learning tools, we conducted a participatory design\nstudy with 15 undergraduate novice programmers and 10 instructors to gather\ninsights on their desired help features and control preferences, as well as a\nfollow-up survey with 172 introductory programming students.\n  Our qualitative findings show that learners prefer help that is encouraging,\nincorporates visual aids, and includes peer-related insights, whereas\ninstructors prioritize scaffolding that reflects learners' progress and\nreinforces best practices. Both groups favor shared control, though learners\ngenerally prefer more autonomy, while instructors lean toward greater system\nguidance to prevent cognitive overload. Additionally, our interviews revealed\nindividual differences in control preferences.\n  Based on our findings, we propose design guidelines for AI-supported\nprogramming tools, particularly regarding user-centered help features and\nadaptive control mechanisms. Our work contributes to the human-centered design\nof AI-supported learning environments by informing the development of systems\nthat effectively balance autonomy and guidance, enhancing AI-supported\neducational tools for programming and beyond.", "journal": ""}
{"doi": "10.48550/arXiv.2305.11282", "date": "2023-05-18", "title": "Statistical Estimation for Covariance Structures with Tail Estimates using Nodewise Quantile Predictive Regression Models", "authors": "Christis Katsouris", "abstract": "This paper considers the specification of covariance structures with tail\nestimates. We focus on two aspects: (i) the estimation of the VaR-CoVaR risk\nmatrix in the case of larger number of time series observations than assets in\na portfolio using quantile predictive regression models without assuming the\npresence of nonstationary regressors and; (ii) the construction of a novel\nvariable selection algorithm, so-called, Feature Ordering by Centrality\nExclusion (FOCE), which is based on an assumption-lean regression framework,\nhas no tuning parameters and is proved to be consistent under general sparsity\nassumptions. We illustrate the usefulness of our proposed methodology with\nnumerical studies of real and simulated datasets when modelling systemic risk\nin a network.", "journal": ""}
{"doi": "10.48550/arXiv.1502.04634", "date": "2015-02-16", "title": "The exp-log normal form of types", "authors": "Danko Ilik", "abstract": "Lambda calculi with algebraic data types lie at the core of functional\nprogramming languages and proof assistants, but conceal at least two\nfundamental theoretical problems already in the presence of the simplest\nnon-trivial data type, the sum type. First, we do not know of an explicit and\nimplemented algorithm for deciding the beta-eta-equality of terms---and this in\nspite of the first decidability results proven two decades ago. Second, it is\nnot clear how to decide when two types are essentially the same, i.e.\nisomorphic, in spite of the meta-theoretic results on decidability of the\nisomorphism.\n  In this paper, we present the exp-log normal form of types---derived from the\nrepresentation of exponential polynomials via the unary exponential and\nlogarithmic functions---that any type built from arrows, products, and sums,\ncan be isomorphically mapped to. The type normal form can be used as a simple\nheuristic for deciding type isomorphism, thanks to the fact that it is a\nsystematic application of the high-school identities.\n  We then show that the type normal form allows to reduce the standard beta-eta\nequational theory of the lambda calculus to a specialized version of itself,\nwhile preserving the completeness of equality on terms. We end by describing an\nalternative representation of normal terms of the lambda calculus with sums,\ntogether with a Coq-implemented converter into/from our new term calculus. The\ndifference with the only other previously implemented heuristic for deciding\ninteresting instances of eta-equality by Balat, Di Cosmo, and Fiore, is that we\nexploit the type information of terms substantially and this often allows us to\nobtain a canonical representation of terms without performing sophisticated\nterm analyses.", "journal": "POPL 2017 Proceedings of the 44th ACM SIGPLAN Symposium on\n  Principles of Programming Languages. Pages 387-399. Paris, France -- January\n  15 - 21, 2017"}
{"doi": "10.48550/arXiv.2105.06319", "date": "2021-05-13", "title": "The Inductive Approach to Verifying Cryptographic Protocols", "authors": "Lawrence C. Paulson", "abstract": "Informal arguments that cryptographic protocols are secure can be made\nrigorous using inductive definitions. The approach is based on ordinary\npredicate calculus and copes with infinite-state systems. Proofs are generated\nusing Isabelle/HOL. The human effort required to analyze a protocol can be as\nlittle as a week or two, yielding a proof script that takes a few minutes to\nrun.\n  Protocols are inductively defined as sets of traces. A trace is a list of\ncommunication events, perhaps comprising many interleaved protocol runs.\nProtocol descriptions incorporate attacks and accidental losses. The model spy\nknows some private keys and can forge messages using components decrypted from\nprevious traffic. Three protocols are analyzed below: Otway-Rees (which uses\nshared-key encryption), Needham-Schroeder (which uses public-key encryption),\nand a recursive protocol by Bull and Otway (which is of variable length).\n  One can prove that event $ev$ always precedes event $ev'$ or that property\n$P$ holds provided $X$ remains secret. Properties can be proved from the\nviewpoint of the various principals: say, if $A$ receives a final message from\n$B$ then the session key it conveys is good.", "journal": "J. Computer Security 6 (1998), 85-128"}
{"doi": "10.48550/arXiv.2001.01840", "date": "2020-01-06", "title": "A Conceptual Paper on SERVQUAL-Framework for Assessing Quality of Internet of Things (IoT) Services", "authors": "Sheikh Muhammad Hizam, Waqas Ahmed", "abstract": "Service quality possesses the vital prominence in usability of innovative\nproducts and services. As technological innovation has made the life\nsynchronized and effective, Internet of Things (IoT) is matter of discussion\neverywhere. From users' perspective, IoT services are always embraced by\nvarious system characteristics of security and performance. A service quality\nmodel can better present the preference of such technology customers. the study\nintends to project theoretical model of service quality for internet of things\n(IoT). Based on the existing models of service quality and the literature in\ninternet of things, a framework is proposed to conceptualize and measure\nservice quality for internet of things.This study established the IoT-Servqual\nmodel with four dimensions (i.e., Privacy, Functionality, Efficiency, and\nTangibility) of multiple service quality models. These dimensions are essential\nand inclined towards the users' leaning of IoT Services. This paper contributes\nto research on internet of things services by development of a comprehensive\nframework for customers' quality apprehension. This model will previse the\nexpression of information secrecy of users related with internet of things\n(IoT). This research will advance understanding of service quality in modern\nday technology and assist firms to devise the fruitful service structure.", "journal": "International Journal of Financial Research, Vol. 10, No. 5,\n  Special Issue; 2019"}
{"doi": "10.48550/arXiv.1502.07634", "date": "2015-02-26", "title": "A finite basis theorem for the description logic ${\\cal ALC}$", "authors": "Marc Aiguier, Jamal Atif, Isabelle Bloch, C\u00e9line Hudelot", "abstract": "The main result of this paper is to prove the existence of a finite basis in\nthe description logic ${\\cal ALC}$. We show that the set of General Concept\nInclusions (GCIs) holding in a finite model has always a finite basis, i.e.\nthese GCIs can be derived from finitely many of the GCIs. This result extends a\nprevious result from Baader and Distel, which showed the existence of a finite\nbasis for GCIs holding in a finite model but for the inexpressive description\nlogics ${\\cal EL}$ and ${\\cal EL}_{gfp}$. We also provide an algorithm for\ncomputing this finite basis, and prove its correctness. As a byproduct, we\nextend our finite basis theorem to any finitely generated complete covariety\n(i.e. any class of models closed under morphism domain, coproduct and quotient,\nand generated from a finite set of finite models).", "journal": ""}
{"doi": "10.48550/arXiv.1411.7139", "date": "2014-11-26", "title": "Certification of programs with computational effects", "authors": "Burak Ekici", "abstract": "In purely functional programming languages imperative features, more\ngenerally computational effects are prohibited. However, non-functional lan-\nguages do involve effects. The theory of decorated logic provides a rigorous\nfor- malism (with a refinement in operation signatures) for proving program\nproperties with respect to computational effects. The aim of this thesis is to\nfirst develop Coq libraries and tools for verifying program properties in\ndecorated settings as- sociated with several effects: states, local state,\nexceptions, non-termination, etc. Then, these tools will be combined to deal\nwith several effects.", "journal": ""}
{"doi": "10.48550/arXiv.1606.02941", "date": "2016-06-09", "title": "A Proof Strategy Language and Proof Script Generation for Isabelle/HOL", "authors": "Yutaka Nagashima, Ramana Kumar", "abstract": "We introduce a language, PSL, designed to capture high level proof strategies\nin Isabelle/HOL. Given a strategy and a proof obligation, PSL's runtime system\ngenerates and combines various tactics to explore a large search space with low\nmemory usage. Upon success, PSL generates an efficient proof script, which\nbypasses a large part of the proof search. We also present PSL's monadic\ninterpreter to show that the underlying idea of PSL is transferable to other\nITPs.", "journal": ""}
{"doi": "10.48550/arXiv.2412.04864", "date": "2024-12-06", "title": "Machine Checked Proofs and Programs in Algebraic Combinatorics", "authors": "Florent Hivert", "abstract": "We present a library of formalized results around symmetric functions and the\ncharacter theory of symmetric groups. Written in Coq/Rocq and based on the\nMathematical Components library, it covers a large part of the contents of a\ngraduate level textbook in the field. The flagship result is a proof of the\nLittlewood-Richardson rule, which computes the structure constants of the\nalgebra of symmetric function in the schur basis which are integer numbers\nappearing in various fields of mathematics, and which has a long history of\nwrong proofs. A specific feature of algebraic combinatorics is the constant\ninterplay between algorithms and algebraic constructions: algorithms are not\nonly in computations, but also are key ingredients in definitions and proofs.\nAs such, the proof of the Littlewood-Richardson rule deeply relies on the\nunderstanding of the execution of the Robinson-Schensted algorithm. Many\nresults in this library are effective and actually used in computer algebra\nsystems, and we discuss their certified implementation.", "journal": "CPP 2025, Proceedings of the 14th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs"}
{"doi": "10.48550/arXiv.0305408", "date": "2003-05-28", "title": "Mathematical analysis of a nonlinear parabolic equation arising in the modelling of non-newtonian flows", "authors": "Eric Canc\u00e8s, Isabelle Catto, Yousra Gati", "abstract": "The mathematical properties of a nonlinear parabolic equation arising in the\nmodelling of non-newtonian flows are investigated. The peculiarity of this\nequation is that it may degenerate into a hyperbolic equation (in fact a linear\nadvection equation). Depending on the initial data, at least two situations can\nbe encountered: the equation may have a unique solution in a convenient class,\nor it may have infinitely many solutions.", "journal": ""}
{"doi": "10.48550/arXiv.2312.10301", "date": "2023-12-16", "title": "FCBench: Cross-Domain Benchmarking of Lossless Compression for Floating-Point Data", "authors": "Xinyu Chen, Jiannan Tian, Ian Beaver, Cynthia Freeman, Yan Yan, Jianguo Wang, Dingwen Tao", "abstract": "While both the database and high-performance computing (HPC) communities\nutilize lossless compression methods to minimize floating-point data size, a\ndisconnect persists between them. Each community designs and assesses methods\nin a domain-specific manner, making it unclear if HPC compression techniques\ncan benefit database applications or vice versa. With the HPC community\nincreasingly leaning towards in-situ analysis and visualization, more\nfloating-point data from scientific simulations are being stored in databases\nlike Key-Value Stores and queried using in-memory retrieval paradigms. This\ntrend underscores the urgent need for a collective study of these compression\nmethods' strengths and limitations, not only based on their performance in\ncompressing data from various domains but also on their runtime\ncharacteristics. Our study extensively evaluates the performance of eight\nCPU-based and five GPU-based compression methods developed by both communities,\nusing 33 real-world datasets assembled in the Floating-point Compressor\nBenchmark (FCBench). Additionally, we utilize the roofline model to profile\ntheir runtime bottlenecks. Our goal is to offer insights into these compression\nmethods that could assist researchers in selecting existing methods or\ndeveloping new ones for integrated database and HPC applications.", "journal": ""}
{"doi": "10.48550/arXiv.2202.12662", "date": "2022-02-25", "title": "Validating Labelled State Transition and Message Production Systems: A Theory for Modelling Faulty Distributed Systems", "authors": "Vlad Zamfir, Mihai Calancea, Denisa Diaconescu, Wojciech Ko\u0142owski, Brandon Moore, Karl Palmskog, Traian Florin \u015eerb\u0103nu\u0163\u0103, Michael Stay, Dafina Trufa\u015f, Jan Tu\u0161il", "abstract": "Modeling and formally reasoning about distributed systems with faults is a\nchallenging task. To address this problem, we propose the theory of Validating\nLabeled State transition and Message production systems (VLSMs). The theory of\nVLSMs provides a general approach to describing and verifying properties of\ndistributed protocols whose executions are subject to faults, supporting a\ncorrect-by-construction system development methodology. The central focus of\nour investigation is equivocation, a mode of faulty behavior that we formally\nmodel, reason about, and then show how to detect from durable evidence that may\nbe available locally to system components. Equivocating components exhibit\nbehavior that is inconsistent with single-trace system executions, while also\nonly interacting with other components by sending and receiving valid messages.\nComponents of system are called validators for that system if their validity\nconstraints validate that the messages they receive are producible by the\nsystem. Our main result shows that for systems of validators, the effect that\nByzantine components can have on honest validators is precisely identical to\nthe effect that equivocating components can have on non-equivocating\nvalidators. Therefore, for distributed systems of potentially faulty\nvalidators, replacing Byzantine components with equivocating components has no\nmaterial analytical consequences, and forms the basis of a sound alternative\nfoundation to Byzantine fault tolerance analysis. All of the results and\nexamples in the paper have been formalised and checked in the Coq proof\nassistant.", "journal": ""}
{"doi": "10.48550/arXiv.1611.01337", "date": "2016-11-04", "title": "Mechanically Proving Determinacy of Hierarchical Block Diagram Translations", "authors": "Viorel Preoteasa, Iulia Dragomir, Stavros Tripakis", "abstract": "Hierarchical block diagrams (HBDs) are at the heart of embedded system design\ntools, including Simulink. Numerous translations exist from HBDs into languages\nwith formal semantics, amenable to formal verification. However, none of these\ntranslations has been proven correct, to our knowledge. We present in this\npaper the first mechanically proven HBD translation algorithm. The algorithm\ntranslates HBDs into an algebra of terms with three basic composition\noperations (serial, parallel, and feedback). In order to capture various\ntranslation strategies resulting in different terms achieving different\ntradeoffs, the algorithm is nondeterministic. Despite this, we prove its\nsemantic determinacy: for every input HBD, all possible terms that can be\ngenerated by the algorithm are semantically equivalent. We apply this result to\nshow how three Simulink translation strategies introduced previously can be\nformalized as determinizations of the algorithm, and derive that these\nstrategies yield semantically equivalent results (a question left open in\nprevious work). All results are formalized and proved in the Isabelle\ntheorem-prover.", "journal": ""}
{"doi": "10.48550/arXiv.2002.10803", "date": "2020-02-25", "title": "A Type Checker for a Logical Framework with Union and Intersection Types", "authors": "Luigi Liquori, Claude Stolze", "abstract": "We present the syntax, semantics, and typing rules of Bull, a prototype\ntheorem prover based on the Delta-Framework, i.e. a fully-typed lambda-calculus\ndecorated with union and intersection types, as described in previous papers by\nthe authors. Bull also implements a subtyping algorithm for the Type Theory Xi\nof Barbanera-Dezani-de'Liguoro. Bull has a command-line interface where the\nuser can declare axioms, terms, and perform computations and some basic\nterminal-style features like error pretty-printing, subexpressions\nhighlighting, and file loading. Moreover, it can typecheck a proof or normalize\nit. These terms can be incomplete, therefore the typechecking algorithm uses\nunification to try to construct the missing subterms. Bull uses the syntax of\nBerardi's Pure Type Systems to improve the compactness and the modularity of\nthe kernel. Abstract and concrete syntax are mostly aligned and similar to the\nconcrete syntax of Coq. Bull uses a higher-order unification algorithm for\nterms, while typechecking and partial type inference are done by a\nbidirectional refinement algorithm, similar to the one found in Matita and\nBeluga. The refinement can be split into two parts: the essence refinement and\nthe typing refinement. Binders are implemented using commonly-used de Bruijn\nindices. We have defined a concrete language syntax that will allow the user to\nwrite Delta-terms. We have defined the reduction rules and an evaluator. We\nhave implemented from scratch a refiner which does partial typechecking and\ntype reconstruction. We have experimented Bull with classical examples of the\nintersection and union literature, such as the ones formalized by Pfenning with\nhis Refinement Types in LF. We hope that this research vein could be useful to\nexperiment, in a proof theoretical setting, forms of polymorphism alternatives\nto Girard's parametric one.", "journal": ""}
{"doi": "10.48550/arXiv.1108.4368", "date": "2011-08-22", "title": "Formalization of Abstract State Transition Systems for SAT", "authors": "Filip Maric, Predrag Janicic", "abstract": "We present a formalization of modern SAT solvers and their properties in a\nform of abstract state transition systems. SAT solving procedures are described\nas transition relations over states that represent the values of the solver's\nglobal variables. Several different SAT solvers are formalized, including both\nthe classical DPLL procedure and its state-of-the-art successors. The\nformalization is made within the Isabelle/HOL system and the total correctness\n(soundness, termination, completeness) is shown for each presented system (with\nrespect to a simple notion of satisfiability that can be manually checked). The\nsystems are defined in a general way and cover procedures used in a wide range\nof modern SAT solvers. Our formalization builds up on the previous work on\nstate transition systems for SAT, but it gives machine-verifiable proofs,\nsomewhat more general specifications, and weaker assumptions that ensure the\nkey correctness properties. The presented proofs of formal correctness of the\ntransition systems can be used as a key building block in proving correctness\nof SAT solvers by using other verification approaches.", "journal": "Logical Methods in Computer Science, Volume 7, Issue 3 (September\n  28, 2011) lmcs:843"}
{"doi": "10.48550/arXiv.1510.03531", "date": "2015-10-13", "title": "A Program Logic for Verifying Secure Routing Protocols", "authors": "Chen Chen, Limin Jia, Hao Xu, Cheng Luo, Wenchao Zhou, Boon Thau Loo", "abstract": "The Internet, as it stands today, is highly vulnerable to attacks. However,\nlittle has been done to understand and verify the formal security guarantees of\nproposed secure inter-domain routing protocols, such as Secure BGP (S-BGP). In\nthis paper, we develop a sound program logic for SANDLog-a declarative\nspecification language for secure routing protocols for verifying properties of\nthese protocols. We prove invariant properties of SANDLog programs that run in\nan adversarial environment. As a step towards automated verification, we\nimplement a verification condition generator (VCGen) to automatically extract\nproof obligations. VCGen is integrated into a compiler for SANDLog that can\ngenerate executable protocol implementations; and thus, both verification and\nempirical evaluation of secure routing protocols can be carried out in this\nunified framework. To validate our framework, we encoded several proposed\nsecure routing mechanisms in SANDLog, verified variants of path authenticity\nproperties by manually discharging the generated verification conditions in\nCoq, and generated executable code based on SANDLog specification and ran the\ncode in simulation.", "journal": "Logical Methods in Computer Science, Volume 11, Issue 4 (December\n  29, 2015) lmcs:1620"}
{"doi": "10.48550/arXiv.2012.10313", "date": "2020-12-18", "title": "Towards Formally Verified Compilation of Tag-Based Policy Enforcement", "authors": "CHR Chhak, Andrew Tolmach, Sean Anderson", "abstract": "Hardware-assisted reference monitoring is receiving increasing attention as a\nway to improve the security of existing software. One example is the PIPE\narchitecture extension, which attaches metadata tags to register and memory\nvalues and executes tag-based rules at each machine instruction to enforce a\nsoftware-defined security policy. To use PIPE effectively, engineers should be\nable to write security policies in terms of source-level concepts like\nfunctions, local variables, and structured control operators, which are not\nvisible at machine level. It is the job of the compiler to generate PIPE-aware\nmachine code that enforces these source-level policies. The compiler thus\nbecomes part of the monitored system's trusted computing base -- and hence a\nprime candidate for verification.\n  To formalize compiler correctness in this setting, we extend the source\nlanguage semantics with its own form of user-specified tag-based monitoring,\nand show that the compiler preserves that monitoring behavior. The challenges\nof compilation include mapping source-level monitoring policies to\ninstruction-level tag rules, preserving fail-stop behaviors, and satisfying the\nsurprisingly complex preconditions for conventional optimizations. In this\npaper, we describe the design and verification of Tagine, a small prototype\ncompiler that translates a simple tagged WHILE language to a tagged register\ntransfer language and performs simple optimizations. Tagine is based on the\nRTLgen and Deadcode phases of the CompCert compiler, and hence is written and\nverified in Coq. This work is a first step toward verification of a full-scale\ncompiler for a realistic tagged source language.", "journal": ""}
{"doi": "10.48550/arXiv.2405.12187", "date": "2024-05-20", "title": "Brewer-Nash Scrutinised: Mechanised Checking of Policies featuring Write Revocation", "authors": "Alfredo Capozucca, Maximiliano Cristi\u00e1, Ross Horne, Ricardo Katz", "abstract": "This paper revisits the Brewer-Nash security policy model inspired by ethical\nChinese Wall policies. We draw attention to the fact that write access can be\nrevoked in the Brewer-Nash model. The semantics of write access were\nunderspecified originally, leading to multiple interpretations for which we\nprovide a modern operational semantics. We go on to modernise the analysis of\ninformation flow in the Brewer-Nash model, by adopting a more precise\ndefinition adapted from Kessler. For our modernised reformulation, we provide\nfull mechanised coverage for all theorems proposed by Brewer & Nash. Most\ntheorems are established automatically using the tool {log} with the exception\nof a theorem regarding information flow, which combines a lemma in {log} with a\ntheorem mechanised in Coq. Having covered all theorems originally posed by\nBrewer-Nash, achieving modern precision and mechanisation, we propose this work\nas a step towards a methodology for automated checking of more complex security\npolicy models.", "journal": ""}
{"doi": "10.48550/arXiv.1907.11454", "date": "2019-07-26", "title": "Using 3D Convolutional Neural Networks to Learn Spatiotemporal Features for Automatic Surgical Gesture Recognition in Video", "authors": "Isabel Funke, Sebastian Bodenstedt, Florian Oehme, Felix von Bechtolsheim, J\u00fcrgen Weitz, Stefanie Speidel", "abstract": "Automatically recognizing surgical gestures is a crucial step towards a\nthorough understanding of surgical skill. Possible areas of application include\nautomatic skill assessment, intra-operative monitoring of critical surgical\nsteps, and semi-automation of surgical tasks. Solutions that rely only on the\nlaparoscopic video and do not require additional sensor hardware are especially\nattractive as they can be implemented at low cost in many scenarios. However,\nsurgical gesture recognition based only on video is a challenging problem that\nrequires effective means to extract both visual and temporal information from\nthe video. Previous approaches mainly rely on frame-wise feature extractors,\neither handcrafted or learned, which fail to capture the dynamics in surgical\nvideo. To address this issue, we propose to use a 3D Convolutional Neural\nNetwork (CNN) to learn spatiotemporal features from consecutive video frames.\nWe evaluate our approach on recordings of robot-assisted suturing on a\nbench-top model, which are taken from the publicly available JIGSAWS dataset.\nOur approach achieves high frame-wise surgical gesture recognition accuracies\nof more than 84%, outperforming comparable models that either extract only\nspatial features or model spatial and low-level temporal information\nseparately. For the first time, these results demonstrate the benefit of\nspatiotemporal CNNs for video-based surgical gesture recognition.", "journal": ""}
{"doi": "10.48550/arXiv.1611.07610", "date": "2016-11-23", "title": "Mutable WadlerFest DOT", "authors": "Marianna Rapoport, Ond\u0159ej Lhot\u00e1k", "abstract": "The Dependent Object Types (DOT) calculus aims to model the essence of Scala,\nwith a focus on abstract type members, path-dependent types, and subtyping.\nOther Scala features could be defined by translation to DOT. Mutation is a\nfundamental feature of Scala currently missing in DOT. Mutation in DOT is\nneeded not only to model effectful computation and mutation in Scala programs,\nbut even to precisely specify how Scala initializes immutable variables and\nfields (vals). We present an extension to DOT that adds typed mutable reference\ncells. We have proven the extension sound with a mechanized proof in Coq. We\npresent the key features of our extended calculus and its soundness proof, and\ndiscuss the challenges that we encountered in our search for a sound design and\nthe alternative solutions that we considered.", "journal": ""}
{"doi": "10.48550/arXiv.2210.09476", "date": "2022-10-17", "title": "Contextuality in distributed systems", "authors": "Nasos Evangelou-Oost, Callum Bannister, Ian J. Hayes", "abstract": "We present a lattice of distributed program specifications, whose ordering\nrepresents implementability/refinement. Specifications are modelled by families\nof subsets of relative execution traces, which encode the local orderings of\nstate transitions, rather than their absolute timing according to a global\nclock. This is to overcome fundamental physical difficulties with\nsynchronisation. The lattice of specifications is assembled and analysed with\nseveral established mathematical tools. Sets of nondegenerate cells of a\nsimplicial set are used to model relative traces, presheaves model the\nparametrisation of these traces by a topological space of variables, and\ninformation algebras reveal novel constraints on program correctness. The\nlatter aspect brings the enterprise of program specification under the widening\numbrella of contextual semantics introduced by Abramsky et al. In this model of\nprogram specifications, contextuality manifests as a failure of a consistency\ncriterion comparable to Lamport's definition of sequential consistency. The\ntheory of information algebras also suggests efficient local computation\nalgorithms for the verification of this criterion. The novel constructions in\nthis paper have been verified in the proof assistant Isabelle/HOL.", "journal": "In: Relational and Algebraic Methods in Computer Science. RAMiCS\n  2023. Lecture Notes in Computer Science, vol 13896. Springer, Cham (2023)"}
{"doi": "10.48550/arXiv.2302.07629", "date": "2023-02-15", "title": "Automated Reasoning for Physical Quantities, Units, and Measurements in Isabelle/HOL", "authors": "Simon Foster, Burkhart Wolff", "abstract": "Formal verification of cyber-physical and robotic systems requires that we\ncan accurately model physical quantities that exist in the real-world. The use\nof explicit units in such quantities can allow a higher degree of rigour, since\nwe can ensure compatibility of quantities in calculations. At the same time,\nimproper use of units can be a barrier to safety and therefore it is highly\ndesirable to have automated sanity checking in physical calculations. In this\npaper, we contribute a mechanisation of the International System of Quantities\n(ISQ) and the associated SI unit system in Isabelle/HOL. We show how Isabelle\ncan be used to provide a type system for physical quantities, and automated\nproof support. Quantities are parameterised by dimension types, which\ncorrespond to base vectors, and thus only quantities of the same dimension can\nbe equated. Since the underlying \"algebra of quantities\" induces congruences on\nquantity and SI types, specific tactic support is developed to capture these.\nOur construction is validated by a test-set of known equivalences between both\nquantities and SI units. Moreover, the presented theory can be used for\ntype-safe conversions between the SI system and others, like the British\nImperial System (BIS).", "journal": ""}
{"doi": "10.48550/arXiv.2102.00378", "date": "2021-01-31", "title": "Model-Based Testing of Networked Applications", "authors": "Yishuai Li, Benjamin C. Pierce, Steve Zdancewic", "abstract": "We present a principled automatic testing framework for application-layer\nprotocols. The key innovation is a domain-specific embedded language for\nwriting nondeterministic models of the behavior of networked servers. These\nmodels are defined within the Coq interactive theorem prover, supporting a\nsmooth transition from testing to formal verification.\n  Given a server model, we show how to automatically derive a tester that\nprobes the server for unexpected behaviors. We address the uncertainties caused\nby both the server's internal choices and the network delaying messages\nnondeterministically. The derived tester accepts server implementations whose\npossible behaviors are a subset of those allowed by the nondeterministic model.\n  We demonstrate the effectiveness of this framework by using it to specify and\ntest a fragment of the HTTP/1.1 protocol, showing that the automatically\nderived tester can capture RFC violations in buggy server implementations,\nincluding the latest versions of Apache and Nginx.", "journal": "Proceedings of the 30th ACM SIGSOFT International Symposium on\n  Software Testing and Analysis (ISSTA 2021). Association for Computing\n  Machinery, New York, NY, USA, 529--539"}
{"doi": "10.48550/arXiv.1301.1952", "date": "2013-01-09", "title": "Ohmic Contact Formation Between Metal and AlGaN/GaN Heterostructure via Graphene Insertion", "authors": "Pil Sung Park, Kongara M. Reddy, Digbijoy N. Nath, Zhichao Yang, Nitin P. Padture, Siddharth Rajan", "abstract": "A simple method for the creation of Ohmic contact to 2-D electron gas (2DEG)\nin AlGaN/GaN high electron-mobility transistors (HEMTs) using Cr/Graphene layer\nis demonstrated. A weak temperature dependence of this Ohmic contact observed\nin the range 77 to 300 K precludes thermionic emission or trap-assisted hopping\nas possible carrier-transport mechanisms. It is suggested that the Cr/Graphene\ncombination acts akin to a doped n-type semiconductor in contact with AlGaN/GaN\nheterostructure, and promotes carrier transport along percolating Al-lean paths\nthrough the AlGaN layer. This new use of graphene offers a simple and reliable\nmethod for making Ohmic contacts to AlGaN/GaN heterostructures, circumventing\ncomplex additional processing steps involving high temperatures. These results\ncould have important implications for the fabrication and manufacturing of\nAlGaN/GaN-based microelectronic and optoelectronic devices/sensors of the\nfuture.", "journal": ""}
{"doi": "10.48550/arXiv.2005.11431", "date": "2020-05-23", "title": "LQR-Assisted Whole-Body Control of a Wheeled Bipedal Robot with Kinematic Loops", "authors": "Victor Klemm, Alessandro Morra, Lionel Gulich, Dominik Mannhart, David Rohr, Mina Kamel, Yvain de Viragh, Roland Siegwart", "abstract": "We present a hierarchical whole-body controller leveraging the full rigid\nbody dynamics of the wheeled bipedal robot Ascento. We derive closed-form\nexpressions for the dynamics of its kinematic loops in a way that readily\ngeneralizes to more complex systems. The rolling constraint is incorporated\nusing a compact analytic solution based on rotation matrices. The non-minimum\nphase balancing dynamics are accounted for by including a linear-quadratic\nregulator as a motion task. Robustness when driving curves is increased by\nregulating the lean angle as a function of the zero-moment point. The proposed\ncontroller is computationally lightweight and significantly extends the\nrough-terrain capabilities and robustness of the system, as we demonstrate in\nseveral experiments.", "journal": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.\n  3745-3752, April 2020"}
{"doi": "10.48550/arXiv.2104.04077", "date": "2021-04-01", "title": "Two Truths and a Lie: Exploring Soft Moderation of COVID-19 Misinformation with Amazon Alexa", "authors": "Donald Gover, Filipo Sharevski", "abstract": "In this paper, we analyzed the perceived accuracy of COVID-19 vaccine Tweets\nwhen they were spoken back by a third-party Amazon Alexa skill. We mimicked the\nsoft moderation that Twitter applies to COVID-19 misinformation content in both\nforms of warning covers and warning tags to investigate whether the third-party\nskill could affect how and when users heed these warnings. The results from a\n304-participant study suggest that the spoken back warning covers may not work\nas intended, even when converted from text to speech. We controlled for\nCOVID-19 vaccination hesitancy and political leanings and found that the\nvaccination hesitant Alexa users ignored any type of warning as long as the\nTweets align with their personal beliefs. The politically independent users\ntrusted Alexa less then their politically-laden counterparts and that helped\nthem accurately perceiving truthful COVID-19 information. We discuss soft\nmoderation adaptations for voice assistants to achieve the intended effect of\ncurbing COVID-19 misinformation.", "journal": ""}
{"doi": "10.48550/arXiv.2201.03374", "date": "2022-01-10", "title": "Personal Mobility With Synchronous Trunk-Knee Passive Exoskeleton: Optimizing Human-Robot Energy Transfer", "authors": "Diego Paez-Granados, Hideki Kadone, Modar Hassan, Yang Chen, Kenji Suzuki", "abstract": "We present a personal mobility device for lower-body impaired users through a\nlight-weighted exoskeleton on wheels. On its core, a novel passive exoskeleton\nprovides postural transition leveraging natural body postures with support to\nthe trunk on sit-to-stand and stand-to-sit (STS) transitions by a single gas\nspring as an energy storage unit. We propose a direction-dependent coupling of\nknees and hip joints through a double-pulley wire system, transferring energy\nfrom the torso motion towards balancing the moment load at the knee joint\nactuator. Herewith, the exoskeleton maximizes energy transfer and the\nnaturalness of the user's movement. We introduce an embodied user interface for\nhands-free navigation through a torso pressure sensing with minimal trunk\nrotations, resulting on average $19^{\\circ} \\pm 13^{\\circ}$ on six unimpaired\nusers. We evaluated the design for STS assistance on 11 unimpaired users\nobserving motions and muscle activity during the transitions. Results comparing\nassisted and unassisted STS transitions validated a significant reduction (up\nto $68\\%$ $p<0.01$) at the involved muscle groups. Moreover, we showed it\nfeasible through natural torso leaning movements of $+12^{\\circ}\\pm\n6.5^{\\circ}$ and $- 13.7^{\\circ} \\pm 6.1^{\\circ}$ for standing and sitting,\nrespectively. Passive postural transition assistance warrants further work on\nincreasing its applicability and broadening the user population.", "journal": "IEEE/ASME Transactions on Mechatronics, vol. 27, no. 5, pp.\n  3613-3623, Oct. 2022"}
{"doi": "10.48550/arXiv.1906.11203", "date": "2019-06-24", "title": "A formalisation of the SPARC TSO memory model for multi-core machine code", "authors": "Zhe Hou, David Sanan, Alwen Tiu, Yang Liu, Jin Song Dong", "abstract": "SPARC processors have many applications in mission-critical industries such\nas aviation and space engineering. Hence, it is important to provide formal\nframeworks that facilitate the verification of hardware and software that run\non or interface with these processors. This paper presents the first mechanised\nSPARC Total Store Ordering (TSO) memory model which operates on top of an\nabstract model of the SPARC Instruction Set Architecture (ISA) for multi-core\nprocessors. Both models are specified in the theorem prover Isabelle/HOL. We\nformalise two TSO memory models: one is an adaptation of the axiomatic SPARC\nTSO model, the other is a novel operational TSO model which is suitable for\nverifying execution results. We prove that the operational model is sound and\ncomplete with respect to the axiomatic model. Finally, we give verification\nexamples with two case studies drawn from the SPARCv9 manual.", "journal": ""}
{"doi": "10.48550/arXiv.1810.10826", "date": "2018-10-25", "title": "All-Path Reachability Logic", "authors": "Andrei Stefanescu, Stefan Ciobaca, Radu Mereuta, Brandon Moore, Traian Florin Serbanuta, Grigore Rosu", "abstract": "This paper presents a language-independent proof system for reachability\nproperties of programs written in non-deterministic (e.g., concurrent)\nlanguages, referred to as all-path reachability logic. It derives\npartial-correctness properties with all-path semantics (a state satisfying a\ngiven precondition reaches states satisfying a given postcondition on all\nterminating execution paths). The proof system takes as axioms any\nunconditional operational semantics, and is sound (partially correct) and\n(relatively) complete, independent of the object language. The soundness has\nalso been mechanized in Coq. This approach is implemented in a tool for\nsemantics-based verification as part of the K framework (http://kframework.org)", "journal": "Logical Methods in Computer Science, Volume 15, Issue 2 (April 30,\n  2019) lmcs:4939"}
{"doi": "10.48550/arXiv.2502.20485", "date": "2025-02-27", "title": "Bounded First-Class Universe Levels in Dependent Type Theory", "authors": "Jonathan Chan, Stephanie Weirich", "abstract": "In dependent type theory, being able to refer to a type universe as a term\nitself increases its expressive power, but requires mechanisms in place to\nprevent Girard's paradox from introducing logical inconsistency in the presence\nof type-in-type. The simplest mechanism is a hierarchy of universes indexed by\na sequence of levels, typically the naturals. To improve reusability of\ndefinitions, they can be made level polymorphic, abstracting over level\nvariables and adding a notion of level expressions. For even more expressive\npower, level expressions can be made first-class as terms themselves, and level\npolymorphism is subsumed by dependent functions quantifying over levels.\nFurthermore, bounded level polymorphism provides more expressivity by being\nable to explicitly state constraints on level variables. While semantics for\nfirst-class levels with constraints are known, syntax and typing rules have not\nbeen explicitly written down. Yet pinning down a well-behaved syntax is not\ntrivial; there exist prior type theories with bounded level polymorphism that\nfail to satisfy subject reduction. In this work, we design an explicit syntax\nfor a type theory with bounded first-class levels, parametrized over arbitrary\nwell-founded sets of levels. We prove the metatheoretic properties of subject\nreduction, type safety, consistency, and canonicity, entirely mechanized from\nsyntax to semantics in Lean.", "journal": ""}
{"doi": "10.48550/arXiv.1503.01406", "date": "2015-03-04", "title": "NF is Consistent", "authors": "M. Randall Holmes, Sky Wilshaw", "abstract": "In this paper we will present a proof of the consistency of Quine's set\ntheory \"New Foundations\" (hereinafter NF), so-called after the title of the\n1937 paper in which it was introduced.\n  This version takes the approach of building a model of tangled type theory\nrather than a model of the usual set theory without choice with a tangled web\nof cardinals; further, details of the construction are refined due to\ninteraction with the now complete verification in Lean by the second author.", "journal": ""}
{"doi": "10.48550/arXiv.2012.05776", "date": "2020-12-10", "title": "Multi-Sense Language Modelling", "authors": "Andrea Lekkas, Peter Schneider-Kamp, Isabelle Augenstein", "abstract": "The effectiveness of a language model is influenced by its token\nrepresentations, which must encode contextual information and handle the same\nword form having a plurality of meanings (polysemy). Currently, none of the\ncommon language modelling architectures explicitly model polysemy. We propose a\nlanguage model which not only predicts the next word, but also its sense in\ncontext. We argue that this higher prediction granularity may be useful for end\ntasks such as assistive writing, and allow for more a precise linking of\nlanguage models with knowledge bases. We find that multi-sense language\nmodelling requires architectures that go beyond standard language models, and\nhere propose a structured prediction framework that decomposes the task into a\nword followed by a sense prediction task. To aid sense prediction, we utilise a\nGraph Attention Network, which encodes definitions and example uses of word\nsenses. Overall, we find that multi-sense language modelling is a highly\nchallenging task, and suggest that future work focus on the creation of more\nannotated training datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2307.09088", "date": "2023-07-18", "title": "Properties of periodic Dirac--Fock functional and minimizers", "authors": "Isabelle Catto, Long Meng", "abstract": "Existence of minimizers for the Dirac--Fock model in crystals was recently\nproved by Paturel and S\\'er\\'e and the authors \\cite{crystals} by a retraction\ntechnique due to S\\'er\\'e \\cite{Ser09}. In this paper, inspired by Ghimenti and\nLewin's result \\cite{ghimenti2009properties} for the periodic Hartree--Fock\nmodel, we prove that the Fermi level of any periodic Dirac--Fock minimizer is\neither empty or totally filled when $\\frac{\\alpha}{c}\\leq C_{\\rm cri}$ and\n$\\alpha>0$. Here $c$ is the speed of light, $\\alpha$ is the fine structure\nconstant, and $C_{\\rm cri}$ is a constant only depending on the number of\nelectrons and on the charge of nuclei per cell. More importantly, we provide an\nexplicit upper bound for $C_{\\rm cri}$.\n  Our result implies that any minimizer of the periodic Dirac--Fock model is a\nprojector when $\\frac{\\alpha}{c}\\leq C_{\\rm cri}$ and $\\alpha>0$. In\nparticular, the non-relativistic regime (i.e., $c\\gg1$) and the weak coupling\nregime (i.e., $0<\\alpha\\ll1$) are covered.\n  The proof is based on a delicate study of a second-order expansion of the\nperiodic Dirac--Fock functional composed with the retraction used in\n\\cite{crystals}.", "journal": ""}
{"doi": "10.48550/arXiv.1705.03639", "date": "2017-05-10", "title": "Sparse Interacting Gaussian Processes: Efficiency and Optimality Theorems of Autonomous Crowd Navigation", "authors": "Pete Trautman", "abstract": "We study the sparsity and optimality properties of crowd navigation and find\nthat existing techniques do not satisfy both criteria simultaneously: either\nthey achieve optimality with a prohibitive number of samples or tractability\nassumptions make them fragile to catastrophe. For example, if the human and\nrobot are modeled independently, then tractability is attained but the planner\nis prone to overcautious or overaggressive behavior. For sampling based motion\nplanning of joint human-robot cost functions, for $n_t$ agents and $T$ step\nlookahead, $\\mathcal O(2^{2n_t T})$ samples are needed for coverage of the\naction space. Advanced approaches statically partition the action space into\nfree-space and then sample in those convex regions. However, if the human is\n\\emph{moving} into free-space, then the partition is misleading and sampling is\nunsafe: free space will soon be occupied. We diagnose the cause of these\ndeficiencies---optimization happens over \\emph{trajectory} space---and propose\na novel solution: optimize over trajectory \\emph{distribution} space by using a\nGaussian process (GP) basis. We exploit the \"kernel trick\" of GPs, where a\ncontinuum of trajectories are captured with a mean and covariance function. By\nusing the mean and covariance as proxies for a trajectory family we reason\nabout collective trajectory behavior without resorting to sampling. The GP\nbasis is sparse and optimal with respect to collision avoidance and robot and\ncrowd intention and flexibility. GP sparsity leans heavily on the insight that\njoint action space decomposes into free regions; however, the decomposition\ncontains feasible solutions only if the partition is dynamically generated. We\ncall our approach \\emph{$\\mathcal O(2^{n_t})$-sparse interacting Gaussian\nprocesses}.", "journal": ""}
{"doi": "10.48550/arXiv.2402.12548", "date": "2024-02-19", "title": "Composite likelihood inference for space-time point processes", "authors": "Abdollah Jalilian, Francisco Cuevas-Pacheco, Ganggang Xu, Rasmus Waagepetersen", "abstract": "The dynamics of a rain forest is extremely complex involving births, deaths\nand growth of trees with complex interactions between trees, animals, climate,\nand environment. We consider the patterns of recruits (new trees) and dead\ntrees between rain forest censuses. For a current census we specify regression\nmodels for the conditional intensity of recruits and the conditional\nprobabilities of death given the current trees and spatial covariates. We\nestimate regression parameters using conditional composite likelihood functions\nthat only involve the conditional first order properties of the data. When\nconstructing assumption lean estimators of covariance matrices of parameter\nestimates we only need mild assumptions of decaying conditional correlations in\nspace while assumptions regarding correlations over time are avoided by\nexploiting conditional centering of composite likelihood score functions. Time\nseries of point patterns from rain forest censuses are quite short while each\npoint pattern covers a fairly big spatial region. To obtain asymptotic results\nwe therefore use a central limit theorem for the fixed timespan - increasing\nspatial domain asymptotic setting. This also allows us to handle the challenge\nof using stochastic covariates constructed from past point patterns.\nConveniently, it suffices to impose weak dependence assumptions on the\ninnovations of the space-time process. We investigate the proposed methodology\nby simulation studies and applications to rain forest data.", "journal": ""}
{"doi": "10.48550/arXiv.1710.09469", "date": "2017-10-25", "title": "Logical relations for coherence of effect subtyping", "authors": "Dariusz Biernacki, Piotr Polesiuk", "abstract": "A coercion semantics of a programming language with subtyping is typically\ndefined on typing derivations rather than on typing judgments. To avoid\nsemantic ambiguity, such a semantics is expected to be coherent, i.e.,\nindependent of the typing derivation for a given typing judgment. In this\narticle we present heterogeneous, biorthogonal, step-indexed logical relations\nfor establishing the coherence of coercion semantics of programming languages\nwith subtyping. To illustrate the effectiveness of the proof method, we develop\na proof of coherence of a type-directed, selective CPS translation from a typed\ncall-by-value lambda calculus with delimited continuations and control-effect\nsubtyping. The article is accompanied by a Coq formalization that relies on a\nnovel shallow embedding of a logic for reasoning about step-indexing.", "journal": "Logical Methods in Computer Science, Volume 14, Issue 1 (January\n  30, 2018) lmcs:4180"}
{"doi": "10.48550/arXiv.1802.00588", "date": "2018-02-02", "title": "When Good Components Go Bad: Formally Secure Compilation Despite Dynamic Compromise", "authors": "Carmine Abate, Arthur Azevedo de Amorim, Roberto Blanco, Ana Nora Evans, Guglielmo Fachini, Catalin Hritcu, Th\u00e9o Laurent, Benjamin C. Pierce, Marco Stronati, J\u00e9r\u00e9my Thibault, Andrew Tolmach", "abstract": "We propose a new formal criterion for evaluating secure compilation schemes\nfor unsafe languages, expressing end-to-end security guarantees for software\ncomponents that may become compromised after encountering undefined\nbehavior---for example, by accessing an array out of bounds.\n  Our criterion is the first to model dynamic compromise in a system of\nmutually distrustful components with clearly specified privileges. It\narticulates how each component should be protected from all the others---in\nparticular, from components that have encountered undefined behavior and become\ncompromised. Each component receives secure compilation guarantees---in\nparticular, its internal invariants are protected from compromised\ncomponents---up to the point when this component itself becomes compromised,\nafter which we assume an attacker can take complete control and use this\ncomponent's privileges to attack other components. More precisely, a secure\ncompilation chain must ensure that a dynamically compromised component cannot\nbreak the safety properties of the system at the target level any more than an\narbitrary attacker-controlled component (with the same interface and\nprivileges, but without undefined behaviors) already could at the source level.\n  To illustrate the model, we construct a secure compilation chain for a small\nunsafe language with buffers, procedures, and components, targeting a simple\nabstract machine with built-in compartmentalization. We give a machine-checked\nproof in Coq that this compiler satisfies our secure compilation criterion.\nFinally, we show that the protection guarantees offered by the\ncompartmentalized abstract machine can be achieved at the machine-code level\nusing either software fault isolation or a tag-based reference monitor.", "journal": ""}
{"doi": "10.48550/arXiv.1410.4439", "date": "2014-10-16", "title": "Principles for Verification Tools: Separation Logic", "authors": "Brijesh Dongol, Victor B. F. Gomes, Georg Struth", "abstract": "A principled approach to the design of program verification and con-\nstruction tools is applied to separation logic. The control flow is modelled by\npower series with convolution as separating conjunction. A generic construction\nlifts resource monoids to assertion and predicate transformer quantales. The\ndata flow is captured by concrete store/heap models. These are linked to the\nseparation algebra by soundness proofs. Verification conditions and\ntransformation laws are derived by equational reasoning within the predicate\ntransformer quantale. This separation of concerns makes an implementation in\nthe Isabelle/HOL proof as- sistant simple and highly automatic. The resulting\ntool is correct by construction; it is explained on the classical linked list\nreversal example.", "journal": ""}
{"doi": "10.48550/arXiv.1909.11143", "date": "2019-09-24", "title": "Spontaneous Fruit Fly Optimisation for truss weight minimisation: Performance evaluation based on the no free lunch theorem", "authors": "Uche Onyekpe, Stratis Kanarachos, Michael E. Fitzpatrick", "abstract": "Over the past decade, several researchers have presented various optimisation\nalgorithms for use in truss design. The no free lunch theorem implies that no\noptimisation algorithm fits all problems; therefore, the interest is not only\nin the accuracy and convergence rate of the algorithm but also the tuning\neffort and population size required for achieving the optimal result. The\nlatter is particularly crucial for computationally intensive or\nhigh-dimensional problems. Contrast-based Fruit-fly Optimisation Algorithm\n(c-FOA) proposed by Kanarachos et al. in 2017 is based on the efficiency of\nfruit flies in food foraging by olfaction and visual contrast. The proposed\nSpontaneous Fruit Fly Optimisation (s-FOA) enhances c-FOA and addresses the\ndifficulty in solving nonlinear optimisation algorithms by presenting standard\nparameters and lean population size for use on all optimisation problems. Six\nbenchmark problems were studied to assess the performance of s-FOA. A\ncomparison of the results obtained from documented literature and other\ninvestigated techniques demonstrates the competence and robustness of the\nalgorithm in truss optimisation.", "journal": ""}
{"doi": "10.48550/arXiv.2007.10030", "date": "2020-07-20", "title": "Modern Random Access: an Age of Information Perspective on Irregular Repetition Slotted ALOHA", "authors": "Andrea Munari", "abstract": "Age of information (AoI) is gaining attention as a valuable performance\nmetric for many IoT systems, in which a large number of devices report\ntime-stamped updates to a central gateway. This is the case, for instance, of\nremote sensing, monitoring, or tracking, with broad applications in the\nindustrial, vehicular, and environmental domain. In these settings, AoI\nprovides insights that are complementary to those offered by throughput or\nlatency, capturing the ability of the system to maintain an up-to-date view of\nthe status of each transmitting device. From this standpoint, while a good\nunderstanding of the metric has been reached for point-to-point links,\nrelatively little attention has been devoted to the impact that link layer\nsolutions employed in IoT systems may have on AoI. In particular, no result is\navailable for modern random access protocols, which have recently emerged as\npromising solutions to support massive machine-type communications. To start\naddressing this gap we provide in this paper the first study of the AoI of a\nscheme in this family, namely irregular repetition slotted ALOHA (IRSA). By\nmeans of a Markovian analysis, we track the AoI evolution at the gateway, prove\nthat the process is ergodic, and derive a compact closed form expression for\nits stationary distribution. Leaning on this, we compute exact formulations for\nthe average AoI and the age violation probability. The study reveals\nnon-trivial design trade-offs for IRSA and highlights the key role played by\nthe protocol operating frame size. Moreover, a comparison with the performance\nof a simpler slotted ALOHA strategy highlights a remarkable potential for\nmodern random access schemes in terms of information freshness.", "journal": ""}
{"doi": "10.48550/arXiv.2305.08496", "date": "2023-05-15", "title": "A Direct-Style Effect Notation for Sequential and Parallel Programs", "authors": "David Richter, Timon B\u00f6hler, Pascal Weisenburger, Mira Mezini", "abstract": "Modeling sequential and parallel composition of effectful computations has\nbeen investigated in a variety of languages for a long time. In particular, the\npopular do-notation provides a lightweight effect embedding for any instance of\na monad. Idiom bracket notation, on the other hand, provides an embedding for\napplicatives. First, while monads force effects to be executed sequentially,\nignoring potential for parallelism, applicatives do not support sequential\neffects. Composing sequential with parallel effects remains an open problem.\nThis is even more of an issue as real programs consist of a combination of both\nsequential and parallel segments. Second, common notations do not support\ninvoking effects in direct-style, instead forcing a rigid structure upon the\ncode.\n  In this paper, we propose a mixed applicative/monadic notation that retains\nparallelism where possible, but allows sequentiality where necessary. We\nleverage a direct-style notation where sequentiality or parallelism is derived\nfrom the structure of the code. We provide a mechanisation of our effectful\nlanguage in Coq and prove that our compilation approach retains the parallelism\nof the source program.", "journal": ""}
{"doi": "10.48550/arXiv.1701.05063", "date": "2017-01-18", "title": "(Mathematical) Logic for Systems Biology (Invited Paper)", "authors": "Jo\u00eblle Despeyroux", "abstract": "We advocates here the use of (mathematical) logic for systems biology, as a\nunified framework well suited for both modeling the dynamic behaviour of\nbiological systems, expressing properties of them, and verifying these\nproperties. The potential candidate logics should have a traditional proof\ntheoretic pedigree (including a sequent calculus presentation enjoying\ncut-elimination and focusing), and should come with (certified) proof tools.\nBeyond providing a reliable framework, this allows the adequate encodings of\nour biological systems. We present two candidate logics (two modal extensions\nof linear logic, called HyLL and SELL), along with biological examples. The\nexamples we have considered so far are very simple ones-coming with completely\nformal (interactive) proofs in Coq. Future works includes using automatic\nprovers, which would extend existing automatic provers for linear logic. This\nshould enable us to specify and study more realistic examples in systems\nbiology, biomedicine (diagnosis and prognosis), and eventually neuroscience.", "journal": "Computational Methods in Systems Biology, Sep 2016, Cambridge,\n  United Kingdom. pp.3 - 12, 2016"}
{"doi": "10.48550/arXiv.1702.05997", "date": "2017-02-20", "title": "Refinement-based Specification and Security Analysis of Separation Kernels", "authors": "Yongwang Zhao, David Sanan, Fuyuan Zhang, Yang Liu", "abstract": "Assurance of information-flow security by formal methods is mandated in\nsecurity certification of separation kernels. As an industrial standard for\nimproving safety, ARINC 653 has been complied with by mainstream separation\nkernels. Due to the new trend of integrating safe and secure functionalities\ninto one separation kernel, security analysis of ARINC 653 as well as a formal\nspecification with security proofs are thus significant for the development and\ncertification of ARINC 653 compliant Separation Kernels (ARINC SKs). This paper\npresents a specification development and security analysis method for ARINC SKs\nbased on refinement. We propose a generic security model and a stepwise\nrefinement framework. Two levels of functional specification are developed by\nthe refinement. A major part of separation kernel requirements in ARINC 653 are\nmodeled, such as kernel initialization, two-level scheduling, partition and\nprocess management, and inter-partition communication. The formal specification\nand its security proofs are carried out in the Isabelle/HOL theorem prover. We\nhave reviewed the source code of one industrial and two open-source ARINC SK\nimplementations, i.e. VxWorks 653, XtratuM, and POK, in accordance with the\nformal specification. During the verification and code review, six security\nflaws, which can cause information leakage, are found in the ARINC 653 standard\nand the implementations.", "journal": ""}
{"doi": "10.48550/arXiv.2201.06648", "date": "2022-01-17", "title": "OmniPrint: A Configurable Printed Character Synthesizer", "authors": "Haozhe Sun, Wei-Wei Tu, Isabelle Guyon", "abstract": "We introduce OmniPrint, a synthetic data generator of isolated printed\ncharacters, geared toward machine learning research. It draws inspiration from\nfamous datasets such as MNIST, SVHN and Omniglot, but offers the capability of\ngenerating a wide variety of printed characters from various languages, fonts\nand styles, with customized distortions. We include 935 fonts from 27 scripts\nand many types of distortions. As a proof of concept, we show various use\ncases, including an example of meta-learning dataset designed for the upcoming\nMetaDL NeurIPS 2021 competition. OmniPrint is available at\nhttps://github.com/SunHaozhe/OmniPrint.", "journal": "35th Conference on Neural Information Processing Systems (NeurIPS\n  2021) Track on Datasets and Benchmarks"}
{"doi": "10.48550/arXiv.2301.09802", "date": "2023-01-24", "title": "Inductive Reasoning for Coinductive Types", "authors": "Alexander Bagnall, Gordon Stewart, Anindya Banerjee", "abstract": "We present AlgCo (Algebraic Coinductives), a practical framework for\ninductive reasoning over commonly used coinductive types such as conats,\nstreams, and infinitary trees with finite branching factor. The key idea is to\nexploit the notion of algebraic complete partial order from domain theory to\ndefine continuous operations over coinductive types via primitive recursion on\n``dense'' collections of their elements, enabling a convenient strategy for\nreasoning about algebraic coinductives by straightforward proofs by induction.\nWe implement the AlgCo framework in Coq and demonstrate its power by verifying\na stream variant of the sieve of Eratosthenes, a regular expression library\nbased on coinductive trie encodings of formal languages, and expected value\nsemantics for coinductive sampling processes over discrete probability\ndistributions in the random bit model.", "journal": ""}
{"doi": "10.48550/arXiv.1405.3017", "date": "2014-05-13", "title": "Formalisation and Analysis of Component Dependencies", "authors": "Maria Spichkova", "abstract": "This set of theories presents a formalisation in Isabelle/HOL+Isar of data\ndependencies between components. The approach allows to analyse system\nstructure oriented towards efficient checking of system: it aims at elaborating\nfor a concrete system, which parts of the system (or system model) are\nnecessary to check a given property.", "journal": ""}
{"doi": "10.48550/arXiv.1012.4898", "date": "2010-12-22", "title": "Beating the Productivity Checker Using Embedded Languages", "authors": "Nils Anders Danielsson", "abstract": "Some total languages, like Agda and Coq, allow the use of guarded corecursion\nto construct infinite values and proofs. Guarded corecursion is a form of\nrecursion in which arbitrary recursive calls are allowed, as long as they are\nguarded by a coinductive constructor. Guardedness ensures that programs are\nproductive, i.e. that every finite prefix of an infinite value can be computed\nin finite time. However, many productive programs are not guarded, and it can\nbe nontrivial to put them in guarded form.\n  This paper gives a method for turning a productive program into a guarded\nprogram. The method amounts to defining a problem-specific language as a data\ntype, writing the program in the problem-specific language, and writing a\nguarded interpreter for this language.", "journal": "EPTCS 43, 2010, pp. 29-48"}
{"doi": "10.48550/arXiv.1904.07298", "date": "2019-04-15", "title": "A Path To DOT: Formalizing Fully Path-Dependent Types", "authors": "Marianna Rapoport, Ond\u0159ej Lhot\u00e1k", "abstract": "The Dependent Object Types (DOT) calculus aims to formalize the Scala\nprogramming language with a focus on path-dependent types $-$ types such as\n$x.a_1\\dots a_n.T$ that depend on the runtime value of a path $x.a_1\\dots a_n$\nto an object. Unfortunately, existing formulations of DOT can model only types\nof the form $x.A$ which depend on variables rather than general paths. This\nrestriction makes it impossible to model nested module dependencies. Nesting\nsmall components inside larger ones is a necessary ingredient of a modular,\nscalable language. DOT's variable restriction thus undermines its ability to\nfully formalize a variety of programming-language features including Scala's\nmodule system, family polymorphism, and covariant specialization.\n  This paper presents the pDOT calculus, which generalizes DOT to support types\nthat depend on paths of arbitrary length, as well as singleton types to track\npath equality. We show that naive approaches to add paths to DOT make it\ninherently unsound, and present necessary conditions for such a calculus to be\nsound. We discuss the key changes necessary to adapt the techniques of the DOT\nsoundness proofs so that they can be applied to pDOT. Our paper comes with a\nCoq-mechanized type-safety proof of pDOT. With support for paths of arbitrary\nlength, pDOT can realize DOT's full potential for formalizing Scala-like\ncalculi.", "journal": ""}
{"doi": "10.48550/arXiv.2212.06956", "date": "2022-12-14", "title": "Verifying term graph optimizations using Isabelle/HOL", "authors": "Brae J. Webb, Ian J. Hayes, Mark Utting", "abstract": "Our objective is to formally verify the correctness of the hundreds of\nexpression optimization rules used within the GraalVM compiler. When defining\nthe semantics of a programming language, expressions naturally form abstract\nsyntax trees, or, terms. However, in order to facilitate sharing of common\nsubexpressions, modern compilers represent expressions as term graphs. Defining\nthe semantics of term graphs is more complicated than defining the semantics of\ntheir equivalent term representations. More significantly, defining\noptimizations directly on term graphs and proving semantics preservation is\nconsiderably more complicated than on the equivalent term representations. On\nterms, optimizations can be expressed as conditional term rewriting rules, and\nproofs that the rewrites are semantics preserving are relatively\nstraightforward. In this paper, we explore an approach to using term rewrites\nto verify term graph transformations of optimizations within the GraalVM\ncompiler. This approach significantly reduces the overall verification effort\nand allows for simpler encoding of optimization rules.", "journal": ""}
{"doi": "10.48550/arXiv.2309.05362", "date": "2023-09-11", "title": "A Mechanized Theory of the Box Calculus", "authors": "Joseph Fourment, Yichen Xu", "abstract": "The capture calculus is an extension of System F<: that tracks free variables\nof terms in their type, allowing one to represent capabilities while limiting\ntheir scope. While previous calculi had mechanized soundness proofs -- notably\nSystem CF<: -- the latest version, namely the box calculus (System CC<:box),\nonly had a paper proof. We present here our work on mechanizing the theory of\nthe box calculus in Coq, and the challenges encountered along the way. While\ndoing so, we motivate the current design of capture calculus, in particular the\nconcept of boxes, from both user and metatheoretical standpoints. Our\nmechanization is complete and available on GitHub.", "journal": ""}
{"doi": "10.48550/arXiv.1507.07697", "date": "2015-07-28", "title": "Featherweight VeriFast", "authors": "Bart Jacobs, Fr\u00e9d\u00e9ric Vogels, Frank Piessens", "abstract": "VeriFast is a leading research prototype tool for the sound modular\nverification of safety and correctness properties of single-threaded and\nmultithreaded C and Java programs. It has been used as a vehicle for\nexploration and validation of novel program verification techniques and for\nindustrial case studies; it has served well at a number of program verification\ncompetitions; and it has been used for teaching by multiple teachers\nindependent of the authors. However, until now, while VeriFast's operation has\nbeen described informally in a number of publications, and specific\nverification techniques have been formalized, a clear and precise exposition of\nhow VeriFast works has not yet appeared. In this article we present for the\nfirst time a formal definition and soundness proof of a core subset of the\nVeriFast program verification approach. The exposition aims to be both\naccessible and rigorous: the text is based on lecture notes for a graduate\ncourse on program verification, and it is backed by an executable\nmachine-readable definition and machine-checked soundness proof in Coq.", "journal": "Logical Methods in Computer Science, Volume 11, Issue 3 (September\n  22, 2015) lmcs:1595"}
{"doi": "10.48550/arXiv.2403.04067", "date": "2024-03-06", "title": "Feel the Bite: Robot-Assisted Inside-Mouth Bite Transfer using Robust Mouth Perception and Physical Interaction-Aware Control", "authors": "Rajat Kumar Jenamani, Daniel Stabile, Ziang Liu, Abrar Anwar, Katherine Dimitropoulou, Tapomayukh Bhattacharjee", "abstract": "Robot-assisted feeding can greatly enhance the lives of those with mobility\nlimitations. Modern feeding systems can pick up and position food in front of a\ncare recipient's mouth for a bite. However, many with severe mobility\nconstraints cannot lean forward and need direct inside-mouth food placement.\nThis demands precision, especially for those with restricted mouth openings,\nand appropriately reacting to various physical interactions - incidental\ncontacts as the utensil moves inside, impulsive contacts due to sudden muscle\nspasms, deliberate tongue maneuvers by the person being fed to guide the\nutensil, and intentional bites. In this paper, we propose an inside-mouth bite\ntransfer system that addresses these challenges with two key components: a\nmulti-view mouth perception pipeline robust to tool occlusion, and a control\nmechanism that employs multimodal time-series classification to discern and\nreact to different physical interactions. We demonstrate the efficacy of these\nindividual components through two ablation studies. In a full system\nevaluation, our system successfully fed 13 care recipients with diverse\nmobility challenges. Participants consistently emphasized the comfort and\nsafety of our inside-mouth bite transfer system, and gave it high technology\nacceptance ratings - underscoring its transformative potential in real-world\nscenarios. Supplementary materials and videos can be found at\nhttp://emprise.cs.cornell.edu/bitetransfer/ .", "journal": ""}
{"doi": "10.48550/arXiv.1507.02999", "date": "2015-07-10", "title": "Compressive Detection of Random Subspace Signals", "authors": "Alireza Razavi, Mikko Valkama, Danijela Cabric", "abstract": "The problem of compressive detection of random subspace signals is studied.\nWe consider signals modeled as $\\mathbf{s} = \\mathbf{H} \\mathbf{x}$ where\n$\\mathbf{H}$ is an $N \\times K$ matrix with $K \\le N$ and $\\mathbf{x} \\sim\n\\mathcal{N}(\\mathbf{0}_{K,1},\\sigma_x^2 \\mathbf{I}_K)$. We say that signal\n$\\mathbf{s}$ lies in or leans toward a subspace if the largest eigenvalue of\n$\\mathbf{H} \\mathbf{H}^T$ is strictly greater than its smallest eigenvalue. We\nfirst design a measurement matrix\n$\\mathbf{\\Phi}=[\\mathbf{\\Phi}_s^T,\\mathbf{\\Phi}_o^T]^T$ comprising of two\nsub-matrices $\\mathbf{\\Phi}_s$ and $\\mathbf{\\Phi}_o$ where $\\mathbf{\\Phi}_s$\nprojects the signals to the strongest left-singular vectors, i.e., the\nleft-singular vectors corresponding to the largest singular values, of subspace\nmatrix $\\mathbf{H}$ and $\\mathbf{\\Phi}_o$ projects it to the weakest\nleft-singular vectors. We then propose two detectors which work based on the\ndifference in energies of the samples measured by two sub-matrices\n$\\mathbf{\\Phi}_s$ and $\\mathbf{\\Phi}_o$ and prove their optimality. Simplified\nversions of the proposed detectors for the case when the variance of noise is\nknown are also provided. Furthermore, we study the performance of the detector\nwhen measurements are imprecise and show how imprecision can be compensated by\nemploying more measurement devices. The problem is then re-formulated for the\ncase when the signal lies in the union of a finite number of linear subspaces\ninstead of a single linear subspace. Finally, we study the performance of the\nproposed methods by simulation examples.", "journal": ""}
{"doi": "10.48550/arXiv.1610.03344", "date": "2016-10-11", "title": "Attention and Anticipation in Fast Visual-Inertial Navigation", "authors": "Luca Carlone, Sertac Karaman", "abstract": "We study a Visual-Inertial Navigation (VIN) problem in which a robot needs to\nestimate its state using an on-board camera and an inertial sensor, without any\nprior knowledge of the external environment. We consider the case in which the\nrobot can allocate limited resources to VIN, due to tight computational\nconstraints. Therefore, we answer the following question: under limited\nresources, what are the most relevant visual cues to maximize the performance\nof visual-inertial navigation? Our approach has four key ingredients. First, it\nis task-driven, in that the selection of the visual cues is guided by a metric\nquantifying the VIN performance. Second, it exploits the notion of\nanticipation, since it uses a simplified model for forward-simulation of robot\ndynamics, predicting the utility of a set of visual cues over a future time\nhorizon. Third, it is efficient and easy to implement, since it leads to a\ngreedy algorithm for the selection of the most relevant visual cues. Fourth, it\nprovides formal performance guarantees: we leverage submodularity to prove that\nthe greedy selection cannot be far from the optimal (combinatorial) selection.\nSimulations and real experiments on agile drones show that our approach ensures\nstate-of-the-art VIN performance while maintaining a lean processing time. In\nthe easy scenarios, our approach outperforms appearance-based feature selection\nin terms of localization errors. In the most challenging scenarios, it enables\naccurate visual-inertial navigation while appearance-based feature selection\nfails to track robot's motion during aggressive maneuvers.", "journal": ""}
{"doi": "10.48550/arXiv.2409.16478", "date": "2024-09-24", "title": "Algorithmic Drift: A Simulation Framework to Study the Effects of Recommender Systems on User Preferences", "authors": "Erica Coppolillo, Simone Mungari, Ettore Ritacco, Francesco Fabbri, Marco Minici, Francesco Bonchi, Giuseppe Manco", "abstract": "Digital platforms such as social media and e-commerce websites adopt\nRecommender Systems to provide value to the user. However, the social\nconsequences deriving from their adoption are still unclear. Many scholars\nargue that recommenders may lead to detrimental effects, such as\nbias-amplification deriving from the feedback loop between algorithmic\nsuggestions and users' choices. Nonetheless, the extent to which recommenders\ninfluence changes in users leaning remains uncertain. In this context, it is\nimportant to provide a controlled environment for evaluating the recommendation\nalgorithm before deployment. To address this, we propose a stochastic\nsimulation framework that mimics user-recommender system interactions in a\nlong-term scenario. In particular, we simulate the user choices by formalizing\na user model, which comprises behavioral aspects, such as the user resistance\ntowards the recommendation algorithm and their inertia in relying on the\nreceived suggestions. Additionally, we introduce two novel metrics for\nquantifying the algorithm's impact on user preferences, specifically in terms\nof drift over time. We conduct an extensive evaluation on multiple synthetic\ndatasets, aiming at testing the robustness of our framework when considering\ndifferent scenarios and hyper-parameters setting. The experimental results\nprove that the proposed methodology is effective in detecting and quantifying\nthe drift over the users preferences by means of the simulation. All the code\nand data used to perform the experiments are publicly available.", "journal": ""}
{"doi": "10.48550/arXiv.2411.12808", "date": "2024-11-19", "title": "Conversational Medical AI: Ready for Practice", "authors": "Antoine Liz\u00e9e, Pierre-Auguste Beaucot\u00e9, James Whitbeck, Marion Doumeingts, Ana\u00ebl Beaugnon, Isabelle Feldhaus", "abstract": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services.", "journal": ""}
{"doi": "10.48550/arXiv.2010.09073", "date": "2020-10-18", "title": "Handling Bidirectional Control Flow: Technical Report", "authors": "Yizhou Zhang, Guido Salvaneschi, Andrew C. Myers", "abstract": "Pressed by the difficulty of writing asynchronous, event-driven code,\nmainstream languages have recently been building in support for a variety of\nadvanced control-flow features. Meanwhile, experimental language designs have\nsuggested effect handlers as a unifying solution to programmer-defined control\neffects, subsuming exceptions, generators, and async--await. Despite these\ntrends, complex control flow---in particular, control flow exhibiting a\nbidirectional pattern---remains challenging to manage.\n  We introduce bidirectional algebraic effects, a new programming abstraction\nthat supports bidirectional control transfer in a more natural way. Handlers of\nbidirectional effects can raise further effects to transfer control back to the\nsite where the initiating effect was raised, and can use themselves to handle\ntheir own effects. We present applications of this expressive power, which\nfalls out naturally as we push toward the unification of effectful programming\nwith object-oriented programming. We pin down the mechanism and the unification\nformally using a core language that generalizes to effect operations and effect\nhandlers.\n  The usual propagation semantics of control effects such as exceptions\nconflicts with modular reasoning in the presence of effect polymorphism---it\nbreaks parametricity. Bidirectionality exacerbates the problem. Hence, we set\nout to show the core language, which builds on the existing tunneling semantics\nfor algebraic effects, is not only type-safe (no effects go unhandled), but\nalso abstraction-safe (no effects are accidentally handled). We devise a\nstep-indexed logical-relations model, and construct its parametricity and\nsoundness proofs. These core results are fully mechanized in Coq. Preliminary\nexperiments show that as a first-class language feature, bidirectional handlers\ncan be implemented efficiently.", "journal": ""}
{"doi": "10.48550/arXiv.1503.08665", "date": "2015-03-30", "title": "A Linear First-Order Functional Intermediate Language for Verified Compilers", "authors": "Sigurd Schneider, Gert Smolka, Sebastian Hack", "abstract": "We present the linear first-order intermediate language IL for verified\ncompilers. IL is a functional language with calls to a nondeterministic\nenvironment. We give IL terms a second, imperative semantic interpretation and\nobtain a register transfer language. For the imperative interpretation we\nestablish a notion of live variables. Based on live variables, we formulate a\ndecidable property called coherence ensuring that the functional and the\nimperative interpretation of a term coincide. We formulate a register\nassignment algorithm for IL and prove its correctness. The algorithm translates\na functional IL program into an equivalent imperative IL program. Correctness\nfollows from the fact that the algorithm reaches a coherent program after\nconsistently renaming local variables. We prove that the maximal number of live\nvariables in the initial program bounds the number of different variables in\nthe final coherent program. The entire development is formalized in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.1810.07560", "date": "2018-10-17", "title": "On the derivatives of the integer-valued polynomials", "authors": "Bakir Farhi", "abstract": "In this paper, we study the derivatives of an integer-valued polynomial of a\ngiven degree. Denoting by $E_n$ the set of the integer-valued polynomials with\ndegree $\\leq n$, we show that the smallest positive integer $c_n$ satisfying\nthe property: $\\forall P \\in E_n, c_n P' \\in E_n$ is $c_n = \\mathrm{lcm}(1 , 2\n, \\dots , n)$. As an application, we deduce an easy proof of the well-known\ninequality $\\mathrm{lcm}(1 , 2 , \\dots , n) \\geq 2^{n - 1}$ ($\\forall n \\geq\n1$). In the second part of the paper, we generalize our result for the\nderivative of a given order $k$ and then we give two divisibility properties\nfor the obtained numbers $c_{n , k}$ (generalizing the $c_n$'s). Leaning on\nthis study, we conclude the paper by determining, for a given natural number\n$n$, the smallest positive integer $\\lambda_n$ satisfying the property:\n$\\forall P \\in E_n$, $\\forall k \\in \\mathbb{N}$: $\\lambda_n P^{(k)} \\in E_n$.\nIn particular, we show that: $\\lambda_n = \\prod_{p \\text{ prime}}\np^{\\lfloor\\frac{n}{p}\\rfloor}$ ($\\forall n \\in \\mathbb{N}$).", "journal": ""}
{"doi": "10.48550/arXiv.2011.10264", "date": "2020-11-20", "title": "Exploring the political pulse of a country using data science tools", "authors": "Miguel G. Folgado, Ver\u00f3nica Sanz", "abstract": "In this paper we illustrate the use of Data Science techniques to analyse\ncomplex human communication. In particular, we consider tweets from leaders of\npolitical parties as a dynamical proxy to political programmes and ideas. We\nalso study the temporal evolution of their contents as a reaction to specific\nevents. We analyse levels of positive and negative sentiment in the tweets\nusing new tools adapted to social media. We also train an Artificial\nIntelligence to recognise the political affiliation of a tweet. The AI is able\nto predict the origin of the tweet with a precision in the range of 71-75\\%,\nand the political leaning (left or right) with a precision of around 90\\%. This\nstudy is meant to be viewed as a proof-of-concept of interdisciplinary nature,\nat the interface between Data Science and political analysis.", "journal": ""}
{"doi": "10.48550/arXiv.2202.06661", "date": "2022-02-14", "title": "Eternalism and the Problem of Hyperplanes", "authors": "Matias Slavov", "abstract": "Eternalism is the view that the past, the present and the future exist\nsimpliciter. A typical argument in favor of this view leans on the relativity\nof simultaneity. The 'equally real with' relation is assumed to be transitive\nbetween spacelike separated events connected by hyperplanes of simultaneity.\nThis reasoning is in tension with the conventionality of simultaneity.\nConventionality indicates that, even within a specific frame, simultaneity is\nbased on the choice of the synchronization parameter. Hence the argument for\neternalism is compromised. This paper lays out alternative eternalist\nstrategies which do not hinge on hyperplanes. While we lack a rigorous proof\nfor eternalism, there are still cogent reasons to prefer eternalism over\npresentism.", "journal": "Ratio 35 (2022) 91-103"}
{"doi": "10.48550/arXiv.2407.12452", "date": "2024-07-17", "title": "A two-sorted theory of nilpotent Lie algebras", "authors": "Christian d'Elb\u00e9e, Isabel M\u00fcller, Nicholas Ramsey, Daoud Siniora", "abstract": "We prove the existence of a model companion of the two-sorted theory of\n$c$-nilpotent Lie algebras over a field satisfying a given theory of fields. We\ndescribe a language in which it admits relative quantifier elimination up to\nthe field sort. Using a new criterion which does not rely on a stationary\nindependence relation, we prove that if the field is NSOP$_1$, then the model\ncompanion is NSOP$_4$. We also prove that if the field is algebraically closed,\nthen the model companion is $c$-NIP.", "journal": ""}
{"doi": "10.48550/arXiv.2307.16177", "date": "2023-07-30", "title": "Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof Classification in the Caribbean", "authors": "Isabelle Tingzon, Nuala Margaret Cowan, Pierre Chrzanowski", "abstract": "Accurate and up-to-date information on building characteristics is essential\nfor vulnerability assessment; however, the high costs and long timeframes\nassociated with conducting traditional field surveys can be an obstacle to\nobtaining critical exposure datasets needed for disaster risk management. In\nthis work, we leverage deep learning techniques for the automated\nclassification of roof characteristics from very high-resolution orthophotos\nand airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017.\nWe demonstrate that the fusion of multimodal earth observation data performs\nbetter than using any single data source alone. Using our proposed methods, we\nachieve F1 scores of 0.93 and 0.92 for roof type and roof material\nclassification, respectively. This work is intended to help governments produce\nmore timely building information to improve resilience and disaster response in\nthe Caribbean.", "journal": ""}
{"doi": "10.48550/arXiv.2110.11075", "date": "2021-10-18", "title": "Enabling a Social Robot to Process Social Cues to Detect when to Help a User", "authors": "Jason R. Wilson, Phyo Thuta Aung, Isabelle Boucher", "abstract": "It is important for socially assistive robots to be able to recognize when a\nuser needs and wants help. Such robots need to be able to recognize human needs\nin a real-time manner so that they can provide timely assistance. We propose an\narchitecture that uses social cues to determine when a robot should provide\nassistance. Based on a multimodal fusion approach upon eye gaze and language\nmodalities, our architecture is trained and evaluated on data collected in a\nrobot-assisted Lego building task. By focusing on social cues, our architecture\nhas minimal dependencies on the specifics of a given task, enabling it to be\napplied in many different contexts. Enabling a social robot to recognize a\nuser's needs through social cues can help it to adapt to user behaviors and\npreferences, which in turn will lead to improved user experiences.", "journal": ""}
{"doi": "10.48550/arXiv.0703026", "date": "2007-03-06", "title": "Formal proof for delayed finite field arithmetic using floating point operators", "authors": "Sylvie Boldo, Marc Daumas, Pascal Giorgi", "abstract": "Formal proof checkers such as Coq are capable of validating proofs of\ncorrection of algorithms for finite field arithmetics but they require\nextensive training from potential users. The delayed solution of a triangular\nsystem over a finite field mixes operations on integers and operations on\nfloating point numbers. We focus in this report on verifying proof obligations\nthat state that no round off error occurred on any of the floating point\noperations. We use a tool named Gappa that can be learned in a matter of\nminutes to generate proofs related to floating point arithmetic and hide\ntechnicalities of formal proof checkers. We found that three facilities are\nmissing from existing tools. The first one is the ability to use in Gappa new\nlemmas that cannot be easily expressed as rewriting rules. We coined the second\none ``variable interchange'' as it would be required to validate loop\ninterchanges. The third facility handles massive loop unrolling and argument\ninstantiation by generating traces of execution for a large number of cases. We\nhope that these facilities may sometime in the future be integrated into\nmainstream code validation.", "journal": ""}
{"doi": "10.48550/arXiv.1811.06203", "date": "2018-11-15", "title": "Combining Axiom Injection and Knowledge Base Completion for Efficient Natural Language Inference", "authors": "Masashi Yoshikawa, Koji Mineshima, Hiroshi Noji, Daisuke Bekki", "abstract": "In logic-based approaches to reasoning tasks such as Recognizing Textual\nEntailment (RTE), it is important for a system to have a large amount of\nknowledge data. However, there is a tradeoff between adding more knowledge data\nfor improved RTE performance and maintaining an efficient RTE system, as such a\nbig database is problematic in terms of the memory usage and computational\ncomplexity. In this work, we show the processing time of a state-of-the-art\nlogic-based RTE system can be significantly reduced by replacing its\nsearch-based axiom injection (abduction) mechanism by that based on Knowledge\nBase Completion (KBC). We integrate this mechanism in a Coq plugin that\nprovides a proof automation tactic for natural language inference.\nAdditionally, we show empirically that adding new knowledge data contributes to\nbetter RTE performance while not harming the processing speed in this\nframework.", "journal": ""}
{"doi": "10.48550/arXiv.2102.12377", "date": "2021-02-23", "title": "Special functions associated with automorphisms of the space of solutions to special double confluent Heun equation", "authors": "S. I. Tertychniy", "abstract": "The family of quads of interrelated functions holomorphic on the universal\ncover of the complex plane without zero (for brevity, pqrs-functions),\nrevealing a number of remarkable properties, is introduced. In particular,\nunder certain conditions the transformations of the argument $z$ of\npqrs-functions represented by lifts of the replacements $ z \\leftarrow -1/z $ $\nz \\leftarrow -z $, and $ z \\leftarrow 1/z $ are equivalent to linear\ntransformations with known coefficients. Pqrs-functions arise in a natural way\nin constructing of certain linear operators acting as automorphisms on the\nspace of solutions to the special double confluent Heun equation (sDCHE).\nEarlier such symmetries were known to exist only in the case of integer value\nof one of the constant parameters when the predecessors of pqrs-functions\nappear as polynomials. In the present work, leaning on the generalized notion\nof pqrs-functions, discrete symmetries of the space of solutions to sDCHE are\nextended to the general case, apart from some natural exceptions.", "journal": ""}
{"doi": "10.48550/arXiv.2306.00617", "date": "2023-06-01", "title": "Multiple-inheritance hazards in dependently-typed algebraic hierarchies", "authors": "Eric Wieser", "abstract": "Abstract algebra provides a large hierarchy of properties that a collection\nof objects can satisfy, such as forming an abelian group or a semiring. These\nclassifications can arranged into a broad and typically acyclic directed graph.\nThis graph perspective encodes naturally in the typeclass system of theorem\nprovers such as Lean, where nodes can be represented as structures (or records)\ncontaining the requisite axioms. This design inevitably needs some form of\nmultiple inheritance; a ring is both a semiring and an abelian group.\n  In the presence of dependently-typed typeclasses that themselves consume\ntypeclasses as type-parameters, such as a vector space typeclass which assumes\nthe presence of an existing additive structure, the implementation details of\nstructure multiple inheritance matter. The type of the outer typeclass is\ninfluenced by the path taken to resolve the typeclasses it consumes. Unless all\npossible paths are considered judgmentally equal, this is a recipe for\ndisaster.\n  This paper provides a concrete explanation of how these situations arise\n(reduced from real examples in mathlib), compares implementation approaches for\nmultiple inheritance by whether judgmental equality is preserved, and outlines\nsolutions (notably: kernel support for $\\eta$-reduction of structures) to the\nproblems discovered.", "journal": ""}
{"doi": "10.48550/arXiv.2401.08287", "date": "2024-01-16", "title": "RichWasm: Bringing Safe, Fine-Grained, Shared-Memory Interoperability Down to WebAssembly", "authors": "Zoe Paraskevopoulou, Michael Fitzgibbons, Michelle Thalakottur, Noble Mushtak, Jose Sulaiman Mazur, Amal Ahmed", "abstract": "Safe, shared-memory interoperability between languages with different type\nsystems and memory-safety guarantees is an intricate problem as crossing\nlanguage boundaries may result in memory-safety violations. In this paper, we\npresent RichWasm, a novel richly typed intermediate language designed to serve\nas a compilation target for typed high-level languages with different\nmemory-safety guarantees. RichWasm is based on WebAssembly and enables safe\nshared-memory interoperability by incorporating a variety of type features that\nsupport fine-grained memory ownership and sharing. RichWasm is rich enough to\nserve as a typed compilation target for both typed garbage-collected languages\nand languages with an ownership-based type system and manually managed memory.\nWe demonstrate this by providing compilers from core ML and L3, a type-safe\nlanguage with strong updates, to RichWasm. RichWasm is compiled to regular\nWasm, allowing for use in existing environments. We formalize RichWasm in Coq\nand prove type safety.", "journal": ""}
{"doi": "10.48550/arXiv.1912.09741", "date": "2019-12-20", "title": "Formalizing Determinacy of Concurrent Revisions", "authors": "Roy Overbeek", "abstract": "Concurrent revisions is a concurrency control model designed to guarantee\ndeterminacy, meaning that the outcomes of programs are uniquely determined.\nThis paper describes an Isabelle/HOL formalization of the model's operational\nsemantics and proof of determinacy. We discuss and resolve subtle ambiguities\nin the operational semantics and simplify the proof of determinacy. Although\nour findings do not appear to correspond to bugs in implementations, the\nformalization highlights some of the challenges involved in the design and\nverification of concurrency control models.", "journal": ""}
{"doi": "10.48550/arXiv.1607.04822", "date": "2016-07-17", "title": "HoTTSQL: Proving Query Rewrites with Univalent SQL Semantics", "authors": "Shumo Chu, Konstantin Weitz, Alvin Cheung, Dan Suciu", "abstract": "Every database system contains a query optimizer that performs query\nrewrites. Unfortunately, developing query optimizers remains a highly\nchallenging task. Part of the challenges comes from the intricacies and rich\nfeatures of query languages, which makes reasoning about rewrite rules\ndifficult. In this paper, we propose a machine-checkable denotational semantics\nfor SQL, the de facto language for relational database, for rigorously\nvalidating rewrite rules. Unlike previously proposed semantics that are either\nnon-mechanized or only cover a small amount of SQL language features, our\nsemantics covers all major features of SQL, including bags, correlated\nsubqueries, aggregation, and indexes. Our mechanized semantics, called HoTTSQL,\nis based on K-Relations and homotopy type theory, where we denote relations as\nmathematical functions from tuples to univalent types. We have implemented\nHoTTSQL in Coq, which takes only fewer than 300 lines of code and have proved a\nwide range of SQL rewrite rules, including those from database research\nliterature (e.g., magic set rewrites) and real-world query optimizers (e.g.,\nsubquery elimination). Several of these rewrite rules have never been\npreviously proven correct. In addition, while query equivalence is generally\nundecidable, we have implemented an automated decision procedure using HoTTSQL\nfor conjunctive queries: a well-studied decidable fragment of SQL that\nencompasses many real-world queries.", "journal": ""}
{"doi": "10.48550/arXiv.1104.5604", "date": "2011-04-29", "title": "Existence of minimal and maximal solutions to first--order differential equations with state--dependent deviated arguments", "authors": "Rub\u00e9n Figueroa, Rodrigo L\u00f3pez Pouso", "abstract": "We prove some new results on existence of solutions to first--order ordinary\ndifferential equations with deviating arguments. Delay differential equations\nare included in our general framework, which even allows deviations to depend\non the unknown solutions. Our existence results lean on new definitions of\nlower and upper solutions introduced in this paper, and we show with an example\nthat similar results with the classical definitions are false. We also\nintroduce an example showing that the problems considered need not have the\nleast (or the greatest) solution between given lower and upper solutions, but\nwe can prove that they do have minimal and maximal solutions in the usual\nset--theoretic sense. Sufficient conditions for the existence of lower and\nupper solutions, with some examples of application, are provided too.", "journal": ""}
{"doi": "10.48550/arXiv.1803.10487", "date": "2018-03-28", "title": "Quandles of cyclic type with several fixed points", "authors": "Ant\u00f3nio Lages, Pedro Lopes", "abstract": "A quandle of cyclic type of order $n$ with $f\\geq 2$ fixed points is such\nthat each of its permutations splits into $f$ cycles of length $1$ and one\ncycle of length $n-f$. In this article we prove that there is only one such\nconnected quandle, up to isomorphism. This is a quandle of order $6$ and $2$\nfixed points, known in the literature as octahedron quandle. We prove also\nthat, for each $f\\geq 2$, the non-connected versions of these quandles only\noccur for orders $n$ in the range $f+2 \\leq n \\leq 2f$ and that, for each\n$f>1$, there is only one such quandle of order $2f$ with $f$ fixed points, up\nto isomorphism. Still in the range $f+2 \\leq n \\leq 2f$, we present sufficient\nconditions for the existence of such quandles, writing down their permutations;\nwe also show how to obtain new quandles form old ones, leaning on the notion of\ncommon fixed point.", "journal": ""}
{"doi": "10.48550/arXiv.1604.00204", "date": "2016-04-01", "title": "Verifying Security Policies using Host Attributes", "authors": "Cornelius Diekmann, Stephan-A. Posselt, Heiko Niedermayer, Holger Kinkelin, Oliver Hanka, Georg Carle", "abstract": "For the formal verification of a network security policy, it is crucial to\nexpress the verification goals. These formal goals, called security invariants,\nshould be easy to express for the end user. Focusing on access control and\ninformation flow security strategies, this work discovers and proves universal\ninsights about security invariants. This enables secure and convenient\nauto-completion of host attribute configurations. We demonstrate our results in\na civil aviation scenario. All results are machine-verified with the\nIsabelle/HOL theorem prover.", "journal": "Formal Techniques for Distributed Objects, Components, and Systems\n  Volume 8461 of the series Lecture Notes in Computer Science, 2014, Springer"}
{"doi": "10.48550/arXiv.2208.10613", "date": "2022-08-22", "title": "Leaning-Based Control of an Immersive-Telepresence Robot", "authors": "Joona Halkola, Markku Suomalainen, Basak Sakcak, Katherine J. Mimnaugh, Juho Kalliokoski, Alexis P. Chambers, Timo Ojala, Steven M. LaValle", "abstract": "In this paper, we present an implementation of a leaning-based control of a\ndifferential drive telepresence robot and a user study in simulation, with the\ngoal of bringing the same functionality to a real telepresence robot. The\nparticipants used a balance board to control the robot and viewed the virtual\nenvironment through a head-mounted display. The main motivation for using a\nbalance board as the control device stems from Virtual Reality (VR) sickness;\neven small movements of your own body matching the motions seen on the screen\ndecrease the sensory conflict between vision and vestibular organs, which lies\nat the heart of most theories regarding the onset of VR sickness. To test the\nhypothesis that the balance board as a control method would be less sickening\nthan using joysticks, we designed a user study (N=32, 15 women) in which the\nparticipants drove a simulated differential drive robot in a virtual\nenvironment with either a Nintendo Wii Balance Board or joysticks. However, our\npre-registered main hypotheses were not supported; the joystick did not cause\nany more VR sickness on the participants than the balance board, and the board\nproved to be statistically significantly more difficult to use, both\nsubjectively and objectively. Analyzing the open-ended questions revealed these\nresults to be likely connected, meaning that the difficulty of use seemed to\naffect sickness; even unlimited training time before the test did not make the\nuse as easy as the familiar joystick. Thus, making the board easier to use is a\nkey to enable its potential; we present a few possibilities towards this goal.", "journal": ""}
{"doi": "10.48550/arXiv.1903.12121", "date": "2019-03-28", "title": "The effective strength of selection in random environment", "authors": "Adri\u00e1n Gonz\u00e1lez Casanova, Dario Span\u00f2, Maite Wilke-Berenguer", "abstract": "We analyse a family of two-types Wright-Fisher models with selection in a\nrandom environment and skewed offspring distribution. We provide a calculable\ncriterion to quantify the impact of different shapes of selection on the fate\nof the weakest allele, and thus compare them. The main mathematical tool is\nduality, which we prove to hold, also in presence of random environment\n(quenched and in some cases annealed), between the population's allele\nfrequencies and genealogy, both in the case of finite population size and in\nthe scaling limit for large size. Duality also yields new insight on properties\nof branching-coalescing processes in random environment, such as their long\nterm behaviour.", "journal": ""}
{"doi": "10.48550/arXiv.2101.03807", "date": "2021-01-11", "title": "Mechanisation of Model-theoretic Conservative Extension for HOL with Ad-hoc Overloading", "authors": "Arve Gengelbach, Johannes \u00c5man Pohjola, Tjark Weber", "abstract": "Definitions of new symbols merely abbreviate expressions in logical\nframeworks, and no new facts (regarding previously defined symbols) should hold\nbecause of a new definition. In Isabelle/HOL, definable symbols are types and\nconstants. The latter may be ad-hoc overloaded, i.e. have different definitions\nfor non-overlapping types. We prove that symbols that are independent of a new\ndefinition may keep their interpretation in a model extension. This work\nrevises our earlier notion of model-theoretic conservative extension and\ngeneralises an earlier model construction. We obtain consistency of theories of\ndefinitions in higher-order logic (HOL) with ad-hoc overloading as a corollary.\nOur results are mechanised in the HOL4 theorem prover.", "journal": "EPTCS 332, 2021, pp. 1-17"}
{"doi": "10.48550/arXiv.2303.15984", "date": "2023-03-28", "title": "Specification-based CSV Support in VDM", "authors": "Leo Freitas, Aaron John Buhagiar", "abstract": "CSV is a widely used format for data representing systems control,\ninformation exchange and processing, logging, etc. Nevertheless, the format is\nriddled with tricky corner cases and inconsistencies, which can make input data\nunreliable, thus, rendering modelling or simulation experiments unusable or\nunsafe. We address this problem by providing a SAFE-CSV VDM-library that is:\nSimple, Accurate, Fast, and Effective. It extends an ecosystem of other VDM\nmathematical toolkit extensions, which also includes a translation and proof\nenvironment for VDM in Isabelle", "journal": ""}
{"doi": "10.48550/arXiv.1102.1323", "date": "2011-02-07", "title": "Type Classes for Mathematics in Type Theory", "authors": "Bas Spitters, Eelis van der Weegen", "abstract": "The introduction of first-class type classes in the Coq system calls for\nre-examination of the basic interfaces used for mathematical formalization in\ntype theory. We present a new set of type classes for mathematics and take full\nadvantage of their unique features to make practical a particularly flexible\napproach formerly thought infeasible. Thus, we address both traditional proof\nengineering challenges as well as new ones resulting from our ambition to build\nupon this development a library of constructive analysis in which abstraction\npenalties inhibiting efficient computation are reduced to a minimum.\n  The base of our development consists of type classes representing a standard\nalgebraic hierarchy, as well as portions of category theory and universal\nalgebra. On this foundation we build a set of mathematically sound abstract\ninterfaces for different kinds of numbers, succinctly expressed using\ncategorical language and universal algebra constructions. Strategic use of type\nclasses lets us support these high-level theory-friendly definitions while\nstill enabling efficient implementations unhindered by gratuitous indirection,\nconversion or projection.\n  Algebra thrives on the interplay between syntax and semantics. The\nProlog-like abilities of type class instance resolution allow us to\nconveniently define a quote function, thus facilitating the use of reflective\ntechniques.", "journal": ""}
{"doi": "10.48550/arXiv.2105.11896", "date": "2021-05-25", "title": "Tracking Captured Variables in Types", "authors": "Aleksander Boruch-Gruszecki, Jonathan Immanuel Brachth\u00e4user, Edward Lee, Ond\u0159ej Lhot\u00e1k, Martin Odersky", "abstract": "Type systems usually characterize the shape of values but not their free\nvariables. However, there are many desirable safety properties one could\nguarantee if one could track how references can escape. For example, one may\nimplement algebraic effect handlers using capabilities -- a value which permits\none to perform the effect -- safely if one can guarantee that the capability\nitself does not escape the scope bound by the effect handler. To this end, we\nstudy the $\\textrm{CF}_{<:}$ calculus, a conservative and lightweight extension\nof $\\textrm{System F}_{<:}$, to track how values and their references can be\ncaptured and escape. We show that existing terms in $\\textrm{System F}_{<:}$\nembed naturally in our calculus, and that many natural problems can be\nexpressed in a system that tracks variable references like we do in\n$\\textrm{CF}_{<:}$. We also give mechanized proofs of the soundness properties\nof $\\textrm{CF}_{<:}$ in Coq. The type system presented in $\\textrm{CF}_{<:}$\nis powerful enough to reason about safety in the context of many natural\nextensions of $\\textrm{CF}_{<:}$ such as region-based memory-management,\nnon-local returns, and effect handlers.", "journal": ""}
{"doi": "10.48550/arXiv.2407.08936", "date": "2024-07-12", "title": "HHLPar: Automated Theorem Prover for Parallel Hybrid Communicating Sequential Processes", "authors": "Xiangyu Jin, Bohua Zhan, Shuling Wang, Naijun Zhan", "abstract": "We present a tool called HHLPar for verifying hybrid systems modelled in\nHybrid Communicating Sequential Processes (HCSP). HHLPar is built upon a Hybrid\nHoare Logic for HCSP, which is able to reason about continuous-time properties\nof differential equations, as well as communication and parallel composition of\nparallel HCSP processes with the help of parameterised trace assertions and\ntheir synchronization. The logic was formalised and proved to be sound in\nIsabelle/HOL, which constitutes a trustworthy foundation for the verification\nconducted by HHLPar. HHLPar implements the Hybrid Hoare Logic in Python and\nsupports automated verification: On one hand, it provides functions for\nsymbolically decomposing HCSP processes, generating specifications for separate\nsequential processes and then composing them via synchronization to obtain the\nfinal specification for the whole parallel HCSP processes; On the other hand,\nit is integrated with external solvers for handling differential equations and\nreal arithmetic properties. We have conducted experiments on a simplified\ncruise control system to validate the performance of the tool.", "journal": ""}
{"doi": "10.48550/arXiv.1510.01044", "date": "2015-10-05", "title": "A Sorted Semantic Framework for Applied Process Calculi", "authors": "Johannes Borgstr\u00f6m, Ram\u016bnas Gutkovas, Joachim Parrow, Bj\u00f6rn Victor, Johannes \u00c5man Pohjola", "abstract": "Applied process calculi include advanced programming constructs such as type\nsystems, communication with pattern matching, encryption primitives, concurrent\nconstraints, nondeterminism, process creation, and dynamic connection\ntopologies. Several such formalisms, e.g. the applied pi calculus, are\nextensions of the the pi-calculus; a growing number is geared towards\nparticular applications or computational paradigms. Our goal is a unified\nframework to represent different process calculi and notions of computation. To\nthis end, we extend our previous work on psi-calculi with novel abstract\npatterns and pattern matching, and add sorts to the data term language, giving\nsufficient criteria for subject reduction to hold. Our framework can directly\nrepresent several existing process calculi; the resulting transition systems\nare isomorphic to the originals up to strong bisimulation. We also demonstrate\ndifferent notions of computation on data terms, including cryptographic\nprimitives and a lambda-calculus with erratic choice. Finally, we prove\nstandard congruence and structural properties of bisimulation; the proof has\nbeen machine-checked using Nominal Isabelle in the case of a single name sort.", "journal": "Logical Methods in Computer Science, Volume 12, Issue 1 (March 31,\n  2016) lmcs:1631"}
{"doi": "10.48550/arXiv.2309.08962", "date": "2023-09-16", "title": "Dynamic Separation Logic", "authors": "Frank S. de Boer, Hans-Dieter A. Hiep, Stijn de Gouw", "abstract": "This paper introduces a dynamic logic extension of separation logic. The\nassertion language of separation logic is extended with modalities for the five\ntypes of the basic instructions of separation logic: simple assignment,\nlook-up, mutation, allocation, and de-allocation. The main novelty of the\nresulting dynamic logic is that it allows to combine different approaches to\nresolving these modalities. One such approach is based on the standard weakest\nprecondition calculus of separation logic. The other approach introduced in\nthis paper provides a novel alternative formalization in the proposed dynamic\nlogic extension of separation logic. The soundness and completeness of this\naxiomatization has been formalized in the Coq theorem prover.", "journal": "Electronic Notes in Theoretical Informatics and Computer Science,\n  Volume 3 - Proceedings of MFPS XXXIX (November 23, 2023) entics:12297"}
{"doi": "10.48550/arXiv.2403.13457", "date": "2024-03-20", "title": "OSVAuto: semi-automatic verifier for functional specifications of operating systems", "authors": "Yulun Wu, Bohua Zhan, Bican Xia", "abstract": "We present the design and implementation of a tool for semi-automatic\nverification of functional specifications of operating system modules. Such\nverification tasks are traditionally done in interactive theorem provers, where\nthe functionalities of the module are specified at abstract and concrete levels\nusing data such as structures, algebraic datatypes, arrays, maps and so on. In\nthis work, we provide encodings to SMT for these commonly occurring data types.\nThis allows verification conditions to be reduced into a form suitable for SMT\nsolvers. The use of SMT solvers combined with a tactic language allows\nsemi-automatic verification of the specification. We apply the tool to verify\nfunctional specification for key parts of the uC-OS/II operating system, based\non earlier work giving full verification of the system in Coq. We demonstrate a\nlarge reduction in the amount of human effort due to increased level of\nautomation.", "journal": ""}
{"doi": "10.48550/arXiv.0603117", "date": "2006-03-29", "title": "Affine functions and series with co-inductive real numbers", "authors": "Yves Bertot", "abstract": "We extend the work of A. Ciaffaglione and P. Di Gianantonio on mechanical\nverification of algorithms for exact computation on real numbers, using\ninfinite streams of digits implemented as co-inductive types. Four aspects are\nstudied: the first aspect concerns the proof that digit streams can be related\nto the axiomatized real numbers that are already axiomatized in the proof\nsystem (axiomatized, but with no fixed representation). The second aspect\nre-visits the definition of an addition function, looking at techniques to let\nthe proof search mechanism perform the effective construction of an algorithm\nthat is correct by construction. The third aspect concerns the definition of a\nfunction to compute affine formulas with positive rational coefficients. This\nshould be understood as a testbed to describe a technique to combine\nco-recursion and recursion to obtain a model for an algorithm that appears at\nfirst sight to be outside the expressive power allowed by the proof system. The\nfourth aspect concerns the definition of a function to compute series, with an\napplication on the series that is used to compute Euler's number e. All these\nexperiments should be reproducible in any proof system that supports\nco-inductive types, co-recursion and general forms of terminating recursion,\nbut we performed with the Coq system [12, 3, 14].", "journal": "Mathematical Structures in Computer Science 17, 1 (2006)"}
{"doi": "10.48550/arXiv.1503.09169", "date": "2015-03-31", "title": "Improving Collaborations in Neuroscientist Community", "authors": "Isabelle Mirbel, Pierre Crescenzo", "abstract": "In this paper, we present our approach, called SATIS (Semantically AnnotaTed\nIntentions for Services), relying on intentional process modeling and semantic\nweb technologies and models, to assist collaboration among the members of a\nneurosciences community. The main expected result of this work is to derive and\nshare semantic web service specifications from a neuro-scientists point of view\nin order to operationalise image analysis pipelines with web services.", "journal": "International Journal of Web Portals, IGI Global, 2011, 1 (3),\n  pp.33-49"}
{"doi": "10.48550/arXiv.2404.03614", "date": "2024-04-04", "title": "Towards Trustworthy Automated Program Verifiers: Formally Validating Translations into an Intermediate Verification Language (extended version)", "authors": "Gaurav Parthasarathy, Thibault Dardinier, Benjamin Bonneau, Peter M\u00fcller, Alexander J. Summers", "abstract": "Automated program verifiers are typically implemented using an intermediate\nverification language (IVL), such as Boogie or Why3. A verifier front-end\ntranslates the input program and specification into an IVL program, while the\nback-end generates proof obligations for the IVL program and employs an SMT\nsolver to discharge them. Soundness of such verifiers therefore requires that\nthe front-end translation faithfully captures the semantics of the input\nprogram and specification in the IVL program, and that the back-end reports\nsuccess only if the IVL program is actually correct. For a verification tool to\nbe trustworthy, these soundness conditions must be satisfied by its actual\nimplementation, not just the program logic it uses.\n  In this paper, we present a novel validation methodology that, given a formal\nsemantics for the input language and IVL, provides formal soundness guarantees\nfor front-end implementations. For each run of the verifier, we automatically\ngenerate a proof in Isabelle showing that the correctness of the produced IVL\nprogram implies the correctness of the input program. This proof can be checked\nindependently from the verifier, in Isabelle, and can be combined with existing\nwork on validating back-ends to obtain an end-to-end soundness result. Our\nmethodology based on forward simulation employs several modularisation\nstrategies to handle the large semantic gap between the input language and the\nIVL, as well as the intricacies of practical, optimised translations. We\npresent our methodology for the widely-used Viper and Boogie languages. Our\nevaluation shows that it is effective in validating the translations performed\nby the existing Viper implementation.", "journal": ""}
{"doi": "10.48550/arXiv.1803.00169", "date": "2018-03-01", "title": "The Effect of Instruction Padding on SFI Overhead", "authors": "Navid Emamdoost, Stephen McCamant", "abstract": "Software-based fault isolation (SFI) is a technique to isolate a potentially\nfaulty or malicious software module from the rest of a system using\ninstruction-level rewriting. SFI implementations on CISC architectures,\nincluding Google Native Client, use instruction padding to enforce an address\nlayout invariant and restrict control flow. However this padding decreases code\ndensity and imposes runtime overhead. We analyze this overhead, and show that\nit can be reduced by allowing some execution of overlapping instructions, as\nlong as those overlapping instructions are still safe according to the original\nper-instruction policy. We implemented this change for both 32-bit and 64-bit\nx86 versions of Native Client, and analyzed why the performance benefit is\nhigher on 32-bit. The optimization leads to a consistent decrease in the number\nof instructions executed and savings averaging 8.6% in execution time (over\ncompatible benchmarks from SPECint2006) for x86-32. We describe how to modify\nthe validation algorithm to check the more permissive policy, and extend a\nmachine-checked Coq proof to confirm that the system's security is preserved.", "journal": ""}
{"doi": "10.48550/arXiv.1302.6890", "date": "2013-02-27", "title": "A Graphical Language for Proof Strategies", "authors": "Gudmund Grov, Aleks Kissinger, Yuhui Lin", "abstract": "Complex automated proof strategies are often difficult to extract, visualise,\nmodify, and debug. Traditional tactic languages, often based on stack-based\ngoal propagation, make it easy to write proofs that obscure the flow of goals\nbetween tactics and are fragile to minor changes in input, proof structure or\nchanges to tactics themselves. Here, we address this by introducing a graphical\nlanguage called PSGraph for writing proof strategies. Strategies are\nconstructed visually by \"wiring together\" collections of tactics and evaluated\nby propagating goal nodes through the diagram via graph rewriting. Tactic nodes\ncan have many output wires, and use a filtering procedure based on goal-types\n(predicates describing the features of a goal) to decide where best to send\nnewly-generated sub-goals.\n  In addition to making the flow of goal information explicit, the graphical\nlanguage can fulfil the role of many tacticals using visual idioms like\nbranching, merging, and feedback loops. We argue that this language enables\ndevelopment of more robust proof strategies and provide several examples, along\nwith a prototype implementation in Isabelle.", "journal": ""}
{"doi": "10.48550/arXiv.1504.03978", "date": "2015-04-15", "title": "Water transport on graphs", "authors": "Olle H\u00e4ggstr\u00f6m, Timo Hirscher", "abstract": "If the nodes of a graph are considered to be identical barrels - featuring\ndifferent water levels - and the edges to be (locked) water-filled pipes in\nbetween the barrels, one might consider the optimization problem of how much\nthe water level in a fixed barrel can be raised with no pumps available, i.e.\nby opening and closing the locks in an elaborate succession. This problem\noriginated from the analysis of an opinion formation process and proved to be\nnot only sufficiently intricate in order to be of independent interest, but\nalso algorithmically complex. We deal with both finite and infinite graphs as\nwell as deterministic and random initial water levels and find that the\ninfinite line graph, due to its leanness, behaves much more like a finite graph\nin this respect.", "journal": ""}
{"doi": "10.48550/arXiv.1806.04119", "date": "2018-06-11", "title": "Valid Post-selection Inference in Assumption-lean Linear Regression", "authors": "Arun Kumar Kuchibhotla, Lawrence D. Brown, Andreas Buja, Edward I. George, Linda Zhao", "abstract": "Construction of valid statistical inference for estimators based on\ndata-driven selection has received a lot of attention in the recent times. Berk\net al. (2013) is possibly the first work to provide valid inference for\nGaussian homoscedastic linear regression with fixed covariates under arbitrary\ncovariate/variable selection. The setting is unrealistic and is extended by\nBachoc et al. (2016) by relaxing the distributional assumptions. A major\ndrawback of the aforementioned works is that the construction of valid\nconfidence regions is computationally intensive. In this paper, we first prove\nthat post-selection inference is equivalent to simultaneous inference and then\nconstruct valid post-selection confidence regions which are computationally\nsimple. Our construction is based on deterministic inequalities and apply to\nindependent as well as dependent random variables without the requirement of\ncorrect distributional assumptions. Finally, we compare the volume of our\nconfidence regions with the existing ones and show that under non-stochastic\ncovariates, our regions are much smaller.", "journal": ""}
{"doi": "10.48550/arXiv.2502.02181", "date": "2025-02-04", "title": "Well-posedness for the dNLS hierarchy", "authors": "Joseph Adams", "abstract": "We prove well-posedness for higher-order equations in the so-called dNLS\nhierarchy (also known as part of the Kaup-Newell hierarchy) in almost critical\nFourier-Lebesgue and in modulation spaces. Leaning in on estimates proven by\nthe author in a previous instalment Adams (2024), where a similar\nwell-posedness theory was developed for the equations of the NLS hierarchy, we\nshow the $j$th equation in the dNLS hierarchy is locally well-posed for initial\ndata in $\\hat H^s_r(\\mathbb{R})$ for $s \\ge \\frac{1}{2} + \\frac{j-1}{r'}$ and\n$1 < r \\le 2$ and also in $M^s_{2, p}(\\mathbb{R})$ for $s \\ge \\frac{j}{2}$ and\n$2 \\le p < \\infty$. Supplementing our results with corresponding ill-posedness\nresults in Fourier-Lebesgue and modulation spaces shows optimality.\n  Our arguments are based on the Fourier restriction norm method in Bourgain\nspaces adapted to our data spaces and the gauge-transformation commonly\nassociated with the dNLS equation. For the latter we establish bi-Lipschitz\ncontinuity between appropriate modulation spaces and that even for higher-order\nequations `bad' cubic nonlinear terms are lifted from the equation.", "journal": ""}
{"doi": "10.48550/arXiv.1806.06811", "date": "2018-06-18", "title": "Temporal coherence-based self-supervised learning for laparoscopic workflow analysis", "authors": "Isabel Funke, Alexander Jenke, S\u00f6ren Torge Mees, J\u00fcrgen Weitz, Stefanie Speidel, Sebastian Bodenstedt", "abstract": "In order to provide the right type of assistance at the right time,\ncomputer-assisted surgery systems need context awareness. To achieve this,\nmethods for surgical workflow analysis are crucial. Currently, convolutional\nneural networks provide the best performance for video-based workflow analysis\ntasks. For training such networks, large amounts of annotated data are\nnecessary. However, collecting a sufficient amount of data is often costly,\ntime-consuming, and not always feasible. In this paper, we address this problem\nby presenting and comparing different approaches for self-supervised\npretraining of neural networks on unlabeled laparoscopic videos using temporal\ncoherence. We evaluate our pretrained networks on Cholec80, a publicly\navailable dataset for surgical phase segmentation, on which a maximum F1 score\nof 84.6 was reached. Furthermore, we were able to achieve an increase of the F1\nscore of up to 10 points when compared to a non-pretrained neural network.", "journal": "CARE 2018, CLIP 2018, OR 2.0 2018, ISIC 2018. Lecture Notes in\n  Computer Science, vol 11041 (2018) 85-93"}
{"doi": "10.48550/arXiv.2201.13394", "date": "2022-01-31", "title": "A Formal Model of Checked C", "authors": "Liyi Li, Yiyun Liu, Deena L. Postol, Leonidas Lampropoulos, David Van Horn, Michael Hicks", "abstract": "We present a formal model of Checked C, a dialect of C that aims to enforce\nspatial memory safety. Our model pays particular attention to the semantics of\ndynamically sized, potentially null-terminated arrays. We formalize this model\nin Coq, and prove that any spatial memory safety errors can be blamed on\nportions of the program labeled unchecked; this is a Checked C feature that\nsupports incremental porting and backward compatibility. While our model's\noperational semantics uses annotated (\"fat\") pointers to enforce spatial\nsafety, we show that such annotations can be safely erased: Using PLT Redex we\nformalize an executable version of our model and a compilation procedure from\nit to an untyped C-like language, and use randomized testing to validate that\ngenerated code faithfully simulates the original. Finally, we develop a custom\nrandom generator for well-typed and almost-well-typed terms in our Redex model,\nand use it to search for inconsistencies between our model and the Clang\nChecked C implementation. We find these steps to be a useful way to co-develop\na language (Checked C is still in development) and a core model of it.", "journal": ""}
{"doi": "10.48550/arXiv.2309.05483", "date": "2023-09-11", "title": "Sound Atomicity Inference for Data-Centric Synchronization", "authors": "Herv\u00e9 Paulino, Ana Almeida Matos, Jan Cederquist, Marco Giunti, Jo\u00e3o Matos, Ant\u00f3nio Ravara", "abstract": "Data-Centric Concurrency Control (DCCC) shifts the reasoning about\nconcurrency restrictions from control structures to data declaration. It is a\nhigh-level declarative approach that abstracts away from the actual concurrency\ncontrol mechanism(s) in use. Despite its advantages, the practical use of DCCC\nis hindered by the fact that it may require many annotations and/or multiple\nimplementations of the same method to cope with differently qualified\nparameters. Moreover, the existing DCCC solutions do not address the use of\ninterfaces, precluding their use in most object-oriented programs. To overcome\nthese limitations, in this paper we present AtomiS, a new DCCC model based on a\nrigorously defined type-sound programming language. Programming with AtomiS\nrequires only (atomic)-qualifying types of parameters and return values in\ninterface definitions, and of fields in class definitions. From this atomicity\nspecification, a static analysis infers the atomicity constraints that are\nlocal to each method, considering valid only the method variants that are\nconsistent with the specification, and performs code generation for all valid\nvariants of each method. The generated code is then the target for automatic\ninjection of concurrency control primitives, by means of the desired automatic\ntechnique and associated atomicity and deadlock-freedom guarantees, which can\nbe plugged-into the model's pipeline. We present the foundations for the AtomiS\nanalysis and synthesis, with formal guarantees that the generated program is\nwell-typed and that it corresponds behaviourally to the original one. The\nproofs are mechanised in Coq. We also provide a Java implementation that\nshowcases the applicability of AtomiS in real-life programs.", "journal": ""}
{"doi": "10.48550/arXiv.1903.01237", "date": "2019-03-04", "title": "Dijkstra Monads for All", "authors": "Kenji Maillard, Danel Ahman, Robert Atkey, Guido Martinez, Catalin Hritcu, Exequiel Rivas, \u00c9ric Tanter", "abstract": "This paper proposes a general semantic framework for verifying programs with\narbitrary monadic side-effects using Dijkstra monads, which we define as\nmonad-like structures indexed by a specification monad. We prove that any monad\nmorphism between a computational monad and a specification monad gives rise to\na Dijkstra monad, which provides great flexibility for obtaining Dijkstra\nmonads tailored to the verification task at hand. We moreover show that a large\nvariety of specification monads can be obtained by applying monad transformers\nto various base specification monads, including predicate transformers and\nHoare-style pre- and postconditions. For defining correct monad transformers,\nwe propose a language inspired by Moggi's monadic metalanguage that is\nparameterized by a dependent type theory. We also develop a notion of algebraic\noperations for Dijkstra monads, and start to investigate two ways of also\naccommodating effect handlers. We implement our framework in both Coq and F*,\nand illustrate that it supports a wide variety of verification styles for\neffects such as exceptions, nondeterminism, state, input-output, and general\nrecursion.", "journal": ""}
{"doi": "10.48550/arXiv.1911.12737", "date": "2019-11-28", "title": "LL(1) Parsing with Derivatives and Zippers", "authors": "Romain Edelmann, Jad Hamza, Viktor Kun\u010dak", "abstract": "In this paper, we present an efficient, functional, and formally verified\nparsing algorithm for LL(1) context-free expressions based on the concept of\nderivatives of formal languages. Parsing with derivatives is an elegant parsing\ntechnique, which, in the general case, suffers from cubic worst-case time\ncomplexity and slow performance in practice. We specialise the parsing with\nderivatives algorithm to LL(1) context-free expressions, where alternatives can\nbe chosen given a single token of lookahead. We formalise the notion of LL(1)\nexpressions and show how to efficiently check the LL(1) property. Next, we\npresent a novel linear-time parsing with derivatives algorithm for LL(1)\nexpressions operating on a zipper-inspired data structure. We prove the\nalgorithm correct in Coq and present an implementation as a parser combinators\nframework in Scala, with enumeration and pretty printing capabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2501.10560", "date": "2025-01-17", "title": "Picachv: Formally Verified Data Use Policy Enforcement for Secure Data Analytics", "authors": "Haobin Hiroki Chen, Hongbo Chen, Mingshen Sun, Chenghong Wang, XiaoFeng Wang", "abstract": "Ensuring the proper use of sensitive data in analytics under complex privacy\npolicies is an increasingly critical challenge. Many existing approaches lack\nportability, verifiability, and scalability across diverse data processing\nframeworks. We introduce Picachv, a novel security monitor that automatically\nenforces data use policies. It works on relational algebra as an abstraction\nfor program semantics, enabling policy enforcement on query plans generated by\nprograms during execution. This approach simplifies analysis across diverse\nanalytical operations and supports various front-end query languages. By\nformalizing both data use policies and relational algebra semantics in Coq, we\nprove that Picachv correctly enforces policies. Picachv also leverages Trusted\nExecution Environments (TEEs) to enhance trust in runtime, providing provable\npolicy compliance to stakeholders that the analytical tasks comply with their\ndata use policies. We integrated Picachv into Polars, a state-of-the-art data\nanalytics framework, and evaluate its performance using the TPC-H benchmark. We\nalso apply our approach to real-world use cases. Our work demonstrates the\npractical application of formal methods in securing data analytics, addressing\nkey challenges.", "journal": ""}
{"doi": "10.48550/arXiv.2111.04134", "date": "2021-11-07", "title": "Mapping Access to Water and Sanitation in Colombia using Publicly Accessible Satellite Imagery, Crowd-sourced Geospatial Information and RandomForests", "authors": "Niccolo Dejito, Ren Avell Flores, Rodolfo de Guzman, Isabelle Tingzon, Liliana Carvajal, Alberto Aroca, Carlos Delgado", "abstract": "Up-to-date, granular, and reliable quality of life data is crucial for\nhumanitarian organizations to develop targeted interventions for vulnerable\ncommunities, especially in times of crisis. One such quality of life data is\naccess to water, sanitation and hygeine (WASH). Traditionally, data collection\nis done through door-to-door surveys sampled over large areas. Unfortunately,\nthe huge costs associated with collecting these data deter more frequent and\nlarge-coverage surveys. To address this challenge, we present a scalable and\ninexpensive end-to-end WASH estimation workflow using a combination of machine\nlearning and census data, publicly available satellite images, and\ncrowd-sourced geospatial information. We generate a map of WASH estimates at a\ngranularity of 250m x 250m across the entire country of Colombia. The model was\nable to explain up to 65% of the variation in predicting access to water\nsupply, sewage, and toilets. The code is made available with MIT License at\nhttps://github.com/thinkingmachines/geoai-immap-wash.", "journal": ""}
{"doi": "10.48550/arXiv.2306.15365", "date": "2023-06-27", "title": "Herb-Drug Interactions: A Holistic Decision Support System in Healthcare", "authors": "Andreia Martins, Eva Maia, Isabel Pra\u00e7a", "abstract": "Complementary and alternative medicine are commonly used concomitantly with\nconventional medications leading to adverse drug reactions and even fatality in\nsome cases. Furthermore, the vast possibility of herb-drug interactions\nprevents health professionals from remembering or manually searching them in a\ndatabase. Decision support systems are a powerful tool that can be used to\nassist clinicians in making diagnostic and therapeutic decisions in patient\ncare. Therefore, an original and hybrid decision support system was designed to\nidentify herb-drug interactions, applying artificial intelligence techniques to\nidentify new possible interactions. Different machine learning models will be\nused to strengthen the typical rules engine used in these cases. Thus, using\nthe proposed system, the pharmacy community, people's first line of contact\nwithin the Healthcare System, will be able to make better and more accurate\ntherapeutic decisions and mitigate possible adverse events.", "journal": "2022 IEEE International Conference on E-health Networking,\n  Application & Services (HealthCom)"}
{"doi": "10.48550/arXiv.2307.14471", "date": "2023-07-26", "title": "Modal Abstractions for Virtualizing Memory Addresses", "authors": "Ismail Kuru, Colin S. Gordon", "abstract": "Operating system kernels employ virtual memory subsystems, which use a CPU's\nmemory management units (MMUs) to virtualize the addresses of memory regions\nOperating systems manipulate these virtualized memory mappings to isolate\nuntrusted processes, restrict which memory is accessible to different\nprocesses, hide memory limits from user programs, ensure process isolation,\nimplement demand-paging and copy-on-write behaviors for performance and\nresource controls.\n  Virtual memory management (VMM) code is a critical piece of general-purpose\nOS kernels, but verification of this functionality is challenging due to the\ncomplexity of the hardware interface. In this paper, we introduce a modal\nabstraction to describe the truth of assertions relative to a specific virtual\naddress space: [r]P indicating that P holds in the virtual address space rooted\nat r. Such modal assertions allow different address spaces to refer to each\nother, enabling complete verification of instruction sequences manipulating\nmultiple address spaces. Using them effectively requires working with other\nassertions, such as points-to assertions in our separation logic, as relative\nto a given address space. We therefore define virtual points-to relations,\nwhich mimic hardware address translation, relative to a page table root. We\ndemonstrate our approach with challenging fragments of VMM code showing that\nour approach handles examples beyond what prior work can address, including\nreasoning about a sequence of instructions as it changes address spaces. All\ndefinitions and theorems mentioned in this paper including the operational\nmodel of a RISC-like fragment of x86-64, a simple language run on this\noperational model, and a logic as an instantiation of the Iris framework are\nmechanized inside Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2109.02991", "date": "2021-09-07", "title": "Abstraction Logic: The Marriage of Contextual Refinement and Separation Logic", "authors": "Youngju Song, Minki Cho, Dongjae Lee, Chung-Kil Hur", "abstract": "Contextual refinement and separation logics are successful verification\ntechniques that are very different in nature. First, the former guarantees\nbehavioral refinement between a concrete program and an abstract program while\nthe latter guarantees safety of a concrete program under certain conditions\n(expressed in terms of pre and post conditions). Second, the former does not\nallow any assumption about the context when locally reasoning about a module\nwhile the latter allows rich assumptions. In this paper, we present a new\nverification technique, called abstraction logic (AL), that inherently combines\ncontextual refinement and separation logics such as Iris and VST, thereby\ntaking the advantages of both. Specifically, AL allows us to locally verify a\nconcrete module against an abstract module under separation-logic-style pre and\npost conditions about external modules. AL are fully formalized in Coq and\nprovides a proof mode that supports a combination of simulation-style reasoning\nusing our own tactics and SL-style reasoning using IPM (Iris Proof Mode). Using\nthe proof mode, we verified various examples to demonstrate reasoning about\nownership (based on partial commutative monoids) and purity ($i.e.$,\ntermination with no system call), cyclic and higher-order reasoning about\nmutual recursion and function pointers, and reusable and gradual verification\nvia intermediate abstractions. Also, the verification results are combined with\nCompCert, so that we formally establish behavioral refinement from top-level\nabstract programs, all the way down to their assembly code.", "journal": ""}
{"doi": "10.48550/arXiv.2007.15126", "date": "2020-07-29", "title": "Towards a Formal Foundation of Intermittent Computing", "authors": "Milijana Surbatovich, Limin Jia, Brandon Lucia", "abstract": "Intermittently powered devices enable new applications in harsh or\ninaccessible environments, such as space or in-body implants, but also\nintroduce problems in programmability and correctness. Researchers have\ndeveloped programming models to ensure that programs make progress and do not\nproduce erroneous results due to memory inconsistencies caused by intermittent\nexecutions. As the technology has matured, more and more features are added to\nintermittently powered devices, such as I/O. Prior work has shown that all\nexisting intermittent execution models have problems with repeated device or\nsensor inputs (RIO). RIOs could leave intermittent executions in an\ninconsistent state. Such problems and the proliferation of existing\nintermittent execution models necessitate a formal foundation for intermittent\ncomputing.\n  In this paper, we formalize intermittent execution models, their correctness\nproperties with respect to memory consistency and inputs, and identify the\ninvariants needed to prove systems correct. We prove equivalence between\nseveral existing intermittent systems. To address RIO problems, we define an\nalgorithm for identifying variables affected by RIOs that need to be restored\nafter reboot and prove the algorithm correct. Finally, we implement the\nalgorithm in a novel intermittent runtime system that is correct with respect\nto input operations and evaluate its performance.", "journal": "Proc. ACM Program. Lang. 4, OOPSLA, Article 163 (November 2020),\n  31 pages (2020)"}
{"doi": "10.48550/arXiv.1903.02306", "date": "2019-03-06", "title": "Video-based surgical skill assessment using 3D convolutional neural networks", "authors": "Isabel Funke, S\u00f6ren Torge Mees, J\u00fcrgen Weitz, Stefanie Speidel", "abstract": "Purpose: A profound education of novice surgeons is crucial to ensure that\nsurgical interventions are effective and safe. One important aspect is the\nteaching of technical skills for minimally invasive or robot-assisted\nprocedures. This includes the objective and preferably automatic assessment of\nsurgical skill. Recent studies presented good results for automatic, objective\nskill evaluation by collecting and analyzing motion data such as trajectories\nof surgical instruments. However, obtaining the motion data generally requires\nadditional equipment for instrument tracking or the availability of a robotic\nsurgery system to capture kinematic data. In contrast, we investigate a method\nfor automatic, objective skill assessment that requires video data only. This\nhas the advantage that video can be collected effortlessly during minimally\ninvasive and robot-assisted training scenarios.\n  Methods: Our method builds on recent advances in deep learning-based video\nclassification. Specifically, we propose to use an inflated 3D ConvNet to\nclassify snippets, i.e., stacks of a few consecutive frames, extracted from\nsurgical video. The network is extended into a Temporal Segment Network during\ntraining.\n  Results: We evaluate the method on the publicly available JIGSAWS dataset,\nwhich consists of recordings of basic robot-assisted surgery tasks performed on\na dry lab bench-top model. Our approach achieves high skill classification\naccuracies ranging from 95.1% to 100.0%.\n  Conclusions: Our results demonstrate the feasibility of deep learning-based\nassessment of technical skill from surgical video. Notably, the 3D ConvNet is\nable to learn meaningful patterns directly from the data, alleviating the need\nfor manual feature engineering. Further evaluation will require more annotated\ndata for training and testing.", "journal": "IJCARS 14.7 (2019) pp. 1217-1225"}
{"doi": "10.48550/arXiv.2207.12039", "date": "2022-07-25", "title": "Isabelle/HOL/GST: A Formal Proof Environment for Generalized Set Theories", "authors": "Ciar\u00e1n Dunne, J. B. Wells", "abstract": "A generalized set theory (GST) is like a standard set theory but also can\nhave non-set structured objects that can contain other structured objects\nincluding sets. This paper presents Isabelle/HOL support for GSTs, which are\ntreated as type classes that combine features that specify kinds of\nmathematical objects, e.g., sets, ordinal numbers, functions, etc. GSTs can\nhave an exception feature that eases representing partial functions and\nundefinedness. When assembling a GST, extra axioms are generated following a\nuser-modifiable policy to fill specification gaps. Specialized type-like\npredicates called soft types are used extensively. Although a GST can be used\nwithout a model, for confidence in its consistency we build a model for each\nGST from components that specify each feature's contribution to each tier of a\nvon-Neumann-style cumulative hierarchy defined via ordinal recursion, and we\nthen connect the model to a separate type which the GST occupies.", "journal": ""}
{"doi": "10.48550/arXiv.1210.6390", "date": "2012-10-23", "title": "Elaborating Inductive Definitions", "authors": "Pierre-Evariste Dagand, Conor McBride", "abstract": "We present an elaboration of inductive definitions down to a universe of\ndatatypes. The universe of datatypes is an internal presentation of strictly\npositive families within type theory. By elaborating an inductive definition --\na syntactic artifact -- to its code -- its semantics -- we obtain an\ninternalized account of inductives inside the type theory itself: we claim that\nreasoning about inductive definitions could be carried in the type theory, not\nin the meta-theory as it is usually the case. Besides, we give a formal\nspecification of that elaboration process. It is therefore amenable to formal\nreasoning too. We prove the soundness of our translation and hint at its\ncorrectness with respect to Coq's Inductive definitions.\n  The practical benefits of this approach are numerous. For the type theorist,\nthis is a small step toward bootstrapping, ie. implementing the inductive\nfragment in the type theory itself. For the programmer, this means better\nsupport for generic programming: we shall present a lightweight deriving\nmechanism, entirely definable by the programmer and therefore not requiring any\nextension to the type theory.", "journal": ""}
{"doi": "10.48550/arXiv.1811.02835", "date": "2018-11-07", "title": "Unification in Matching Logic - Extended Version", "authors": "Andrei Arusoaie, Dorel Lucanu", "abstract": "Matching Logic is a framework for specifying programming language semantics\nand reasoning about programs. Its formulas are called patterns and are built\nwith variables, symbols, connectives and quantifiers. A pattern is a\ncombination of structural components (term patterns), which must be matched,\nand constraints (predicate patterns), which must be satisfied. Dealing with\nmore than one structural component in a pattern could be cumbersome because it\ninvolves multiple matching operations. A source for getting patterns with many\nstructural components is the conjunction of patterns. Here, we propose a method\nthat uses a syntactic unification algorithm to transform conjunctions of\nstructural patterns into equivalent patterns having only one structural\ncomponent and some additional constraints. We prove the soundness of our\napproach, we discuss why the approach is not complete and we provide sound\nstrategies to generate certificates for the equivalences, validated using Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2406.14787", "date": "2024-06-20", "title": "Story of Your Lazy Function's Life: A Bidirectional Demand Semantics for Mechanized Cost Analysis of Lazy Programs", "authors": "Li-yao Xia, Laura Israel, Maite Kramarz, Nicholas Coltharp, Koen Claessen, Stephanie Weirich, Yao Li", "abstract": "Lazy evaluation is a powerful tool that enables better compositionality and\npotentially better performance in functional programming, but it is challenging\nto analyze its computation cost. Existing works either require manually\nannotating sharing, or rely on separation logic to reason about heaps of\nmutable cells. In this paper, we propose a bidirectional demand semantics that\nallows for extrinsic reasoning about the computation cost of lazy programs\nwithout relying on special program logics. To show the effectiveness of our\napproach, we apply the demand semantics to a variety of case studies including\ninsertion sort, selection sort, Okasaki's banker's queue, and the implicit\nqueue. We formally prove that the banker's queue and the implicit queue are\nboth amortized and persistent using the Rocq Prover (formerly known as Coq). We\nalso propose the reverse physicist's method, a novel variant of the classical\nphysicist's method, which enables mechanized, modular and compositional\nreasoning about amortization and persistence with the demand semantics.", "journal": ""}
{"doi": "10.48550/arXiv.2009.14001", "date": "2020-09-29", "title": "Improving Interpretability for Computer-aided Diagnosis tools on Whole Slide Imaging with Multiple Instance Learning and Gradient-based Explanations", "authors": "Antoine Pirovano, Hippolyte Heuberger, Sylvain Berlemont, Sa\u00efd Ladjal, Isabelle Bloch", "abstract": "Deep learning methods are widely used for medical applications to assist\nmedical doctors in their daily routines. While performances reach expert's\nlevel, interpretability (highlight how and what a trained model learned and why\nit makes a specific decision) is the next important challenge that deep\nlearning methods need to answer to be fully integrated in the medical field. In\nthis paper, we address the question of interpretability in the context of whole\nslide images (WSI) classification. We formalize the design of WSI\nclassification architectures and propose a piece-wise interpretability\napproach, relying on gradient-based methods, feature visualization and multiple\ninstance learning context. We aim at explaining how the decision is made based\non tile level scoring, how these tile scores are decided and which features are\nused and relevant for the task. After training two WSI classification\narchitectures on Camelyon-16 WSI dataset, highlighting discriminative features\nlearned, and validating our approach with pathologists, we propose a novel\nmanner of computing interpretability slide-level heat-maps, based on the\nextracted features, that improves tile-level classification performances by\nmore than 29% for AUC.", "journal": ""}
{"doi": "10.48550/arXiv.1502.00461", "date": "2015-02-02", "title": "Hexagonal Projected Symmetries", "authors": "Juliane F. Oliveira, Sofia S. B. S. D. Castro, Isabel S. Labouriau", "abstract": "In the study of pattern formation in symmetric physical systems a\n3-dimensional structure in thin domains is often modelled as 2-dimensional one.\nWe are concerned with functions in $R^3$ that are invariant under the action of\na crystallographic group and the symmetries of their projections into a\nfunction defined on a plane. We obtain a list of the crystallographic groups\nfor which the projected functions have a hexagonal lattice of periods. The\nproof is constructive and the result may be used in the study of observed\npatterns in thin domains, whose symmetries are not expected in 2-dimensional\nmodels, like the black-eye pattern.", "journal": ""}
{"doi": "10.48550/arXiv.1907.09334", "date": "2019-07-17", "title": "LinTO : Assistant vocal open-source respectueux des donn\u00e9es personnelles pour les r\u00e9unions d'entreprise", "authors": "Jean-Pierre Lorr\u00e9, Isabelle Ferran\u00e9, Francisco Madrigal, Michalis Vazirgiannis, Christophe Bourguignat", "abstract": "This paper presents the first results of the PIA \"Grands D\\'efis du\nNum\\'erique\" research project LinTO. The goal of this project is to develop a\nconversational assistant to help the company's employees, particularly during\nmeetings. LinTO is an interactive device equipped with microphones, a screen\nand a 360$^\\circ$ camera, which allows to control the room, query company's\ninformation system, helps facilitate the meeting and provides an environment to\naid minute writing. Distributed according to an open model that respects\nprivate data LinTO is the first open-source enterprise's assistant designed to\ncomply with the GDPR requirements.", "journal": ""}
{"doi": "10.48550/arXiv.2007.00548", "date": "2020-07-01", "title": "Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse Surgical Instrument Usage for Context-aware Assistance", "authors": "Dominik Rivoir, Sebastian Bodenstedt, Isabel Funke, Felix von Bechtolsheim, Marius Distler, J\u00fcrgen Weitz, Stefanie Speidel", "abstract": "Intra-operative anticipation of instrument usage is a necessary component for\ncontext-aware assistance in surgery, e.g. for instrument preparation or\nsemi-automation of robotic tasks. However, the sparsity of instrument\noccurrences in long videos poses a challenge. Current approaches are limited as\nthey assume knowledge on the timing of future actions or require dense temporal\nsegmentations during training and inference. We propose a novel learning task\nfor anticipation of instrument usage in laparoscopic videos that overcomes\nthese limitations. During training, only sparse instrument annotations are\nrequired and inference is done solely on image data. We train a probabilistic\nmodel to address the uncertainty associated with future events. Our approach\noutperforms several baselines and is competitive to a variant using richer\nannotations. We demonstrate the model's ability to quantify task-relevant\nuncertainties. To the best of our knowledge, we are the first to propose a\nmethod for anticipating instruments in surgery.", "journal": ""}
{"doi": "10.48550/arXiv.1509.01967", "date": "2015-09-07", "title": "Dirichlet principal eigenvalue comparison theorems in geometry with torsion", "authors": "Ana Cristina Ferreira, Isabel Salavessa", "abstract": "We describe min-max formulas for the principal eigenvalue of a $V$-drift\nLaplacian defined by a vector field $V$ on a geodesic ball of a Riemannian\nmanifold $N$. Then we derive comparison results for the principal eigenvalue\nwith the one of a spherically symmetric model space endowed with a radial\nvector field, under pointwise comparison of the corresponding radial sectional\nand Ricci curvatures, and of the radial component of the vector fields. These\nresults generalize the known case $V=0$.", "journal": "Journal of Mathematical Analysis and Applications Volume 453,\n  Issue 2, 15 September 2017, 700-723"}
{"doi": "10.48550/arXiv.2309.09141", "date": "2023-09-17", "title": "Event-based Compositional Reasoning of Information-Flow Security for Concurrent Systems", "authors": "Yongwang Zhao, David Sanan, Fuyuan Zhang, Yang Liu", "abstract": "High assurance of information-flow security (IFS) for concurrent systems is\nchallenging. A promising way for formal verification of concurrent systems is\nthe rely-guarantee method. However, existing compositional reasoning approaches\nfor IFS concentrate on language-based IFS. It is often not applicable for\nsystem-level security, such as multicore operating system kernels, in which\nsecrecy of actions should also be considered. On the other hand, existing\nstudies on the rely-guarantee method are basically built on concurrent\nprogramming languages, by which semantics of concurrent systems cannot be\ncompletely captured in a straightforward way. In order to formally verify\nstate-action based IFS for concurrent systems, we propose a\nrely-guarantee-based compositional reasoning approach for IFS in this paper. We\nfirst design a language by incorporating ``Event'' into concurrent languages\nand give the IFS semantics of the language. As a primitive element, events\noffer an extremely neat framework for modeling system and are not necessarily\natomic in our language. For compositional reasoning of IFS, we use\nrely-guarantee specification to define new forms of unwinding conditions (UCs)\non events, i.e., event UCs. By a rely-guarantee proof system of the language\nand the soundness of event UCs, we have that event UCs imply IFS of concurrent\nsystems. In such a way, we relax the atomicity constraint of actions in\ntraditional UCs and provide a compositional reasoning way for IFS in which\nsecurity proof of systems can be discharged by independent security proof on\nindividual events. Finally, we mechanize the approach in Isabelle/HOL and\ndevelop a formal specification and its IFS proof for multicore separation\nkernels as a study case according to an industrial standard -- ARINC 653.", "journal": ""}
{"doi": "10.48550/arXiv.1610.08476", "date": "2016-10-26", "title": "Gradual Typing in an Open World", "authors": "Michael M. Vitousek, Jeremy G. Siek", "abstract": "Gradual typing combines static and dynamic typing in the same language,\noffering the benefits of both to programmers. Static typing provides error\ndetection and strong guarantees while dynamic typing enables rapid prototyping\nand flexible programming idioms. For programmers to fully take advantage of a\ngradual type system, however, they must be able to trust their type\nannotations, and so runtime checks must be performed at the boundaries of\nstatic and dynamic code to ensure that static types are respected. Higher order\nand mutable values cannot be completely checked at these boundaries, and so\nadditional checks must be performed at their use sites. Traditionally, this has\nbeen achieved by installing wrappers or proxies on such values that moderate\nthe flow of data between static and dynamic, but these can cause problems if\nthe language supports comparison of object identity or has a foreign function\ninterface.\n  Reticulated Python is a gradually typed variant of Python implemented via a\nsource-to-source translator for Python 3. It implements a proxy-free\nalternative design named transient casts. This paper presents a formal\nsemantics for transient casts and shows that not only are they sound, but they\nwork in an open-world setting in which the Reticulated translator has only been\napplied to some of the program; the rest is untranslated Python. We formalize\nthis open world soundness property and use Coq to prove that it holds for\nAnthill Python, a calculus that models Reticulated Python.", "journal": ""}
{"doi": "10.48550/arXiv.2011.15115", "date": "2020-11-30", "title": "The degree of the central curve in semidefinite, linear, and quadratic programming", "authors": "Serkan Ho\u015ften, Isabelle Shankar, Ang\u00e9lica Torres", "abstract": "The Zariski closure of the central path which interior point algorithms track\nin convex optimization problems such as linear, quadratic, and semidefinite\nprograms is an algebraic curve. The degree of this curve has been studied in\nrelation to the complexity of these interior point algorithms, and for linear\nprograms it was computed by De Loera, Sturmfels, and Vinzant in 2012. We show\nthat the degree of the central curve for generic semidefinite programs is equal\nto the maximum likelihood degree of linear concentration models. New results\nfrom the intersection theory of the space of complete quadrics imply that this\nis a polynomial in the size of semidefinite matrices with degree equal to the\nnumber of constraints. Besides its degree we explore the arithmetic genus of\nthe same curve. We also compute the degree of the central curve for generic\nlinear programs with different techniques which extend to bounding the same\ndegree for generic quadratic programs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.03465", "date": "2024-10-04", "title": "Formalizing MLTL Formula Progression in Isabelle/HOL", "authors": "Katherine Kosaian, Zili Wang, Elizabeth Sloan, Kristin Rozier", "abstract": "Mission-time Linear Temporal Logic (MLTL) is rapidly increasing in popularity\nas a specification logic, e.g., for runtime verification and model checking,\ndriving a need for a trustworthy tool base for analyzing MLTL. In this work, we\nformalize the syntax and semantics of MLTL and a library of key properties,\nincluding useful custom induction rules. We envision this library as being\nuseful for future formalizations involving MLTL and as serving as a reference\npoint for theoretical work using or developing MLTL. We then formalize the\nalgorithm and correctness theorems for MLTL formula progression; along the way,\nwe identify and fix several errors and gaps in the source material. A main\nmotivation for our work is tool validation; we ensure the executability of our\nalgorithms by using Isabelle's built-in code generator.", "journal": ""}
{"doi": "10.48550/arXiv.0604050", "date": "2006-04-03", "title": "Mathematical study of the betaplane model: Equatorial waves and convergence results", "authors": "Isabelle Gallagher, Laure Saint-Raymond", "abstract": "We are interested in a model of rotating fluids, describing the motion of the\nocean in the equatorial zone. This model is known as the Saint-Venant, or\nshallow-water type system, to which a rotation term is added whose amplitude is\nlinear with respect to the latitude; in particular it vanishes at the equator.\nAfter a physical introduction to the model, we describe the various waves\ninvolved and study in detail the resonances associated with those waves. We\nthen exhibit the formal limit system (as the rotation becomes large), obtained\nas usual by filtering out the waves, and prove its wellposedness. Finally we\nprove three types of convergence results: a weak convergence result towards a\nlinear, geostrophic equation, a strong convergence result of the filtered\nsolutions towards the unique strong solution to the limit system, and finally a\n\"hybrid\" strong convergence result of the filtered solutions towards a weak\nsolution to the limit system. In particular we obtain that there are no\nconfined equatorial waves in the mean motion as the rotation becomes large.", "journal": ""}
{"doi": "10.48550/arXiv.2401.15041", "date": "2024-01-26", "title": "Computationally Bounded Robust Compilation and Universally Composable Security", "authors": "Robert K\u00fcnnemann, Marco Patrignani, Ethan Cecchetti", "abstract": "Universal Composability (UC) is the gold standard for cryptographic security,\nbut mechanizing proofs of UC is notoriously difficult. A recently-discovered\nconnection between UC and Robust Compilation (RC)$\\unicode{x2014}$a novel\ntheory of secure compilation$\\unicode{x2014}$provides a means to verify UC\nproofs using tools that mechanize equality results. Unfortunately, the existing\nmethods apply only to perfect UC security, and real-world protocols relying on\ncryptography are only computationally secure.\n  This paper addresses this gap by lifting the connection between UC and RC to\nthe computational setting, extending techniques from the RC setting to apply to\ncomputational UC security. Moreover, it further generalizes the\nUC$\\unicode{x2013}$RC connection beyond computational security to arbitrary\nequalities, providing a framework to subsume the existing perfect case, and to\ninstantiate future theories with more complex notions of security. This\nconnection allows the use of tools for proofs of computational\nindistinguishability to properly mechanize proofs of computational UC security.\nWe demonstrate this power by using CryptoVerif to mechanize a proof that parts\nof the Wireguard protocol are computationally UC secure. Finally, all proofs of\nthe framework itself are verified in Isabelle/HOL.", "journal": "Proceedings of the 2024 IEEE Computer Security Foundations\n  Symposium (CSF)"}
{"doi": "10.48550/arXiv.2502.09083", "date": "2025-02-13", "title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking", "authors": "Greta Warren, Irina Shklovski, Isabelle Augenstein", "abstract": "The pervasiveness of large language models and generative AI in online media\nhas amplified the need for effective automated fact-checking to assist\nfact-checkers in tackling the increasing volume and sophistication of\nmisinformation. The complex nature of fact-checking demands that automated\nfact-checking systems provide explanations that enable fact-checkers to\nscrutinise their outputs. However, it is unclear how these explanations should\nalign with the decision-making and reasoning processes of fact-checkers to be\neffectively integrated into their workflows. Through semi-structured interviews\nwith fact-checking professionals, we bridge this gap by: (i) providing an\naccount of how fact-checkers assess evidence, make decisions, and explain their\nprocesses; (ii) examining how fact-checkers use automated tools in practice;\nand (iii) identifying fact-checker explanation requirements for automated\nfact-checking tools. The findings show unmet explanation needs and identify\nimportant criteria for replicable fact-checking explanations that trace the\nmodel's reasoning path, reference specific evidence, and highlight uncertainty\nand information gaps.", "journal": ""}
{"doi": "10.48550/arXiv.2203.01643", "date": "2022-03-03", "title": "Improving X-ray Diagnostics through Eye-Tracking and XR", "authors": "Catarina Moreira, Isabel Blanco Nobre, Sandra Costa Sousa, Jo\u00e3o Madeiras Pereira, Joaquim Jorge", "abstract": "There is a growing need to assist radiologists in performing X-ray readings\nand diagnoses fast, comfortably, and effectively. As radiologists strive to\nmaximize productivity, it is essential to consider the impact of reading rooms\nin interpreting complex examinations and ensure that higher volume and\nreporting speeds do not compromise patient outcomes. Virtual Reality (VR) is a\ndisruptive technology for clinical practice in assessing X-ray images. We argue\nthat conjugating eye-tracking with VR devices and Machine Learning may overcome\nobstacles posed by inadequate ergonomic postures and poor room conditions that\noften cause erroneous diagnostics when professionals examine digital images.", "journal": "1st International Workshop on XR for Healthcare and Wellbeing,\n  2022"}
{"doi": "10.48550/arXiv.1709.04255", "date": "2017-09-13", "title": "On the Generation of Initial Contexts for Effective Deadlock Detection", "authors": "Elvira Albert, Miguel G\u00f3mez-Zamalloa, Miguel Isabel", "abstract": "It has been recently proposed that testing based on symbolic execution can be\nused in conjunction with static deadlock analysis to define a deadlock\ndetection framework that: (i) can show deadlock presence, in that case a\nconcrete test-case and trace are obtained, and (ii) can also prove deadlock\nfreedom. Such symbolic execution starts from an initial distributed context,\ni.e., a set of locations and their initial tasks. Considering all possibilities\nresults in a combinatorial explosion on the different distributed contexts that\nmust be considered. This paper proposes a technique to effectively generate\ninitial contexts that can lead to deadlock, using the possible conflicting task\ninteractions identified by static analysis, discarding other distributed\ncontexts that cannot lead to deadlock. The proposed technique has been\nintegrated in the above-mentioned deadlock detection framework hence enabling\nit to analyze systems without the need of any user supplied initial context.", "journal": ""}
{"doi": "10.48550/arXiv.1809.02375", "date": "2018-09-07", "title": "W-types in setoids", "authors": "Jacopo Emmenegger", "abstract": "We present a construction of W-types in the setoid model of extensional\nMartin-L\\\"of type theory using dependent W-types in the underlying intensional\ntheory. More precisely, we prove that the internal category of setoids has\ninitial algebras for polynomial endofunctors. In particular, we characterise\nthe setoid of algebra morphisms from the initial algebra to a given algebra as\na setoid on a dependent W-type. We conclude by discussing the case of free\nsetoids. We work in a fully intensional theory and, in fact, we assume identity\ntypes only when discussing free setoids. By using dependent W-types we can also\navoid elimination into a type universe. The results have been verified in Coq\nand a formalisation is available on the author's GitHub page.", "journal": "Logical Methods in Computer Science, Volume 17, Issue 3 (September\n  24, 2021) lmcs:5764"}
{"doi": "10.48550/arXiv.2502.21156", "date": "2025-02-28", "title": "Cryptis: Cryptographic Reasoning in Separation Logic", "authors": "Arthur Azevedo de Amorim, Amal Ahmed, Marco Gaboardi", "abstract": "We introduce Cryptis, an extension of the Iris separation logic that can be\nused to verify cryptographic components using the symbolic model of\ncryptography. The combination of separation logic and cryptographic reasoning\nallows us to prove the correctness of a protocol and later reuse this result to\nverify larger systems that rely on the protocol. To make this integration\npossible, we propose novel specifications for authentication protocols that\nallow agents in a network to agree on the use of system resources. We evaluate\nour approach by verifying various authentication protocols and a key-value\nstore server that uses these authentication protocols to connect to clients.\nOur results are formalized in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2005.10554", "date": "2020-05-21", "title": "Repairing and Mechanising the JavaScript Relaxed Memory Model", "authors": "Conrad Watt, Christopher Pulte, Anton Podkopaev, Guillaume Barbier, Stephen Dolan, Shaked Flur, Jean Pichon-Pharabod, Shu-yu Guo", "abstract": "Modern JavaScript includes the SharedArrayBuffer feature, which provides\naccess to true shared memory concurrency. SharedArrayBuffers are simple linear\nbuffers of bytes, and the JavaScript specification defines an axiomatic relaxed\nmemory model to describe their behaviour. While this model is heavily based on\nthe C/C++11 model, it diverges in some key areas. JavaScript chooses to give a\nwell-defined semantics to data-races, unlike the \"undefined behaviour\" of\nC/C++11. Moreover, the JavaScript model is mixed-size. This means that its\naccesses are not to discrete locations, but to (possibly overlapping) ranges of\nbytes.\n  We show that the model, in violation of the design intention, does not\nsupport a compilation scheme to ARMv8 which is used in practice. We propose a\ncorrection, which also incorporates a previously proposed fix for a failure of\nthe model to provide Sequential Consistency of Data-Race-Free programs\n(SC-DRF), an important correctness condition. We use model checking, in Alloy,\nto generate small counter-examples for these deficiencies, and investigate our\ncorrection. To accomplish this, we also develop a mixed-size extension to the\nexisting ARMv8 axiomatic model.\n  Guided by our Alloy experimentation, we mechanise (in Coq) the JavaScript\nmodel (corrected and uncorrected), our ARMv8 model, and, for the corrected\nJavaScript model, a \"model-internal\" SC-DRF proof and a compilation scheme\ncorrectness proof to ARMv8. In addition, we investigate a non-mixed-size subset\nof the corrected JavaScript model, and give proofs of compilation correctness\nfor this subset to x86-TSO, Power, RISC-V, ARMv7, and (again) ARMv8, via the\nIntermediate Memory Model (IMM).\n  As a result of our work, the JavaScript standards body (ECMA TC39) will\ninclude fixes for both issues in an upcoming edition of the specification.", "journal": ""}
{"doi": "10.48550/arXiv.1809.00866", "date": "2018-09-04", "title": "p-Multigrid matrix-free discontinuous Galerkin solution strategies for the under-resolved simulation of incompressible turbulent flows", "authors": "Matteo Franciolini, Lorenzo Botti, Alessandro Colombo, Andrea Crivellini", "abstract": "In recent years several research efforts focused on the development of\nhigh-order discontinuous Galerkin (dG) methods for scale resolving simulations\nof turbulent flows. Nevertheless, in the context of incompressible flow\ncomputations, the computational expense of solving large scale equation systems\ncharacterized by indefinite Jacobian matrices has often prevented from dealing\nwith industrially-relevant computations. In this work we seek to improve the\nefficiency of Rosenbrock-type linearly-implicit Runge-Kutta methods by devising\nrobust, scalable and memory-lean solution strategies. In particular, we\nintroduce memory saving p-multigrid preconditioners coupling matrix-free and\nmatrix-based Krylov iterative smoothers. The p-multigrid preconditioner relies\non cheap block-diagonal smoother's preconditioners on the fine space to reduce\nassembly costs and memory allocation, and ensures an adequate resolution of the\ncoarsest space of the multigrid iteration using Additive Schwarz precondioned\nsmoothers to obtain satisfactory convergence rates and optimal parallel\nefficiency of the method. Extensive numerical validation is performed. The\nRosenbrock formulation is applied to test cases of growing complexity: the\nlaminar unsteady flow around a two-dimensional cylinder at Re=200 and around a\nsphere at Re=300, the transitional flow problem of the ERCOFTAC T3L test case\nsuite with different levels of free-stream turbulence. As proof of concept, the\nnumerical solution of the Boeing Rudimentary Landing Gear test case at Re=10^6\nis reported. A good agreement of the solutions with experimental data is\ndocumented, as well as strong memory savings and execution time gains with\nrespect to state-of-the art solution strategies.", "journal": ""}
{"doi": "10.48550/arXiv.1410.5476", "date": "2014-10-20", "title": "Certified Connection Tableaux Proofs for HOL Light and TPTP", "authors": "Cezary Kaliszyk, Josef Urban, Jiri Vyskocil", "abstract": "In the recent years, the Metis prover based on ordered paramodulation and\nmodel elimination has replaced the earlier built-in methods for general-purpose\nproof automation in HOL4 and Isabelle/HOL. In the annual CASC competition, the\nleanCoP system based on connection tableaux has however performed better than\nMetis. In this paper we show how the leanCoP's core algorithm can be\nimplemented inside HOLLight. leanCoP's flagship feature, namely its\nminimalistic core, results in a very simple proof system. This plays a crucial\nrole in extending the MESON proof reconstruction mechanism to connection\ntableaux proofs, providing an implementation of leanCoP that certifies its\nproofs. We discuss the differences between our direct implementation using an\nexplicit Prolog stack, to the continuation passing implementation of MESON\npresent in HOLLight and compare their performance on all core HOLLight goals.\nThe resulting prover can be also used as a general purpose TPTP prover. We\ncompare its performance against the resolution based Metis on TPTP and other\ninteresting datasets.", "journal": ""}
{"doi": "10.48550/arXiv.0805.3215", "date": "2008-05-21", "title": "Remarks on the blow-up of solutions to a toy model for the Navier-Stokes equations", "authors": "Isabelle Gallagher, Marius Paicu", "abstract": "S. Montgomery-Smith provided a one dimensional model for the three\ndimensional, incompressible Navier-Stokes equations, for which he proved the\nblow up of solutions associated to a class of large initial data, while the\nsame global existence results as for the Navier-Stokes equations hold for small\ndata. In this note the model is adapted to the case of two and three space\ndimensions, with the additional feature that the divergence free condition is\npreserved. It is checked that the family of initial data constructed previously\nby J.-Y Chemin and I. Gallagher which is arbitrarily large but yet generates a\nglobal solution to the Navier-Stokes equations in three space dimensions,\nactually causes blow up for the toy model -- meaning that the precise structure\nof the nonlinear term is crucial to understand the dynamics of large solutions\nto the Navier-Stokes equations.", "journal": ""}
{"doi": "10.48550/arXiv.1907.05100", "date": "2019-07-11", "title": "A prey-predator model with three interacting species", "authors": "Uygun Jamilov, Michael Scheutzow, Isabell Vorkastner", "abstract": "In this paper we consider a class of discrete time prey-predator models with\nthree interacting species defined on the two-dimensional simplex. For some\nchoices of parameters of the operator describing the evolution of the relative\nfrequencies, we show that the ergodic hypothesis does not hold. Moreover, we\nprove that any order Ces\\`aro mean of the trajectories diverges. For another\nclass of parameters, we show that all orbits starting from the interior of the\nsimplex converge to the unique fixed point of the operator while for the\nremaining choices of parameters all orbits converge to one of the vertices of\nthe simplex. Contrary to many authors we study discrete time models but we\ninclude a speed function $f$ in the dynamics which allows us to approximate the\ncontinuous-time case arbitrarily well when $f$ is small.", "journal": ""}
{"doi": "10.48550/arXiv.2007.12310", "date": "2020-07-24", "title": "The Effect of the Slit Configuration on the H$_2$ 1-0~S(1) to Br$\u03b3$ Line Ratio of Spatially Resolved Planetary Nebulae", "authors": "Isabel Aleman", "abstract": "The H$_2$ 1-0~S(1)/Br$\\gamma$ ratio (R(Br$\\gamma$)) is used in many studies\nof the molecular content in planetary nebulae (PNe). As these lines are\nproduced in different regions, the slit configuration used in spectroscopic\nobservations may have an important effect on their ratio. In this work,\nobservations and numerical simulations are used to demonstrate and quantify\nsuch effect in PNe. The study aims to assist the interpretation of observations\nand their comparison to models. The analysis shows that observed R(Br$\\gamma$)\nratios reach only values up to 0.3 when the slit encompasses the entire nebula.\nValues higher than that are only obtained when the slit covers a limited region\naround the H$_2$ peak emission and the Br$\\gamma$ emission is then minimised.\nThe numerical simulations presented show that, when the effect of the slit\nconfiguration is taken into account, photoionization models can reproduce the\nwhole range of observed R(Br$\\gamma$) in PNe, as well as the behaviour\ndescribed above. The argument that shocks are needed to explain the higher\nvalues of R(Br$\\gamma$) is thus not valid. Therefore, this ratio is not a good\nindicator of the H$_2$ excitation mechanism as suggested in the literature.", "journal": ""}
{"doi": "10.48550/arXiv.2206.04699", "date": "2022-06-09", "title": "Pulsars Do Not Produce Sharp Features in the Cosmic-Ray Electron and Positron Spectra", "authors": "Isabelle John, Tim Linden", "abstract": "Pulsars are considered to be the leading explanation for the excess in\ncosmic-ray positrons detected by PAMELA and AMS-02. A notable feature of\nstandard pulsar models is the sharp spectral cutoff produced by the\nincreasingly efficient cooling of very-high-energy electrons by synchrotron and\ninverse-Compton processes. This spectral break has been employed to: (1)\nconstrain the age of pulsars that contribute to the excess, (2) argue that a\nlarge number of pulsars must significantly contribute to the positron flux, and\n(3) argue that spectral cutoffs cannot distinguish between dark matter and\npulsar models. We prove that this spectral feature does not exist -- it appears\ndue to approximations that treat inverse-Compton scattering as a continuous,\ninstead of as a discrete and catastrophic, energy-loss process. Astrophysical\nsources do not produce sharp spectral features via cooling, reopening the\npossibility that such a feature would provide incontrovertible evidence for\ndark matter.", "journal": ""}
{"doi": "10.48550/arXiv.2303.15020", "date": "2023-03-27", "title": "A Generalized Hybrid Hoare Logic", "authors": "Naijun Zhan, Xiangyu Jin, Bohua Zhan, Shuling Wang, Dimitar Guelev", "abstract": "Deductive verification of hybrid systems (HSs) increasingly attracts more\nattention in recent years because of its power and scalability, where a\npowerful specification logic for HSs is the cornerstone. Often, HSs are\nnaturally modelled by concurrent processes that communicate with each other.\nHowever, existing specification logics cannot easily handle such models. In\nthis paper, we present a specification logic and proof system for Hybrid\nCommunicating Sequential Processes (HCSP), that extends CSP with ordinary\ndifferential equations (ODE) and interrupts to model interactions between\ncontinuous and discrete evolution. Because it includes a rich set of algebraic\noperators, complicated hybrid systems can be easily modelled in an algebra-like\ncompositional way in HCSP. Our logic can be seen as a generalization and\nsimplification of existing hybrid Hoare logics (HHL) based on duration calculus\n(DC), as well as a conservative extension of existing Hoare logics for\nconcurrent programs. Its assertion logic is the first-order theory of\ndifferential equations (FOD), together with assertions about traces recording\ncommunications, readiness, and continuous evolution. We prove continuous\nrelative completeness of the logic w.r.t. FOD, as well as discrete relative\ncompleteness in the sense that continuous behaviour can be arbitrarily\napproximated by discretization. Finally, we implement the above logic in\nIsabelle/HOL, and apply it to verify two case studies to illustrate the power\nand scalability of our logic.", "journal": ""}
{"doi": "10.48550/arXiv.1709.09527", "date": "2017-09-27", "title": "Introducing machine learning for power system operation support", "authors": "Benjamin Donnot, Isabelle Guyon, Marc Schoenauer, Patrick Panciatici, Antoine Marot", "abstract": "We address the problem of assisting human dispatchers in operating power\ngrids in today's changing context using machine learning, with theaim of\nincreasing security and reducing costs. Power networks are highly regulated\nsystems, which at all times must meet varying demands of electricity with a\ncomplex production system, including conventional power plants, less\npredictable renewable energies (such as wind or solar power), and the\npossibility of buying/selling electricity on the international market with more\nand more actors involved at a Europeanscale. This problem is becoming ever more\nchallenging in an aging network infrastructure. One of the primary goals of\ndispatchers is to protect equipment (e.g. avoid that transmission lines\noverheat) with few degrees of freedom: we are considering in this paper solely\nmodifications in network topology, i.e. re-configuring the way in which lines,\ntransformers, productions and loads are connected in sub-stations. Using years\nof historical data collected by the French Transmission Service Operator (TSO)\n\"R\\'eseau de Transport d'Electricit\\'e\" (RTE), we develop novel machine\nlearning techniques (drawing on \"deep learning\") to mimic human decisions to\ndevise \"remedial actions\" to prevent any line to violate power flow limits\n(so-called \"thermal limits\"). The proposed technique is hybrid. It does not\nrely purely on machine learning: every action will be tested with actual\nsimulators before being proposed to the dispatchers or implemented on the grid.", "journal": ""}
{"doi": "10.48550/arXiv.2111.12682", "date": "2021-11-22", "title": "CQS: A Formally-Verified Framework for Fair and Abortable Synchronization", "authors": "Nikita Koval, Dmitry Khalanskiy, Dan Alistarh", "abstract": "Writing concurrent code that is both correct and efficient is notoriously\ndifficult. Thus, programmers often prefer to use synchronization abstractions,\nwhich render code simpler and easier to reason about. Despite a wealth of work\non this topic, there is still a gap between the rich semantics provided by\nsynchronization abstractions in modern programming languages -- specifically,\n\\emph{fair} FIFO ordering of synchronization requests and support for\n\\emph{abortable} operations -- and frameworks for implementing it correctly and\nefficiently. Supporting such semantics is critical given the rising popularity\nof constructs for asynchronous programming, such as coroutines, which abort\nfrequently and are cheaper to suspend and resume compared to native threads.\n  This paper introduces a new framework called\n\\texttt{CancellableQueueSynchronizer} (CQS), which enables simple yet efficient\nimplementations of a wide range of fair and abortable synchronization\nprimitives: mutexes, semaphores, barriers, count-down latches, and blocking\npools. Our main contribution is algorithmic, as implementing both fairness and\nabortability efficiently at this level of generality is non-trivial.\nImportantly, all our algorithms, including the CQS framework and the primitives\nbuilt on top of it, come with \\emph{formal proofs} in the Iris framework for\nCoq for many of their properties. These proofs are modular, so it is easy to\nshow correctness for new primitives implemented on top of CQS. From a practical\nperspective, implementation of CQS for native threads on the JVM significantly\nimproves Java's \\texttt{AbstractQueuedSynchronizer}, the only practical\nabstraction offering similar semantics. In sum,\n\\texttt{CancellableQueueSynchronizer} is the first framework to combine\nexpressiveness with formal guarantees and solid practical performance.", "journal": ""}
{"doi": "10.48550/arXiv.2010.06216", "date": "2020-10-13", "title": "Resolution as Intersection Subtyping via Modus Ponens", "authors": "Koar Marntirosian, Tom Schrijvers, Bruno C. d. S. Oliveira, Georgios Karachalias", "abstract": "Resolution and subtyping are two common mechanisms in programming languages.\nResolution is used by features such as type classes or Scala-style implicits to\nsynthesize values automatically from contextual type information. Subtyping is\ncommonly used to automatically convert the type of a value into another\ncompatible type. So far the two mechanisms have been considered independently\nof each other. This paper shows that, with a small extension, subtyping with\nintersection types can subsume resolution. This has three main consequences.\nFirstly, resolution does not need to be implemented as a separate mechanism.\nSecondly, the interaction between resolution and subtyping becomes apparent.\nFinally, the integration of resolution into subtyping enables first-class\n(implicit) environments. The extension that recovers the power of resolution\nvia subtyping is the modus ponens rule of propositional logic. While it is\neasily added to declarative subtyping, significant care needs to be taken to\nretain desirable properties, such as transitivity and decidability of\nalgorithmic subtyping, and coherence. To materialize these ideas we develop\n$\\lambda_i^{\\mathsf{MP}}$, a calculus that extends a iprevious calculus with\ndisjoint intersection types, and develop its metatheory in the Coq theorem\nprover.", "journal": ""}
{"doi": "10.48550/arXiv.2311.09704", "date": "2023-11-16", "title": "International System of Quantities library in VDM", "authors": "Leo Freitas", "abstract": "The International Systems of Quantities (ISQ) standard was published in 1960\nto tame the wide diversity of measurement systems being developed across the\nworld, such as the centimetre-gram-second versus the meter-kilogram-second for\nexample. Such a standard is highly motivated by the potential of ``trivial''\n(rather error-prone) mistakes in converting between incompatible units. There\nhave been such accidents in space missions, medical devices, etc. Thus,\nrendering modelling or simulation experiments unusable or unsafe. We address\nthis problem by providing a \\textbf{SAFE}-ISQ VDM-library that is: Simple,\nAccurate, Fast, and Effective. It extends an ecosystem of other VDM\nmathematical toolkit extensions, which include a translation and proof\nenvironment for VDM in Isabelle at https://github.com/leouk/VDM_Toolkit.", "journal": ""}
{"doi": "10.48550/arXiv.2405.20083", "date": "2024-05-30", "title": "Tachis: Higher-Order Separation Logic with Credits for Expected Costs", "authors": "Philipp G. Haselwarter, Kwing Hei Li, Markus de Medeiros, Simon Oddershede Gregersen, Alejandro Aguirre, Joseph Tassarotti, Lars Birkedal", "abstract": "We present Tachis, a higher-order separation logic to reason about the\nexpected cost of probabilistic programs. Inspired by the uses of time credits\nfor reasoning about the running time of deterministic programs, we introduce a\nnovel notion of probabilistic cost credit. Probabilistic cost credits are a\nseparation logic resource that can be used to pay for the cost of operations in\nprograms, and that can be distributed across all possible branches of sampling\ninstructions according to their weight, thus enabling us to reason about\nexpected cost. The representation of cost credits as separation logic resources\ngives Tachis a great deal of flexibility and expressivity. In particular, it\npermits reasoning about amortized expected cost by storing excess credits as\npotential into data structures to pay for future operations. Tachis further\nsupports a range of cost models, including running time and entropy usage. We\nshowcase the versatility of this approach by applying our techniques to prove\nupper bounds on the expected cost of a variety of probabilistic algorithms and\ndata structures, including randomized quicksort, hash tables, and meldable\nheaps.\n  All of our results have been mechanized using Coq, Iris, and the Coquelicot\nreal analysis library.", "journal": "Proc. ACM Program. Lang. 8, OOPSLA2, Article 313 (October 2024),\n  30 pages"}
{"doi": "10.48550/arXiv.2211.12923", "date": "2022-11-23", "title": "A Calculus for Amortized Expected Runtimes", "authors": "Kevin Batz, Benjamin Lucien Kaminski, Joost-Pieter Katoen, Christoph Matheja, Lena Verscht", "abstract": "We develop a weakest-precondition-style calculus \\`a la Dijkstra for\nreasoning about amortized expected runtimes of randomized algorithms with\naccess to dynamic memory - the $\\textsf{aert}$ calculus. Our calculus is truly\nquantitative, i.e. instead of Boolean valued predicates, it manipulates\nreal-valued functions.\n  En route to the $\\textsf{aert}$ calculus, we study the $\\textsf{ert}$\ncalculus for reasoning about expected runtimes of Kaminski et al. [2018]\nextended by capabilities for handling dynamic memory, thus enabling\ncompositional and local reasoning about randomized data structures. This\nextension employs runtime separation logic, which has been foreshadowed by\nMatheja [2020] and then implemented in Isabelle/HOL by Haslbeck [2021]. In\naddition to Haslbeck's results, we further prove soundness of the so-extended\n$\\textsf{ert}$ calculus with respect to an operational Markov decision process\nmodel featuring countably-branching nondeterminism, provide intuitive\nexplanations, and provide proof rules enabling separation logic-style\nverification for upper bounds on expected runtimes. Finally, we build the\nso-called potential method for amortized analysis into the $\\textsf{ert}$\ncalculus, thus obtaining the $\\textsf{aert}$ calculus.\n  Since one needs to be able to handle changes in potential which can be\nnegative, the $\\textsf{aert}$ calculus needs to be capable of handling signed\nrandom variables. A particularly pleasing feature of our solution is that,\nunlike e.g. Kozen [1985], we obtain a loop rule for our signed random\nvariables, and furthermore, unlike e.g. Kaminski and Katoen [2017], the\n$\\textsf{aert}$ calculus makes do without the need for involved technical\nmachinery keeping track of the integrability of the random variables.\n  Finally, we present case studies, including a formal analysis of a randomized\ndelete-insert-find-any set data structure [Brodal et al. 1996].", "journal": ""}
{"doi": "10.48550/arXiv.2208.14384", "date": "2022-08-30", "title": "Expert Opinion Elicitation for Assisting Deep Learning based Lyme Disease Classifier with Patient Data", "authors": "Sk Imran Hossain, Jocelyn de Go\u00ebr de Herve, David Abrial, Richard Emillion, Isabelle Lebertb, Yann Frendo, Delphine Martineau, Olivier Lesens, Engelbert Mephu Nguifo", "abstract": "Diagnosing erythema migrans (EM) skin lesion, the most common early symptom\nof Lyme disease using deep learning techniques can be effective to prevent\nlong-term complications. Existing works on deep learning based EM recognition\nonly utilizes lesion image due to the lack of a dataset of Lyme disease related\nimages with associated patient data. Physicians rely on patient information\nabout the background of the skin lesion to confirm their diagnosis. In order to\nassist the deep learning model with a probability score calculated from patient\ndata, this study elicited opinion from fifteen doctors. For the elicitation\nprocess, a questionnaire with questions and possible answers related to EM was\nprepared. Doctors provided relative weights to different answers to the\nquestions. We converted doctors evaluations to probability scores using\nGaussian mixture based density estimation. For elicited probability model\nvalidation, we exploited formal concept analysis and decision tree. The\nelicited probability scores can be utilized to make image based deep learning\nLyme disease pre-scanners robust.", "journal": ""}
{"doi": "10.48550/arXiv.2203.07431", "date": "2022-03-14", "title": "Conditional Contextual Refinement (CCR)", "authors": "Youngju Song, Minki Cho, Dongjae Lee, Chung-Kil Hur", "abstract": "Contextual refinement (CR) is one of the standard notions of specifying open\nprograms. CR has two main advantages: (i) (horizontal and vertical)\ncompositionality that allows us to decompose a large contextual refinement into\nmany smaller ones enabling modular and incremental verification, and (ii) no\nrestriction on programming features thereby allowing, e.g., mutually recursive,\npointer-value passing, and higher-order functions. However, CR has a downside\nthat it cannot impose conditions on the context since it quantifies over all\ncontexts, which indeed plays a key role in support of full compositionality and\nprogramming features.\n  In this paper, we address the problem of finding a notion of refinement that\nsatisfies all three requirements: support of full compositionality, full\n(sequential) programming features, and rich conditions on the context. As a\nsolution, we propose a new theory of refinement, called CCR (Conditional\nContextual Refinement), and develop a verification framework based on it, which\nallows us to modularly and incrementally verify a concrete module against an\nabstract module under separation-logic-style pre and post conditions about\nexternal modules. It is fully formalized in Coq and provides a proof mode that\ncombines (i) simulation reasoning about preservation of sideffects such as IO\nevents and termination and (ii) propositional reasoning about pre and post\nconditions. Also, the verification results are combined with CompCert, so that\nwe formally establish behavioral refinement from top-level abstract programs,\nall the way down to their assembly code.", "journal": ""}
{"doi": "10.48550/arXiv.2010.04749", "date": "2020-10-09", "title": "Igloo: Soundly Linking Compositional Refinement and Separation Logic for Distributed System Verification", "authors": "Christoph Sprenger, Tobias Klenze, Marco Eilers, Felix A. Wolf, Peter M\u00fcller, Martin Clochard, David Basin", "abstract": "Lighthouse projects such as CompCert, seL4, IronFleet, and DeepSpec have\ndemonstrated that full verification of entire systems is feasible by\nestablishing a refinement relation between an abstract system specification and\nan executable implementation. Existing approaches however impose severe\nrestrictions on either the abstract system specifications due to their limited\nexpressiveness or versatility, or on the executable code due to their reliance\non suboptimal code extraction or inexpressive program logics. We propose a\nnovel methodology that combines the compositional refinement of abstract,\nevent-based models of distributed systems with the verification of full-fledged\nprogram code using expressive separation logics, which support features of\nrealistic programming languages like mutable heap data structures and\nconcurrency. The main technical contribution of our work is a formal framework\nthat soundly relates event-based system models to program specifications in\nseparation logics, such that successful verification establishes a refinement\nrelation between the model and the code. We formalized our framework, Igloo, in\nIsabelle/HOL. Our framework enables the sound combination of tools for protocol\ndevelopment with existing program verifiers. We report on three case studies, a\nleader election protocol, a replication protocol, and a security protocol, for\nwhich we refine formal requirements into program specifications (in\nIsabelle/HOL) that we implement in Java and Python and prove correct using the\nVeriFast and Nagini tools.", "journal": ""}
{"doi": "10.48550/arXiv.2411.14975", "date": "2024-11-22", "title": "Exploring Foundation Models Fine-Tuning for Cytology Classification", "authors": "Manon Dausort, Tiffanie Godelaine, Maxime Zanella, Karim El Khoury, Isabelle Salmon, Beno\u00eet Macq", "abstract": "Cytology slides are essential tools in diagnosing and staging cancer, but\ntheir analysis is time-consuming and costly. Foundation models have shown great\npotential to assist in these tasks. In this paper, we explore how existing\nfoundation models can be applied to cytological classification. More\nparticularly, we focus on low-rank adaptation, a parameter-efficient\nfine-tuning method suited to few-shot learning. We evaluated five foundation\nmodels across four cytological classification datasets. Our results demonstrate\nthat fine-tuning the pre-trained backbones with LoRA significantly improves\nmodel performance compared to fine-tuning only the classifier head, achieving\nstate-of-the-art results on both simple and complex classification tasks while\nrequiring fewer data samples.", "journal": ""}
{"doi": "10.48550/arXiv.1607.02225", "date": "2016-07-08", "title": "Hybrid Information Flow Analysis for Programs with Arrays", "authors": "Gerg\u00f6 Barany", "abstract": "Information flow analysis checks whether certain pieces of (confidential)\ndata may affect the results of computations in unwanted ways and thus leak\ninformation. Dynamic information flow analysis adds instrumentation code to the\ntarget software to track flows at run time and raise alarms if a flow policy is\nviolated; hybrid analyses combine this with preliminary static analysis.\n  Using a subset of C as the target language, we extend previous work on hybrid\ninformation flow analysis that handled pointers to scalars. Our extended\nformulation handles arrays, pointers to array elements, and pointer arithmetic.\nInformation flow through arrays of pointers is tracked precisely while arrays\nof non-pointer types are summarized efficiently.\n  A prototype of our approach is implemented using the Frama-C program analysis\nand transformation framework. Work on a full machine-checked proof of the\ncorrectness of our approach using Isabelle/HOL is well underway; we present the\nexisting parts and sketch the rest of the correctness argument.", "journal": "EPTCS 216, 2016, pp. 5-23"}
{"doi": "10.48550/arXiv.2306.17765", "date": "2023-06-30", "title": "Speculative SAT Modulo SAT", "authors": "Hari Govind V K, Isabel Garcia-Contreras, Sharon Shoham, Arie Gurfinkel", "abstract": "State-of-the-art model-checking algorithms like IC3/PDR are based on\nuni-directional modular SAT solving for finding and/or blocking\ncounterexamples. Modular SAT solvers divide a SAT-query into multiple\nsub-queries, each solved by a separate SAT solver (called a module), and\npropagate information (lemmas, proof obligations, blocked clauses, etc.)\nbetween modules. While modular solving is key to IC3/PDR, it is obviously not\nas effective as monolithic solving, especially when individual sub-queries are\nharder to solve than the combined query. This is partially addressed in SAT\nmodulo SAT (SMS) by propagating unit literals back and forth between the\nmodules and using information from one module to simplify the sub-query in\nanother module as soon as possible (i.e., before the satisfiability of any\nsub-query is established). However, bi-directionality of SMS is limited because\nof the strict order between decisions and propagation -- only one module is\nallowed to make decisions, until its sub-query is SAT. In this paper, we\npropose a generalization of SMS, called SPEC SMS, that speculates decisions\nbetween modules. This makes it bi-directional -- decisions are made in multiple\nmodules, and learned clauses are exchanged in both directions. We further\nextend DRUP proofs and interpolation, these are useful in model checking, to\nSPEC SMS. We have implemented SPEC SMS in Z3 and show that it performs\nexponentially better on a series of benchmarks that are provably hard for SMS.", "journal": ""}
{"doi": "10.48550/arXiv.1704.02961", "date": "2017-04-10", "title": "A nonlocal model of epidemic network with nonlimited transmission: Global existence and uniqueness", "authors": "Elisabeth Logak, Isabelle Passat", "abstract": "Following \\cite{ipel1}, we consider a nonlinear SIS-type nonlocal system\ndescribing the spread of epidemics on networks, assuming nonlimited\ntransmission, We prove local existence of a unique solution for any diffusion\ncoefficients and global existence in the case of equal diffusion coefficients.\nNext we study the asymptotic behaviour of the solution and show that the\ndisease-free equilibrium (DFE) is linearly and globally asymptotically stable\nwhen the total mean population is small. Finally, we prove that the solution of\nthe system converge to the $DFE$.", "journal": ""}
{"doi": "10.48550/arXiv.2203.08726", "date": "2022-03-16", "title": "Differential Phase-Shift QKD in a 2:16-Split Lit PON with 19 Carrier-Grade Channels", "authors": "Nemanja Vokiic, Dinka Milovanvcev, Bernhard Schrenk, Michael Hentschel, Hannes Hubel", "abstract": "We investigate the practical network integration of differential phase shift\nquantum key distribution following a cost-optimized deployment scheme where\ncomplexity is off-loaded to a centralized location. User terminal equipment for\nquantum state preparation at 1 GHz symbol rate is kept technologically lean\nthrough use of a directly-modulated laser as optical encoder. Integration in a\npassive optical network infrastructure is experimentally studied for legacy and\nmodern optical access standards. We analyze the implications that result from\nRaman scattering arising from different spectral allocations of the classical\nchannels in the O-, S-, C- and L-band, and prove that the quantum channel can\nco-exist with up to 19 classical channels of a fully-loaded modern access\nstandard. Secure-key generation at a rate of 5.1 times 10e-7 bits per pulse at\na quantum bit error ratio of 3.28 percent is obtained over a 13.5 km reach, 2\nto 16 split passive network configuration. The high power difference of 93.8 dB\nbetween launched classical and quantum signals in the lit access network leads\nto a low penalty of 0.52 percent in terms of error ratio.", "journal": "IEEE Journal of Selected Topics in Quantum Electronics ( Volume:\n  26, Issue: 3, May-June 2020)"}
{"doi": "10.48550/arXiv.2204.10679", "date": "2022-04-22", "title": "Deterministic Sensitivity Oracles for Diameter, Eccentricities and All Pairs Distances", "authors": "Davide Bil\u00f2, Keerti Choudhary, Sarel Cohen, Tobias Friedrich, Martin Schirneck", "abstract": "We construct data structures for extremal and pairwise distances in directed\ngraphs in the presence of transient edge failures. Henzinger et al. [ITCS 2017]\ninitiated the study of fault-tolerant (sensitivity) oracles for the diameter\nand vertex eccentricities. We extend this with a special focus on space\nefficiency. We present several new data structures, among them the first\nfault-tolerant eccentricity oracle for dual failures in subcubic space. We\nfurther prove lower bounds that show limits to approximation vs. space and\ndiameter vs. space trade-offs for fault-tolerant oracles. They highlight key\ndifferences between data structures for undirected and directed graphs.\n  Initially, our oracles are randomized leaning on a sampling technique\nfrequently used in sensitivity analysis. Building on the work of Alon, Chechik,\nand Cohen [ICALP 2019] as well as Karthik and Parter [SODA 2021], we develop a\nhierarchical framework to derandomize fault-tolerant data structures. We first\napply it to our own diameter and eccentricity oracles and then show its\nversatility by derandomizing algorithms from the literature: the distance\nsensitivity oracle of Ren [JCSS 2022] and the Single-Source Replacement Path\nalgorithm of Chechik and Magen [ICALP 2020]. This way, we obtain the first\ndeterministic distance sensitivity oracle with subcubic preprocessing time.", "journal": ""}
{"doi": "10.48550/arXiv.2205.11325", "date": "2022-05-23", "title": "Sound Automation of Magic Wands (extended version)", "authors": "Thibault Dardinier, Gaurav Parthasarathy, No\u00e9 Weeks, Alexanders J. Summers, Peter M\u00fcller", "abstract": "The magic wand $\\mathbin{-\\!\\!*}$ (also called separating implication) is a\nseparation logic connective commonly used to specify properties of partial data\nstructures, for instance during iterative traversals. A footprint of a magic\nwand formula $A \\mathbin{-\\!\\!*} B$ is a state that, combined with any state in\nwhich $A$ holds, yields a state in which $B$ holds. The key challenge of\nproving a magic wand (also called packaging a wand) is to find such a\nfootprint. Existing package algorithms either have a high annotation overhead\nor, as we show in this paper, are unsound. We present a formal framework that\nprecisely characterises a wide design space of possible package algorithms\napplicable to a large class of separation logics. We prove in Isabelle/HOL that\nour formal framework is sound and complete, and use it to develop a novel\npackage algorithm that offers competitive automation and is sound. Moreover, we\npresent a novel, restricted definition of wands and prove in Isabelle/HOL that\nit is possible to soundly combine fractions of such wands, which is not the\ncase for arbitrary wands. We have implemented our techniques for the Viper\nlanguage, and demonstrate that they are effective in practice.", "journal": ""}
{"doi": "10.48550/arXiv.1510.04873", "date": "2015-10-16", "title": "Translating Hierarchical Block Diagrams into Composite Predicate Transformers", "authors": "Iulia Dragomir, Viorel Preoteasa, Stavros Tripakis", "abstract": "Simulink is the de facto industrial standard for designing embedded control\nsystems. When dealing with the formal verification of Simulink models, we face\nthe problem of translating the graphical language of Simulink, namely,\nhierarchical block diagrams (HBDs), into a formalism suitable for verification.\nIn this paper, we study the translation of HBDs into the compositional\nrefinement calculus framework for reactive systems. Specifically, we consider\nas target language an algebra of atomic predicate transformers to capture basic\nSimulink blocks (both stateless and stateful), composed in series, in parallel,\nand in feedback. For a given HBD, there are many possible ways to translate it\ninto a term in this algebra, with different tradeoffs. We explore these\ntradeoffs, and present three translation algorithms. We report on a prototype\nimplementation of these algorithms in a tool that translates Simulink models\ninto algebra terms implemented in the Isabelle theorem prover. We test our tool\non several case studies including a benchmark Simulink model by Toyota. We\ncompare the three translation algorithms, with respect to size and readability\nof generated terms, simplifiability of the corresponding formulas, and other\nmetrics.", "journal": ""}
{"doi": "10.48550/arXiv.2311.15003", "date": "2023-11-25", "title": "Enumerating Error Bounded Polytime Algorithms Through Arithmetical Theories", "authors": "Melissa Antonelli, Ugo Dal Lago, Davide Davoli, Isabel Oitavem, Paolo Pistone", "abstract": "We consider a minimal extension of the language of arithmetic, such that the\nbounded formulas provably total in a suitably-defined theory \\`a la Buss\n(expressed in this new language) precisely capture polytime random functions.\nThen, we provide two new characterizations of the semantic class BPP obtained\nby internalizing the error-bound check within a logical system: the first\nrelies on measure-sensitive quantifiers, while the second is based on standard\nfirst-order quantification. This leads us to introduce a family of effectively\nenumerable subclasses of BPP, called BPP_T and consisting of languages captured\nby those probabilistic Turing machines whose underlying error can be proved\nbounded in the theory T. As a paradigmatic example of this approach, we\nestablish that polynomial identity testing is in BPP_T where\nT=$\\mathrm{I}\\Delta_0+\\mathrm{Exp}$ is a well-studied theory based on bounded\ninduction.", "journal": ""}
{"doi": "10.48550/arXiv.2307.13844", "date": "2023-07-25", "title": "Polymorphic Reachability Types: Tracking Freshness, Aliasing, and Separation in Higher-Order Generic Programs", "authors": "Guannan Wei, Oliver Bra\u010devac, Songlin Jia, Yuyan Bao, Tiark Rompf", "abstract": "Reachability types are a recent proposal that has shown promise in scaling to\nhigher-order but monomorphic settings, tracking aliasing and separation on top\nof a substrate inspired by separation logic. The prior $\\lambda^*$ reachability\ntype system qualifies types with sets of reachable variables and guarantees\nseparation if two terms have disjoint qualifiers. However, naive extensions\nwith type polymorphism and/or precise reachability polymorphism are unsound,\nmaking $\\lambda^*$ unsuitable for adoption in real languages. Combining\nreachability and type polymorphism that is precise, sound, and parametric\nremains an open challenge.\n  This paper presents a rethinking of the design of reachability tracking and\nproposes a solution to the key challenge of reachability polymorphism. Instead\nof always tracking the transitive closure of reachable variables as in the\noriginal design, we only track variables reachable in a single step and compute\ntransitive closures only when necessary, thus preserving chains of reachability\nover known variables that can be refined using substitution. To enable this\nproperty, we introduce a new freshness qualifier, which indicates variables\nwhose reachability sets may grow during evaluation steps. These ideas yield the\nsimply-typed $\\lambda^\\diamond$-calculus with precise lightweight, i.e.,\nquantifier-free, reachability polymorphism, and the\n$\\mathsf{F}_{<:}^\\diamond$-calculus with bounded parametric polymorphism over\ntypes and reachability qualifiers. We prove type soundness and a preservation\nof separation property in Coq.", "journal": ""}
{"doi": "10.48550/arXiv.2307.09997", "date": "2023-07-19", "title": "TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition", "authors": "Isabel Funke, Dominik Rivoir, Stefanie Krell, Stefanie Speidel", "abstract": "Objective: To enable context-aware computer assistance in the operating room\nof the future, cognitive systems need to understand automatically which\nsurgical phase is being performed by the medical team. The primary source of\ninformation for surgical phase recognition is typically video, which presents\ntwo challenges: extracting meaningful features from the video stream and\neffectively modeling temporal information in the sequence of visual features.\nMethods: For temporal modeling, attention mechanisms have gained popularity due\nto their ability to capture long-range dependencies. In this paper, we explore\ndesign choices for attention in existing temporal models for surgical phase\nrecognition and propose a novel approach that uses attention more effectively\nand does not require hand-crafted constraints: TUNeS, an efficient and simple\ntemporal model that incorporates self-attention at the core of a convolutional\nU-Net structure. In addition, we propose to train the feature extractor, a\nstandard CNN, together with an LSTM on preferably long video segments, i.e.,\nwith long temporal context. Results: In our experiments, almost all temporal\nmodels performed better on top of feature extractors that were trained with\nlonger temporal context. On these contextualized features, TUNeS achieves\nstate-of-the-art results on the Cholec80 dataset. Conclusion: This study offers\nnew insights on how to use attention mechanisms to build accurate and efficient\ntemporal models for surgical phase recognition. Significance: Implementing\nautomatic surgical phase recognition is essential to automate the analysis and\noptimization of surgical workflows and to enable context-aware computer\nassistance during surgery, thus ultimately improving patient care.", "journal": ""}
{"doi": "10.48550/arXiv.1706.01740", "date": "2017-06-06", "title": "Label-Dependencies Aware Recurrent Neural Networks", "authors": "Yoann Dupont, Marco Dinarelli, Isabelle Tellier", "abstract": "In the last few years, Recurrent Neural Networks (RNNs) have proved effective\non several NLP tasks. Despite such great success, their ability to model\n\\emph{sequence labeling} is still limited. This lead research toward solutions\nwhere RNNs are combined with models which already proved effective in this\ndomain, such as CRFs. In this work we propose a solution far simpler but very\neffective: an evolution of the simple Jordan RNN, where labels are re-injected\nas input into the network, and converted into embeddings, in the same way as\nwords. We compare this RNN variant to all the other RNN models, Elman and\nJordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language\nUnderstanding (SLU). Thanks to label embeddings and their combination at the\nhidden layer, the proposed variant, which uses more parameters than Elman and\nJordan RNNs, but far fewer than LSTM and GRU, is more effective than other\nRNNs, but also outperforms sophisticated CRF models.", "journal": ""}
{"doi": "10.48550/arXiv.1610.08452", "date": "2016-10-26", "title": "Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment", "authors": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi", "abstract": "Automated data-driven decision making systems are increasingly being used to\nassist, or even replace humans in many settings. These systems function by\nlearning from historical decisions, often taken by humans. In order to maximize\nthe utility of these systems (or, classifiers), their training involves\nminimizing the errors (or, misclassifications) over the given historical data.\nHowever, it is quite possible that the optimally trained classifier makes\ndecisions for people belonging to different social groups with different\nmisclassification rates (e.g., misclassification rates for females are higher\nthan for males), thereby placing these groups at an unfair disadvantage. To\naccount for and avoid such unfairness, in this paper, we introduce a new notion\nof unfairness, disparate mistreatment, which is defined in terms of\nmisclassification rates. We then propose intuitive measures of disparate\nmistreatment for decision boundary-based classifiers, which can be easily\nincorporated into their formulation as convex-concave constraints. Experiments\non synthetic as well as real world datasets show that our methodology is\neffective at avoiding disparate mistreatment, often at a small cost in terms of\naccuracy.", "journal": ""}
{"doi": "10.48550/arXiv.2305.18029", "date": "2023-05-29", "title": "Faithfulness Tests for Natural Language Explanations", "authors": "Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, Isabelle Augenstein", "abstract": "Explanations of neural models aim to reveal a model's decision-making process\nfor its predictions. However, recent work shows that current methods giving\nexplanations such as saliency maps or counterfactuals can be misleading, as\nthey are prone to present reasons that are unfaithful to the model's inner\nworkings. This work explores the challenging question of evaluating the\nfaithfulness of natural language explanations (NLEs). To this end, we present\ntwo tests. First, we propose a counterfactual input editor for inserting\nreasons that lead to counterfactual predictions but are not reflected by the\nNLEs. Second, we reconstruct inputs from the reasons stated in the generated\nNLEs and check how often they lead to the same predictions. Our tests can\nevaluate emerging NLE models, proving a fundamental tool in the development of\nfaithful NLEs.", "journal": "The 61st Annual Meeting of the Association for Computational\n  Linguistics (ACL 2023)"}
{"doi": "10.48550/arXiv.1412.6579", "date": "2014-12-20", "title": "A Hoare logic for the coinductive trace-based big-step semantics of While", "authors": "Keiko Nakata, Tarmo Uustalu", "abstract": "In search for a foundational framework for reasoning about observable\nbehavior of programs that may not terminate, we have previously devised a\ntrace-based big-step semantics for While. In this semantics, both traces and\nevaluation (relating initial states of program runs to traces they produce) are\ndefined coinductively. On terminating runs, this semantics agrees with the\nstandard inductive state-based semantics. Here we present a Hoare logic\ncounterpart of our coinductive trace-based semantics and prove it sound and\ncomplete. Our logic subsumes the standard partial-correctness state-based Hoare\nlogic as well as the total-correctness variation: they are embeddable. In the\nconverse direction, projections can be constructed: a derivation of a Hoare\ntriple in our trace-based logic can be translated into a derivation in the\nstate-based logic of a translated, weaker Hoare triple. Since we work with a\nconstructive underlying logic, the range of program properties we can reason\nabout has a fine structure; in particular, we can distinguish between\ntermination and nondivergence, e.g., unbounded classically total search fails\nto be terminating, but is nonetheless nondivergent. Our meta-theory is entirely\nconstructive as well, and we have formalized it in Coq.", "journal": "Logical Methods in Computer Science, Volume 11, Issue 1 (February\n  11, 2015) lmcs:692"}
{"doi": "10.48550/arXiv.1712.10233", "date": "2017-12-29", "title": "Unifying Theories of Reactive Design Contracts", "authors": "Simon Foster, Ana Cavalcanti, Samuel Canham, Jim Woodcock, Frank Zeyda", "abstract": "Design-by-contract is an important technique for model-based design in which\na composite system is specified by a collection of contracts that specify the\nbehavioural assumptions and guarantees of each component. In this paper, we\ndescribe a unifying theory for reactive design contracts that provides the\nbasis for modelling and verification of reactive systems. We provide a language\nfor expression and composition of contracts that is supported by a rich\ncalculational theory. In contrast with other semantic models in the literature,\nour theory of contracts allow us to specify both the evolution of state\nvariables and the permissible interactions with the environment. Moreover, our\nmodel of interaction is abstract, and supports, for instance, discrete time,\ncontinuous time, and hybrid computational models. Being based in Unifying\nTheories of Programming (UTP), our theory can be composed with further\ncomputational theories to support semantics for multi-paradigm languages.\nPractical reasoning support is provided via our proof framework, Isabelle/UTP,\nincluding a proof tactic that reduces a conjecture about a reactive program to\nthree predicates, symbolically characterising its assumptions and guarantees\nabout intermediate and final observations. This allows us to verify programs\nwith a large or infinite state space. Our work advances the state-of-the-art in\nsemantics for reactive languages, description of their contractual\nspecifications, and compositional verification.", "journal": ""}
{"doi": "10.48550/arXiv.2210.12095", "date": "2022-10-21", "title": "Learning shape distributions from large databases of healthy organs: applications to zero-shot and few-shot abnormal pancreas detection", "authors": "Rebeca V\u00e9til, Cl\u00e9ment Abi Nader, Alexandre B\u00f4ne, Marie-Pierre Vullierme, Marc-Michel Rohe\u00e9, Pietro Gori, Isabelle Bloch", "abstract": "We propose a scalable and data-driven approach to learn shape distributions\nfrom large databases of healthy organs. To do so, volumetric segmentation masks\nare embedded into a common probabilistic shape space that is learned with a\nvariational auto-encoding network. The resulting latent shape representations\nare leveraged to derive zeroshot and few-shot methods for abnormal shape\ndetection. The proposed distribution learning approach is illustrated on a\nlarge database of 1200 healthy pancreas shapes. Downstream qualitative and\nquantitative experiments are conducted on a separate test set of 224 pancreas\nfrom patients with mixed conditions. The abnormal pancreas detection AUC\nreached up to 65.41% in the zero-shot configuration, and 78.97% in the few-shot\nconfiguration with as few as 15 abnormal examples, outperforming a baseline\napproach based on the sole volume.", "journal": "Medical Image Computing and Computer Assisted Intervention 2022,\n  Lecture Notes in Computer Science volume 13432, pp 464-473"}
{"doi": "10.48550/arXiv.1410.4097", "date": "2014-10-15", "title": "Extreme value statistics for truncated Pareto-type distributions", "authors": "Jan Beirlant, Isabel Fraga Alves, Ivette Gomes, Mark M. Meerschaert", "abstract": "Recently attention has been drawn to practical problems with the use of\nunbounded Pareto distributions, for instance when there are natural upper\nbounds that truncate the probability tail. Aban, Meerschaert and Panorska\n(2006) derived the maximum likelihood estimator for the Pareto tail index of a\ntruncated Pareto distribution with a right truncation point $T$. The Hill\n(1975) estimator is then obtained by letting $T \\to \\infty$. The problem of\nextreme value estimation under right truncation was also introduced in Nuyts\n(2010) who proposed a similar estimator for the tail index and considered\ntrimming of the number of extreme order statistics. Given that in practice one\ndoes not always know whether the distribution is truncated or not, we discuss\nestimators for the Pareto index and extreme quantiles both under truncated and\nnon-truncated Pareto-type distributions. We also propose a truncated Pareto\nQQ-plot in order to help deciding between a truncated and a non-truncated case.\nIn this way we extend the classical extreme value methodology adding the\ntruncated Pareto-type model with truncation point $T \\to \\infty$ as the sample\nsize $n \\to \\infty$. Finally we present some practical examples, asymptotics\nand simulation results.", "journal": ""}
{"doi": "10.48550/arXiv.2006.09823", "date": "2020-06-05", "title": "Verifying Strong Eventual Consistency in $\u03b4$-CRDTs", "authors": "Taylor Blau", "abstract": "Conflict-free replicated data types (CRDTs) are a natural structure with\nwhich to communicate information about a shared computation in a distributed\nsetting where coordination overhead may not be tolerated, and individual\nparticipants are allowed to temporarily diverge from the overall computation.\nWithin this setting, there are two classical approaches: state- and\noperation-based CRDTs. The former define a commutative, associative, and\nidempotent join operation, and their states a monotone join semi-lattice.\nState-based CRDTs may be further distinguished into classical- and\n$\\delta$-state CRDTs. The former communicate their full state after each\nupdate, whereas the latter communicate only the changed state. Op-based CRDTs\ncommunicate operations (not state), thus making their updates non-idempotent.\nWhereas op-based CRDTs require little information to be exchanged, they demand\nrelatively strong network guarantees (exactly-once message delivery), and\nstate-based CRDTs suffer the opposite problem. Both satisfy strong eventual\nconsistency (SEC).\n  We posit that $\\delta$-state CRDTs both (1) require less communication\noverhead from payload size, and (2) tolerate relatively weak network\nenvironments, making them an ideal candidate for real-world use of CRDTs. Our\ncentral intuition is a pair of reductions between state-, $\\delta$-state, and\nop-based CRDTs. We formalize this intuition in the Isabelle interactive theorem\nprover and show that state-based CRDTs achieve SEC. We present a relaxed\nnetwork model in Isabelle and show that state-based CRDTs still maintain SEC.\nFinally, we extend our work to show that $\\delta$-state CRDTs maintain SEC when\nonly communicating $\\delta$-state fragments, even under relatively weak network\nconditions.", "journal": ""}
{"doi": "10.48550/arXiv.2204.02625", "date": "2022-04-06", "title": "Bridging the Gap of AutoGraph between Academia and Industry: Analysing AutoGraph Challenge at KDD Cup 2020", "authors": "Zhen Xu, Lanning Wei, Huan Zhao, Rex Ying, Quanming Yao, Wei-Wei Tu, Isabelle Guyon", "abstract": "Graph structured data is ubiquitous in daily life and scientific areas and\nhas attracted increasing attention. Graph Neural Networks (GNNs) have been\nproved to be effective in modeling graph structured data and many variants of\nGNN architectures have been proposed. However, much human effort is often\nneeded to tune the architecture depending on different datasets. Researchers\nnaturally adopt Automated Machine Learning on Graph Learning, aiming to reduce\nthe human effort and achieve generally top-performing GNNs, but their methods\nfocus more on the architecture search. To understand GNN practitioners'\nautomated solutions, we organized AutoGraph Challenge at KDD Cup 2020,\nemphasizing on automated graph neural networks for node classification. We\nreceived top solutions especially from industrial tech companies like Meituan,\nAlibaba and Twitter, which are already open sourced on Github. After detailed\ncomparisons with solutions from academia, we quantify the gaps between academia\nand industry on modeling scope, effectiveness and efficiency, and show that (1)\nacademia AutoML for Graph solutions focus on GNN architecture search while\nindustrial solutions, especially the winning ones in the KDD Cup, tend to\nobtain an overall solution (2) by neural architecture search only, academia\nsolutions achieve on average 97.3% accuracy of industrial solutions (3)\nacademia solutions are cheap to obtain with several GPU hours while industrial\nsolutions take a few months' labors. Academic solutions also contain much fewer\nparameters.", "journal": ""}
{"doi": "10.48550/arXiv.1111.0090", "date": "2011-11-01", "title": "An Improved Implementation and Abstract Interface for Hybrid", "authors": "Alan J. Martin, Amy P. Felty", "abstract": "Hybrid is a formal theory implemented in Isabelle/HOL that provides an\ninterface for representing and reasoning about object languages using\nhigher-order abstract syntax (HOAS). This interface is built around an HOAS\nvariable-binding operator that is constructed definitionally from a de Bruijn\nindex representation. In this paper we make a variety of improvements to\nHybrid, culminating in an abstract interface that on one hand makes Hybrid a\nmore mathematically satisfactory theory, and on the other hand has important\npractical benefits. We start with a modification of Hybrid's type of terms that\nbetter hides its implementation in terms of de Bruijn indices, by excluding at\nthe type level terms with dangling indices. We present an improved set of\ndefinitions, and a series of new lemmas that provide a complete\ncharacterization of Hybrid's primitives in terms of properties stated at the\nHOAS level. Benefits of this new package include a new proof of adequacy and\nimprovements to reasoning about object logics. Such proofs are carried out at\nthe higher level with no involvement of the lower level de Bruijn syntax.", "journal": "EPTCS 71, 2011, pp. 76-90"}
{"doi": "10.48550/arXiv.2212.01748", "date": "2022-12-04", "title": "Differential Testing of a Verification Framework for Compiler Optimizations (Experience Paper)", "authors": "Mark Utting, Brae J. Webb, Ian J. Hayes", "abstract": "We want to verify the correctness of optimization phases in the GraalVM\ncompiler, which consist of many thousands of lines of complex Java code\nperforming sophisticated graph transformations. We have built high-level models\nof the data structures and operations of the code using the Isabelle/HOL\ntheorem prover, and can formally verify the correctness of those high-level\noperations. But the remaining challenge is: how can we be sure that those\nhigh-level operations accurately reflect what the Java is doing? This paper\naddresses that issue by applying several different kinds of differential\ntesting to validate that the formal model and the Java code have the same\nsemantics. Many of these validation techniques should be applicable to other\nprojects that are building formal models of real-world code.", "journal": ""}
{"doi": "10.48550/arXiv.2402.16303", "date": "2024-02-26", "title": "A Note on Explicit Convergence Rates of Nonlocal Peridynamic Operators in $L^q$-Norm", "authors": "Adam Larios, Isabel Safarik", "abstract": "This note investigates the explicit convergence rates of nonlocal peridynamic\noperators to their classical (local) counterparts in $L^q$-norm. Previous\nresults used Fourier series and hence were restricted to showing convergence in\n$L^2$. Moreover, convergence rates were not explicit due to the use of the\nLebesgue Dominated Convergence Theorem. Some previous results have also used\nthe Taylor Remainder Theorem in differential form, but this often required an\nassumption of bounded fifth-order derivatives. We do not use these tools, but\ninstead use the Hardy-Littlewood Maximal function, and combine it with the\nintegral form of the Taylor Remainder Theorem. This approach allows us to\nestablish convergence in the $L^q$-norm ($1 \\leq q \\leq \\infty$) for nonlocal\nperidynamic partial derivatives, which immediately yields convergence rates for\nthe corresponding nonlocal peridynamic divergence, gradient, and curl operators\nto their local counterparts as the radius (a.k.a., ``horizon'') of the nonlocal\ninteraction $\\delta \\to 0$. Moreover, we obtain an explicit rate of order\n$\\mathcal{O}(\\delta^2)$. This result contributes to the understanding of the\nrelationship between nonlocal and local models, which is essential for\napplications in multiscale modeling and simulations.", "journal": ""}
{"doi": "10.48550/arXiv.2406.18211", "date": "2024-06-26", "title": "AI Cards: Towards an Applied Framework for Machine-Readable AI and Risk Documentation Inspired by the EU AI Act", "authors": "Delaram Golpayegani, Isabelle Hupont, Cecilia Panigutti, Harshvardhan J. Pandit, Sven Schade, Declan O'Sullivan, Dave Lewis", "abstract": "With the upcoming enforcement of the EU AI Act, documentation of high-risk AI\nsystems and their risk management information will become a legal requirement\nplaying a pivotal role in demonstration of compliance. Despite its importance,\nthere is a lack of standards and guidelines to assist with drawing up AI and\nrisk documentation aligned with the AI Act. This paper aims to address this gap\nby providing an in-depth analysis of the AI Act's provisions regarding\ntechnical documentation, wherein we particularly focus on AI risk management.\nOn the basis of this analysis, we propose AI Cards as a novel holistic\nframework for representing a given intended use of an AI system by encompassing\ninformation regarding technical specifications, context of use, and risk\nmanagement, both in human- and machine-readable formats. While the\nhuman-readable representation of AI Cards provides AI stakeholders with a\ntransparent and comprehensible overview of the AI use case, its\nmachine-readable specification leverages on state of the art Semantic Web\ntechnologies to embody the interoperability needed for exchanging documentation\nwithin the AI value chain. This brings the flexibility required for reflecting\nchanges applied to the AI system and its context, provides the scalability\nneeded to accommodate potential amendments to legal requirements, and enables\ndevelopment of automated tools to assist with legal compliance and conformity\nassessment tasks. To solidify the benefits, we provide an exemplar AI Card for\nan AI-based student proctoring system and further discuss its potential\napplications within and beyond the context of the AI Act.", "journal": ""}
{"doi": "10.48550/arXiv.2202.12572", "date": "2022-02-25", "title": "Spatially-Resolved Band Gap and Dielectric Function in 2D Materials from Electron Energy Loss Spectroscopy", "authors": "Abel Brokkelkamp, Jaco ter Hoeve, Isabel Postmes, Sabrya E. van Heijst, Louis Maduro, Albert V. Davydov, Sergiy Krylyuk, Juan Rojo, Sonia Conesa-Boj", "abstract": "The electronic properties of two-dimensional (2D) materials depend\nsensitively on the underlying atomic arrangement down to the monolayer level.\nHere we present a novel strategy for the determination of the band gap and\ncomplex dielectric function in 2D materials achieving a spatial resolution down\nto a few nanometers. This approach is based on machine learning techniques\ndeveloped in particle physics and makes possible the automated processing and\ninterpretation of spectral images from electron energy-loss spectroscopy\n(EELS). Individual spectra are classified as a function of the thickness with\n$K$-means clustering and then used to train a deep-learning model of the\nzero-loss peak background. As a proof-of-concept we assess the band gap and\ndielectric function of InSe flakes and polytypic WS$_2$ nanoflowers, and\ncorrelate these electrical properties with the local thickness. Our flexible\napproach is generalizable to other nanostructured materials and to\nhigher-dimensional spectroscopies, and is made available as a new release of\nthe open-source EELSfitter framework.", "journal": "J. Phys. Chem. A 2022, 126, 7, 1255-1262"}
{"doi": "10.48550/arXiv.1306.4624", "date": "2013-06-19", "title": "Automated Certification of Authorisation Policy Resistance", "authors": "Andreas Griesmayer, Charles Morisset", "abstract": "Attribute-based Access Control (ABAC) extends traditional Access Control by\nconsidering an access request as a set of pairs attribute name-value, making it\nparticularly useful in the context of open and distributed systems, where\nsecurity relevant information can be collected from different sources. However,\nABAC enables attribute hiding attacks, allowing an attacker to gain some access\nby withholding information. In this paper, we first introduce the notion of\npolicy resistance to attribute hiding attacks. We then propose the tool ATRAP\n(Automatic Term Rewriting for Authorisation Policies), based on the recent\nformal ABAC language PTaCL, which first automatically searches for resistance\ncounter-examples using Maude, and then automatically searches for an Isabelle\nproof of resistance. We illustrate our approach with two simple examples of\npolicies and propose an evaluation of ATRAP performances.", "journal": ""}
{"doi": "10.48550/arXiv.2206.10411", "date": "2022-06-09", "title": "Audio-video fusion strategies for active speaker detection in meetings", "authors": "Lionel Pibre, Francisco Madrigal, Cyrille Equoy, Fr\u00e9d\u00e9ric Lerasle, Thomas Pellegrini, Julien Pinquier, Isabelle Ferran\u00e9", "abstract": "Meetings are a common activity in professional contexts, and it remains\nchallenging to endow vocal assistants with advanced functionalities to\nfacilitate meeting management. In this context, a task like active speaker\ndetection can provide useful insights to model interaction between meeting\nparticipants. Motivated by our application context related to advanced meeting\nassistant, we want to combine audio and visual information to achieve the best\npossible performance. In this paper, we propose two different types of fusion\nfor the detection of the active speaker, combining two visual modalities and an\naudio modality through neural networks. For comparison purpose, classical\nunsupervised approaches for audio feature extraction are also used. We expect\nvisual data centered on the face of each participant to be very appropriate for\ndetecting voice activity, based on the detection of lip and facial gestures.\nThus, our baseline system uses visual data and we chose a 3D Convolutional\nNeural Network architecture, which is effective for simultaneously encoding\nappearance and movement. To improve this system, we supplemented the visual\ninformation by processing the audio stream with a CNN or an unsupervised\nspeaker diarization system. We have further improved this system by adding\nvisual modality information using motion through optical flow. We evaluated our\nproposal with a public and state-of-the-art benchmark: the AMI corpus. We\nanalysed the contribution of each system to the merger carried out in order to\ndetermine if a given participant is currently speaking. We also discussed the\nresults we obtained. Besides, we have shown that, for our application context,\nadding motion information greatly improves performance. Finally, we have shown\nthat attention-based fusion improves performance while reducing the standard\ndeviation.", "journal": ""}
{"doi": "10.48550/arXiv.1301.7462", "date": "2013-01-30", "title": "A Framework for the Verification of Certifying Computations", "authors": "Eyad Alkassar, Sascha B\u00f6hme, Kurt Mehlhorn, Christine Rizkallah", "abstract": "Formal verification of complex algorithms is challenging. Verifying their\nimplementations goes beyond the state of the art of current automatic\nverification tools and usually involves intricate mathematical theorems.\nCertifying algorithms compute in addition to each output a witness certifying\nthat the output is correct. A checker for such a witness is usually much\nsimpler than the original algorithm - yet it is all the user has to trust. The\nverification of checkers is feasible with current tools and leads to\ncomputations that can be completely trusted. We describe a framework to\nseamlessly verify certifying computations. We use the automatic verifier VCC\nfor establishing the correctness of the checker and the interactive theorem\nprover Isabelle/HOL for high-level mathematical properties of algorithms. We\ndemonstrate the effectiveness of our approach by presenting the verification of\ntypical examples of the industrial-level and widespread algorithmic library\nLEDA.", "journal": ""}
{"doi": "10.48550/arXiv.2402.13949", "date": "2024-02-21", "title": "Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements", "authors": "Jhon P. F. Charaja, Isabell Wochner, Pierre Schumacher, Winfried Ilg, Martin Giese, Christophe Maufroy, Andreas Bulling, Syn Schmitt, Georg Martius, Daniel F. B. Haeufle", "abstract": "The mimicking of human-like arm movement characteristics involves the\nconsideration of three factors during control policy synthesis: (a) chosen task\nrequirements, (b) inclusion of noise during movement execution and (c) chosen\noptimality principles. Previous studies showed that when considering these\nfactors (a-c) individually, it is possible to synthesize arm movements that\neither kinematically match the experimental data or reproduce the stereotypical\ntriphasic muscle activation pattern. However, to date no quantitative\ncomparison has been made on how realistic the arm movement generated by each\nfactor is; as well as whether a partial or total combination of all factors\nresults in arm movements with human-like kinematic characteristics and a\ntriphasic muscle pattern. To investigate this, we used reinforcement learning\nto learn a control policy for a musculoskeletal arm model, aiming to discern\nwhich combination of factors (a-c) results in realistic arm movements according\nto four frequently reported stereotypical characteristics. Our findings\nindicate that incorporating velocity and acceleration requirements into the\nreaching task, employing reward terms that encourage minimization of mechanical\nwork, hand jerk, and control effort, along with the inclusion of noise during\nmovement, leads to the emergence of realistic human arm movements in\nreinforcement learning. We expect that the gained insights will help in the\nfuture to better predict desired arm movements and corrective forces in\nwearable assistive devices.", "journal": ""}
{"doi": "10.48550/arXiv.2106.07591", "date": "2021-06-14", "title": "A biomathematical model of tumor response to radioimmunotherapy with $\u03b1$PDL1 and $\u03b1$CTLA4", "authors": "Isabel Gonz\u00e1lez-Crespo, Antonio G\u00f3mez-Caama\u00f1o, \u00d3scar L\u00f3pez Pouso, John D. Fenwick, Juan Pardo-Montero", "abstract": "There is evidence of synergy between radiotherapy and immunotherapy.\nRadiotherapy can increase liberation of tumor antigens, causing activation of\nantitumor T-cells. This effect can be boosted with immunotherapy.\nRadioimmunotherapy has potential to increase tumor control rates.\nBiomathematical models of response to radioimmunotherapy may help on\nunderstanding of the mechanisms affecting response, and assist clinicians on\nthe design of optimal treatment strategies. In this work we present a\nbiomathematical model of tumor response to radioimmunotherapy. The model uses\nthe linear-quadratic response of tumor cells to radiation (or variation of it),\nand builds on previous developments to include the radiation-induced immune\neffect. We have focused this study on the combined effect of radiotherapy and\n$\\alpha$PDL1/$\\alpha$CTLA4 therapies. The model can fit preclinical data of\nvolume dynamics and control obtained with different dose fractionations and\n$\\alpha$PDL1/$\\alpha$CTLA4. A biomathematical study of optimal combination\nstrategies suggests that a good understanding of the involved biological\ndelays, the biokinetics of the immunotherapy drug, and the interplay between\nthem, may be of paramount importance to design optimal radioimmunotherapy\nschedules. Biomathematical models like the one we present can help to interpret\nexperimental data on the synergy between radiotherapy and immunotherapy, and to\nassist in the design of more effective treatments.", "journal": "IEEE/ACM Transactions on Computational Biology and Bioinformatics\n  2023;20:808-821"}
{"doi": "10.48550/arXiv.2106.14465", "date": "2021-06-28", "title": "Exploring convolutional neural networks with transfer learning for diagnosing Lyme disease from skin lesion images", "authors": "Sk Imran Hossain, Jocelyn de Go\u00ebr de Herve, Md Shahriar Hassan, Delphine Martineau, Evelina Petrosyan, Violaine Corbain, Jean Beytout, Isabelle Lebert, Elisabeth Baux, C\u00e9line Cazorla, Carole Eldin, Yves Hansmann, Solene Patrat-Delon, Thierry Prazuck, Alice Raffetin, Pierre Tattevin, Gwena\u00ebl Vourc'H, Olivier Lesens, Engelbert Nguifo", "abstract": "Lyme disease which is one of the most common infectious vector-borne diseases\nmanifests itself in most cases with erythema migrans (EM) skin lesions. Recent\nstudies show that convolutional neural networks (CNNs) perform well to identify\nskin lesions from images. Lightweight CNN based pre-scanner applications for\nresource-constrained mobile devices can help users with early diagnosis of Lyme\ndisease and prevent the transition to a severe late form thanks to appropriate\nantibiotic therapy. Also, resource-intensive CNN based robust computer\napplications can assist non-expert practitioners with an accurate diagnosis.\nThe main objective of this study is to extensively analyze the effectiveness of\nCNNs for diagnosing Lyme disease from images and to find out the best CNN\narchitectures considering resource constraints. First, we created an EM dataset\nwith the help of expert dermatologists from Clermont-Ferrand University\nHospital Center of France. Second, we benchmarked this dataset for twenty-three\nCNN architectures customized from VGG, ResNet, DenseNet, MobileNet, Xception,\nNASNet, and EfficientNet architectures in terms of predictive performance,\ncomputational complexity, and statistical significance. Third, to improve the\nperformance of the CNNs, we used custom transfer learning from ImageNet\npre-trained models as well as pre-trained the CNNs with the skin lesion dataset\nHAM10000. Fourth, for model explainability, we utilized Gradient-weighted Class\nActivation Mapping to visualize the regions of input that are significant to\nthe CNNs for making predictions. Fifth, we provided guidelines for model\nselection based on predictive performance and computational complexity.", "journal": ""}
{"doi": "10.48550/arXiv.2207.10152", "date": "2022-07-20", "title": "Automated Kantian Ethics: A Faithful Implementation", "authors": "Lavanya Singh", "abstract": "As we grant artificial intelligence increasing power and independence in\ncontexts like healthcare, policing, and driving, AI faces moral dilemmas but\nlacks the tools to solve them. Warnings from regulators, philosophers, and\ncomputer scientists about the dangers of unethical artificial intelligence have\nspurred interest in automated ethics-i.e., the development of machines that can\nperform ethical reasoning. However, prior work in automated ethics rarely\nengages with philosophical literature. Philosophers have spent centuries\ndebating moral dilemmas so automated ethics will be most nuanced, consistent,\nand reliable when it draws on philosophical literature. In this paper, I\npresent an implementation of automated Kantian ethics that is faithful to the\nKantian philosophical tradition. I formalize Kant's categorical imperative in\nDyadic Deontic Logic, implement this formalization in the Isabelle theorem\nprover, and develop a testing framework to evaluate how well my implementation\ncoheres with expected properties of Kantian ethic. My system is an early step\ntowards philosophically mature ethical AI agents and it can make nuanced\njudgements in complex ethical dilemmas because it is grounded in philosophical\nliterature. Because I use an interactive theorem prover, my system's judgements\nare explainable.", "journal": ""}
{"doi": "10.48550/arXiv.2304.12773", "date": "2023-04-25", "title": "Modeling Adaptive Self-healing Systems", "authors": "Habtom Kahsay Gidey, Diego Marmsoler, Dominik Ascher", "abstract": "Motivation: Smart grids design requires energy distribution operations to be\nadaptable to abnormality. This requirement entails distribution system\noperators (DSOs) to optimize restoration to normal operational states\ndynamically. However, these design challenges demand collaborative research\nefforts on sophisticated modeling and simulation approaches. Approach: In the\nESOSEG research project, analyzing the smart grid domain as a\nsoftware-intensive system, we employed a dynamic architecture approach,\nparticularly the FOCUS theory, to model and assure the domains' self-healing\nrequirements. Although some works specify various self-healing systems, to the\nbest of our knowledge, the use of the approach in smart grids is the first work\nto enable a formal specification and verification of self-healing properties in\nsmart grids. Results: As a result, to support the modeling and verification\nprocess, we developed tool support with Eclipse Modeling Framework (EMF),\nXtext, and other languages in the EMF ecosystem. The tool includes a grammar or\na meta-model of the DSL, an interface to enable textual and graphical modeling\nof architectural patterns and code transformer engine for verification.\nFurthermore, we evaluated the modeling and verification features of the tool\nsupport with an e-Car charging scenario for modeling adaptive self-healing\nproperties. Futureworks: As an outlook, future works could include\ninvestigation of comprehensive case studies. These, for instance, could be\nfurther particular adaptability scenarios addressing challenges in DSOs.\nAnother interesting aspect could be the evaluation of the modeling approach by\ninvestigating its use with engineers involved in a smart grid design. Next, the\nevaluation could be followed with abstractions of the verification process to\nmake it useable by system architects with no knowledge of the proof language,\nIsabelle/HOL.", "journal": ""}
{"doi": "10.48550/arXiv.2110.14707", "date": "2021-10-27", "title": "New $Z_c$ spectroscopy or Landau singularities? Searches in Heavy Ion Collisions could help decide", "authors": "Felipe J. Llanes-Estrada, Luciano M. Abreu", "abstract": "A constellation of quarkonium-mass peaks has been reported in the last\ndecade, opening what could be an entire new spectroscopy of nuclear-physics\nlike complexity. Salient among these structures are the $Z_c$, much analyzed at\nBESIII as nicely summarised by J.~Zhao at this conference. Their stature as new\nhadron states has often been challenged by proposals that the peaks arise from\ntriangle singularities instead, because they appear in three-body reactions and\noften satisfy the special kinematic conditions of the Coleman-Norton theorem.\nTo experimentally decide, we should like to resort to different reactions\nerasing the kinematic coincidence, and see whether each peak survives. We\nobserve that the most universal \"eraser\" is the medium created in Heavy Ion\nCollisions, because its high temperature often affects the masses and widths of\nthe particles participating in the reaction: if additionally the triangle\nreaction is completed during the lifetime of the fireball, our computations in\nthermal field theory show that the triangle singularities do not survive the\nhot hadron phase (much less the hotter quark-gluon plasma). On the contrary,\nordinary quarkonia are affected but often survive the fireball and\nmolecular-like states are also routinely detected. We have provided several\nexamples, and here investigate the exotic $Z_{c}(4020)^\\pm\\to \\pi^{\\pm} h_c $\npeak (produced against a $\\pi^\\mp$ in $e^+e^-$ collisions). If, as has been\nproposed during the debates, it is produced or enhanced by a $D_{1} D^{*} D^*$\ntriangle loop, the estimate here reported suggests its disappearance near the\ncritical temperature of the confinement/deconfinement phase transition, so any\nfuture detection in heavy-ion collision data would suggest it to be a real\nhadron, with its absence leaning towards the singularity interpretation.", "journal": ""}
{"doi": "10.48550/arXiv.1610.09796", "date": "2016-10-31", "title": "A two-scale Stefan problem arising in a model for tree sap exudation", "authors": "Isabell Konrad, Malte A. Peter, John M. Stockie", "abstract": "The study of tree sap exudation, in which a (leafless) tree generates\nelevated stem pressure in response to repeated daily freeze-thaw cycles, gives\nrise to an interesting multi-scale problem involving heat and multiphase\nliquid/gas transport. The pressure generation mechanism is a cellular-level\nprocess that is governed by differential equations for sap transport through\nporous cell membranes, phase change, heat transport, and generation of osmotic\npressure. By assuming a periodic cellular structure based on an appropriate\nreference cell, we derive an homogenized heat equation governing the global\ntemperature on the scale of the tree stem, with all the remaining physics\nrelegated to equations defined on the reference cell. We derive a corresponding\nstrong formulation of the limit problem and use it to design an efficient\nnumerical solution algorithm. Numerical simulations are then performed to\nvalidate the results and draw conclusions regarding the phenomenon of sap\nexudation, which is of great importance in trees such as sugar maple and a few\nother related species. The particular form of our homogenized temperature\nequation is obtained using periodic homogenization techniques with two-scale\nconvergence, which we investigate theoretically in the context of a simpler\ntwo-phase Stefan-type problem corresponding to a periodic array of melting\ncylindrical ice bars with a constant thermal diffusion coefficient. For this\nreduced model, we prove results on existence, uniqueness and convergence of the\ntwo-scale limit solution in the weak form, clearly identifying the missing\npieces required to extend the proofs to the fully nonlinear sap exudation\nmodel. Numerical simulations of the reduced equations are then compared with\nresults from the complete sap exudation model.", "journal": "IMA Journal of Applied Mathematics, 82(4):726-762, 2017"}
{"doi": "10.48550/arXiv.2306.12535", "date": "2023-06-21", "title": "A formalization of the CHSH inequality and Tsirelson's upper-bound in Isabelle/HOL", "authors": "Mnacho Echenim, Mehdi Mhalla", "abstract": "We present a formalization of several fundamental notions and results from\nQuantum Information theory, including density matrices and projective\nmeasurements, along with the proof that the local hidden-variable hypothesis\nadvocated by Einstein to model quantum mechanics cannot hold. The proof of the\nlatter result is based on the so-called CHSH inequality, and it is the\nviolation of this inequality that was experimentally evidenced by Aspect who\nearned the Nobel Prize in 2022 for his work. We also formalize various results\nrelated to the violation of the CHSH inequality, such as Tsirelson's bound\nwhich permits to obtain the maximum violation of this inequality in a quantum\nsetting.", "journal": ""}
{"doi": "10.48550/arXiv.1702.03715", "date": "2017-02-13", "title": "An efficient algorithm to decide periodicity of b-recognisable sets using MSDF convention", "authors": "Bernard Boigelot, Isabelle Mainz, Victor Marsault, Michel Rigo", "abstract": "Given an integer base $b>1$, a set of integers is represented in base $b$ by\na language over $\\{0,1,...,b-1\\}$. The set is said to be $b$-recognisable if\nits representation is a regular language. It is known that eventually periodic\nsets are $b$-recognisable in every base $b$, and Cobham's theorem implies the\nconverse: no other set is $b$-recognisable in every base $b$.\n  We are interested in deciding whether a $b$-recognisable set of integers\n(given as a finite automaton) is eventually periodic. Honkala showed that this\nproblem decidable in 1986 and recent developments give efficient decision\nalgorithms. However, they only work when the integers are written with the\nleast significant digit first.\n  In this work, we consider the natural order of digits (Most Significant Digit\nFirst) and give a quasi-linear algorithm to solve the problem in this case.", "journal": ""}
{"doi": "10.48550/arXiv.2105.10914", "date": "2021-05-23", "title": "Quantum references", "authors": "Dominique Unruh", "abstract": "We present a theory of \"quantum references\", similar to lenses in classical\nfunctional programming, that allow to point to a subsystem of a larger quantum\nsystem, and to mutate/measure that part. Mutable classical variables, quantum\nregisters, and wires in quantum circuits are examples of this, but also\nreferences to parts of larger quantum datastructures. Quantum references in our\nsetting can also refer to subparts of other references, or combinations of\nparts from different references, or quantum references seen in a different\nbasis, etc. Our modeling is intended to be well suited for formalization in\ntheorem provers and as a foundation for modeling variables in quantum programs.\nWe study quantum references in greater detail and cover the\ninfinite-dimensional case as well, but also provide a more general treatment\nnot specific to the quantum case. We implemented a large part of our results\n(including a small quantum Hoare logic and an analysis of quantum\nteleportation) in the Isabelle/HOL theorem prover.", "journal": ""}
{"doi": "10.48550/arXiv.2310.17595", "date": "2023-10-26", "title": "Model-theoretic properties of nilpotent groups and Lie algebras", "authors": "Christian d'Elb\u00e9e, Isabel M\u00fcller, Nicholas Ramsey, Daoud Siniora", "abstract": "We give a systematic study of the model theory of generic nilpotent groups\nand Lie algebras. We show that the Fra\\\"iss\\'e limit of 2-nilpotent groups of\nexponent $p$ studied by Baudisch is 2-dependent and NSOP$_{1}$. We prove that\nthe class of $c$-nilpotent Lie algebras over an arbitrary field, in a language\nwith predicates for a Lazard series, is closed under free amalgamation. We show\nthat for $2 < c$, the generic $c$-nilpotent Lie algebra over $\\mathbb{F}_{p}$\nis strictly NSOP$_{4}$ and $c$-dependent. Via the Lazard correspondence, we\nobtain the same result for $c$-nilpotent groups of exponent $p$, for an odd\nprime $p > c$.", "journal": ""}
{"doi": "10.48550/arXiv.1604.08345", "date": "2016-04-28", "title": "Advances in Property-Based Testing for $\u03b1$Prolog", "authors": "James Cheney, Alberto Momigliano, Matteo Pessina", "abstract": "$\\alpha$Check is a light-weight property-based testing tool built on top of\n$\\alpha$Prolog, a logic programming language based on nominal logic.\n$\\alpha$Prolog is particularly suited to the validation of the meta-theory of\nformal systems, for example correctness of compiler translations involving\nname-binding, alpha-equivalence and capture-avoiding substitution. In this\npaper we describe an alternative to the negation elimination algorithm\nunderlying $\\alpha$Check that substantially improves its effectiveness. To\nsubstantiate this claim we compare the checker performances w.r.t. two of its\nmain competitors in the logical framework niche, namely the QuickCheck/Nitpick\ncombination offered by Isabelle/HOL and the random testing facility in\nPLT-Redex.", "journal": ""}
{"doi": "10.48550/arXiv.2204.14050", "date": "2022-04-28", "title": "Polarimetric imaging for the detection of synthetic models of SARS-CoV-2: a proof of concept", "authors": "Emilio Gomez-Gonzalez, Olga Mu\u00f1oz, Juan Carlos Gomez-Martin, Jesus Aceituno-Castro, Beatriz Fernandez-Mu\u00f1oz, Jose Manuel Navas-Garcia, Alejandro Barriga-Rivera, Isabel Fernandez-Lizaranzu, Francisco Javier Munoz-Gonzalez, Ruben Parrilla-Giraldez, Desiree Requena-Lancharro, Pedro Gil-Gamboa, Jos\u00e9 Luis Ramos, Cristina Rosell-Valle, Carmen Gomez-Gonzalez, Maria Martin-Lopez, Maria Isabel Relimpio-Lopez, Manuel A. Perales-Esteve, Antonio Puppo-Moreno, Francisco Jose Garcia-Cozar, Lucia Olvera-Collantes, Silvia de los Santos-Trigo, Emilia Gomez, Rosario Sanchez-Pernaute, Javier Padillo-Ruiz, Javier Marquez-Rivas", "abstract": "Objective: To conduct a proof-of-concept study of the detection of two\nsynthetic models of severe acute respiratory syndrome coronavirus 2\n(SARS-CoV-2) using polarimetric imaging. Methods: Two SARS-CoV-2 models were\nprepared as engineered lentiviruses pseudotyped with the G protein of the\nvesicular stomatitis virus, and with the characteristic Spike protein of\nSARS-CoV-2. Samples were preparations in two biofluids (saline solution and\nartificial saliva), in four concentrations, and deposited as 5-{\\mu}L droplets\non a supporting plate. The angles of maximal degree of linear polarization\n(DLP) of light diffusely scattered from dry residues were determined using\nMueller polarimetry of 87 samples at 405 nm and 514 nm. A polarimetric camera\nwas used for simultaneous imaging of several samples under 380-420 nm\nillumination at angles similar to those of maximal DLP. A per-pixel image\nanalysis included quantification and combination of polarization feature\ndescriptors in other 475 samples. Results: The angles (from sample surface) of\nmaximal DLP were 3 degrees for 405 nm and 6 degrees for 514 nm. Similar viral\nparticles that differ only in the characteristic spike protein of the\nSARS-CoV-2, their corresponding negative controls, fluids, and the sample\nholder were discerned from polarimetric image analysis at 10-degree and\n15-degree configurations. Conclusion: Polarimetric imaging in the visible\nspectrum has the potential for non-contact, reagent-free detection of viruses\nin multiple dry fluid residues simultaneously. Further analysis including real\nSARS-CoV-2 in human samples -- particularly, fresh saliva -- are required.\nSignificance: Polarimetric imaging under visible light could contribute to\nfast, cost-effective screening of SARS-CoV-2 and other pathogens.", "journal": ""}
{"doi": "10.48550/arXiv.2105.14381", "date": "2021-05-29", "title": "Formally Validating a Practical Verification Condition Generator (extended version)", "authors": "Gaurav Parthasarathy, Peter M\u00fcller, Alexander J. Summers", "abstract": "A program verifier produces reliable results only if both the logic used to\njustify the program's correctness is sound, and the implementation of the\nprogram verifier is itself correct. Whereas it is common to formally prove\nsoundness of the logic, the implementation of a verifier typically remains\nunverified. Bugs in verifier implementations may compromise the trustworthiness\nof successful verification results. Since program verifiers used in practice\nare complex, evolving software systems, it is generally not feasible to\nformally verify their implementation.\n  In this paper, we present an alternative approach: we validate successful\nruns of the widely-used Boogie verifier by producing a certificate which proves\ncorrectness of the obtained verification result. Boogie performs a complex\nseries of program translations before ultimately generating a verification\ncondition whose validity should imply the correctness of the input program. We\nshow how to certify three of Boogie's core transformation phases: the\nelimination of cyclic control flow paths, the (SSA-like) replacement of\nassignments by assumptions using fresh variables (passification), and the final\ngeneration of verification conditions. Similar translations are employed by\nother verifiers. Our implementation produces certificates in Isabelle, based on\na novel formalisation of the Boogie language.", "journal": ""}
{"doi": "10.48550/arXiv.2007.13529", "date": "2020-07-27", "title": "Automated Verification of Reactive and Concurrent Programs by Calculation", "authors": "Simon Foster, Kangfeng Ye, Ana Cavalcanti, Jim Woodcock", "abstract": "Reactive programs combine traditional sequential programming constructs with\nprimitives to allow communication with other concurrent agents. They are\nubiquitous in modern applications, ranging from components systems and web\nservices, to cyber-physical systems and autonomous robots. In this paper, we\npresent an algebraic verification strategy for concurrent reactive programs,\nwith a large or infinite state space. We define novel operators to characterise\ninteractions and state updates, and an associated equational theory. With this\nwe can calculate a reactive program's denotational semantics, and thereby\nfacilitate automated proof. Of note is our reasoning support for iterative\nprograms with reactive invariants, based on Kleene algebra, and for parallel\ncomposition. We illustrate our strategy by verifying a reactive buffer. Our\nlaws and strategy are mechanised in Isabelle/UTP, our implementation of Hoare\nand He's Unifying Theories of Programming (UTP) framework, to provide soundness\nguarantees and practical verification support.", "journal": ""}
{"doi": "10.48550/arXiv.2009.09215", "date": "2020-09-19", "title": "Faster Smarter Induction in Isabelle/HOL", "authors": "Yutaka Nagashima", "abstract": "Proof by induction plays a critical role in formal verification and\nmathematics at large. However, its automation remains as one of the\nlong-standing challenges in Computer Science. To address this problem, we\ndeveloped sem_ind. Given inductive problem, sem_ind recommends what arguments\nto pass to the induct method. To improve the accuracy of sem_ind, we introduced\ndefinitional quantifiers, a new kind of quantifiers that allow us to\ninvestigate not only the syntactic structures of inductive problems but also\nthe definitions of relevant constants in a domain-agnostic style. Our\nevaluation shows that compared to its predecessor sem_ind improves the accuracy\nof recommendation from 20.1% to 38.2% for the most promising candidates within\n5.0 seconds of timeout while decreasing the median value of execution time from\n2.79 seconds to 1.06 seconds.", "journal": ""}
{"doi": "10.48550/arXiv.2403.03035", "date": "2024-03-05", "title": "Mars 2.0: A Toolchain for Modeling, Analysis, Verification and Code Generation of Cyber-Physical Systems", "authors": "Bohua Zhan, Xiong Xu, Qiang Gao, Zekun Ji, Xiangyu Jin, Shuling Wang, Naijun Zhan", "abstract": "We introduce Mars 2.0 for modeling, analysis, verification and code\ngeneration of Cyber-Physical Systems. Mars 2.0 integrates Mars 1.0 with several\nimportant extensions and improvements, allowing the design of cyber-physical\nsystems using the combination of AADL and Simulink/Stateflow, which provide a\nunified graphical framework for modeling the functionality, physicality and\narchitecture of the system to be developed. For a safety-critical system,\nformal analysis and verification of its combined AADL and Simulink/Stateflow\nmodel can be conducted via the following steps. First, the toolchain\nautomatically translates AADL and Simulink/Stateflow models into Hybrid CSP\n(HCSP), an extension of CSP for formally modeling hybrid systems. Second, the\nHCSP processes can be simulated using the HCSP simulator, and to complement\nincomplete simulation, they can be verified using the Hybrid Hoare Logic prover\nin Isabelle/HOL, as well as the more automated HHLPy prover. Finally,\nimplementations in SystemC or C can be automatically generated from the\nverified HCSP processes. The transformation from AADL and Simulink/Stateflow to\nHCSP, and the one from HCSP to SystemC or C, are both guaranteed to be correct\nwith formal proofs. This approach allows model-driven design of safety-critical\ncyber-physical systems based on graphical and formal models and proven-correct\ntranslation procedures. We demonstrate the use of the toolchain on several\nbenchmarks of varying complexity, including several industrial-sized examples.", "journal": ""}
{"doi": "10.48550/arXiv.2107.11674", "date": "2021-07-24", "title": "Case Studies in Formal Reasoning About Lambda-Calculus: Semantics, Church-Rosser, Standardization and HOAS", "authors": "Lorenzo Gheri, Andrei Popescu", "abstract": "We have previously published the Isabelle/HOL formalization of a general\ntheory of syntax with bindings. In this companion paper, we instantiate the\ngeneral theory to the syntax of lambda-calculus and formalize the development\nleading to several fundamental constructions and results: sound semantic\ninterpretation, the Church-Rosser and standardization theorems, and\nhigher-order abstract syntax (HOAS) encoding. For Church-Rosser and\nstandardization, our work covers both the call-by-name and call-by-value\nversions of the calculus, following classic papers by Takahashi and Plotkin.\nDuring the formalization, we were able to stay focused on the high-level ideas\nof the development -- thanks to the arsenal provided by our general theory: a\nwealth of basic facts about the substitution, swapping and freshness operators,\nas well as recursive-definition and reasoning principles, including a\nspecialization to semantic interpretation of syntax.", "journal": ""}
{"doi": "10.48550/arXiv.2004.08200", "date": "2020-04-17", "title": "Defining and Verifying Durable Opacity: Correctness for Persistent Software Transactional Memory", "authors": "Eleni Bila, Simon Doherty, Brijesh Dongol, John Derrick, Gerhard Schellhorn, Heike Wehrheim", "abstract": "Non-volatile memory (NVM), aka persistent memory, is a new paradigm for\nmemory that preserves its contents even after power loss. The expected ubiquity\nof NVM has stimulated interest in the design of novel concepts ensuring\ncorrectness of concurrent programming abstractions in the face of persistency.\nSo far, this has lead to the design of a number of persistent concurrent data\nstructures, built to satisfy an associated notion of correctness: durable\nlinearizability.\n  In this paper, we transfer the principle of durable concurrent correctness to\nthe area of software transactional memory (STM). Software transactional memory\nalgorithms allow for concurrent access to shared state. Like linearizability\nfor concurrent data structures, opacity is the established notion of\ncorrectness for STMs. First, we provide a novel definition of durable opacity\nextending opacity to handle crashes and recovery in the context of NVM. Second,\nwe develop a durably opaque version of an existing STM algorithm, namely the\nTransactional Mutex Lock (TML). Third, we design a proof technique for durable\nopacity based on refinement between TML and an operational characterisation of\ndurable opacity by adapting the TMS2 specification. Finally, we apply this\nproof technique to show that the durable version of TML is indeed durably\nopaque. The correctness proof is mechanized within Isabelle.", "journal": ""}
{"doi": "10.48550/arXiv.2407.00514", "date": "2024-06-29", "title": "Combining Classical and Probabilistic Independence Reasoning to Verify the Security of Oblivious Algorithms (Extended Version)", "authors": "Pengbo Yan, Toby Murray, Olga Ohrimenko, Van-Thuan Pham, Robert Sison", "abstract": "We consider the problem of how to verify the security of probabilistic\noblivious algorithms formally and systematically. Unfortunately, prior program\nlogics fail to support a number of complexities that feature in the semantics\nand invariant needed to verify the security of many practical probabilistic\noblivious algorithms. We propose an approach based on reasoning over perfectly\noblivious approximations, using a program logic that combines both classical\nHoare logic reasoning and probabilistic independence reasoning to support all\nthe needed features. We formalise and prove our new logic sound in Isabelle/HOL\nand apply our approach to formally verify the security of several challenging\ncase studies beyond the reach of prior methods for proving obliviousness.", "journal": ""}
{"doi": "10.48550/arXiv.1912.08858", "date": "2019-12-18", "title": "General parametrization of Majorana neutrino mass models", "authors": "Isabel Cordero-Carri\u00f3n, Martin Hirsch, Avelino Vicente", "abstract": "We discuss a general formula which allows to automatically reproduce\nexperimental data for Majorana neutrino mass models, while keeping the complete\nset of the remaining model parameters free for general scans, as necessary in\norder to provide reliable predictions for observables outside the neutrino\nsector. We provide a proof of this master parametrization and show how to apply\nit for several well-known neutrino mass models from the literature. We also\ndiscuss a list of special cases, in which the Yukawa couplings have to fulfill\nsome particular additional conditions.", "journal": "Phys. Rev. D 101, 075032 (2020)"}
{"doi": "10.48550/arXiv.2501.11467", "date": "2025-01-20", "title": "Fixed Point Certificates for Reachability and Expected Rewards in MDPs", "authors": "Krishnendu Chatterjee, Tim Quatmann, Maximilian Sch\u00e4ffeler, Maximilian Weininger, Tobias Winkler, Daniel Zilken", "abstract": "The possibility of errors in human-engineered formal verification software,\nsuch as model checkers, poses a serious threat to the purpose of these tools.\nAn established approach to mitigate this problem are certificates --\nlightweight, easy-to-check proofs of the verification results. In this paper,\nwe develop novel certificates for model checking of Markov decision processes\n(MDPs) with quantitative reachability and expected reward properties. Our\napproach is conceptually simple and relies almost exclusively on elementary\nfixed point theory. Our certificates work for arbitrary finite MDPs and can be\nreadily computed with little overhead using standard algorithms. We formalize\nthe soundness of our certificates in Isabelle/HOL and provide a formally\nverified certificate checker. Moreover, we augment existing algorithms in the\nprobabilistic model checker Storm with the ability to produce certificates and\ndemonstrate practical applicability by conducting the first formal\ncertification of the reference results in the Quantitative Verification\nBenchmark Set.", "journal": ""}
{"doi": "10.48550/arXiv.1707.05383", "date": "2017-07-13", "title": "A Flexible Approach for Finding Optimal Paths with Minimal Conflicts", "authors": "Juliana Bowles, Marco B. Caminati", "abstract": "Complex systems are usually modelled through a combination of structural and\nbehavioural models, where separate behavioural models make it easier to design\nand understand partial behaviour. When partial models are combined, we need to\nguarantee that they are consistent, and several automated techniques have been\ndeveloped to check this. We argue that in some cases it is impossible to\nguarantee total consistency, and instead we want to find execution paths across\nsuch models with minimal conflicts with respect to a certain metric of\ninterest. We present an efficient and scalable solution to find optimal paths\nthrough a combination of the theorem prover Isabelle with the constraint solver\nZ3. Our approach has been inspired by a healthcare problem, namely how to\ndetect conflicts between medications taken by patients with multiple chronic\nconditions, and how to find preferable alternatives automatically.", "journal": ""}
{"doi": "10.48550/arXiv.2407.20002", "date": "2024-07-29", "title": "Formal Foundations for Translational Separation Logic Verifiers (extended version)", "authors": "Thibault Dardinier, Michael Sammler, Gaurav Parthasarathy, Alexander J. Summers, Peter M\u00fcller", "abstract": "Program verification tools are often implemented as front-end translations of\nan input program into an intermediate verification language (IVL) such as\nBoogie, GIL, Viper, or Why3. The resulting IVL program is then verified using\nan existing back-end verifier. A soundness proof for such a translational\nverifier needs to relate the input program and verification logic to the\nsemantics of the IVL, which in turn needs to be connected with the verification\nlogic implemented in the back-end verifiers. Performing such proofs is\nchallenging due to the large semantic gap between the input and output programs\nand logics, especially for complex verification logics such as separation\nlogic.\n  This paper presents a formal framework for reasoning about translational\nseparation logic verifiers. At its center is a generic core IVL that captures\nthe essence of different separation logics. We define its operational semantics\nand formally connect it to two different back-end verifiers, which use symbolic\nexecution and verification condition generation, resp. Crucially, this\nsemantics uses angelic non-determinism to enable the application of different\nproof search algorithms and heuristics in the back-end verifiers. An axiomatic\nsemantics for the core IVL simplifies reasoning about the front-end translation\nby performing essential proof steps once and for all in the equivalence proof\nwith the operational semantics rather than for each concrete front-end\ntranslation.\n  We illustrate the usefulness of our formal framework by instantiating our\ncore IVL with elements of Viper and connecting it to two Viper back-ends as\nwell as a front-end for concurrent separation logic. All our technical results\nhave been formalized in Isabelle/HOL, including the core IVL and its semantics,\nthe semantics of two back-ends for a subset of Viper, and all proofs.", "journal": "Proc. ACM Program. Lang. 9, POPL, Article 20 (January 2025)"}
{"doi": "10.48550/arXiv.1707.09646", "date": "2017-07-30", "title": "Correct Composition of Dephased Behavioural Models", "authors": "Juliana Bowles, Marco B. Caminati", "abstract": "Scenarios of execution are commonly used to specify partial behaviour and\ninteractions between different objects and components in a system. To avoid\noverall inconsistency in specifications, various automated methods have emerged\nin the literature to compose (behavioural) models. In recent work, we have\nshown how the theorem prover Isabelle can be combined with the constraint\nsolver Z3 to efficiently detect inconsistencies in two or more behavioural\nmodels and, in their absence, generate the composition. Here, we extend our\napproach further and show how to generate the correct composition (as a set of\nvalid traces) of dephased models. This work has been inspired by a problem from\na medical domain where different care pathways (for chronic conditions) may be\napplied to the same patient with different starting points.", "journal": ""}
{"doi": "10.48550/arXiv.1604.00206", "date": "2016-04-01", "title": "Semantics-Preserving Simplification of Real-World Firewall Rule Sets", "authors": "Cornelius Diekmann, Lars Hupel, Georg Carle", "abstract": "The security provided by a firewall for a computer network almost completely\ndepends on the rules it enforces. For over a decade, it has been a well-known\nand unsolved problem that the quality of many firewall rule sets is\ninsufficient. Therefore, there are many tools to analyze them. However, we\nfound that none of the available tools could handle typical, real-world\niptables rulesets. This is due to the complex chain model used by iptables, but\nalso to the vast amount of possible match conditions that occur in real-world\nfirewalls, many of which are not understood by academic and open source tools.\n  In this paper, we provide algorithms to transform firewall rulesets. We\nreduce the execution model to a simple list model and use ternary logic to\nabstract over all unknown match conditions. These transformations enable\nexisting tools to understand real-world firewall rules, which we demonstrate on\nfour decently-sized rulesets. %After preparation with our algorithms, tools\ncould understand them. Using the Isabelle theorem prover, we formally show that\nall our algorithms preserve the firewall's filtering behavior.", "journal": "FM 2015: Formal Methods Volume 9109 of the series Lecture Notes in\n  Computer Science pp 195-212, Springer"}
{"doi": "10.48550/arXiv.1609.00118", "date": "2016-09-01", "title": "An algebra of synchronous atomic steps", "authors": "Ian J. Hayes, Robert Colvin, Larissa Meinicke, Kirsten Winter, Andrius Velykis", "abstract": "This research started with an algebra for reasoning about rely/guarantee\nconcurrency for a shared memory model. The approach taken led to a more\nabstract algebra of atomic steps, in which atomic steps synchronise (rather\nthan interleave) when composed in parallel. The algebra of rely/guarantee\nconcurrency then becomes an interpretation of the more abstract algebra. Many\nof the core properties needed for rely/guarantee reasoning can be shown to hold\nin the abstract algebra where their proofs are simpler and hence allow a higher\ndegree of automation. Moreover, the realisation that the synchronisation\nmechanisms of standard process algebras, such as CSP and CCS/SCCS, can be\ninterpreted in our abstract algebra gives evidence of its unifying power. The\nalgebra has been encoded in Isabelle/HOL to provide a basis for tool support.", "journal": "Fitzgerald J., Heitmeyer C., Gnesi S., Philippou A. (eds) FM 2016:\n  Formal Methods. FM 2016. Lecture Notes in Computer Science, vol 9995.\n  Springer, Cham"}
{"doi": "10.48550/arXiv.2401.06764", "date": "2024-01-12", "title": "Covert Quantum Communication Over Optical Channels", "authors": "Evan J. D. Anderson, Christopher K. Eyre, Isabel M. Dailey, Filip Rozp\u0119dek, Boulat A. Bash", "abstract": "We explore covert communication of qubits over the lossy thermal-noise\nbosonic channel, which is a quantum-mechanical model of many practical\nchannels, including optical. Covert communication ensures that an adversary is\nunable to detect the presence of transmissions, which are concealed in channel\nnoise. We show a \\emph{square root law} (SRL) for quantum covert communication\nsimilar to that for classical: $\\propto\\sqrt{n}$ qubits can be transmitted\ncovertly and reliably over $n$ uses of an optical channel. Our achievability\nproof uses photonic dual-rail qubit encoding, which has been proposed for\nlong-range repeater-based quantum communication and entanglement distribution.\nOur converse employs prior covert signal power limit results and adapts\nwell-known methods to upper bound quantum capacity of optical channels.\nFinally, we believe that the gap between our lower and upper bounds for the\nnumber of reliable covert qubits can be mitigated by improving the quantum\nerror correction codes and quantum channel capacity bounds.", "journal": ""}
{"doi": "10.48550/arXiv.2411.14581", "date": "2024-11-21", "title": "Semantics for Linear-time Temporal Logic with Finite Observations", "authors": "Rayhana Amjad, Rob van Glabbeek, Liam O'Connor", "abstract": "LTL3 is a multi-valued variant of Linear-time Temporal Logic for runtime\nverification applications. The semantic descriptions of LTL3 in previous work\nare given only in terms of the relationship to conventional LTL. Our approach,\nby contrast, gives a full model-based inductive accounting of the semantics of\nLTL3, in terms of families of definitive prefix sets. We show that our\ndefinitive prefix sets are isomorphic to linear-time temporal properties (sets\nof infinite traces), and thereby show that our semantics of LTL3 directly\ncorrespond to the semantics of conventional LTL. In addition, we formalise the\nformula progression evaluation technique, popularly used in runtime\nverification and testing contexts, and show its soundness and completeness up\nto finite traces with respect to our semantics. All of our definitions and\nproofs are mechanised in Isabelle/HOL.", "journal": "EPTCS 412, 2024, pp. 35-50"}
{"doi": "10.48550/arXiv.2401.11942", "date": "2024-01-22", "title": "Single-Photon-Assisted Two-Photon Polymerization", "authors": "Buse Unlu, Maria Isabel \u00c1lvarez-Casta\u00f1o, Antoine Boniface, Ye Pu, Christophe Moser", "abstract": "Light-based additive manufacturing (AM) has revolutionized the fabrication of\ncomplex three-dimensional (3D) objects offering a cost-effective and high-speed\nalternative to traditional machining. One-photon polymerization is a key\nprocess in this advancement, standing out for rapid printing time, albeit with\nlimited resolution. Two-photon polymerization (2PP) empowers AM with\nunprecedented resolution but is accompanied by a tradeoff of prolonged printing\ntimes. We propose combining the single-photon absorption (1PA) and 2PP to\nbenefit from the dual capabilities, allowing for faster printing while\nmaintaining high resolution and improved depth sectioning, respectively. In\nthis study, we employ a blue light source to pre-excite a photocurable resin by\n1PA followed by a precisely focused femtosecond (fs) beam to provide the\nmissing energy necessary to reach the polymerization threshold to solidify the\nresin through two-photon absorption. First, we investigate the impact of\npre-sensitization by blue light illumination on 2PP and demonstrate one order\nof magnitude faster printing time for a voxel size of 150 nm as compared to the\nsame voxel size printed by 2PP only. Then, we build a custom 2PP printer\nutilizing blue light sensitization in a light-sheet mode and demonstrate\nsuccessful 3D prints.", "journal": ""}
{"doi": "10.48550/arXiv.2202.02278", "date": "2022-02-04", "title": "LTU Attacker for Membership Inference", "authors": "Joseph Pedersen, Rafael Mu\u00f1oz-G\u00f3mez, Jiangnan Huang, Haozhe Sun, Wei-Wei Tu, Isabelle Guyon", "abstract": "We address the problem of defending predictive models, such as machine\nlearning classifiers (Defender models), against membership inference attacks,\nin both the black-box and white-box setting, when the trainer and the trained\nmodel are publicly released. The Defender aims at optimizing a dual objective:\nutility and privacy. Both utility and privacy are evaluated with an external\napparatus including an Attacker and an Evaluator. On one hand, Reserved data,\ndistributed similarly to the Defender training data, is used to evaluate\nUtility; on the other hand, Reserved data, mixed with Defender training data,\nis used to evaluate membership inference attack robustness. In both cases\nclassification accuracy or error rate are used as the metric: Utility is\nevaluated with the classification accuracy of the Defender model; Privacy is\nevaluated with the membership prediction error of a so-called\n\"Leave-Two-Unlabeled\" LTU Attacker, having access to all of the Defender and\nReserved data, except for the membership label of one sample from each. We\nprove that, under certain conditions, even a \"na\\\"ive\" LTU Attacker can achieve\nlower bounds on privacy loss with simple attack strategies, leading to concrete\nnecessary conditions to protect privacy, including: preventing over-fitting and\nadding some amount of randomness. However, we also show that such a na\\\"ive LTU\nAttacker can fail to attack the privacy of models known to be vulnerable in the\nliterature, demonstrating that knowledge must be complemented with strong\nattack strategies to turn the LTU Attacker into a powerful means of evaluating\nprivacy. Our experiments on the QMNIST and CIFAR-10 datasets validate our\ntheoretical results and confirm the roles of over-fitting prevention and\nrandomness in the algorithms to protect against privacy attacks.", "journal": ""}
{"doi": "10.48550/arXiv.1207.3208", "date": "2012-07-13", "title": "Formal Verification of Monad Transformers", "authors": "Brian Huffman", "abstract": "We present techniques for reasoning about constructor classes that (like the\nmonad class) fix polymorphic operations and assert polymorphic axioms. We do\nnot require a logic with first-class type constructors, first-class\npolymorphism, or type quantification; instead, we rely on a domain-theoretic\nmodel of the type system in a universal domain to provide these features.\n  These ideas are implemented in the Tycon library for the Isabelle theorem\nprover, which builds on the HOLCF library of domain theory. The Tycon library\nprovides various axiomatic type constructor classes, including functors and\nmonads. It also provides automation for instantiating those classes, and for\ndefining further subclasses.\n  We use the Tycon library to formalize three Haskell monad transformers: the\nerror transformer, the writer transformer, and the resumption transformer. The\nerror and writer transformers do not universally preserve the monad laws;\nhowever, we establish datatype invariants for each, showing that they are valid\nmonads when viewed as abstract datatypes.", "journal": ""}
{"doi": "10.48550/arXiv.1410.4381", "date": "2014-10-16", "title": "ALICE: An Advanced Logic for Interactive Component Engineering", "authors": "Borislav Gajanovic, Bernhard Rumpe", "abstract": "This paper presents an overview of the verification framework ALICE in its\ncurrent version 0.7. It is based on the generic theorem prover Isabelle\n[Pau03a]. Within ALICE a software or hardware component is specified as a\nstate-full black-box with directed communication channels. Components send and\nreceive asynchronous messages via these channels. The behavior of a component\nis generally described as a relation on the observations in form of streams of\nmessages flowing over its input and output channels. Untimed and timed as well\nas state-based, recursive, relational, equational, assumption/guarantee, and\nfunctional styles of specification are supported. Hence, ALICE is well suited\nfor the formalization and verification of distributed systems modeled with this\nstream-processing paradigm.", "journal": ""}
{"doi": "10.48550/arXiv.2311.12447", "date": "2023-11-21", "title": "Designing Long-term Group Fair Policies in Dynamical Systems", "authors": "Miriam Rateike, Isabel Valera, Patrick Forr\u00e9", "abstract": "Neglecting the effect that decisions have on individuals (and thus, on the\nunderlying data distribution) when designing algorithmic decision-making\npolicies may increase inequalities and unfairness in the long term - even if\nfairness considerations were taken in the policy design process. In this paper,\nwe propose a novel framework for achieving long-term group fairness in\ndynamical systems, in which current decisions may affect an individual's\nfeatures in the next step, and thus, future decisions. Specifically, our\nframework allows us to identify a time-independent policy that converges, if\ndeployed, to the targeted fair stationary state of the system in the long term,\nindependently of the initial data distribution. We model the system dynamics\nwith a time-homogeneous Markov chain and optimize the policy leveraging the\nMarkov chain convergence theorem to ensure unique convergence. We provide\nexamples of different targeted fair states of the system, encompassing a range\nof long-term goals for society and policymakers. Furthermore, we show how our\napproach facilitates the evaluation of different long-term targets by examining\ntheir impact on the group-conditional population distribution in the long term\nand how it evolves until convergence.", "journal": ""}
{"doi": "10.48550/arXiv.2410.00676", "date": "2024-10-01", "title": "User-Guided Verification of Security Protocols via Sound Animation", "authors": "Kangfeng Ye, Roberto Metere, Poonam Yadav", "abstract": "Current formal verification of security protocols relies on specialized\nresearchers and complex tools, inaccessible to protocol designers who\ninformally evaluate their work with emulators. This paper addresses this gap by\nembedding symbolic analysis into the design process. Our approach implements\nthe Dolev-Yao attack model using a variant of CSP based on Interaction Trees\n(ITrees) to compile protocols into animators -- executable programs that\ndesigners can use for debugging and inspection. To guarantee the soundness of\nour compilation, we mechanised our approach in the theorem prover Isabelle/HOL.\nAs traditionally done with symbolic tools, we refer to the Diffie-Hellman key\nexchange and the Needham-Schroeder public-key protocol (and Lowe's patched\nvariant). We demonstrate how our animator can easily reveal the mechanics of\nattacks and verify corrections. This work facilitates security integration at\nthe design level and supports further security property analysis and\nsoftware-engineered integrations.", "journal": ""}
{"doi": "10.48550/arXiv.2501.17444", "date": "2025-01-29", "title": "Formally Verifying a Transformation from MLTL Formulas to Regular Expressions", "authors": "Zili Wang, Katherine Kosaian, Kristin Yvonne Rozier", "abstract": "Mission-time Linear Temporal Logic (MLTL), a widely used subset of popular\nspecification logics like STL and MTL, is often used to model and verify real\nworld systems in safety-critical contexts. As the results of formal\nverification are only as trustworthy as their input specifications, the WEST\ntool was created to facilitate writing MLTL specifications. Accordingly, it is\nvital to demonstrate that WEST itself works correctly. To that end, we verify\nthe WEST algorithm, which converts MLTL formulas to (logically equivalent)\nregular expressions, in the theorem prover Isabelle/HOL. Our top-level result\nestablishes the correctness of the regular expression transformation; we then\ngenerate a code export from our verified development and use this to\nexperimentally validate the existing WEST tool. To facilitate this, we develop\nsome verified support for checking the equivalence of two regular expressions.", "journal": ""}
{"doi": "10.48550/arXiv.2305.07526", "date": "2023-05-12", "title": "A note on composition operators on model spaces", "authors": "Isabelle Chalendar, Pavel Gumenyuk, John E. McCarthy", "abstract": "Motivated by the study of composition operators on model spaces launched by\nMashreghi and Shabankha we consider the following problem: for a given inner\nfunction $\\phi\\not\\in\\mathsf{Aut}(\\mathbb D)$, find a non-constant inner\nfunction $\\Psi$ satisfying the functional equation $\\Psi\\circ\\phi=\\tau\\Psi$,\nwhere $\\tau$ is a unimodular constant. We prove that this problem has a\nsolution if and only if $\\phi$ is of positive hyperbolic step. More precisely,\nif this condition holds, we show that there is an infinite Blaschke product $B$\nsatisfying the equation for $\\tau=1$. If in addition, $\\phi$ is parabolic, we\nprove that the problem has a solution $\\Psi$ for $any$ unimodular $\\tau$.\nFinally, we show that if $\\phi$ is of zero hyperbolic step, then no\nnon-constant Bloch function $f$ and no unimodular constant $\\tau$ satisfy\n$f\\circ\\phi=\\tau f$.", "journal": ""}
{"doi": "10.48550/arXiv.2001.06296", "date": "2020-01-15", "title": "Overly Optimistic Prediction Results on Imbalanced Data: a Case Study of Flaws and Benefits when Applying Over-sampling", "authors": "Gilles Vandewiele, Isabelle Dehaene, Gy\u00f6rgy Kov\u00e1cs, Lucas Sterckx, Olivier Janssens, Femke Ongenae, Femke De Backere, Filip De Turck, Kristien Roelens, Johan Decruyenaere, Sofie Van Hoecke, Thomas Demeester", "abstract": "Information extracted from electrohysterography recordings could potentially\nprove to be an interesting additional source of information to estimate the\nrisk on preterm birth. Recently, a large number of studies have reported\nnear-perfect results to distinguish between recordings of patients that will\ndeliver term or preterm using a public resource, called the Term/Preterm\nElectrohysterogram database. However, we argue that these results are overly\noptimistic due to a methodological flaw being made. In this work, we focus on\none specific type of methodological flaw: applying over-sampling before\npartitioning the data into mutually exclusive training and testing sets. We\nshow how this causes the results to be biased using two artificial datasets and\nreproduce results of studies in which this flaw was identified. Moreover, we\nevaluate the actual impact of over-sampling on predictive performance, when\napplied prior to data partitioning, using the same methodologies of related\nstudies, to provide a realistic view of these methodologies' generalization\ncapabilities. We make our research reproducible by providing all the code under\nan open license.", "journal": "Artificial Intelligence in Medicine. 111 (2021). 101987"}
{"doi": "10.48550/arXiv.2306.03201", "date": "2023-06-05", "title": "Analysis of Integral Field Spectroscopy observations of the planetary nebula Hen 2-108 and its central star", "authors": "B\u00e1rbara L. Miranda Marques, Hektor Monteiro, Isabel Aleman, Stavros Akras, Helge Todt, Romano L. M. Corradi", "abstract": "The study of planetary nebulae provides important constraints for many\naspects of stellar and Galactic evolution. Hen 2-108 is a poorly known\nplanetary nebula with a slight elliptical morphology and a peculiar central\nstar (CS), which has defied classification. In this work, we present the first\ndetailed integral field spectroscopic study of the planetary nebula Hen 2-108\nand its CS. We provide spatially resolved flux maps for important emission\nlines, as well as diagnostic maps of extinction and electronic density and\ntemperature. Physical conditions and chemical abundances were also calculated\nfrom the integrated spectrum. The analysis was also performed with the code\nsatellite which uses a distinct strategy to evaluate physical and chemical\nproperties. Both satellite and traditional procedure give consistent results,\nshowing some variation in physical and chemical properties. We detect and\nmeasure a number of faint heavy element recombination lines from which we find\na significant abundance discrepancy factor for O/H, and possibly for N/H.\nPseudo 3D photoionization models were used to assist in the interpretation with\nresults supporting the low-ionisation nature of this nebula, indicating a CS\nwith Teff = 40 kK and a shell structure. The spectrum of the CS has been\nanalysed with a detailed model for expanding atmospheres to infer stellar\nparameters, finding that it is a [Of/WN8] type with T* = 41.5 kK, making it a\nnew addition to a small set (~20) of rare objects.", "journal": ""}
{"doi": "10.48550/arXiv.2211.08459", "date": "2022-11-15", "title": "CommCSL: Proving Information Flow Security for Concurrent Programs using Abstract Commutativity", "authors": "Marco Eilers, Thibault Dardinier, Peter M\u00fcller", "abstract": "Information flow security ensures that the secret data manipulated by a\nprogram does not influence its observable output. Proving information flow\nsecurity is especially challenging for concurrent programs, where operations on\nsecret data may influence the execution time of a thread and, thereby, the\ninterleaving between different threads. Such internal timing channels may\naffect the observable outcome of a program even if an attacker does not observe\nexecution times. Existing verification techniques for information flow security\nin concurrent programs attempt to prove that secret data does not influence the\nrelative timing of threads. However, these techniques are often restrictive\n(for instance because they disallow branching on secret data) and make strong\nassumptions about the execution platform (ignoring caching, processor\ninstructions with data-dependent runtime, and other common features that affect\nexecution time). In this paper, we present a novel verification technique for\nsecure information flow in concurrent programs that lifts these restrictions\nand does not make any assumptions about timing behavior. The key idea is to\nprove that all mutating operations performed on shared data commute, such that\ndifferent thread interleavings do not influence its final value. Crucially,\ncommutativity is required only for an abstraction of the shared data that\ncontains the information that will be leaked to a public output. Abstract\ncommutativity is satisfied by many more operations than standard commutativity,\nwhich makes our technique widely applicable. We formalize our technique in\nCommCSL, a relational concurrent separation logic with support for\ncommutativity-based reasoning, and prove its soundness in Isabelle/HOL. We\nimplemented CommCSL in HyperViper, an automated verifier based on the Viper\nverification infrastructure, and demonstrate its ability to verify challenging\nexamples.", "journal": ""}
{"doi": "10.48550/arXiv.2301.10311", "date": "2023-01-24", "title": "Relation-Algebraic Verification of Disjoint-Set Forests", "authors": "Walter Guttmann", "abstract": "This paper studies how to use relation algebras, which are useful for\nhigh-level specification and verification, for proving the correctness of\nlower-level array-based implementations of algorithms. We give a simple\nrelation-algebraic semantics of read and write operations on associative\narrays. The array operations seamlessly integrate with assignments in\ncomputation models supporting while-programs. As a result, relation algebras\ncan be used for verifying programs with associative arrays. We verify the\ncorrectness of an array-based implementation of disjoint-set forests using the\nunion-by-rank strategy and find operations with path compression, path\nsplitting and path halving. All results are formally proved in Isabelle/HOL.\nThis paper is an extended version of [1].", "journal": "Fundamenta Informaticae, Volume 192, Issue 1 (November 10, 2024)\n  fi:10856"}
{"doi": "10.48550/arXiv.2411.08913", "date": "2024-10-30", "title": "System Reliability Engineering in the Age of Industry 4.0: Challenges and Innovations", "authors": "Antoine Tordeux, Tim M. Julitz, Isabelle M\u00fcller, Zikai Zhang, Jannis Pietruschka, Nicola Fricke, Nadine Schl\u00fcter, Stefan Bracke, Manuel L\u00f6wer", "abstract": "In the era of Industry 4.0, system reliability engineering faces both\nchallenges and opportunities. On the one hand, the complexity of cyber-physical\nsystems, the integration of novel numerical technologies, and the handling of\nlarge amounts of data pose new difficulties for ensuring system reliability. On\nthe other hand, innovations such as AI-driven prognostics, digital twins, and\nIoT-enabled systems enable the implementation of new methodologies that are\ntransforming reliability engineering. Condition-based monitoring and predictive\nmaintenance are examples of key advancements, leveraging real-time sensor data\ncollection and AI to predict and prevent equipment failures. These approaches\nreduce failures and downtime, lower costs, and extend equipment lifespan and\nsustainability. However, it also brings challenges such as data management,\nintegrating complexity, and the need for fast and accurate models and\nalgorithms. Overall, the convergence of advanced technologies in Industry 4.0\nrequires a rethinking of reliability tasks, emphasising adaptability and\nreal-time data processing. In this chapter, we propose to review recent\ninnovations in the field, related methods and applications, as well as\nchallenges and barriers that remain to be explored. In the red lane, we focus\non smart manufacturing and automotive engineering applications with\nsensor-based monitoring and driver assistance systems.", "journal": ""}
{"doi": "10.48550/arXiv.2107.01815", "date": "2021-07-05", "title": "A Formal Semantics of the GraalVM Intermediate Representation", "authors": "Brae J. Webb, Mark Utting, Ian J. Hayes", "abstract": "The optimization phase of a compiler is responsible for transforming an\nintermediate representation (IR) of a program into a more efficient form.\nModern optimizers, such as that used in the GraalVM compiler, use an IR\nconsisting of a sophisticated graph data structure that combines data flow and\ncontrol flow into the one structure. As part of a wider project on the\nverification of optimization passes of GraalVM, this paper describes a\nsemantics for its IR within Isabelle/HOL. The semantics consists of a big-step\noperational semantics for data nodes (which are represented in a graph-based\nstatic single assignment (SSA) form) and a small-step operational semantics for\nhandling control flow including heap-based reads and writes, exceptions, and\nmethod calls. We have proved a suite of canonicalization optimizations and\nconditional elimination optimizations with respect to the semantics.", "journal": ""}
{"doi": "10.48550/arXiv.2405.06074", "date": "2024-05-09", "title": "Protocols to Code: Formal Verification of a Next-Generation Internet Router", "authors": "Jo\u00e3o C. Pereira, Tobias Klenze, Sofia Giampietro, Markus Limbeck, Dionysios Spiliopoulos, Felix A. Wolf, Marco Eilers, Christoph Sprenger, David Basin, Peter M\u00fcller, Adrian Perrig", "abstract": "We present the first formally-verified Internet router, which is part of the\nSCION Internet architecture. SCION routers run a cryptographic protocol for\nsecure packet forwarding in an adversarial environment. We verify both the\nprotocol's network-wide security properties and low-level properties of its\nimplementation. More precisely, we develop a series of protocol models by\nrefinement in Isabelle/HOL and we use an automated program verifier to prove\nthat the router's Go code satisfies memory safety, crash freedom, freedom from\ndata races, and adheres to the protocol model. Both verification efforts are\nsoundly linked together. Our work demonstrates the feasibility of coherently\nverifying a critical network component from high-level protocol models down to\nperformance-optimized production code, developed by an independent team. In the\nprocess, we uncovered critical bugs in both the protocol and its\nimplementation, which were confirmed by the code developers, and we\nstrengthened the protocol's security properties. This paper explains our\napproach, summarizes the main results, and distills lessons for the design and\nimplementation of verifiable systems, for the handling of continuous changes,\nand for the verification techniques and tools employed.", "journal": ""}
{"doi": "10.48550/arXiv.2212.01142", "date": "2022-12-02", "title": "Existence of minimizers for the Dirac-Fock model of crystals", "authors": "Isabelle Catto, Long Meng, Eric Paturel, Eric S\u00e9r\u00e9", "abstract": "Whereas many different models exist in the mathematical and physics\nliterature for ground states of non-relativistic crystals, the relativistic\ncase has been much less studied and we are not aware of any mathematical result\non a fully relativistic treatment of crystals. In this paper, we introduce a\nmean-field relativistic energy for crystals in terms of periodic density\nmatrices. This model is inspired both from a recent definition of the\nDirac-Fock ground state for atoms and molecules, due to one of us, and from the\nnon-relativistic Hartree-Fock model for crystals. We prove the existence of a\nground state when the number of electrons per cell is not too large.", "journal": "Archive for Rational Mechanics and Analysis 248 (2024), issue 4,\n  article 63"}
{"doi": "10.48550/arXiv.1005.4563", "date": "2010-05-25", "title": "Physically-based particle simulation and visualization of pastes and gels", "authors": "Claire Guilbaud, Annie Luciani, Nicolas Castagn\u00e9", "abstract": "This paper is focused on the question of simulation and visualiza- tion of 3D\ngel and paste dynamic effects. In a first part, we introduce a 3D physically\nbased particle (or mass-interaction) model, with a small number of masses and\nfew powerful interaction parameters, which is able to generate the dynamic\nfeatures of both gels and pastes. This model proves that the 3D\nmass-interaction method is relevant for the simulation of such phenomena,\nwithout an explicit knowledge of their underly- ing physics. In a second part,\nwe expose an original rendering process, the Flow Structuring Method that\nenhances the dynamic properties of the simulation and offers a realistic\nvisualization. This process ignores all the properties of the underlying\nphysical model. It leads to a reconstruction of the spatial structure of the\ngel (or paste) flow only through an analysis of the output of the simula- tion\nwhich is a set of unorganized points moving in a 3D space. Finally, the paper\npresents realistic renderings obtained by using implicit surfaces and\nray-tracing techniques on the Structured Flow previously obtained.", "journal": ""}
{"doi": "10.48550/arXiv.1407.5819", "date": "2014-07-22", "title": "Concurrent Dynamic Algebra", "authors": "Hitoshi Furusawa, Georg Struth", "abstract": "We reconstruct Peleg's concurrent dynamic logic in the context of modal\nKleene algebras. We explore the algebraic structure of its multirelational\nsemantics and develop an abstract axiomatisation of concurrent dynamic algebras\nfrom that basis. In this axiomatisation, sequential composition is not\nassociative. It interacts with concurrent composition through a weak\ndistributivity law. The modal operators of concurrent dynamic algebra are\nobtained from abstract axioms for domain and antidomain operators; the Kleene\nstar is modelled as a least fixpoint. Algebraic variants of Peleg's axioms are\nshown to be valid in these algebras and their soundness is proved relative to\nthe multirelational model. Additional results include iteration principles for\nthe Kleene star and a refutation of variants of Segerberg's axiom in the\nmultirelational setting. The most important results have been verified formally\nwith Isabelle/HOL.", "journal": ""}
{"doi": "10.48550/arXiv.1704.05320", "date": "2017-04-18", "title": "EPTL - A temporal logic for weakly consistent systems", "authors": "Mathias Weber, Annette Bieniusa, Arnd Poetzsch-Heffter", "abstract": "The high availability and scalability of weakly-consistent systems attracts\nsystem designers. Yet, writing correct application code for this type of\nsystems is difficult; even how to specify the intended behavior of such systems\nis still an open question. There has not been established any standard method\nto specify the intended dynamic behavior of a weakly consistent system. There\nexist specifications of various consistency models for distributed and\nconcurrent systems; and the semantics of replicated datatypes like CRDTs have\nbeen specified in axiomatic and operational models based on visibility\nrelations.\n  In this paper, we present a temporal logic, EPTL, that is tailored to specify\nproperties of weakly consistent systems. In contrast to LTL and CTL, EPTL takes\ninto account that operations of weakly consistent systems are in many cases not\nserializable and have to be treated respectively to capture the behavior. We\nembed our temporal logic in Isabelle/HOL and can thereby leverage strong\nsemi-automatic proving capabilities.", "journal": ""}
{"doi": "10.48550/arXiv.2406.07307", "date": "2024-06-11", "title": "The effective cone conjecture for Calabi--Yau pairs", "authors": "C\u00e9cile Gachet, Hsueh-Yung Lin, Isabel Stenger, Long Wang", "abstract": "We formulate an {\\it effective cone conjecture} for klt Calabi--Yau pairs\n$(X,\\Delta)$, pertaining to the structure of the cone of effective divisors\n$\\mathrm{Eff}(X)$ modulo the action of the subgroup of pseudo-automorphisms\n$\\mathrm{PsAut}(X,\\Delta)$. Assuming the existence of good minimal models in\ndimension $\\dim(X)$, known to hold in dimension up to $3$, we prove that the\neffective cone conjecture for $(X,\\Delta)$ is equivalent to the\nKawamata--Morrison--Totaro movable cone conjecture for $(X,\\Delta)$. As an\napplication, we show that the movable cone conjecture unconditionally holds for\nthe smooth Calabi--Yau threefolds introduced by Schoen and studied by Namikawa,\nGrassi and Morrison. We also show that for such a Calabi--Yau threefold $X$,\nall of its minimal models, apart from $X$ itself, have rational polyhedral nef\ncones.", "journal": ""}
{"doi": "10.48550/arXiv.2210.03151", "date": "2022-10-06", "title": "Integrative Imaging Informatics for Cancer Research: Workflow Automation for Neuro-oncology (I3CR-WANO)", "authors": "Satrajit Chakrabarty, Syed Amaan Abidi, Mina Mousa, Mahati Mokkarala, Isabelle Hren, Divya Yadav, Matthew Kelsey, Pamela LaMontagne, John Wood, Michael Adams, Yuzhuo Su, Sherry Thorpe, Caroline Chung, Aristeidis Sotiras, Daniel S. Marcus", "abstract": "Efforts to utilize growing volumes of clinical imaging data to generate tumor\nevaluations continue to require significant manual data wrangling owing to the\ndata heterogeneity. Here, we propose an artificial intelligence-based solution\nfor the aggregation and processing of multisequence neuro-oncology MRI data to\nextract quantitative tumor measurements. Our end-to-end framework i) classifies\nMRI sequences using an ensemble classifier, ii) preprocesses the data in a\nreproducible manner, iii) delineates tumor tissue subtypes using convolutional\nneural networks, and iv) extracts diverse radiomic features. Moreover, it is\nrobust to missing sequences and adopts an expert-in-the-loop approach, where\nthe segmentation results may be manually refined by radiologists. Following the\nimplementation of the framework in Docker containers, it was applied to two\nretrospective glioma datasets collected from the Washington University School\nof Medicine (WUSM; n = 384) and the M.D. Anderson Cancer Center (MDA; n = 30)\ncomprising preoperative MRI scans from patients with pathologically confirmed\ngliomas. The scan-type classifier yielded an accuracy of over 99%, correctly\nidentifying sequences from 380/384 and 30/30 sessions from the WUSM and MDA\ndatasets, respectively. Segmentation performance was quantified using the Dice\nSimilarity Coefficient between the predicted and expert-refined tumor masks.\nMean Dice scores were 0.882 ($\\pm$0.244) and 0.977 ($\\pm$0.04) for whole tumor\nsegmentation for WUSM and MDA, respectively. This streamlined framework\nautomatically curated, processed, and segmented raw MRI data of patients with\nvarying grades of gliomas, enabling the curation of large-scale neuro-oncology\ndatasets and demonstrating a high potential for integration as an assistive\ntool in clinical practice.", "journal": ""}
{"doi": "10.48550/arXiv.1201.3988", "date": "2012-01-19", "title": "Experimental study of the delayed threshold phenomenon in a semiconductor laser", "authors": "Abdelkrim El Amili, Gr\u00e9gory Gredat, Mehdi Alouini, Isabelle Sagnes, Fabien Bretenaker", "abstract": "An experimental study of the delayed threshold phenomenon in a Vertical\nExtended Cavity Semiconductor Emitting Laser is carried out. Under modulation\nof the pump power, the laser intensity exhibits a hysteresis behavior in the\nvicinity of the threshold. The temporal width of this hysteresis is measured as\na function of the modulation frequency, and is proved to follow the predicted\nscaling law. A model based on the rate equations is derived and used to analyze\nthe experimental observations. A frequency variation of the laser around the\ndelayed threshold and induced by the phase-amplitude coupling is predicted and\nestimated.", "journal": ""}
{"doi": "10.48550/arXiv.1603.05632", "date": "2016-03-17", "title": "A quasilinear bistable equation in cylinders and timelike heteroclinics in special relativity", "authors": "Denis Bonheure, Isabel Coelho, Manon Nys", "abstract": "In this note we consider the action functional \\[ \\int_{\\mathbb{R} \\times\n\\omega} \\left( 1 - \\sqrt{ 1 - |\\nabla u|^2 } + W(u) \\right) \\, \\mathrm{d}t, \\]\nwhere $W$ is a double well potential and $\\omega$ is a bounded domain of\n$\\mathbb{R}^{N-1}$. We prove existence, one-dimensionality and uniqueness (up\nto translation) of a smooth minimizing phase transition between the two stable\nstates $u=1$ and $u=-1$. The question of existence of at least one minimal\nheteroclinic connection for the non autonomous model \\[ \\int_{\\mathbb{R}}\n\\left( 1 - \\sqrt{1-|u'|^2} + a(t) W(u) \\right) \\, \\mathrm{d}t \\] is also\naddressed. For this, we look for the possible assumptions on $a(t)$ ensuring\nthe existence of a minimizer.", "journal": ""}
{"doi": "10.48550/arXiv.2001.04314", "date": "2020-01-13", "title": "Formal specification of a security framework for smart contracts", "authors": "Mikhail Mandrykin, Jake O'Shannessy, Jacob Payne, Ilya Shchepetkov", "abstract": "As smart contracts are growing in size and complexity, it becomes harder and\nharder to ensure their correctness and security. Due to the lack of isolation\nmechanisms a single mistake or vulnerability in the code can bring the whole\nsystem down, and due to this smart contract upgrades can be especially\ndangerous. Traditional ways to ensure the security of a smart contract,\nincluding DSLs, auditing and static analysis, are used before the code is\ndeployed to the blockchain, and thus offer no protection after the deployment.\nAfter each upgrade the whole code need to be verified again, which is a\ndifficult and time-consuming process that is prone to errors. To address these\nissues a security protocol and framework for smart contracts called Cap9 was\ndeveloped. It provides developers the ability to perform upgrades in a secure\nand robust manner, and improves isolation and transparency through the use of a\nlow level capability-based security model. We have used Isabelle/HOL to develop\na formal specification of the Cap9 framework and prove its consistency. The\npaper presents a refinement-based approach that we used to create the\nspecification, as well as discussion of some encountered difficulties during\nthis process.", "journal": ""}
{"doi": "10.48550/arXiv.2102.10923", "date": "2021-02-22", "title": "Approximation of dilation-based spatial relations to add structural constraints in neural networks", "authors": "Mateus Riva, Pietro Gori, Florian Yger, Roberto Cesar, Isabelle Bloch", "abstract": "Spatial relations between objects in an image have proved useful for\nstructural object recognition. Structural constraints can act as regularization\nin neural network training, improving generalization capability with small\ndatasets. Several relations can be modeled as a morphological dilation of a\nreference object with a structuring element representing the semantics of the\nrelation, from which the degree of satisfaction of the relation between another\nobject and the reference object can be derived. However, dilation is not\ndifferentiable, requiring an approximation to be used in the context of\ngradient-descent training of a network. We propose to approximate dilations\nusing convolutions based on a kernel equal to the structuring element. We show\nthat the proposed approximation, even if slightly less accurate than previous\napproximations, is definitely faster to compute and therefore more suitable for\ncomputationally intensive neural network applications.", "journal": ""}
{"doi": "10.48550/arXiv.2302.01625", "date": "2023-02-03", "title": "Stability of local tip pool sizes", "authors": "Sebastian M\u00fcller, Isabel Amigo, Alexandre Reiffers-Masson, Santiago Ruano-Rinc\u00f3n", "abstract": "In distributed ledger technologies (DLTs) with a directed acyclic graph (DAG)\ndata structure, a block-issuing node can decide where to append new blocks and,\nconsequently, how the DAG grows. This DAG data structure is typically\ndecomposed into two pools of blocks, dependent on whether another block already\nreferences them. The unreferenced blocks are called the tips. Due to network\ndelay, nodes can perceive the set of tips differently, giving rise to local tip\npools.\n  We present a new mathematical model to analyse the stability of the different\nlocal perceptions of the tip pools and allow heterogeneous and random network\ndelay in the underlying peer-to-peer communication layer. Under natural\nassumptions, we prove that the number of tips is ergodic, converges to a\nstationary distribution, and provide quantitative bounds on the tip pool sizes.\nWe conclude our study with agent-based simulations to illustrate the\nconvergence of the tip pool sizes and the pool sizes' dependence on the\ncommunication delay and degree of centralization.", "journal": ""}
{"doi": "10.48550/arXiv.2109.14956", "date": "2021-09-30", "title": "Comparative Validation of Machine Learning Algorithms for Surgical Workflow and Skill Analysis with the HeiChole Benchmark", "authors": "Martin Wagner, Beat-Peter M\u00fcller-Stich, Anna Kisilenko, Duc Tran, Patrick Heger, Lars M\u00fcndermann, David M Lubotsky, Benjamin M\u00fcller, Tornike Davitashvili, Manuela Capek, Annika Reinke, Tong Yu, Armine Vardazaryan, Chinedu Innocent Nwoye, Nicolas Padoy, Xinyang Liu, Eung-Joo Lee, Constantin Disch, Hans Meine, Tong Xia, Fucang Jia, Satoshi Kondo, Wolfgang Reiter, Yueming Jin, Yonghao Long, Meirui Jiang, Qi Dou, Pheng Ann Heng, Isabell Twick, Kadir Kirtac, Enes Hosgor, Jon Lindstr\u00f6m Bolmgren, Michael Stenzel, Bj\u00f6rn von Siemens, Hannes G. Kenngott, Felix Nickel, Moritz von Frankenberg, Franziska Mathis-Ullrich, Lena Maier-Hein, Stefanie Speidel, Sebastian Bodenstedt", "abstract": "PURPOSE: Surgical workflow and skill analysis are key technologies for the\nnext generation of cognitive surgical assistance systems. These systems could\nincrease the safety of the operation through context-sensitive warnings and\nsemi-autonomous robotic assistance or improve training of surgeons via\ndata-driven feedback. In surgical workflow analysis up to 91% average precision\nhas been reported for phase recognition on an open data single-center dataset.\nIn this work we investigated the generalizability of phase recognition\nalgorithms in a multi-center setting including more difficult recognition tasks\nsuch as surgical action and surgical skill. METHODS: To achieve this goal, a\ndataset with 33 laparoscopic cholecystectomy videos from three surgical centers\nwith a total operation time of 22 hours was created. Labels included annotation\nof seven surgical phases with 250 phase transitions, 5514 occurences of four\nsurgical actions, 6980 occurences of 21 surgical instruments from seven\ninstrument categories and 495 skill classifications in five skill dimensions.\nThe dataset was used in the 2019 Endoscopic Vision challenge, sub-challenge for\nsurgical workflow and skill analysis. Here, 12 teams submitted their machine\nlearning algorithms for recognition of phase, action, instrument and/or skill\nassessment. RESULTS: F1-scores were achieved for phase recognition between\n23.9% and 67.7% (n=9 teams), for instrument presence detection between 38.5%\nand 63.8% (n=8 teams), but for action recognition only between 21.8% and 23.3%\n(n=5 teams). The average absolute error for skill assessment was 0.78 (n=1\nteam). CONCLUSION: Surgical workflow and skill analysis are promising\ntechnologies to support the surgical team, but are not solved yet, as shown by\nour comparison of algorithms. This novel benchmark can be used for comparable\nevaluation and validation of future work.", "journal": ""}
{"doi": "10.48550/arXiv.1910.08992", "date": "2019-10-20", "title": "Checking Timed Bisimulation with Bounded Zone-History Graphs -- Technical Report", "authors": "Lars Luthmann, Hendrik G\u00f6ttmann, Isabelle Bacher, Malte Lochau", "abstract": "Timed automata (TA) are a well-established formalism for specifying\ndiscrete-state/continuous-time behavior of time-critical reactive systems.\nConcerning the fundamental analysis problem of comparing a candidate\nimplementation against a specification, both given as TA, it has been shown\nthat timed trace equivalence is undecidable, whereas timed bisimulation\nequivalence is decidable. The corresponding proof utilizes region graphs, a\nfinite, but generally very space-consuming characterization of TA semantics.\nHence, most practical TA tools utilize zone graphs instead, a symbolic and\ngenerally more efficient representation of TA semantics, to automate analysis\ntasks. However, zone graphs only produce sound results for analysis tasks being\nreducible to plain reachability problems thus being too imprecise for checking\ntimed bisimilarity. In this paper, we propose bounded zone-history graphs, a\nnovel characterization of TA semantics facilitating an adjustable trade-off\nbetween precision and scalability of timed-bisimilarity checking. Our tool\nTimBrCheck is, to the best of our knowledge, the only currently available tool\nfor effectively checking timed bisimilarity and even supports non-deterministic\nTA with silent moves. We further present experimental results gained from\napplying our tool to a collection of community benchmarks, providing insights\ninto trade-offs between precision and efficiency, depending on the bound value.", "journal": ""}
{"doi": "10.48550/arXiv.0609104", "date": "2006-09-18", "title": "On Verifying Complex Properties using Symbolic Shape Analysis", "authors": "Thomas Wies, Viktor Kuncak, Karen Zee, Andreas Podelski, Martin Rinard", "abstract": "One of the main challenges in the verification of software systems is the\nanalysis of unbounded data structures with dynamic memory allocation, such as\nlinked data structures and arrays. We describe Bohne, a new analysis for\nverifying data structures. Bohne verifies data structure operations and shows\nthat 1) the operations preserve data structure invariants and 2) the operations\nsatisfy their specifications expressed in terms of changes to the set of\nobjects stored in the data structure. During the analysis, Bohne infers loop\ninvariants in the form of disjunctions of universally quantified Boolean\ncombinations of formulas. To synthesize loop invariants of this form, Bohne\nuses a combination of decision procedures for Monadic Second-Order Logic over\ntrees, SMT-LIB decision procedures (currently CVC Lite), and an automated\nreasoner within the Isabelle interactive theorem prover. This architecture\nshows that synthesized loop invariants can serve as a useful communication\nmechanism between different decision procedures. Using Bohne, we have verified\noperations on data structures such as linked lists with iterators and back\npointers, trees with and without parent pointers, two-level skip lists, array\ndata structures, and sorted lists. We have deployed Bohne in the Hob and Jahob\ndata structure analysis systems, enabling us to combine Bohne with analyses of\ndata structure clients and apply it in the context of larger programs. This\nreport describes the Bohne algorithm as well as techniques that Bohne uses to\nreduce the ammount of annotations and the running time of the analysis.", "journal": ""}
{"doi": "10.48550/arXiv.2010.14032", "date": "2020-10-27", "title": "Verified Secure Compilation for Mixed-Sensitivity Concurrent Programs", "authors": "Robert Sison, Toby Murray", "abstract": "Proving only over source code that programs do not leak sensitive data leaves\na gap between reasoning and reality that can only be filled by accounting for\nthe behaviour of the compiler. Furthermore, software does not always have the\nluxury of limiting itself to single-threaded computation with resources\nstatically dedicated to each user to ensure the confidentiality of their data.\nThis results in mixed-sensitivity concurrent programs, which might reuse memory\nshared between their threads to hold data of different sensitivity levels at\ndifferent times; for such programs, a compiler must preserve the\nvalue-dependent coordination of such mixed-sensitivity reuse despite the impact\nof concurrency.\n  Here we demonstrate, using Isabelle/HOL, that it is feasible to verify that a\ncompiler preserves noninterference, the strictest kind of confidentiality\nproperty, for mixed-sensitivity concurrent programs. First, we present notions\nof refinement that preserve a concurrent value-dependent notion of\nnoninterference that we have designed to support such programs. As proving\nnoninterference-preserving refinement can be considerably more complex than the\nstandard refinements typically used to verify semantics-preserving compilation,\nour notions include a decomposition principle that separates the\nsemantics-preservation from security-preservation concerns. Second, we\ndemonstrate that these refinement notions are applicable to verified secure\ncompilation, by exercising them on a single-pass compiler for mixed-sensitivity\nconcurrent programs that synchronise using mutex locks, from a generic\nimperative language to a generic RISC-style assembly language. Finally, we\nexecute our compiler on a nontrivial mixed-sensitivity concurrent program\nmodelling a real-world use case, thus preserving its source-level\nnoninterference properties down to an assembly-level model automatically.\n  (Full abstract in paper)", "journal": "J. Funct. Prog. 31 (2021) e18"}
{"doi": "10.48550/arXiv.2006.13859", "date": "2020-06-11", "title": "Multi-scale modelling of concrete structures affected by alkali-silica reaction: Coupling the mesoscopic damage evolution and the macroscopic concrete deterioration", "authors": "Emil R. Gallyamov, Aurelia Isabel Cuba Ramos, Mauro Corrado, Roozbeh Rezakhani, Jean-Francois Molinari", "abstract": "A finite-element approach based on the first-order FE 2 homogenisation\ntechnique is formulated to analyse the alkali-silica reaction-induced damage in\nconcrete structures, by linking the concrete degradation at the macro-scale to\nthe reaction extent at the meso-scale. At the meso-scale level, concrete is\nconsidered as a heterogeneous material consisting of aggregates embedded in a\nmortar matrix. The mechanical effects of the Alkali-Silica Reaction (ASR) are\nmodelled through the application of temperature-dependent eigenstrains in\nseveral localised spots inside the aggregates and the mechanical degradation of\nconcrete is modelled using continuous damage model, which is capable of\nreproducing the complex ASR crack networks. Then, the effective stiffness\ntensor and the effective stress tensor for each macroscopic finite element are\ncomputed by homogenising the mechanical response of the corresponding\nrepresentative volume element (RVE), thus avoiding the use of phenomenological\nconstitutive laws at the macro-scale. Convergence between macro- and\nmeso-scales is achieved via an iterative procedure. A 2D model of an ASR\nlaboratory specimen is analysed as a proof of concept. The model is able to\naccount for the loading applied at the macro-scale and the ASR-product\nexpansion at the meso-scale. The results demonstrate that the macroscopic\nstress state influences the orientation of damage inside the underlying RVEs.\nThe effective stiffness becomes anisotropic in cases where damage is aligned\ninside the RVE.", "journal": "International Journal of Solids and Structures 207 (2020) 262-278"}
{"doi": "10.48550/arXiv.1405.1386", "date": "2014-05-06", "title": "Homogenization Model for Aberrant Crypt Foci", "authors": "Isabel N. Figueiredo, Carlos Leal, Giuseppe Romanazzi, Bjorn Engquist", "abstract": "Several explanations can be found in the literature about the origin of\ncolorectal cancer. There is however some agreement on the fact that the\ncarcinogenic process is a result of several genetic mutations of normal cells.\nThe colon epithelium is characterized by millions of invaginations, very small\ncavities, called crypts, where most of the cellular activity occurs. It is\nconsensual in the medical community, that a potential first manifestation of\nthe carcinogenic process, observed in conventional colonoscopy images, is the\nappearance of Aberrant Crypt Foci (ACF). These are clusters of abnormal crypts,\nmorphologically characterized by an atypical behavior of the cells that\npopulate the crypts. In this work an homogenization model is proposed, for\nrepresenting the cellular dynamics in the colon epithelium. The goal is to\nsimulate and predict, in silico, the spread and evolution of ACF, as it can be\nobserved in colonoscopy images. By assuming that the colon is an heterogeneous\nmedia, exhibiting a periodic distribution of crypts, we start this work by\ndescribing a periodic model, that represents the ACF cell-dynamics in a\ntwo-dimensional setting. Then, homogenization techniques are applied to this\nperiodic model, to find a simpler model, whose solution symbolizes the averaged\nbehavior of ACF at the tissue level. Some theoretical results concerning the\nexistence of solution of the homogenized model are proven, applying a fixed\npoint theorem. Numerical results showing the convergence of the periodic model\nto the homogenized model are presented.", "journal": ""}
{"doi": "10.48550/arXiv.2412.12309", "date": "2024-12-16", "title": "Data-Driven Modeling for On-Demand Flow Prescription in Fan-Array Wind Tunnels", "authors": "Alejandro A. Stefan-Zavala, Isabel Scherl, Ioannis Mandralis, Steven L. Brunton, Morteza Gharib", "abstract": "Fan-array wind tunnels are an emerging technology to design bespoke wind\nfields through grids of individually controllable fans. This design is\nespecially suited for the turbulent, dynamic, non-uniform flow conditions found\nclose to the ground, and has enabled applications from entomology to flight on\nMars. However, due to the high dimensionality of fan-array actuation and the\ncomplexity of unsteady fluid flow, the physics of fan arrays are not fully\ncharacterized, making it difficult to prescribe arbitrary flow fields.\nAccessing the full capability of fan arrays requires resolving the map from\ntime-varying grids of fan speeds to three-dimensional unsteady flow fields,\nwhich remains an open problem. This map is unfeasible to span in a single\nstudy, but it can be partitioned and studied in subsets. In this paper, we\nstudy the special case of constant fan-speeds and time-averaged streamwise\nvelocities with one homogeneous spanwise axis. We produce a proof-of-concept\nsurrogate model by fitting a regularized linear map to a dataset of fan-array\nmeasurements. We use this model as the basis for an open-loop control scheme to\ndesign flow profiles subject to constraints on fan speeds. In experimental\nvalidation, our model scored a mean prediction error of 1.02 m/s and our\ncontrol scheme a mean tracking error of 1.05 m/s in a fan array with velocities\nup to 12 m/s. We empirically conclude that the physics relating constant fan\nspeeds to time-averaged streamwise velocities are dominated by linear dynamics,\nand present our method as a foundational step to fully resolve fan-array wind\ntunnel control.", "journal": ""}
{"doi": "10.48550/arXiv.1511.04170", "date": "2015-11-13", "title": "Controlled Owicki-Gries Concurrency: Reasoning about the Preemptible eChronos Embedded Operating System", "authors": "June Andronick, Corey Lewis, Carroll Morgan", "abstract": "We introduce a controlled concurrency framework, derived from the\nOwicki-Gries method, for describing a hardware interface in detail sufficient\nto support the modelling and verification of small, embedded operating systems\n(OS's) whose run-time responsiveness is paramount. Such real-time systems run\nwith interrupts mostly enabled, including during scheduling. That differs from\nmany other successfully modelled and verified OS's that typically reduce the\ncomplexity of concurrency by running on uniprocessor platforms and by switching\ninterrupts off as much as possible. Our framework builds on the traditional\nOwicki-Gries method, for its fine-grained concurrency is needed for\nhigh-performance system code. We adapt it to support explicit concurrency\ncontrol, by providing a simple, faithful representation of the hardware\ninterface that allows software to control the degree of interleaving between\nuser code, OS code, interrupt handlers and a scheduler that controls context\nswitching. We then apply this framework to model the interleaving behavior of\nthe eChronos OS, a preemptible real-time OS for embedded micro-controllers. We\ndiscuss the accuracy and usability of our approach when instantiated to model\nthe eChronos OS. Both our framework and the eChronos model are formalised in\nthe Isabelle/HOL theorem prover, taking advantage of the high level of\nautomation in modern reasoning tools.", "journal": "EPTCS 196, 2015, pp. 10-24"}
{"doi": "10.48550/arXiv.2208.14424", "date": "2022-08-30", "title": "Inevitability of knowing less than nothing", "authors": "Gilad Gour, Mark M. Wilde, Sarah Brandsen, Isabelle Jianing Geng", "abstract": "A colloquial interpretation of entropy is that it is the knowledge gained\nupon learning the outcome of a random experiment. Conditional entropy is then\ninterpreted as the knowledge gained upon learning the outcome of one random\nexperiment after learning the outcome of another, possibly statistically\ndependent, random experiment. In the classical world, entropy and conditional\nentropy take only non-negative values, consistent with the intuition that one\nhas regarding the aforementioned interpretations. However, for certain\nentangled states, one obtains negative values when evaluating commonly accepted\nand information-theoretically justified formulas for the quantum conditional\nentropy, leading to the confounding conclusion that one can know less than\nnothing in the quantum world. Here, we introduce a physically motivated\nframework for defining quantum conditional entropy, based on two simple\npostulates inspired by the second law of thermodynamics (non-decrease of\nentropy) and extensivity of entropy, and we argue that all plausible\ndefinitions of quantum conditional entropy should respect these two postulates.\nWe then prove that all plausible quantum conditional entropies take on negative\nvalues for certain entangled states, so that it is inevitable that one can know\nless than nothing in the quantum world. All of our arguments are based on\nconstructions of physical processes that respect the first postulate, the one\ninspired by the second law of thermodynamics.", "journal": "Quantum 8, 1529 (2024)"}
{"doi": "10.48550/arXiv.2112.11463", "date": "2021-12-21", "title": "New Variable Hot Subdwarf Stars Identified from Anomalous Gaia Flux Errors, Observed by TESS, and Classified via Fourier Diagnostics", "authors": "Brad N. Barlow, Kyle A. Corcoran, Isabelle M. Parker, Thomas Kupfer, P\u00e9ter N\u00e9meth, J. J. Hermes, Isaac D. Lopez, Will J. Frondorf, David Vestal, Jazzmyn Holden", "abstract": "Hot subdwarf stars are mostly stripped red giants that can exhibit\nphotometric variations due to stellar pulsations, eclipses, the reflection\neffect, ellipsoidal modulation, and Doppler beaming. Detailed studies of their\nlight curves help constrain stellar parameters through asteroseismological\nanalyses or binary light curve modeling and generally improve our capacity to\ndraw a statistically meaningful picture of this enigmatic stage of stellar\nevolution. From an analysis of Gaia DR2 flux errors, we have identified around\n1200 candidate hot subdwarfs with inflated flux errors for their magnitudes - a\nstrong indicator of photometric variability. As a pilot study, we obtained\n2-min cadence TESS Cycle 2 observations of 187 candidate hot subdwarfs with\nanomalous Gaia flux errors. More than 90% of our targets show significant\nphotometric variations in their TESS light curves. Many of the new systems\nfound are cataclysmic variables, but we report the discovery of several new\nvariable hot subdwarfs, including HW Vir binaries, reflection effect systems,\npulsating sdBVs stars, and ellipsoidally modulated systems. We determine\natmospheric parameters for select systems using follow-up spectroscopy from the\n3-m Shane telescope. Finally, we present a Fourier diagnostic plot for\nclassifying binary light curves using the relative amplitudes and phases of\ntheir fundamental and harmonic signals in their periodograms. This plot makes\nit possible to identify certain types of variables efficiently, without\ndirectly investigating their light curves, and may assist in the rapid\nclassification of systems observed in large photometric surveys.", "journal": ""}
{"doi": "10.48550/arXiv.1010.0518", "date": "2010-10-04", "title": "Influence of nuclear de-excitation on observables relevant for space exploration", "authors": "Davide Mancusi, Alain Boudard, Joseph Cugnon, Jean-Christophe David, Sylvie Leray", "abstract": "The composition of the space radiation environment inside spacecrafts is\nmodified by the interaction with shielding material, with equipment and even\nwith the astronauts' bodies. Accurate quantitative estimates of the effects of\nnuclear reactions are necessary, for example, for dose estimation and\nprediction of single-event-upset rates. To this end, it is necessary to\nconstruct predictive models for nuclear reactions, which usually consist of an\nintranuclear-cascade or quantum-molecular-dynamics stage, followed by a\nnuclear-de-excitation stage.\n  While it is generally acknowledged that it is necessary to accurately\nsimulate the first reaction stage, transport-code users often neglect or\nunderestimate the importance of the choice of the de-excitation code. The\npurpose of this work is to prove that the de-excitation model is in fact a\nnon-negligible source of uncertainty for the prediction of several observables\nof crucial importance for space applications. For some particular observables,\nthe systematic uncertainty due to the de-excitation model actually dominates\nthe total uncertainty. Our point will be illustrated by making use of\nnucleon-nucleus calculations performed with several\nintranuclear-cascade/de-excitation models, such as the Li\\`{e}ge Intranuclear\nCascade model (INCL) and Isabel (for the cascade part) and ABLA07, Dresner,\nGEM, GEMINI++ and SMM (on the de-excitation side).", "journal": "Adv.Space Res.47:1194-1199,2011"}
{"doi": "10.48550/arXiv.2312.09319", "date": "2023-12-14", "title": "A bubble VEM-fully discrete polytopal scheme for mixed-dimensional poromechanics with frictional contact at matrix fracture interfaces", "authors": "J\u00e9r\u00f4me Droniou, Guillaume Ench\u00e9ry, Isabelle Faille, Ali Haidar, Roland Masson", "abstract": "The objective of this article is to address the discretisation of\nfractured/faulted poromechanical models using 3D polyhedral meshes in order to\ncope with the geometrical complexity of faulted geological models.\n  A polytopal scheme is proposed for contact-mechanics, based on a mixed\nformulation combining a fully discrete space and suitable reconstruction\noperators for the displacement field with a face-wise constant approximation of\nthe Lagrange multiplier accounting for the surface tractions along the\nfracture/fault network. To ensure the inf--sup stability of the mixed\nformulation, a bubble-like degree of freedom is included in the discrete space\nof displacements (and taken into account in the reconstruction operators). It\nis proved that this fully discrete scheme for the displacement is equivalent to\na low-order Virtual Element scheme, with a bubble enrichment of the VEM space.\nThis $\\mathbb{P}^1$-bubble VEM--$\\mathbb{P}^0$ mixed discretization is combined\nwith an Hybrid Finite Volume scheme for the Darcy flow. All together, the\nproposed approach is adapted to complex geometry accounting for network of\nplanar faults/fractures including corners, tips and intersections; it leads to\nefficient semi-smooth Newton solvers for the contact-mechanics and preserve the\ndissipative properties of the fully coupled model. Our approach is investigated\nin terms of convergence and robustness on several 2D and 3D test cases using\neither analytical or numerical reference solutions both for the stand alone\nstatic contact mechanical model and the fully coupled poromechanical model.", "journal": "Comput. Methods Appl. Mech. Engrg. 422, Paper no. 116838, 25p,\n  2024"}
{"doi": "10.48550/arXiv.2312.03356", "date": "2023-12-06", "title": "Bile Duct Segmentation Methods Under 3D Slicer Applied to ERCP: Advantages and Disadvantages", "authors": "Abdelhadi Essamlali, Vincent Millot-Maysounabe, Marion Chartier, Gr\u00e9goire Salin, Aymeric Becq, Lionel Arriv\u00e9, Marine Duboc Camus, J\u00e9r\u00f4me Szewczyk, Isabelle Claude", "abstract": "This article presents an evaluation of biliary tract segmentation methods\nused for 3D reconstruction, which may be very usefull in various critical\ninterventions, such as endoscopic retrograde cholangiopancreatography (ERCP),\nusing the 3D Slicer software. This article provides an assessment of biliary\ntract segmentation techniques employed for 3D reconstruction, which can prove\nhighly valuable in diverse critical procedures like endoscopic retrograde\ncholangiopancreatography (ERCP) through the utilization of 3D Slicer software.\nThree different methods, namely thresholding, flood filling, and region\ngrowing, were assessed in terms of their advantages and disadvantages. The\nstudy involved 10 patient cases and employed quantitative indices and\nqualitative evaluation to assess the segmentations obtained by the different\nsegmentation methods against ground truth. The results indicate that the\nthresholding method is almost manual and time-consuming, while the flood\nfilling method is semi-automatic and also time-consuming. Although both methods\nimprove segmentation quality, they are not reproducible. Therefore, an\nautomatic method based on region growing was developed to reduce segmentation\ntime, albeit at the expense of quality. These findings highlight the pros and\ncons of different conventional segmentation methods and underscore the need to\nexplore alternative approaches, such as deep learning, to optimize biliary\ntract segmentation in the context of ERCP.", "journal": "International Journal of Biomedical Engineering and Clinical\n  Science, 2023, 9 (4), pp.66-74"}
{"doi": "10.48550/arXiv.2303.00489", "date": "2023-03-01", "title": "The miniJPAS survey quasar selection II: Machine learning classification with photometric measurements and uncertainties", "authors": "Nat\u00e1lia V. N. Rodrigues, L. Raul Abramo, Carolina Queiroz, Gin\u00e9s Mart\u00ednez-Solaeche, Ignasi P\u00e9rez-R\u00e0fols, Silvia Bonoli, Jon\u00e1s Chaves-Montero, Matthew M. Pieri, Rosa M. Gonz\u00e1lez Delgado, Sean S. Morrison, Valerio Marra, Isabel M\u00e1rquez, A. Hern\u00e1n-Caballero, L. A. D\u00edaz-Garc\u00eda, Narciso Ben\u00edtez, A. Javier Cenarro, Renato A. Dupke, Alessandro Ederoclite, Carlos L\u00f3pez-Sanjuan, Antonio Mar\u00edn-Franch, Claudia Mendes de Oliveira, Mariano Moles, Laerte Sodr\u00e9 Jr., Jes\u00fas Varela, H\u00e9ctor V\u00e1zquez Rami\u00f3, Keith Taylor", "abstract": "Astrophysical surveys rely heavily on the classification of sources as stars,\ngalaxies or quasars from multi-band photometry. Surveys in narrow-band filters\nallow for greater discriminatory power, but the variety of different types and\nredshifts of the objects present a challenge to standard template-based\nmethods. In this work, which is part of larger effort that aims at building a\ncatalogue of quasars from the miniJPAS survey, we present a Machine\nLearning-based method that employs Convolutional Neural Networks (CNNs) to\nclassify point-like sources including the information in the measurement\nerrors. We validate our methods using data from the miniJPAS survey, a\nproof-of-concept project of the J-PAS collaboration covering $\\sim$ 1 deg$^2$\nof the northern sky using the 56 narrow-band filters of the J-PAS survey. Due\nto the scarcity of real data, we trained our algorithms using mocks that were\npurpose-built to reproduce the distributions of different types of objects that\nwe expect to find in the miniJPAS survey, as well as the properties of the real\nobservations in terms of signal and noise. We compare the performance of the\nCNNs with other well-established Machine Learning classification methods based\non decision trees, finding that the CNNs improve the classification when the\nmeasurement errors are provided as inputs. The predicted distribution of\nobjects in miniJPAS is consistent with the putative luminosity functions of\nstars, quasars and unresolved galaxies. Our results are a proof-of-concept for\nthe idea that the J-PAS survey will be able to detect unprecedented numbers of\nquasars with high confidence.", "journal": "Monthly Notices of the Royal Astronomical Society, 2023, 520,\n  3494-3509"}
{"doi": "10.48550/arXiv.1411.3039", "date": "2014-11-12", "title": "Homogenization of the Stefan problem, with application to maple sap exudation", "authors": "Isabell Graf, John M. Stockie", "abstract": "The technique of periodic homogenization with two-scale convergence is\napplied to the analysis of a two-phase Stefan-type problem that arises in the\nstudy of a periodic array of melting ice bars. For this \"reduced model\" we\nprove results on existence, uniqueness and convergence of the two-scale limit\nsolution in the weak form, which requires solving a macroscale problem for the\nglobal temperature field and a reference cell problem at each point in space\nwhich captures the underlying phase change process occurring on the microscale.\nWe state a corresponding strong formulation of the limit problem and use it to\ndesign an efficient numerical solution algorithm. The same homogenized\ntemperature equations are then applied to solve a much more complicated problem\ninvolving multi-phase flow and heat transport in trees, where the sap is\npresent in both frozen and liquid forms and a third gas phase is also present.\nOur homogenization approach has the advantage that the global temperature field\nis a solution of the same reduced model equations, while all the remaining\nphysics are relegated to the reference cell problem. Numerical simulations are\nperformed to validate our results and draw conclusions regarding the phenomenon\nknown as sap exudation, which is of great importance in sugar maple trees and\nfew other related species.", "journal": ""}
{"doi": "10.48550/arXiv.2410.23492", "date": "2024-10-30", "title": "Fractional Voigt-regularization of the 3D Navier--Stokes and Euler equations: Global well-posedness and limiting behavior", "authors": "Zdzislaw Brze\u017aniak, Adam Larios, Isabel Safarik", "abstract": "The Voigt regularization is a technique used to model turbulent flows,\noffering advantages such as sharing steady states with the Navier-Stokes\nequations and requiring no modification of boundary conditions; however, the\nparabolic dissipative character of the equation is lost. In this work we\npropose and study a generalization of the Voigt regularization technique by\nintroducing a fractional power $r$ in the Helmholtz operator, which allows for\ndissipation in the system, at least in the viscous case. We examine the\nresulting fractional Navier-Stokes-Voigt (fNSV) and fractional Euler-Voigt\n(fEV) and show that global well-posedness holds in the 3D periodic case for\nfNSV when the fractional power $r \\geq \\frac{1}{2}$ and for fEV when\n$r>\\frac{5}{6}$. Moreover, we show that the solutions of these fractional\nVoigt-regularized systems converge to solutions of the original equations, on\nthe corresponding time interval of existence and uniqueness of the latter, as\nthe regularization parameter $\\alpha \\to 0$. Additionally, we prove convergence\nof solutions of fNSV to solutions of fEV as the viscosity $\\nu \\to 0$ as well\nas the convergence of solutions of fNSV to solutions of the 3D Euler equations\nas both $\\alpha, \\nu \\to 0$. Furthermore, we derive a criterion for finite-time\nblow-up for each system based on this regularization. These results may be of\nuse to researchers in both pure and applied fluid dynamics, particularly in\nterms of approximate models for turbulence and as tools to investigate\npotential blow-up of solutions.", "journal": ""}
{"doi": "10.48550/arXiv.1608.05968", "date": "2016-08-21", "title": "Coulomb-blockade effect in nonlinear mesoscopic capacitors", "authors": "M. I. Alomar, Jong Soo Lim, David S\u00e1nchez", "abstract": "We consider an interacting quantum dot working as a coherent source of single\nelectrons. The dot is tunnel coupled to a reservoir and capacitively coupled to\na gate terminal with an applied ac potential. At low frequencies, this is the\nquantum analog of the RC circuit with a purely dynamical response. We\ninvestigate the quantized dynamics as a consequence of ac pulses with large\namplitude. Within a Keldysh-Green function formalism we derive the\ntime-dependent current in the Coulomb blockade regime. Our theory thus extends\nprevious models that considered either noninteracting electrons in nonlinear\nresponse or interacting electrons in the linear regime. We prove that the\nelectron emission and absorption resonances undergo a splitting when the\ncharging energy is larger than the tunnel broadening. For very large charging\nenergies, the additional peaks collapse and the original resonances are\nrecovered, though with a reduced amplitude. Quantization of the charge emitted\nby the capacitor is reduced due to Coulomb repulsion and additional plateaus\narise. Additionally, we discuss the differential capacitance and resistance as\na function of time. We find that to leading order in driving frequency the\ncurrent can be expressed as a weighted sum of noninteracting currents shifted\nby the charging energy.", "journal": "Phys. Rev. B 94, 165425 (2016)"}
{"doi": "10.48550/arXiv.2201.10742", "date": "2022-01-26", "title": "Effect of Chiral Damping on the dynamics of chiral domain walls and skyrmions", "authors": "C. K. Safeer, Mohamed-Ali Nsibi, Jayshankar Nath, Mihai Sebastian Gabor, Haozhe Yang, Isabelle Joumard, Stephane Auffret, Gilles Gaudin, Ioan-Mihai Miron", "abstract": "Friction plays an essential role in most physical processes that we\nexperience in our everyday life. Examples range from our ability to walk or\nswim, to setting boundaries of speed and fuel efficiency of moving vehicles. In\nmagnetic systems, the displacement of chiral domain walls (DW) and skyrmions\n(SK) by Spin Orbit Torques (SOT), is also prone to friction. Chiral damping,\nthe dissipative counterpart of the Dzyaloshinskii Moriya Interaction (DMI),\nplays a central role in these dynamics. Despite experimental observation, and\nnumerous theoretical studies confirming its existence, the influence of chiral\ndamping on DW and SK dynamics has remained elusive due to the difficulty of\ndiscriminating from DMI. Here we unveil the effect that chiral damping has on\nthe flow motion of DWs and SKs driven by current and magnetic field. We use a\nstatic in-plane field to lift the chiral degeneracy. As the in-plane field is\nincreased, the chiral asymmetry changes sign. When considered separately,\nneither DMI nor chiral damping can explain the sign reversal of the asymmetry,\nwhich we prove to be the result of their competing effects. Finally, numerical\nmodelling unveils the non-linear nature of chiral dissipation and its critical\nrole for the stabilization of moving SKs.", "journal": ""}
{"doi": "10.48550/arXiv.1710.03352", "date": "2017-10-09", "title": "A synchronous program algebra: a basis for reasoning about shared-memory and event-based concurrency", "authors": "Ian J. Hayes, Larissa A. Meinicke, Kirsten Winter, Robert J. Colvin", "abstract": "This research started with an algebra for reasoning about rely/guarantee\nconcurrency for a shared memory model. The approach taken led to a more\nabstract algebra of atomic steps, in which atomic steps synchronise (rather\nthan interleave) when composed in parallel. The algebra of rely/guarantee\nconcurrency then becomes an instantiation of the more abstract algebra. Many of\nthe core properties needed for rely/guarantee reasoning can be shown to hold in\nthe abstract algebra where their proofs are simpler and hence allow a higher\ndegree of automation. The algebra has been encoded in Isabelle/HOL to provide a\nbasis for tool support for program verification.\n  In rely/guarantee concurrency, programs are specified to guarantee certain\nbehaviours until assumptions about the behaviour of their environment are\nviolated. When assumptions are violated, program behaviour is unconstrained\n(aborting), and guarantees need no longer hold. To support these guarantees a\nsecond synchronous operator, weak conjunction, was introduced: both processes\nin a weak conjunction must agree to take each atomic step, unless one aborts in\nwhich case the whole aborts. In developing the laws for parallel and weak\nconjunction we found many properties were shared by the operators and that the\nproofs of many laws were essentially the same. This insight led to the idea of\ngeneralising synchronisation to an abstract operator with only the axioms that\nare shared by the parallel and weak conjunction operator, so that those two\noperators can be viewed as instantiations of the abstract synchronisation\noperator. The main differences between parallel and weak conjunction are how\nthey combine individual atomic steps; that is left open in the axioms for the\nabstract operator.", "journal": ""}
{"doi": "10.48550/arXiv.1511.03197", "date": "2015-11-10", "title": "Radii, masses, and ages of 18 bright stars using interferometry and new estimations of exoplanetary parameters", "authors": "Roxanne Ligi, Orlagh Creevey, Denis Mourard, Aur\u00e9lien Crida, Anne-Marie Lagrange, Nicolas Nardetto, Karine Perraut, Mathias Schultheis, Isabelle Tallon-Bosc, Theo ten Brummelaar", "abstract": "Accurate stellar parameters are needed in numerous domains of astrophysics.\nThe position of stars on the H-R diagram is an important indication of their\nstructure and evolution, and it helps improve stellar models. Furthermore, the\nage and mass of stars hosting planets are required elements for studying\nexoplanetary systems. We aim at determining accurate parameters of a set of 18\nbright exoplanet host and potential host stars from interferometric\nmeasurements, photometry, and stellar models. Using the VEGA/CHARA\ninterferometer, we measured the angular diameters of 18 stars, ten of which\nhost exoplanets. We combined them with their distances to estimate their radii.\nWe used photometry to derive their bolometric flux and, then, their effective\ntemperature and luminosity to place them on the H-R diagram. We then used the\nPARSEC models to derive their best fit ages and masses, with error bars derived\nfrom MC calculations. Our interferometric measurements lead to an average of\n1.9% uncertainty on angular diameters and 3% on stellar radii. There is good\nagreement between measured and indirect estimations of angular diameters (from\nSED fitting or SB relations) for MS stars, but not as good for more evolved\nstars. For each star, we provide a likelihood map in the mass-age plane;\ntypically, two distinct sets of solutions appear (an old and a young age). The\nerrors on the ages and masses that we provide account for the metallicity\nuncertainties, which are often neglected by other works. From measurements of\nits radius and density, we also provide the mass of 55 Cnc independently of\nmodels. From the stellar masses, we provide new estimates of semi-major axes\nand minimum masses of exoplanets with reliable uncertainties. We also derive\nthe radius, density, and mass of 55 Cnc e, a super-Earth that transits its\nstellar host. Our exoplanetary parameters reflect the known population of\nexoplanets.", "journal": "A&A 586, A94 (2016)"}
{"doi": "10.48550/arXiv.2406.09327", "date": "2024-06-13", "title": "Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN Pipeline applied on PSMA PET/CT Scans", "authors": "Stefan P. Hein, Manuel Schultheiss, Andrei Gafita, Raphael Zaum, Farid Yagubbayli, Robert Tauber, Isabel Rauscher, Matthias Eiber, Franz Pfeiffer, Wolfgang A. Weber", "abstract": "Assessing tumor response to systemic therapies is one of the main\napplications of PET/CT. Routinely, only a small subset of index lesions out of\nmultiple lesions is analyzed. However, this operator dependent selection may\nbias the results due to possible significant inter-metastatic heterogeneity of\nresponse to therapy. Automated, AI based approaches for lesion tracking hold\npromise in enabling the analysis of many more lesions and thus providing a\nbetter assessment of tumor response. This work introduces a Siamese CNN\napproach for lesion tracking between PET/CT scans. Our approach is applied on\nthe laborious task of tracking a high number of bone lesions in full-body\nbaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles\nof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer\npatients. Data preparation includes lesion segmentation and affine\nregistration. Our algorithm extracts suitable lesion patches and forwards them\ninto a Siamese CNN trained to classify the lesion patch pairs as corresponding\nor non-corresponding lesions. Experiments have been performed with different\ninput patch types and a Siamese network in 2D and 3D. The CNN model\nsuccessfully learned to classify lesion assignments, reaching a lesion tracking\naccuracy of 83 % in its best configuration with an AUC = 0.91. For remaining\nlesions the pipeline accomplished a re-identification rate of 89 %. We proved\nthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT\nscans. Future clinical studies are necessary if this improves the prediction of\nthe outcome of therapies.", "journal": ""}
{"doi": "10.48550/arXiv.2302.05004", "date": "2023-02-10", "title": "Initial validation of a soil-based mass-balance approach for empirical monitoring of enhanced rock weathering rates", "authors": "Tom Reershemius, Mike E. Kelland, Jacob S. Jordan, Isabelle R. Davis, Rocco D'Ascanio, Boriana Kalderon-Asael, Dan Asael, T. Jesper Suhrhoff, Dimitar Z. Epihov, David J. Beerling, Christopher T. Reinhard, Noah J. Planavsky", "abstract": "Enhanced Rock Weathering (ERW) is a promising scalable and cost-effective\nCarbon Dioxide Removal (CDR) strategy with significant environmental and\nagronomic co-benefits. A major barrier to large-scale implementation of ERW is\na robust Monitoring, Reporting, and Verification (MRV) framework. To\nsuccessfully quantify the amount of carbon dioxide removed by ERW, MRV must be\naccurate, precise, and cost-effective. Here, we outline a mass-balance-based\nmethod where analysis of the chemical composition of soil samples is used to\ntrack in-situ silicate rock weathering. We show that signal-to-noise issues of\nin-situ soil analysis can be mitigated by using isotope-dilution mass\nspectrometry to reduce analytical error. We implement a proof-of-concept\nexperiment demonstrating the method in controlled mesocosms. In our experiment,\nbasalt rock feedstock is added to soil columns containing the cereal crop\nSorghum bicolor at a rate equivalent to 50 t ha$^{-1}$. Using our approach, we\ncalculate rock weathering corresponding to an average initial CDR value of 1.44\n+/- 0.27 tCO$_2$eq ha$^{-1}$ from our experiments after 235 days, within error\nof an independent estimate calculated using conventional elemental budgeting of\nreaction products. Our method provides a robust time-integrated estimate of\ninitial CDR, to feed into models that track and validate large-scale carbon\nremoval through ERW.", "journal": ""}
{"doi": "10.48550/arXiv.1309.6489", "date": "2013-09-25", "title": "Variation in dust properties in a dense filament of the Taurus molecular complex (L1506)", "authors": "Nathalie Ysard, Alain Abergel, Isabelle Ristorcelli, Mika Juvela, Laurent Pagani, Vera Konyves, Locke Spencer, Glenn White, Annie Zavagno", "abstract": "We observed the L1506 filament, which is located in the Taurus molecular\ncomplex, with the Herschel PACS and SPIRE instruments. Our aim is to prove the\nvariation in grain properties along the entire length of the filament. In\nparticular, we want to determine above which gas density this variation arises\nand what changes in the grain optical properties/size distribution are\nrequired. We use the 3D radiative transfer code CRT, coupled to the dust\nemission and extinction code DustEM, to model the emission and extinction of\nthe dense filament. We test a range of optical properties and size\ndistributions for the grains: dust of the diffuse interstellar medium\n(interstellar PAHs and amorphous carbons and silicates) and both compact and\nfluffy aggregates. We find that the grain opacity has to increase across the\nfilament to fit simultaneously the near-IR extinction and Herschel emission\nprofiles of L1506. We interpret this change to be a consequence of the\ncoagulation of dust grains to form fluffy aggregates. Grains similar to those\nin the diffuse medium have to be present in the outer layers of the cloud,\nwhereas aggregates must prevail above gas densities of a few 1000 H/cm3. This\ncorresponds to line-of-sights with visual extinction in the V band of the order\nof 2 to 3. The dust opacity at 250 microns is raised by a factor of 1.8 to 2.2,\nwhile the grain average size is increased by a factor of 5. These exact numbers\ndepend naturally on the dust model chosen to fit the data. Our findings agree\nwith the constraints given by the study of the gas molecular lines. Using a\nsimple approach, we show that the aggregates may have time to form inside the\nfilament within the cloud lifetime. Our model also characterises the density\nstructure of the filament, showing that the filament width is not constant\nalong L1506 but instead varies by a factor of the order of 4.", "journal": ""}
{"doi": "10.48550/arXiv.1412.3212", "date": "2014-12-10", "title": "Millimeter-wave Evolution for 5G Cellular Networks", "authors": "Kei Sakaguchi, Gia Khanh Tran, Hidekazu Shimodaira, Shinobu Nanba, Toshiaki Sakurai, Koji Takinami, Isabelle Siaud, Emilio Calvanese Strinati, Antonio Capone, Ingolf Karls, Reza Arefi, Thomas Haustein", "abstract": "Triggered by the explosion of mobile traffic, 5G (5th Generation) cellular\nnetwork requires evolution to increase the system rate 1000 times higher than\nthe current systems in 10 years. Motivated by this common problem, there are\nseveral studies to integrate mm-wave access into current cellular networks as\nmulti-band heterogeneous networks to exploit the ultra-wideband aspect of the\nmm-wave band. The authors of this paper have proposed comprehensive\narchitecture of cellular networks with mm-wave access, where mm-wave small cell\nbasestations and a conventional macro basestation are connected to\nCentralized-RAN (C-RAN) to effectively operate the system by enabling power\nefficient seamless handover as well as centralized resource control including\ndynamic cell structuring to match the limited coverage of mm-wave access with\nhigh traffic user locations via user-plane/control-plane splitting. In this\npaper, to prove the effectiveness of the proposed 5G cellular networks with\nmm-wave access, system level simulation is conducted by introducing an expected\nfuture traffic model, a measurement based mm-wave propagation model, and a\ncentralized cell association algorithm by exploiting the C-RAN architecture.\nThe numerical results show the effectiveness of the proposed network to realize\n1000 times higher system rate than the current network in 10 years which is not\nachieved by the small cells using commonly considered 3.5 GHz band.\nFurthermore, the paper also gives latest status of mm-wave devices and\nregulations to show the feasibility of using mm-wave in the 5G systems.", "journal": "IEICE Trans. Commun., Vol. E98-B, No. 3, Mar. 2015"}
{"doi": "10.48550/arXiv.1810.03586", "date": "2018-10-08", "title": "Hierarchical segmentation using equivalence test (HiSET): Application to DCE image sequences", "authors": "Fuchen Liu, Charles-Andr\u00e9 Cu\u00e9nod, Isabelle Thomassin-Naggara, St\u00e9phane Chemouny, Yves Rozenholc", "abstract": "Dynamical contrast enhanced (DCE) imaging allows non invasive access to\ntissue micro-vascularization. It appears as a promising tool to build imaging\nbiomark-ers for diagnostic, prognosis or anti-angiogenesis treatment monitoring\nof cancer. However, quantitative analysis of DCE image sequences suffers from\nlow signal to noise ratio (SNR). SNR may be improved by averaging functional\ninformation in a large region of interest when it is functionally homogeneous.\n  We propose a novel method for automatic segmentation of DCE image sequences\ninto functionally homogeneous regions, called DCE-HiSET. Using an observation\nmodel which depends on one parameter a and is justified a posteri-ori,\nDCE-HiSET is a hierarchical clustering algorithm. It uses the p-value of a\nmultiple equivalence test as dissimilarity measure and consists of two steps.\nThe first exploits the spatial neighborhood structure to reduce complexity and\ntakes advantage of the regularity of anatomical features, while the second\nrecovers (spatially) disconnected homogeneous structures at a larger (global)\nscale. Given a minimal expected homogeneity discrepancy for the multiple\nequivalence test, both steps stop automatically by controlling the Type I\nerror. This provides an adaptive choice for the number of clusters. Assuming\nthat the DCE image sequence is functionally piecewise constant with signals on\neach piece sufficiently separated, we prove that DCE-HiSET will retrieve the\nexact partition with high probability as soon as the number of images in the\nsequence is large enough. The minimal expected homogeneity discrepancy appears\nas the tuning parameter controlling the size of the segmentation.\n  DCE-HiSET has been implemented in C++ for 2D and 3D image sequences with\ncompetitive speed.\n  Keywords : DCE imaging, automatic clustering, hierarchical segmentation,\nequivalence test", "journal": ""}
{"doi": "10.48550/arXiv.2309.12325", "date": "2023-08-11", "title": "FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare", "authors": "Karim Lekadir, Aasa Feragen, Abdul Joseph Fofanah, Alejandro F Frangi, Alena Buyx, Anais Emelie, Andrea Lara, Antonio R Porras, An-Wen Chan, Arcadi Navarro, Ben Glocker, Benard O Botwe, Bishesh Khanal, Brigit Beger, Carol C Wu, Celia Cintas, Curtis P Langlotz, Daniel Rueckert, Deogratias Mzurikwao, Dimitrios I Fotiadis, Doszhan Zhussupov, Enzo Ferrante, Erik Meijering, Eva Weicken, Fabio A Gonz\u00e1lez, Folkert W Asselbergs, Fred Prior, Gabriel P Krestin, Gary Collins, Geletaw S Tegenaw, Georgios Kaissis, Gianluca Misuraca, Gianna Tsakou, Girish Dwivedi, Haridimos Kondylakis, Harsha Jayakody, Henry C Woodruf, Horst Joachim Mayer, Hugo JWL Aerts, Ian Walsh, Ioanna Chouvarda, Ir\u00e8ne Buvat, Isabell Tributsch, Islem Rekik, James Duncan, Jayashree Kalpathy-Cramer, Jihad Zahir, Jinah Park, John Mongan, Judy W Gichoya, Julia A Schnabel, Kaisar Kushibar, Katrine Riklund, Kensaku Mori, Kostas Marias, Lameck M Amugongo, Lauren A Fromont, Lena Maier-Hein, Leonor Cerd\u00e1 Alberich, Leticia Rittner, Lighton Phiri, Linda Marrakchi-Kacem, Llu\u00eds Donoso-Bach, Luis Mart\u00ed-Bonmat\u00ed, M Jorge Cardoso, Maciej Bobowicz, Mahsa Shabani, Manolis Tsiknakis, Maria A Zuluaga, Maria Bielikova, Marie-Christine Fritzsche, Marina Camacho, Marius George Linguraru, Markus Wenzel, Marleen De Bruijne, Martin G Tolsgaard, Marzyeh Ghassemi, Md Ashrafuzzaman, Melanie Goisauf, Mohammad Yaqub, M\u00f3nica Cano Abad\u00eda, Mukhtar M E Mahmoud, Mustafa Elattar, Nicola Rieke, Nikolaos Papanikolaou, Noussair Lazrak, Oliver D\u00edaz, Olivier Salvado, Oriol Pujol, Ousmane Sall, Pamela Guevara, Peter Gordebeke, Philippe Lambin, Pieta Brown, Purang Abolmaesumi, Qi Dou, Qinghua Lu, Richard Osuala, Rose Nakasi, S Kevin Zhou, Sandy Napel, Sara Colantonio, Shadi Albarqouni, Smriti Joshi, Stacy Carter, Stefan Klein, Steffen E Petersen, Susanna Auss\u00f3, Suyash Awate, Tammy Riklin Raviv, Tessa Cook, Tinashe E M Mutsvangwa, Wendy A Rogers, Wiro J Niessen, X\u00e8nia Puig-Bosch, Yi Zeng, Yunusa G Mohammed, Yves Saint James Aquino, Zohaib Salahuddin, Martijn P A Starmans", "abstract": "Despite major advances in artificial intelligence (AI) for medicine and\nhealthcare, the deployment and adoption of AI technologies remain limited in\nreal-world clinical practice. In recent years, concerns have been raised about\nthe technical, clinical, ethical and legal risks associated with medical AI. To\nincrease real world adoption, it is essential that medical AI tools are trusted\nand accepted by patients, clinicians, health organisations and authorities.\nThis work describes the FUTURE-AI guideline as the first international\nconsensus framework for guiding the development and deployment of trustworthy\nAI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and\ncurrently comprises 118 inter-disciplinary experts from 51 countries\nrepresenting all continents, including AI scientists, clinicians, ethicists,\nand social scientists. Over a two-year period, the consortium defined guiding\nprinciples and best practices for trustworthy AI through an iterative process\ncomprising an in-depth literature review, a modified Delphi survey, and online\nconsensus meetings. The FUTURE-AI framework was established based on 6 guiding\nprinciples for trustworthy AI in healthcare, i.e. Fairness, Universality,\nTraceability, Usability, Robustness and Explainability. Through consensus, a\nset of 28 best practices were defined, addressing technical, clinical, legal\nand socio-ethical dimensions. The recommendations cover the entire lifecycle of\nmedical AI, from design, development and validation to regulation, deployment,\nand monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which\nprovides a structured approach for constructing medical AI tools that will be\ntrusted, deployed and adopted in real-world practice. Researchers are\nencouraged to take the recommendations into account in proof-of-concept stages\nto facilitate future translation towards clinical practice of medical AI.", "journal": ""}
{"doi": "10.48550/arXiv.1707.09322", "date": "2017-07-28", "title": "The Fourteenth Data Release of the Sloan Digital Sky Survey: First Spectroscopic Data from the extended Baryon Oscillation Spectroscopic Survey and from the second phase of the Apache Point Observatory Galactic Evolution Experiment", "authors": "Bela Abolfathi, D. S. Aguado, Gabriela Aguilar, Carlos Allende Prieto, Andres Almeida, Tonima Tasnim Ananna, Friedrich Anders, Scott F. Anderson, Brett H. Andrews, Borja Anguiano, Alfonso Aragon-Salamanca, Maria Argudo-Fernandez, Eric Armengaud, Metin Ata, Eric Aubourg, Vladimir Avila-Reese, Carles Badenes, Stephen Bailey, Christophe Balland, Kathleen A. Barger, Jorge Barrera-Ballesteros, Curtis Bartosz, Fabienne Bastien, Dominic Bates, Falk Baumgarten, Julian Bautista, Rachael Beaton, Timothy C. Beers, Francesco Belfiore, Chad F. Bender, Mariangela Bernardi, Matthew A. Bershady, Florian Beutler, Jonathan C. Bird, Dmitry Bizyaev, Guillermo A. Blanc, Michael R. Blanton, Michael Blomqvist, Adam S. Bolton, Mederic Boquien, Jura Borissova, Jo Bovy, Christian Andres Bradna Diaz, William Nielsen Brandt, Jonathan Brinkmann, Joel R. Brownstein, Kevin Bundy, Adam J. Burgasser, Etienne Burtin, Nicolas G. Busca, Caleb I. Canas, Mariana Cano-Diaz, Michele Cappellari, Ricardo Carrera, Andrew R. Casey, Bernardo Cervantes Sodi, Yanping Chen, Brian Cherinka, Cristina Chiappini, Peter Doohyun Choi, Drew Chojnowski, Chia-Hsun Chuang, Haeun Chung, Nicolas Clerc, Roger E. Cohen, Julia M. Comerford, Johan Comparat, Janaina Correa do Nascimento, Luiz da Costa, Marie-Claude Cousinou, Kevin Covey, Jeffrey D. Crane, Irene Cruz-Gonzalez, Katia Cunha, Gabriele da Silva Ilha, Guillermo J. Damke, Jeremy Darling, James W. Davidson Jr., Kyle Dawson, Miguel Angel C. de Icaza Lizaola, Axel de la Macorra, Sylvain de la Torre, Nathan De Lee, Victoria de Sainte Agathe, Alice Deconto Machado, Flavia Dell'Agli, Timothee Delubac, Aleksandar M. Diamond-Stanic, John Donor, Juan Jose Downes, Niv Drory, Helion du Mas des Bourboux, Christopher J. Duckworth, Tom Dwelly, Jamie Dyer, Garrett Ebelke, Arthur Davis Eigenbrot, Daniel J. Eisenstein, Yvonne P. Elsworth, Eric Emsellem, Mike Eracleous, Ghazaleh Erfanianfar, Stephanie Escoffier, Xiaohui Fan, Emma Fernandez Alvar, J. G. Fernandez-Trincado, Rafael Fernando Cirolini, Diane Feuillet, Alexis Finoguenov, Scott W. Fleming, Andreu Font-Ribera, Gordon Freischlad, Peter Frinchaboy, Hai Fu, Yilen Gomez Maqueo Chew, Lluis Galbany, Ana E. Garcia Perez, R. Garcia-Dias, D. A. Garcia-Hernandez, Luis Alberto Garma Oehmichen, Patrick Gaulme, Joseph Gelfand, Hector Gil-Marin, Bruce A. Gillespie, Daniel Goddard, Jonay I. Gonzalez Hernandez, Violeta Gonzalez-Perez, Kathleen Grabowski, Paul J. Green, Catherine J. Grier, Alain Gueguen, Hong Guo, Julien Guy, Alex Hagen, Patrick Hall, Paul Harding, Sten Hasselquist, Suzanne Hawley, Christian R. Hayes, Fred Hearty, Saskia Hekker, Jesus Hernandez, Hector Hernandez Toledo, David W. Hogg, Kelly Holley-Bockelmann, Jon Holtzman, Jiamin Hou, Bau-Ching Hsieh, Jason A. S. Hunt, Timothy A. Hutchinson, Ho Seong Hwang, Camilo Eduardo Jimenez Angel, Jennifer A. Johnson, Amy Jones, Henrik Jonsson, Eric Jullo, Fahim Sakil Khan, Karen Kinemuchi, David Kirkby, Charles C. Kirkpatrick IV, Francisco-Shu Kitaura, Gillian R. Knapp, Jean-Paul Kneib, Juna A. Kollmeier, Ivan Lacerna, Richard R. Lane, Dustin Lang, David R. Law, Jean-Marc Le Goff, Young-Bae Lee, Hongyu Li, Cheng Li, Jianhui Lian, Yu Liang, Marcos Lima, Lihwai Lin, Dan Long, Sara Lucatello, Britt Lundgren, J. Ted Mackereth, Chelsea L. MacLeod, Suvrath Mahadevan, Marcio Antonio Geimba Maia, Steven Majewski, Arturo Manchado, Claudia Maraston, Vivek Mariappan, Rui Marques-Chaves, Thomas Masseron, Karen L. Masters, Richard M. McDermid, Ian D. McGreer, Matthew Melendez, Sofia Meneses-Goytia, Andrea Merloni, Michael R. Merrifield, Szabolcs Meszaros, Andres Meza, Ivan Minchev, Dante Minniti, Eva-Maria Mueller, Francisco Muller-Sanchez, Demitri Muna, Ricardo R. Munoz, Adam D. Myers, Preethi Nair, Kirpal Nandra, Melissa Ness, Jeffrey A. Newman, Robert C. Nichol, David L. Nidever, Christian Nitschelm, Pasquier Noterdaeme, Julia O'Connell, Ryan James Oelkers, Audrey Oravetz, Daniel Oravetz, Erik Aquino Ortiz, Yeisson Osorio, Zach Pace, Nelson Padilla, Nathalie Palanque-Delabrouille, Pedro Alonso Palicio, Hsi-An Pan, Kaike Pan, Taniya Parikh, Isabelle Paris, Changbom Park, Sebastien Peirani, Marcos Pellejero-Ibanez, Samantha Penny, Will J. Percival, Ismael Perez-Fournon, Patrick Petitjean, Matthew M. Pieri, Marc Pinsonneault, Alice Pisani, Francisco Prada, Abhishek Prakash, Anna Barbara de Andrade Queiroz, M. Jordan Raddick, Anand Raichoor, Sandro Barboza Rembold, Hannah Richstein, Rogemar A. Riffel, Rogerio Riffel, Hans-Walter Rix, Annie C. Robin, Sergio Rodriguez Torres, Carlos Roman-Zuniga, Ashley J. Ross, Graziano Rossi, John Ruan, Rossana Ruggeri, Jose Ruiz, Mara Salvato, Ariel G. Sanchez, Sebastian F. Sanchez, Jorge Sanchez Almeida, Jose R. Sanchez-Gallego, Felipe Antonio Santana Rojas, Basilio Xavier Santiago, Ricardo P. Schiavon, Jaderson S. Schimoia, Edward Schlafly, David Schlegel, Donald P. Schneider, William J. Schuster, Axel Schwope, Hee-Jong Seo, Aldo Serenelli, Shiyin Shen, Yue Shen, Matthew Shetrone, Michael Shull, Victor Silva Aguirre, Joshua D. Simon, Mike Skrutskie, Anze Slosar, Rebecca Smethurst, Verne Smith, Jennifer Sobeck, Garrett Somers, Barbara J. Souter, Diogo Souto, Ashley Spindler, David V. Stark, Keivan Stassun, Matthias Steinmetz, Dennis Stello, Thaisa Storchi-Bergmann, Alina Streblyanska, Guy Stringfellow, Genaro Suarez, Jing Sun, Laszlo Szigeti, Manuchehr Taghizadeh-Popp, Michael S. Talbot, Baitian Tang, Charling Tao, Jamie Tayar, Mita Tembe, Johanna Teske, Aniruddha R. Thaker, Daniel Thomas, Patricia Tissera, Rita Tojeiro, Christy Tremonti, Nicholas W. Troup, Meg Urry, O. Valenzuela, Remco van den Bosch, Jaime Vargas-Gonzalez, Mariana Vargas-Magana, Jose Alberto Vazquez, Sandro Villanova, Nicole Vogt, David Wake, Yuting Wang, Benjamin Alan Weaver, Anne-Marie Weijmans, David H. Weinberg, Kyle B. Westfall, David G. Whelan, Eric Wilcots, Vivienne Wild, Rob A. Williams, John Wilson, W. M. Wood-Vasey, Dominika Wylezalek, Ting Xiao, Renbin Yan, Meng Yang, Jason E. Ybarra, Christophe Yeche, Nadia Zakamska, Olga Zamora, Pauline Zarrouk, Gail Zasowski, Kai Zhang, Cheng Zhao, Gong-Bo Zhao, Zheng Zheng, Zheng Zheng, Zhi-Min Zhou, Guangtun Zhu, Joel C. Zinn, Hu Zou", "abstract": "The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) has been in\noperation since July 2014. This paper describes the second data release from\nthis phase, and the fourteenth from SDSS overall (making this, Data Release\nFourteen or DR14). This release makes public data taken by SDSS-IV in its first\ntwo years of operation (July 2014-2016). Like all previous SDSS releases, DR14\nis cumulative, including the most recent reductions and calibrations of all\ndata taken by SDSS since the first phase began operations in 2000. New in DR14\nis the first public release of data from the extended Baryon Oscillation\nSpectroscopic Survey (eBOSS); the first data from the second phase of the\nApache Point Observatory (APO) Galactic Evolution Experiment (APOGEE-2),\nincluding stellar parameter estimates from an innovative data driven machine\nlearning algorithm known as \"The Cannon\"; and almost twice as many data cubes\nfrom the Mapping Nearby Galaxies at APO (MaNGA) survey as were in the previous\nrelease (N = 2812 in total). This paper describes the location and format of\nthe publicly available data from SDSS-IV surveys. We provide references to the\nimportant technical papers describing how these data have been taken (both\ntargeting and observation details) and processed for scientific use. The SDSS\nwebsite (www.sdss.org) has been updated for this release, and provides links to\ndata downloads, as well as tutorials and examples of data use. SDSS-IV is\nplanning to continue to collect astronomical data until 2020, and will be\nfollowed by SDSS-V.", "journal": ""}
