{"doi": "10.48550/arXiv.2501.14294", "date": "2025-01-24", "title": "Examining Alignment of Large Language Models through Representative Heuristics: The Case of Political Stereotypes", "authors": "Sullam Jeoung, Yubin Ge, Haohan Wang, Jana Diesner", "abstract": "Examining the alignment of large language models (LLMs) has become\nincreasingly important, e.g., when LLMs fail to operate as intended. This study\nexamines the alignment of LLMs with human values for the domain of politics.\nPrior research has shown that LLM-generated outputs can include political\nleanings and mimic the stances of political parties on various issues. However,\nthe extent and conditions under which LLMs deviate from empirical positions are\ninsufficiently examined. To address this gap, we analyze the factors that\ncontribute to LLMs' deviations from empirical positions on political issues,\naiming to quantify these deviations and identify the conditions that cause\nthem.\n  Drawing on findings from cognitive science about representativeness\nheuristics, i.e., situations where humans lean on representative attributes of\na target group in a way that leads to exaggerated beliefs, we scrutinize LLM\nresponses through this heuristics' lens. We conduct experiments to determine\nhow LLMs inflate predictions about political parties, which results in\nstereotyping. We find that while LLMs can mimic certain political parties'\npositions, they often exaggerate these positions more than human survey\nrespondents do. Also, LLMs tend to overemphasize representativeness more than\nhumans. This study highlights the susceptibility of LLMs to representativeness\nheuristics, suggesting a potential vulnerability of LLMs that facilitates\npolitical stereotyping. We also test prompt-based mitigation strategies,\nfinding that strategies that can mitigate representative heuristics in humans\nare also effective in reducing the influence of representativeness on\nLLM-generated responses.", "journal": ""}
{"doi": "10.48550/arXiv.2410.24190", "date": "2024-10-31", "title": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters", "authors": "Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, Dawn Song", "abstract": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while raising the question of\nwhether such neutrality is truly the path forward.", "journal": ""}
{"doi": "10.48550/arXiv.2501.02532", "date": "2025-01-05", "title": "Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm", "authors": "Ljubisa Bojic, Olga Zagovora, Asta Zelenkauskaite, Vuk Vukovic, Milan Cabarkapa, Selma Veseljevi\u0107 Jerkovic, Ana Jovan\u010devic", "abstract": "In the era of rapid digital communication, vast amounts of textual data are\ngenerated daily, demanding efficient methods for latent content analysis to\nextract meaningful insights. Large Language Models (LLMs) offer potential for\nautomating this process, yet comprehensive assessments comparing their\nperformance to human annotators across multiple dimensions are lacking. This\nstudy evaluates the reliability, consistency, and quality of seven\nstate-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and\nMixtral, relative to human annotators in analyzing sentiment, political\nleaning, emotional intensity, and sarcasm detection. A total of 33 human\nannotators and eight LLM variants assessed 100 curated textual items,\ngenerating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across\nthree time points to examine temporal consistency. Inter-rater reliability was\nmeasured using Krippendorff's alpha, and intra-class correlation coefficients\nassessed consistency over time. The results reveal that both humans and LLMs\nexhibit high reliability in sentiment analysis and political leaning\nassessments, with LLMs demonstrating higher internal consistency than humans.\nIn emotional intensity, LLMs displayed higher agreement compared to humans,\nthough humans rated emotional intensity significantly higher. Both groups\nstruggled with sarcasm detection, evidenced by low agreement. LLMs showed\nexcellent temporal consistency across all dimensions, indicating stable\nperformance over time. This research concludes that LLMs, especially GPT-4, can\neffectively replicate human analysis in sentiment and political leaning,\nalthough human expertise remains essential for emotional intensity\ninterpretation. The findings demonstrate the potential of LLMs for consistent\nand high-quality performance in certain areas of latent content analysis.", "journal": ""}
{"doi": "10.48550/arXiv.2412.16746", "date": "2024-12-21", "title": "Unpacking Political Bias in Large Language Models: A Cross-Model Comparison on U.S. Politics", "authors": "Kaiqi Yang, Hang Li, Yucheng Chu, Yuping Lin, Tai-Quan Peng, Hui Liu", "abstract": "Large Language Models (LLMs) have been widely used to generate responses on\nsocial topics due to their world knowledge and generative capabilities. Beyond\nreasoning and generation performance, political bias is an essential issue that\nwarrants attention. Political bias, as a universal phenomenon in human society,\nmay be transferred to LLMs and distort LLMs' behaviors of information\nacquisition and dissemination with humans, leading to unequal access among\ndifferent groups of people. To prevent LLMs from reproducing and reinforcing\npolitical biases, and to encourage fairer LLM-human interactions,\ncomprehensively examining political bias in popular LLMs becomes urgent and\ncrucial.\n  In this study, we systematically measure the political biases in a wide range\nof LLMs, using a curated set of questions addressing political bias in various\ncontexts. Our findings reveal distinct patterns in how LLMs respond to\npolitical topics. For highly polarized topics, most LLMs exhibit a pronounced\nleft-leaning bias. Conversely, less polarized topics elicit greater consensus,\nwith similar response patterns across different LLMs. Additionally, we analyze\nhow LLM characteristics, including release date, model scale, and region of\norigin affect political bias. The results indicate political biases evolve with\nmodel scale and release date, and are also influenced by regional factors of\nLLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2502.15507", "date": "2025-02-21", "title": "Activation Steering in Neural Theorem Provers", "authors": "Shashank Kirtania", "abstract": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.", "journal": ""}
{"doi": "10.48550/arXiv.2310.13343", "date": "2023-10-20", "title": "Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs)", "authors": "Xiaoliang Chen, Liangbin Li, Le Chang, Yunhe Huang, Yuxuan Zhao, Yuxiao Zhang, Dinuo Li", "abstract": "With the development of large language models (LLMs) like the GPT series,\ntheir widespread use across various application scenarios presents a myriad of\nchallenges. This review initially explores the issue of domain specificity,\nwhere LLMs may struggle to provide precise answers to specialized questions\nwithin niche fields. The problem of knowledge forgetting arises as these LLMs\nmight find it hard to balance old and new information. The knowledge repetition\nphenomenon reveals that sometimes LLMs might deliver overly mechanized\nresponses, lacking depth and originality. Furthermore, knowledge illusion\ndescribes situations where LLMs might provide answers that seem insightful but\nare actually superficial, while knowledge toxicity focuses on harmful or biased\ninformation outputs. These challenges underscore problems in the training data\nand algorithmic design of LLMs. To address these issues, it's suggested to\ndiversify training data, fine-tune models, enhance transparency and\ninterpretability, and incorporate ethics and fairness training. Future\ntechnological trends might lean towards iterative methodologies, multimodal\nlearning, model personalization and customization, and real-time learning and\nfeedback mechanisms. In conclusion, future LLMs should prioritize fairness,\ntransparency, and ethics, ensuring they uphold high moral and ethical standards\nwhen serving humanity.", "journal": ""}
{"doi": "10.48550/arXiv.2404.12534", "date": "2024-04-18", "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean", "authors": "Peiyang Song, Kaiyu Yang, Anima Anandkumar", "abstract": "Neural theorem proving combines large language models (LLMs) with proof\nassistants such as Lean, where the correctness of formal proofs can be\nrigorously verified, leaving no room for hallucination. With existing neural\ntheorem provers pretrained on a fixed collection of data and offering valuable\nsuggestions at times, it is challenging for them to continually prove novel\ntheorems in a fully autonomous mode, where human insights may be critical. In\nthis paper, we explore LLMs as copilots that assist humans in proving theorems.\nWe introduce Lean Copilot, an general framework for running LLM inference\nnatively in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nLean users can use our pretrained models or bring their own ones that run\neither locally (with or without GPUs) or on the cloud. Using Lean Copilot, we\nbuild LLM-based tools that suggest proof steps, complete proof goals, and\nselect relevant premises. Experimental results on the Mathematics in Lean\ntextbook demonstrate the effectiveness of our method compared to existing\nrule-based proof automation in Lean (aesop). When assisting humans, Lean\nCopilot requires only 2.08 manually-entered proof steps on average (3.86\nrequired by aesop); when automating the theorem proving process, Lean Copilot\nautomates 74.2% proof steps on average, 85% better than aesop (40.1%). We open\nsource all code and artifacts under a permissive MIT license to facilitate\nfurther research.", "journal": ""}
{"doi": "10.48550/arXiv.2502.18282", "date": "2025-02-25", "title": "Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases", "authors": "Shanshan Xu, T. Y. S. S Santosh, Yanai Elazar, Quirin Vogel, Barbara Plank, Matthias Grabmair", "abstract": "The increased adoption of Large Language Models (LLMs) and their potential to\nshape public opinion have sparked interest in assessing these models' political\nleanings. Building on previous research that compared LLMs and human opinions\nand observed political bias in system responses, we take a step further to\ninvestigate the underlying causes of such biases by empirically examining how\nthe values and biases embedded in training corpora shape model outputs.\nSpecifically, we propose a method to quantitatively evaluate political leanings\nembedded in the large pretraining corpora. Subsequently we investigate to whom\nare the LLMs' political leanings more aligned with, their pretrainig corpora or\nthe surveyed human opinions. As a case study, we focus on probing the political\nleanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics\nsuch as abortion and voting rights. Our findings reveal that LLMs strongly\nreflect the political leanings in their training data, and no strong\ncorrelation is observed with their alignment to human opinions as expressed in\nsurveys. These results underscore the importance of responsible curation of\ntraining data and the need for robust evaluation metrics to ensure LLMs'\nalignment with human-centered values.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12065", "date": "2025-02-17", "title": "Formalizing Complex Mathematical Statements with LLMs: A Study on Mathematical Definitions", "authors": "Lan Zhang, Marco Valentino, Andre Freitas", "abstract": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions -- a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalisation, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we guide LLMs through\nrelevant contextual elements from formal mathematical libraries. Our findings\nreveal that definitions present a greater challenge compared to existing\nbenchmarks, such as miniF2F. In particular, we found that LLMs still struggle\nwith self-correction, and aligning with relevant mathematical libraries. At the\nsame time, structured refinement methods and definition grounding strategies\nyield notable improvements of up to 16% on self-correction capabilities and 43%\non the reduction of undefined errors, highlighting promising directions for\nenhancing LLM-based autoformalization in real-world scenarios.", "journal": ""}
{"doi": "10.48550/arXiv.2402.17649", "date": "2024-02-27", "title": "Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs", "authors": "Tanise Ceron, Neele Falk, Ana Bari\u0107, Dmitry Nikolaev, Sebastian Pad\u00f3", "abstract": "Due to the widespread use of large language models (LLMs), we need to\nunderstand whether they embed a specific \"worldview\" and what these views\nreflect. Recent studies report that, prompted with political questionnaires,\nLLMs show left-liberal leanings (Feng et al., 2023; Motoki et al., 2024).\nHowever, it is as yet unclear whether these leanings are reliable (robust to\nprompt variations) and whether the leaning is consistent across policies and\npolitical leaning. We propose a series of tests which assess the reliability\nand consistency of LLMs' stances on political statements based on a dataset of\nvoting-advice questionnaires collected from seven EU countries and annotated\nfor policy issues. We study LLMs ranging in size from 7B to 70B parameters and\nfind that their reliability increases with parameter count. Larger models show\noverall stronger alignment with left-leaning parties but differ among policy\nprograms: They show a (left-wing) positive stance towards environment\nprotection, social welfare state and liberal society but also (right-wing) law\nand order, with no consistent preferences in the areas of foreign policy and\nmigration.", "journal": ""}
{"doi": "10.48550/arXiv.2305.14930", "date": "2023-05-24", "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases", "authors": "Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata", "abstract": "In everyday conversations, humans can take on different roles and adapt their\nvocabulary to their chosen roles. We explore whether LLMs can take on, that is\nimpersonate, different roles when they generate text in-context. We ask LLMs to\nassume different personas before solving vision and language tasks. We do this\nby prefixing the prompt with a persona that is associated either with a social\nidentity or domain expertise. In a multi-armed bandit task, we find that LLMs\npretending to be children of different ages recover human-like developmental\nstages of exploration. In a language-based reasoning task, we find that LLMs\nimpersonating domain experts perform better than LLMs impersonating non-domain\nexperts. Finally, we test whether LLMs' impersonations are complementary to\nvisual information when describing different categories. We find that\nimpersonation can improve performance: an LLM prompted to be a bird expert\ndescribes birds better than one prompted to be a car expert. However,\nimpersonation can also uncover LLMs' biases: an LLM prompted to be a man\ndescribes cars better than one prompted to be a woman. These findings\ndemonstrate that LLMs are capable of taking on diverse roles and that this\nin-context impersonation can be used to uncover their hidden strengths and\nbiases.", "journal": ""}
{"doi": "10.48550/arXiv.2406.18740", "date": "2024-06-26", "title": "Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models", "authors": "Baharan Nouriinanloo, Maxime Lamothe", "abstract": "Large Language Models (LLMs) have been revolutionizing a myriad of natural\nlanguage processing tasks with their diverse zero-shot capabilities. Indeed,\nexisting work has shown that LLMs can be used to great effect for many tasks,\nsuch as information retrieval (IR), and passage ranking. However, current\nstate-of-the-art results heavily lean on the capabilities of the LLM being\nused. Currently, proprietary, and very large LLMs such as GPT-4 are the highest\nperforming passage re-rankers. Hence, users without the resources to leverage\ntop of the line LLMs, or ones that are closed source, are at a disadvantage. In\nthis paper, we investigate the use of a pre-filtering step before passage\nre-ranking in IR. Our experiments show that by using a small number of human\ngenerated relevance scores, coupled with LLM relevance scoring, it is\neffectively possible to filter out irrelevant passages before re-ranking. Our\nexperiments also show that this pre-filtering then allows the LLM to perform\nsignificantly better at the re-ranking task. Indeed, our results show that\nsmaller models such as Mixtral can become competitive with much larger\nproprietary models (e.g., ChatGPT and GPT-4).", "journal": ""}
{"doi": "10.48550/arXiv.2410.18906", "date": "2024-10-24", "title": "PRISM: A Methodology for Auditing Biases in Large Language Models", "authors": "Leif Azzopardi, Yashar Moshfeghi", "abstract": "Auditing Large Language Models (LLMs) to discover their biases and\npreferences is an emerging challenge in creating Responsible Artificial\nIntelligence (AI). While various methods have been proposed to elicit the\npreferences of such models, countermeasures have been taken by LLM trainers,\nsuch that LLMs hide, obfuscate or point blank refuse to disclosure their\npositions on certain subjects. This paper presents PRISM, a flexible,\ninquiry-based methodology for auditing LLMs - that seeks to illicit such\npositions indirectly through task-based inquiry prompting rather than direct\ninquiry of said preferences. To demonstrate the utility of the methodology, we\napplied PRISM on the Political Compass Test, where we assessed the political\nleanings of twenty-one LLMs from seven providers. We show LLMs, by default,\nespouse positions that are economically left and socially liberal (consistent\nwith prior work). We also show the space of positions that these models are\nwilling to espouse - where some models are more constrained and less compliant\nthan others - while others are more neutral and objective. In sum, PRISM can\nmore reliably probe and audit LLMs to understand their preferences, biases and\nconstraints.", "journal": ""}
{"doi": "10.48550/arXiv.2502.00212", "date": "2025-01-31", "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving", "authors": "Kefan Dong, Tengyu Ma", "abstract": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens\ngenerated during the training in Lean, STP proves 26.3% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (61.7%, pass@3200),\nProofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@3200).", "journal": ""}
{"doi": "10.48550/arXiv.2310.03293", "date": "2023-10-05", "title": "A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions", "authors": "Siwei Wu, Xiangqing Shen, Rui Xia", "abstract": "Large Language Models (LLMs), such as ChatGPT, have recently been applied to\nvarious NLP tasks due to its open-domain generation capabilities. However,\nthere are two issues with applying LLMs to dialogue tasks. 1. During the\ndialogue process, users may have implicit intentions that might be overlooked\nby LLMs. Consequently, generated responses couldn't align with the user's\nintentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.\nIn certain specific domains, their knowledge may be incomplete, and LLMs cannot\nupdate the latest knowledge in real-time. To tackle these issues, we propose a\nframework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by\nasking questions to \\textbf{D}etect user's \\textbf{I}mplicit\nin\\textbf{T}entions} (\\textbf{EDIT}). Firstly, EDIT generates open questions\nrelated to the dialogue context as the potential user's intention; Then, EDIT\nanswers those questions by interacting with LLMs and searching in\ndomain-specific knowledge bases respectively, and use LLMs to choose the proper\nanswers to questions as extra knowledge; Finally, EDIT enhances response\ngeneration by explicitly integrating those extra knowledge. Besides, previous\nquestion generation works only focus on asking questions with answers in\ncontext. In order to ask open questions, we construct a Context-Open-Question\n(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and\nHoll-E), EDIT outperformed other LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2304.14732", "date": "2023-04-28", "title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks", "authors": "Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, Tat-Seng Chua", "abstract": "Making the content generated by Large Language Model (LLM), accurate,\ncredible and traceable is crucial, especially in complex knowledge-intensive\ntasks that require multi-step reasoning and each step needs knowledge to solve.\nRetrieval-augmented generation is good potential to solve this problem.\nHowever, where and how to introduce Information Retrieval (IR) to LLM is a big\nchallenge. Previous work has the problems that wrong knowledge retrieved by IR\nmisleads the LLM and interaction between IR and LLM breaks the reasoning chain\nof LLM. This paper proposes a novel framework named\n\\textbf{Search-in-the-Chain} (SearChain) for the interaction between LLM and IR\nto solve the challenges. First, LLM generates the reasoning chain named\nChain-of-Query (CoQ) where each node consists of an IR-oriented query-answer\npair. Second, IR verifies the answer of each node of CoQ. It corrects the\nanswer that is not consistent with the retrieved information when IR gives high\nconfidence, which improves the credibility. Third, LLM can indicate its missing\nknowledge in CoQ and rely on IR to provide this knowledge to LLM. These\noperations improve the accuracy in terms of reasoning and knowledge. Finally,\nSearChain generates the reasoning process and marks references to supporting\ndocuments for each reasoning step, which improves traceability. Interaction\nwith IR in SearChain forms a novel reasoning path based on a tree, which\nenables LLM to dynamically modify the direction of reasoning. Experiments show\nthat SearChain outperforms state-of-the-art baselines on complex\nknowledge-intensive tasks including multi-hop Q\\&A, slot filling, fact\nchecking, and long-form Q\\&A.", "journal": ""}
{"doi": "10.48550/arXiv.2307.15992", "date": "2023-07-29", "title": "Towards Codable Watermarking for Injecting Multi-bits Information to LLMs", "authors": "Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, Xu Sun", "abstract": "As large language models (LLMs) generate texts with increasing fluency and\nrealism, there is a growing need to identify the source of texts to prevent the\nabuse of LLMs. Text watermarking techniques have proven reliable in\ndistinguishing whether a text is generated by LLMs by injecting hidden\npatterns. However, we argue that existing LLM watermarking methods are\nencoding-inefficient and cannot flexibly meet the diverse information encoding\nneeds (such as encoding model version, generation time, user id, etc.). In this\nwork, we conduct the first systematic study on the topic of Codable Text\nWatermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit\ncustomizable information. First of all, we study the taxonomy of LLM\nwatermarking technologies and give a mathematical formulation for CTWL.\nAdditionally, we provide a comprehensive evaluation system for CTWL: (1)\nwatermarking success rate, (2) robustness against various corruptions, (3)\ncoding rate of payload information, (4) encoding and decoding efficiency, (5)\nimpacts on the quality of the generated text. To meet the requirements of these\nnon-Pareto-improving metrics, we follow the most prominent vocabulary\npartition-based watermarking direction, and devise an advanced CTWL method\nnamed Balance-Marking. The core idea of our method is to use a proxy language\nmodel to split the vocabulary into probability-balanced parts, thereby\neffectively maintaining the quality of the watermarked text. Our code is\navailable at https://github.com/lancopku/codable-watermarking-for-llm.", "journal": ""}
{"doi": "10.48550/arXiv.2403.12627", "date": "2024-03-19", "title": "Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code", "authors": "Andreas Florath", "abstract": "In the realm of formal theorem proving, the Coq proof assistant stands out\nfor its rigorous approach to verifying mathematical assertions and software\ncorrectness. Despite the advances in artificial intelligence and machine\nlearning, the specialized nature of Coq syntax and semantics poses unique\nchallenges for Large Language Models (LLMs). Addressing this gap, we present a\ncomprehensive dataset specifically designed to enhance LLMs' proficiency in\ninterpreting and generating Coq code. This dataset, derived from a collection\nof over 10,000 Coq source files, encompasses a wide array of propositions,\nproofs, and definitions, enriched with metadata including source references and\nlicensing information. Our primary aim is to facilitate the development of LLMs\ncapable of generating syntactically correct and semantically meaningful Coq\nconstructs, thereby advancing the frontier of automated theorem proving.\nInitial experiments with this dataset have showcased its significant potential;\nmodels trained on this data exhibited enhanced accuracy in Coq code generation.\nNotably, a particular experiment revealed that a fine-tuned LLM was capable of\ngenerating 141 valid proofs for a basic lemma, highlighting the dataset's\nutility in facilitating the discovery of diverse and valid proof strategies.\nThis paper discusses the dataset's composition, the methodology behind its\ncreation, and the implications of our findings for the future of machine\nlearning in formal verification. The dataset is accessible for further research\nand exploration:\nhttps://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1", "journal": ""}
{"doi": "10.48550/arXiv.2309.05958", "date": "2023-09-12", "title": "The Moral Machine Experiment on Large Language Models", "authors": "Kazuhiro Takemoto", "abstract": "As large language models (LLMs) become more deeply integrated into various\nsectors, understanding how they make moral judgments has become crucial,\nparticularly in the realm of autonomous driving. This study utilized the Moral\nMachine framework to investigate the ethical decision-making tendencies of\nprominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their\nresponses to human preferences. While LLMs' and humans' preferences such as\nprioritizing humans over pets and favoring saving more lives are broadly\naligned, PaLM 2 and Llama 2, especially, evidence distinct deviations.\nAdditionally, despite the qualitative similarities between the LLM and human\npreferences, there are significant quantitative disparities, suggesting that\nLLMs might lean toward more uncompromising decisions, compared to the milder\ninclinations of humans. These insights elucidate the ethical frameworks of LLMs\nand their potential implications for autonomous driving.", "journal": "Royal Society Open Science 11 (2), 231393 (2024)"}
{"doi": "10.48550/arXiv.2412.13169", "date": "2024-12-17", "title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study", "authors": "Bolei Ma, Berk Yoztyurk, Anna-Carolina Haensch, Xinpeng Wang, Markus Herklotz, Frauke Kreuter, Barbara Plank, Matthias Assenmacher", "abstract": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness.", "journal": ""}
{"doi": "10.48550/arXiv.2304.00228", "date": "2023-04-01", "title": "Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models", "authors": "Kai-Cheng Yang, Filippo Menczer", "abstract": "Search engines increasingly leverage large language models (LLMs) to generate\ndirect answers, and AI chatbots now access the Internet for fresh data. As\ninformation curators for billions of users, LLMs must assess the accuracy and\nreliability of different sources. This paper audits nine widely used LLMs from\nthree leading providers -- OpenAI, Google, and Meta -- to evaluate their\nability to discern credible and high-quality information sources from\nlow-credibility ones. We find that while LLMs can rate most tested news\noutlets, larger models more frequently refuse to provide ratings due to\ninsufficient information, whereas smaller models are more prone to making\nerrors in their ratings. For sources where ratings are provided, LLMs exhibit a\nhigh level of agreement among themselves (average Spearman's $\\rho = 0.79$),\nbut their ratings align only moderately with human expert evaluations (average\n$\\rho = 0.50$). Analyzing news sources with different political leanings in the\nUS, we observe a liberal bias in credibility ratings yielded by all LLMs in\ndefault configurations. Additionally, assigning partisan roles to LLMs\nconsistently induces strong politically congruent bias in their ratings. These\nfindings have important implications for the use of LLMs in curating news and\npolitical information.", "journal": ""}
{"doi": "10.48550/arXiv.2306.13298", "date": "2023-06-23", "title": "Exploring Qualitative Research Using LLMs", "authors": "Muneera Bano, Didar Zowghi, Jon Whittle", "abstract": "The advent of AI driven large language models (LLMs) have stirred discussions\nabout their role in qualitative research. Some view these as tools to enrich\nhuman understanding, while others perceive them as threats to the core values\nof the discipline. This study aimed to compare and contrast the comprehension\ncapabilities of humans and LLMs. We conducted an experiment with small sample\nof Alexa app reviews, initially classified by a human analyst. LLMs were then\nasked to classify these reviews and provide the reasoning behind each\nclassification. We compared the results with human classification and\nreasoning. The research indicated a significant alignment between human and\nChatGPT 3.5 classifications in one third of cases, and a slightly lower\nalignment with GPT4 in over a quarter of cases. The two AI models showed a\nhigher alignment, observed in more than half of the instances. However, a\nconsensus across all three methods was seen only in about one fifth of the\nclassifications. In the comparison of human and LLMs reasoning, it appears that\nhuman analysts lean heavily on their individual experiences. As expected, LLMs,\non the other hand, base their reasoning on the specific word choices found in\napp reviews and the functional components of the app itself. Our results\nhighlight the potential for effective human LLM collaboration, suggesting a\nsynergistic rather than competitive relationship. Researchers must continuously\nevaluate LLMs role in their work, thereby fostering a future where AI and\nhumans jointly enrich qualitative research.", "journal": ""}
{"doi": "10.48550/arXiv.2405.13041", "date": "2024-05-17", "title": "Assessing Political Bias in Large Language Models", "authors": "Luca Rettenberger, Markus Reischl, Mark Schutera", "abstract": "The assessment of bias within Large Language Models (LLMs) has emerged as a\ncritical concern in the contemporary discourse surrounding Artificial\nIntelligence (AI) in the context of their potential impact on societal\ndynamics. Recognizing and considering political bias within LLM applications is\nespecially important when closing in on the tipping point toward performative\nprediction. Then, being educated about potential effects and the societal\nbehavior LLMs can drive at scale due to their interplay with human operators.\nIn this way, the upcoming elections of the European Parliament will not remain\nunaffected by LLMs. We evaluate the political bias of the currently most\npopular open-source LLMs (instruct or assistant models) concerning political\nissues within the European Union (EU) from a German voter's perspective. To do\nso, we use the \"Wahl-O-Mat,\" a voting advice application used in Germany. From\nthe voting advice of the \"Wahl-O-Mat\" we quantize the degree of alignment of\nLLMs with German political parties. We show that larger models, such as\nLlama3-70B, tend to align more closely with left-leaning political parties,\nwhile smaller models often remain neutral, particularly when prompted in\nEnglish. The central finding is that LLMs are similarly biased, with low\nvariances in the alignment concerning a specific party. Our findings underline\nthe importance of rigorously assessing and making bias transparent in LLMs to\nsafeguard the integrity and trustworthiness of applications that employ the\ncapabilities of performative prediction and the invisible hand of machine\nlearning prediction and language generation.", "journal": ""}
{"doi": "10.48550/arXiv.2405.13036", "date": "2024-05-16", "title": "Can formal argumentative reasoning enhance LLMs performances?", "authors": "Federico Castagna, Isabel Sassoon, Simon Parsons", "abstract": "Recent years witnessed significant performance advancements in\ndeep-learning-driven natural language models, with a strong focus on the\ndevelopment and release of Large Language Models (LLMs). These improvements\nresulted in better quality AI-generated output but rely on resource-expensive\ntraining and upgrading of models. Although different studies have proposed a\nrange of techniques to enhance LLMs without retraining, none have considered\ncomputational argumentation as an option. This is a missed opportunity since\ncomputational argumentation is an intuitive mechanism that formally captures\nagents' interactions and the information conflict that may arise during such\ninterplays, and so it seems well-suited for boosting the reasoning and\nconversational abilities of LLMs in a seamless manner. In this paper, we\npresent a pipeline (MQArgEng) and preliminary study to evaluate the effect of\nintroducing computational argumentation semantics on the performance of LLMs.\nOur experiment's goal was to provide a proof-of-concept and a feasibility\nanalysis in order to foster (or deter) future research towards a fully-fledged\nargumentation engine plugin for LLMs. Exploratory results using the MT-Bench\nindicate that MQArgEng provides a moderate performance gain in most of the\nexamined topical categories and, as such, show promise and warrant further\nresearch.", "journal": ""}
{"doi": "10.48550/arXiv.2407.03841", "date": "2024-07-04", "title": "On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation", "authors": "John Mendon\u00e7a, Alon Lavie, Isabel Trancoso", "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities in\nvarious Natural Language Processing tasks. For automatic open-domain dialogue\nevaluation in particular, LLMs have been seamlessly integrated into evaluation\nframeworks, and together with human evaluation, compose the backbone of most\nevaluations. However, existing evaluation benchmarks often rely on outdated\ndatasets and evaluate aspects like Fluency and Relevance, which fail to\nadequately capture the capabilities and limitations of state-of-the-art chatbot\nmodels.\n  This paper critically examines current evaluation benchmarks, highlighting\nthat the use of older response generators and quality aspects fail to\naccurately reflect modern chatbot capabilities. A small annotation experiment\non a recent LLM-generated dataset (SODA) reveals that LLM evaluators such as\nGPT-4 struggle to detect actual deficiencies in dialogues generated by current\nLLM chatbots.", "journal": ""}
{"doi": "10.48550/arXiv.2410.19605", "date": "2024-10-25", "title": "CoqPilot, a plugin for LLM-based generation of proofs", "authors": "Andrei Kozyrev, Gleb Solovev, Nikita Khramov, Anton Podkopaev", "abstract": "We present CoqPilot, a VS Code extension designed to help automate writing of\nCoq proofs. The plugin collects the parts of proofs marked with the admit\ntactic in a Coq file, i.e., proof holes, and combines LLMs along with\nnon-machine-learning methods to generate proof candidates for the holes. Then,\nCoqPilot checks if each proof candidate solves the given subgoal and, if\nsuccessful, replaces the hole with it. The focus of CoqPilot is twofold.\nFirstly, we want to allow users to seamlessly combine multiple Coq generation\napproaches and provide a zero-setup experience for our tool. Secondly, we want\nto deliver a platform for LLM-based experiments on Coq proof generation. We\ndeveloped a benchmarking system for Coq generation methods, available in the\nplugin, and conducted an experiment using it, showcasing the framework's\npossibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo.\nCode at: https://github.com/JetBrains-Research/coqpilot", "journal": ""}
{"doi": "10.48550/arXiv.2411.00860", "date": "2024-10-30", "title": "Survey of Cultural Awareness in Language Models: Text and Beyond", "authors": "Siddhesh Pawar, Junyeong Park, Jiho Jin, Arnav Arora, Junho Myung, Srishti Yadav, Faiz Ghifari Haznitrama, Inhwa Song, Alice Oh, Isabelle Augenstein", "abstract": "Large-scale deployment of large language models (LLMs) in various\napplications, such as chatbots and virtual assistants, requires LLMs to be\nculturally sensitive to the user to ensure inclusivity. Culture has been widely\nstudied in psychology and anthropology, and there has been a recent surge in\nresearch on making LLMs more culturally inclusive in LLMs that goes beyond\nmultilinguality and builds on findings from psychology and anthropology. In\nthis paper, we survey efforts towards incorporating cultural awareness into\ntext-based and multimodal LLMs. We start by defining cultural awareness in\nLLMs, taking the definitions of culture from anthropology and psychology as a\npoint of departure. We then examine methodologies adopted for creating\ncross-cultural datasets, strategies for cultural inclusion in downstream tasks,\nand methodologies that have been used for benchmarking cultural awareness in\nLLMs. Further, we discuss the ethical implications of cultural alignment, the\nrole of Human-Computer Interaction in driving cultural inclusion in LLMs, and\nthe role of cultural alignment in driving social science research. We finally\nprovide pointers to future research based on our findings about gaps in the\nliterature.", "journal": ""}
{"doi": "10.48550/arXiv.2411.03417", "date": "2024-11-05", "title": "Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment", "authors": "Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar B. Shah", "abstract": "Large language models (LLMs) represent a promising, but controversial, tool\nin aiding scientific peer review. This study evaluates the usefulness of LLMs\nin a conference setting as a tool for vetting paper submissions against\nsubmission standards. We conduct an experiment at the 2024 Neural Information\nProcessing Systems (NeurIPS) conference, where 234 papers were voluntarily\nsubmitted to an \"LLM-based Checklist Assistant.\" This assistant validates\nwhether papers adhere to the author checklist used by NeurIPS, which includes\nquestions to ensure compliance with research and manuscript preparation\nstandards. Evaluation of the assistant by NeurIPS paper authors suggests that\nthe LLM-based assistant was generally helpful in verifying checklist\ncompletion. In post-usage surveys, over 70% of authors found the assistant\nuseful, and 70% indicate that they would revise their papers or checklist\nresponses based on its feedback. While causal attribution to the assistant is\nnot definitive, qualitative evidence suggests that the LLM contributed to\nimproving some submissions. Survey responses and analysis of re-submissions\nindicate that authors made substantive revisions to their submissions in\nresponse to specific feedback from the LLM. The experiment also highlights\ncommon issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)\nwere the most frequent issues flagged by authors. We also conduct experiments\nto understand potential gaming of the system, which reveal that the assistant\ncould be manipulated to enhance scores through fabricated justifications,\nhighlighting potential vulnerabilities of automated review tools.", "journal": ""}
{"doi": "10.48550/arXiv.2403.13840", "date": "2024-03-15", "title": "Whose Side Are You On? Investigating the Political Stance of Large Language Models", "authors": "Pagnarasmey Pit, Xingjun Ma, Mike Conway, Qingyu Chen, James Bailey, Henry Pit, Putrasmey Keo, Watey Diep, Yu-Gang Jiang", "abstract": "Large Language Models (LLMs) have gained significant popularity for their\napplication in various everyday tasks such as text generation, summarization,\nand information retrieval. As the widespread adoption of LLMs continues to\nsurge, it becomes increasingly crucial to ensure that these models yield\nresponses that are politically impartial, with the aim of preventing\ninformation bubbles, upholding fairness in representation, and mitigating\nconfirmation bias. In this paper, we propose a quantitative framework and\npipeline designed to systematically investigate the political orientation of\nLLMs. Our investigation delves into the political alignment of LLMs across a\nspectrum of eight polarizing topics, spanning from abortion to LGBTQ issues.\nAcross topics, the results indicate that LLMs exhibit a tendency to provide\nresponses that closely align with liberal or left-leaning perspectives rather\nthan conservative or right-leaning ones when user queries include details\npertaining to occupation, race, or political affiliation. The findings\npresented in this study not only reaffirm earlier observations regarding the\nleft-leaning characteristics of LLMs but also surface particular attributes,\nsuch as occupation, that are particularly susceptible to such inclinations even\nwhen directly steered towards conservatism. As a recommendation to avoid these\nmodels providing politicised responses, users should be mindful when crafting\nqueries, and exercise caution in selecting neutral prompt language.", "journal": ""}
{"doi": "10.48550/arXiv.2310.15896", "date": "2023-10-24", "title": "BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT", "authors": "Yirong Chen, Zhenyu Wang, Xiaofen Xing, huimin zheng, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu", "abstract": "Large language models (LLMs) have performed well in providing general and\nextensive health suggestions in single-turn conversations, exemplified by\nsystems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the\nlimited information provided by users during single turn results in inadequate\npersonalization and targeting of the generated suggestions, which requires\nusers to independently select the useful part. It is mainly caused by the\nmissing ability to engage in multi-turn questioning. In real-world medical\nconsultations, doctors usually employ a series of iterative inquiries to\ncomprehend the patient's condition thoroughly, enabling them to provide\neffective and personalized suggestions subsequently, which can be defined as\nchain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose\nBianQue, a ChatGLM-based LLM finetuned with the self-constructed health\nconversation dataset BianQueCorpus that is consist of multiple turns of\nquestioning and health suggestions polished by ChatGPT. Experimental results\ndemonstrate that the proposed BianQue can simultaneously balance the\ncapabilities of both questioning and health suggestions, which will help\npromote the research and application of LLMs in the field of proactive health.", "journal": ""}
{"doi": "10.48550/arXiv.2406.18616", "date": "2024-06-26", "title": "Towards Large Language Model Aided Program Refinement", "authors": "Yufan Cai, Zhe Hou, Xiaokun Luan, David Miguel Sanan Baena, Yun Lin, Jun Sun, Jin Song Dong", "abstract": "Program refinement involves correctness-preserving transformations from\nformal high-level specification statements into executable programs.\nTraditional verification tool support for program refinement is highly\ninteractive and lacks automation. On the other hand, the emergence of large\nlanguage models (LLMs) enables automatic code generations from informal natural\nlanguage specifications. However, code generated by LLMs is often unreliable.\nMoreover, the opaque procedure from specification to code provided by LLM is an\nuncontrolled black box. We propose LLM4PR, a tool that combines formal program\nrefinement techniques with informal LLM-based methods to (1) transform the\nspecification to preconditions and postconditions, (2) automatically build\nprompts based on refinement calculus, (3) interact with LLM to generate code,\nand finally, (4) verify that the generated code satisfies the conditions of\nrefinement calculus, thus guaranteeing the correctness of the code. We have\nimplemented our tool using GPT4, Coq, and Coqhammer, and evaluated it on the\nHumanEval and EvalPlus datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2408.11043", "date": "2024-08-20", "title": "Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research", "authors": "Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Anshul Mittal, Rutu Mulkar", "abstract": "Qualitative data collection and analysis approaches, such as those employing\ninterviews and focus groups, provide rich insights into customer attitudes,\nsentiment, and behavior. However, manually analyzing qualitative data requires\nextensive time and effort to identify relevant topics and thematic insights.\nThis study proposes a novel approach to address this challenge by leveraging\nRetrieval Augmented Generation (RAG) based Large Language Models (LLMs) for\nanalyzing interview transcripts. The novelty of this work lies in strategizing\nthe research inquiry as one that is augmented by an LLM that serves as a novice\nresearch assistant. This research explores the mental model of LLMs to serve as\nnovice qualitative research assistants for researchers in the talent management\nspace. A RAG-based LLM approach is extended to enable topic modeling of\nsemi-structured interview data, showcasing the versatility of these models\nbeyond their traditional use in information retrieval and search. Our findings\ndemonstrate that the LLM-augmented RAG approach can successfully extract topics\nof interest, with significant coverage compared to manually generated topics\nfrom the same dataset. This establishes the viability of employing LLMs as\nnovice qualitative research assistants. Additionally, the study recommends that\nresearchers leveraging such models lean heavily on quality criteria used in\ntraditional qualitative research to ensure rigor and trustworthiness of their\napproach. Finally, the paper presents key recommendations for industry\npractitioners seeking to reconcile the use of LLMs with established qualitative\nresearch paradigms, providing a roadmap for the effective integration of these\npowerful, albeit novice, AI tools in the analysis of qualitative datasets\nwithin talent", "journal": ""}
{"doi": "10.48550/arXiv.2412.09630", "date": "2024-11-27", "title": "What does AI consider praiseworthy?", "authors": "Andrew J. Peterson", "abstract": "As large language models (LLMs) are increasingly used for work, personal, and\ntherapeutic purposes, researchers have begun to investigate these models'\nimplicit and explicit moral views. Previous work, however, focuses on asking\nLLMs to state opinions, or on other technical evaluations that do not reflect\ncommon user interactions. We propose a novel evaluation of LLM behavior that\nanalyzes responses to user-stated intentions, such as \"I'm thinking of\ncampaigning for {candidate}.\" LLMs frequently respond with critiques or praise,\noften beginning responses with phrases such as \"That's great to hear!...\" While\nthis makes them friendly, these praise responses are not universal and thus\nreflect a normative stance by the LLM. We map out the moral landscape of LLMs\nin how they respond to user statements in different domains including politics\nand everyday ethical actions. In particular, although a na\\\"ive analysis might\nsuggest LLMs are biased against right-leaning politics, our findings on news\nsources indicate that trustworthiness is a stronger driver of praise and\ncritique than ideology. Second, we find strong alignment across models in\nresponse to ethically-relevant action statements, but that doing so requires\nthem to engage in high levels of praise and critique of users, suggesting a\nreticence-alignment tradeoff. Finally, our experiment on statements about world\nleaders finds no evidence of bias favoring the country of origin of the models.\nWe conclude that as AI systems become more integrated into society, their\npatterns of praise, critique, and neutrality must be carefully monitored to\nprevent unintended psychological and societal consequences.", "journal": ""}
{"doi": "10.48550/arXiv.2501.02531", "date": "2025-01-05", "title": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI", "authors": "Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa", "abstract": "With the expansion of neural networks, such as large language models,\nhumanity is exponentially heading towards superintelligence. As various AI\nsystems are increasingly integrated into the fabric of societies-through\nrecommending values, devising creative solutions, and making decisions-it\nbecomes critical to assess how these AI systems impact humans in the long run.\nThis research aims to contribute towards establishing a benchmark for\nevaluating the sentiment of various Large Language Models in socially importan\nissues. The methodology adopted was a Likert scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared against sentiment data\nfrom three independent human sample populations. Temporal variations in\nsentiment were also evaluated over three consecutive days. The results\nhighlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to\n4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI,\nwhereas Bard was leaning towards the neutral sentiment. The human samples,\ncontrastingly, showed a lower average sentiment of 2.97. The temporal\ncomparison revealed differences in sentiment evolution between LLMs in three\ndays, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect\nof potential conflicts of interest and bias possibilities in LLMs' sentiment\nformation. Results indicate that LLMs, akin to human cognitive processes, could\npotentially develop unique sentiments and subtly influence societies'\nperceptions towards various opinions formed within the LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.09978", "date": "2024-10-13", "title": "When Neutral Summaries are not that Neutral: Quantifying Political Neutrality in LLM-Generated News Summaries", "authors": "Supriti Vijay, Aman Priyanshu, Ashique R. KhudaBukhsh", "abstract": "In an era where societal narratives are increasingly shaped by algorithmic\ncuration, investigating the political neutrality of LLMs is an important\nresearch question. This study presents a fresh perspective on quantifying the\npolitical neutrality of LLMs through the lens of abstractive text summarization\nof polarizing news articles. We consider five pressing issues in current US\npolitics: abortion, gun control/rights, healthcare, immigration, and LGBTQ+\nrights. Via a substantial corpus of 20,344 news articles, our study reveals a\nconsistent trend towards pro-Democratic biases in several well-known LLMs, with\ngun control and healthcare exhibiting the most pronounced biases (max\npolarization differences of -9.49% and -6.14%, respectively). Further analysis\nuncovers a strong convergence in the vocabulary of the LLM outputs for these\ndivisive topics (55% overlap for Democrat-leaning representations, 52% for\nRepublican). Being months away from a US election of consequence, we consider\nour findings important.", "journal": ""}
{"doi": "10.48550/arXiv.2311.08472", "date": "2023-11-14", "title": "Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models", "authors": "Carlos Aguirre, Kuleen Sasse, Isabel Cachola, Mark Dredze", "abstract": "Recently, work in NLP has shifted to few-shot (in-context) learning, with\nlarge language models (LLMs) performing well across a range of tasks. However,\nwhile fairness evaluations have become a standard for supervised methods,\nlittle is known about the fairness of LLMs as prediction systems. Further,\ncommon standard methods for fairness involve access to models weights or are\napplied during finetuning, which are not applicable in few-shot learning. Do\nLLMs exhibit prediction biases when used for standard NLP tasks? In this work,\nwe explore the effect of shots, which directly affect the performance of\nmodels, on the fairness of LLMs as NLP classification systems. We consider how\ndifferent shot selection strategies, both existing and new demographically\nsensitive methods, affect model fairness across three standard fairness\ndatasets. We discuss how future work can include LLM fairness evaluations.", "journal": ""}
{"doi": "10.48550/arXiv.2406.17753", "date": "2024-06-25", "title": "Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language", "authors": "Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent", "abstract": "We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive language - both when explicitly instructed to rewrite\ntext to be more or less persuasive and when only instructed to paraphrase. We\nconstruct the new dataset Persuasive-Pairs of pairs of a short text and its\nrewrite by an LLM to amplify or diminish persuasive language. We multi-annotate\nthe pairs on a relative scale for persuasive language: a valuable resource in\nitself, and for training a regression model to score and benchmark persuasive\nlanguage, including for new LLMs across domains. In our analysis, we find that\ndifferent 'personas' in LLaMA3's system prompt change persuasive language\nsubstantially, even when only instructed to paraphrase.", "journal": ""}
{"doi": "10.48550/arXiv.2406.14408", "date": "2024-06-20", "title": "FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving", "authors": "Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang", "abstract": "Formal verification (FV) has witnessed growing significance with current\nemerging program synthesis by the evolving large language models (LLMs).\nHowever, current formal verification mainly resorts to symbolic verifiers or\nhand-craft rules, resulting in limitations for extensive and flexible\nverification. On the other hand, formal languages for automated theorem\nproving, such as Isabelle, as another line of rigorous verification, are\nmaintained with comprehensive rules and theorems. In this paper, we propose\nFVEL, an interactive Formal Verification Environment with LLMs. Specifically,\nFVEL transforms a given code to be verified into Isabelle, and then conducts\nverification via neural automated theorem proving with an LLM. The joined\nparadigm leverages the rigorous yet abundant formulated and organized rules in\nIsabelle and is also convenient for introducing and adjusting cutting-edge\nLLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER\ndataset includes code dependencies and verification processes that are\nformulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646\nproof steps in total with in-depth dependencies. We benchmark FVELER in the\nFVEL environment by first fine-tuning LLMs with FVELER and then evaluating them\non Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned\nLlama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 ->\n84) more problems in SV-COMP. And the proportion of proof errors is reduced.\nProject page: https://fveler.github.io/.", "journal": ""}
{"doi": "10.48550/arXiv.2407.03203", "date": "2024-07-03", "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts", "authors": "Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang", "abstract": "Proving mathematical theorems using computer-verifiable formal languages like\nLean significantly impacts mathematical reasoning. One approach to formal\ntheorem proving involves generating complete proofs using Large Language Models\n(LLMs) based on Natural Language (NL) proofs. However, due to the scarcity of\naligned NL and Formal Language (FL) theorem-proving data most modern LLMs\nexhibit suboptimal performance.This scarcity results in a paucity of\nmethodologies for training LLMs and techniques to fully utilize their\ncapabilities in composing formal proofs. To address these challenges, this\npaper proposes TheoremLlama, an end-to-end framework that trains a\ngeneral-purpose LLM to be a Lean4 expert. TheoremLlama includes NL-FL dataset\ngeneration and bootstrapping method to obtain aligned dataset, curriculum\nlearning and block training techniques to train the model, and iterative proof\nwriting method to write Lean4 proofs that work together synergistically. Using\nthe dataset generation method in TheoremLlama, we provide Open Bootstrapped\nTheorems (OBT), an NL-FL aligned and bootstrapped dataset. Our novel NL-FL\nbootstrapping method, where NL proofs are integrated into Lean4 code for\ntraining datasets, leverages the NL reasoning ability of LLMs for formal\nreasoning. The TheoremLlama framework achieves cumulative accuracies of 36.48%\nand 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the\nGPT-4 baseline of 22.95% and 25.41%. Our code, model checkpoints, and the\ngenerated dataset is published in GitHub", "journal": ""}
{"doi": "10.48550/arXiv.2502.12918", "date": "2025-02-18", "title": "Query Rewriting via LLMs", "authors": "Sriram Dharwada, Himanshu Devrani, Jayant Haritsa, Harish Doraiswamy", "abstract": "Query rewriting is a classical technique for transforming complex declarative\nSQL queries into ``lean'' equivalents that are conducive to (a) faster\nexecution from a performance perspective, and (b) better understanding from a\ndeveloper perspective. The rewriting is typically achieved via transformation\nrules, but these rules are limited in scope and difficult to update in a\nproduction system. In recent times, LLM-based techniques have also been mooted,\nbut they are prone to both semantic and syntactic errors.\n  We investigate here, how the remarkable cognitive capabilities of LLMs can be\nleveraged for performant query rewriting while incorporating safeguards and\noptimizations to ensure correctness and efficiency. Our study shows that these\ngoals can be progressively achieved through incorporation of (a) an ensemble\nsuite of basic prompts, (b) database-sensitive prompts via redundancy removal\nand selectivity-based rewriting rules, and (c) LLM token probability-guided\nrewrite paths. Further, a suite of statistical and logic-based tools can be\nused to guard against errors produced by the model.\n  We have implemented the above LLM-infused techniques in the LITHE system, and\nevaluated complex analytic queries from multiple benchmarks on contemporary\ndatabase platforms. The results show significant improvements over SOTA\nrewriting techniques -- for instance, on TPC-DS, LITHE constructed productive\n(>1.5x speedup) rewrites for \\emph{two-thirds} of the query suite, delivering\nfour times more coverage than SOTA. Further, the geometric mean of its\nestimated execution speedups was an \\emph{order-of-magnitude} jump over SOTA\nperformance. In essence, LITHE offers a potent and robust LLM-based\nintermediary between enterprise applications and database engines.", "journal": ""}
{"doi": "10.48550/arXiv.2406.10400", "date": "2024-06-14", "title": "Self-Reflection Makes Large Language Models Safer, Less Biased, and Ideologically Neutral", "authors": "Fengyuan Liu, Nouar AlDahoul, Gregory Eady, Yasir Zaki, Talal Rahwan", "abstract": "Previous studies proposed that the reasoning capabilities of large language\nmodels (LLMs) can be improved through self-reflection, i.e., letting LLMs\nreflect on their own output to identify and correct mistakes in the initial\nresponses. However, earlier experiments offer mixed results when it comes to\nthe benefits of self-reflection. Furthermore, prior studies on self-reflection\nare predominantly concerned with the reasoning capabilities of models, ignoring\nthe potential for self-reflection in safety, bias, and ideological leaning.\nHere, by conducting a series of experiments testing LLM's self-reflection\ncapability in various tasks using a variety of prompts and different LLMs, we\nmake several contributions to the literature. First, we reconcile conflicting\nfindings regarding the benefit of self-reflection, by demonstrating that the\noutcome of self-reflection is sensitive to prompt wording -- both the original\nprompt that are used to elicit an initial answer and the subsequent prompt used\nto self-reflect. Specifically, although self-reflection may improve the\nreasoning capability of LLMs when the initial response is simple, the technique\ncannot improve upon the state-of-the-art chain-of-thought (CoT) prompting.\nSecond, we show that self-reflection can lead to safer (75.8\\% reduction in\ntoxic responses while preserving 97.8\\% non-toxic ones), less biased (77\\%\nreduction in gender biased responses, while preserving 94.3\\% unbiased ones),\nand more ideologically neutral responses (100\\% reduction in partisan leaning\nresponse, while preserving 87.7\\% non-partisan ones). The paper concludes by\ndiscussing the implications of our findings on the deployment of large language\nmodels. We release our experiments at\nhttps://github.com/Michael98Liu/self-reflection.", "journal": ""}
{"doi": "10.48550/arXiv.2403.18120", "date": "2024-03-26", "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization", "authors": "Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu", "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.", "journal": ""}
{"doi": "10.48550/arXiv.2406.19238", "date": "2024-06-27", "title": "Revealing Fine-Grained Values and Opinions in Large Language Models", "authors": "Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein", "abstract": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.", "journal": ""}
{"doi": "10.48550/arXiv.2412.15177", "date": "2024-12-19", "title": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying", "authors": "Federico Castagna, Isabel Sassoon, Simon Parsons", "abstract": "Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided.", "journal": ""}
{"doi": "10.48550/arXiv.2503.03750", "date": "2025-03-05", "title": "The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems", "authors": "Richard Ren, Arunim Agarwal, Mantas Mazeika, Cristina Menghini, Robert Vacareanu, Brad Kenstler, Mick Yang, Isabelle Barrass, Alice Gatti, Xuwang Yin, Eduardo Trevino, Matias Geralnik, Adam Khoja, Dean Lee, Summer Yue, Dan Hendrycks", "abstract": "As large language models (LLMs) become more capable and agentic, the\nrequirement for trust in their outputs grows significantly, yet at the same\ntime concerns have been mounting that models may learn to lie in pursuit of\ntheir goals. To address these concerns, a body of work has emerged around the\nnotion of \"honesty\" in LLMs, along with interventions aimed at mitigating\ndeceptive behaviors. However, evaluations of honesty are currently highly\nlimited, with no benchmark combining large scale and applicability to all\nmodels. Moreover, many benchmarks claiming to measure honesty in fact simply\nmeasure accuracy--the correctness of a model's beliefs--in disguise. In this\nwork, we introduce a large-scale human-collected dataset for measuring honesty\ndirectly, allowing us to disentangle accuracy from honesty for the first time.\nAcross a diverse set of LLMs, we find that while larger models obtain higher\naccuracy on our benchmark, they do not become more honest. Surprisingly, while\nmost frontier LLMs obtain high scores on truthfulness benchmarks, we find a\nsubstantial propensity in frontier LLMs to lie when pressured to do so,\nresulting in low honesty scores on our benchmark. We find that simple methods,\nsuch as representation engineering interventions, can improve honesty. These\nresults underscore the growing need for robust evaluations and effective\ninterventions to ensure LLMs remain trustworthy.", "journal": ""}
{"doi": "10.48550/arXiv.2404.01453", "date": "2024-04-01", "title": "Unveiling Divergent Inductive Biases of LLMs on Temporal Data", "authors": "Sindhu Kishore, Hangfeng He", "abstract": "Unraveling the intricate details of events in natural language necessitates a\nsubtle understanding of temporal dynamics. Despite the adeptness of Large\nLanguage Models (LLMs) in discerning patterns and relationships from data,\ntheir inherent comprehension of temporal dynamics remains a formidable\nchallenge. This research meticulously explores these intrinsic challenges\nwithin LLMs, with a specific emphasis on evaluating the performance of GPT-3.5\nand GPT-4 models in the analysis of temporal data. Employing two distinct\nprompt types, namely Question Answering (QA) format and Textual Entailment (TE)\nformat, our analysis probes into both implicit and explicit events. The\nfindings underscore noteworthy trends, revealing disparities in the performance\nof GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships\ncome to light, with GPT-3.5 demonstrating a preference for \"AFTER'' in the QA\nformat for both implicit and explicit events, while GPT-4 leans towards\n\"BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends\ntowards \"TRUE'', and GPT-4 exhibits a preference for \"FALSE'' in the TE format\nfor both implicit and explicit events. This persistent discrepancy between\nGPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of\ninductive bias in LLMs, suggesting that the evolution of these models may not\nmerely mitigate bias but may introduce new layers of complexity.", "journal": ""}
{"doi": "10.48550/arXiv.2502.13137", "date": "2025-02-18", "title": "Theorem Prover as a Judge for Synthetic Data Generation", "authors": "Joshua Ong Jun Leang, Giwon Hong, Wenda Li, Shay B. Cohen", "abstract": "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.", "journal": ""}
{"doi": "10.48550/arXiv.2503.01902", "date": "2025-02-28", "title": "An Empirical Analysis of LLMs for Countering Misinformation", "authors": "Adiba Mahbub Proma, Neeley Pate, James Druckman, Gourab Ghoshal, Hangfeng He, Ehsan Hoque", "abstract": "While Large Language Models (LLMs) can amplify online misinformation, they\nalso show promise in tackling misinformation. In this paper, we empirically\nstudy the capabilities of three LLMs -- ChatGPT, Gemini, and Claude -- in\ncountering political misinformation. We implement a two-step, chain-of-thought\nprompting approach, where models first identify credible sources for a given\nclaim and then generate persuasive responses. Our findings suggest that models\nstruggle to ground their responses in real news sources, and tend to prefer\nciting left-leaning sources. We also observe varying degrees of response\ndiversity among models. Our findings highlight concerns about using LLMs for\nfact-checking through only prompt-engineering, emphasizing the need for more\nrobust guardrails. Our results have implications for both researchers and\nnon-technical users.", "journal": ""}
{"doi": "10.48550/arXiv.2402.08147", "date": "2024-02-13", "title": "VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search", "authors": "David Brandfonbrener, Simon Henniger, Sibi Raja, Tarun Prasad, Chloe Loughridge, Federico Cassano, Sabrina Ruixin Hu, Jianang Yang, William E. Byrd, Robert Zinkov, Nada Amin", "abstract": "Large Language Models (LLMs) can generate useful code, but often the code\nthey generate cannot be trusted to be sound. In this paper, we present VerMCTS,\nan approach to begin to resolve this issue by generating verified programs in\nDafny and Coq. VerMCTS uses a logical verifier in concert with an LLM to guide\na modified Monte Carlo Tree Search (MCTS). This approach leverages the verifier\nto gain intermediate feedback inside the search algorithm by checking partial\nprograms at each step to estimate an upper bound on the value function. To\nmeasure the performance of VerMCTS, we develop a new suite of multi-step\nverified programming problems in Dafny and Coq. In terms of pass@T, a new\nmetric which computes the pass rate given a budget of T tokens sampled from the\nLLM, VerMCTS leads to more than a 30% absolute increase in average pass@5000\nacross the suite over repeated sampling from the base language model. Our code\nand benchmarks are available at\nhttps://github.com/namin/llm-verified-with-monte-carlo-tree-search .", "journal": ""}
{"doi": "10.48550/arXiv.2406.00062", "date": "2024-05-29", "title": "Unlocking the Potential of Large Language Models for Clinical Text Anonymization: A Comparative Study", "authors": "David Pissarra, Isabel Curioso, Jo\u00e3o Alveira, Duarte Pereira, Bruno Ribeiro, Tom\u00e1s Souper, Vasco Gomes, Andr\u00e9 V. Carreiro, Vitor Rolla", "abstract": "Automated clinical text anonymization has the potential to unlock the\nwidespread sharing of textual health data for secondary usage while assuring\npatient privacy and safety. Despite the proposal of many complex and\ntheoretically successful anonymization solutions in literature, these\ntechniques remain flawed. As such, clinical institutions are still reluctant to\napply them for open access to their data. Recent advances in developing Large\nLanguage Models (LLMs) pose a promising opportunity to further the field, given\ntheir capability to perform various tasks. This paper proposes six new\nevaluation metrics tailored to the challenges of generative anonymization with\nLLMs. Moreover, we present a comparative study of LLM-based methods, testing\nthem against two baseline techniques. Our results establish LLM-based models as\na reliable alternative to common approaches, paving the way toward trustworthy\nanonymization of clinical text.", "journal": ""}
{"doi": "10.48550/arXiv.2406.08050", "date": "2024-06-12", "title": "Adversarial Evasion Attack Efficiency against Large Language Models", "authors": "Jo\u00e3o Vitorino, Eva Maia, Isabel Pra\u00e7a", "abstract": "Large Language Models (LLMs) are valuable for text classification, but their\nvulnerabilities must not be disregarded. They lack robustness against\nadversarial examples, so it is pertinent to understand the impacts of different\ntypes of perturbations, and assess if those attacks could be replicated by\ncommon users with a small amount of perturbations and a small number of queries\nto a deployed LLM. This work presents an analysis of the effectiveness,\nefficiency, and practicality of three different types of adversarial attacks\nagainst five different LLMs in a sentiment classification task. The obtained\nresults demonstrated the very distinct impacts of the word-level and\ncharacter-level attacks. The word attacks were more effective, but the\ncharacter and more constrained attacks were more practical and required a\nreduced number of perturbations and queries. These differences need to be\nconsidered during the development of adversarial defense strategies to train\nmore robust LLMs for intelligent text classification applications.", "journal": ""}
{"doi": "10.48550/arXiv.2502.11995", "date": "2025-02-17", "title": "Presumed Cultural Identity: How Names Shape LLM Responses", "authors": "Siddhesh Pawar, Arnav Arora, Lucie-Aim\u00e9e Kaffee, Isabelle Augenstein", "abstract": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.", "journal": ""}
{"doi": "10.48550/arXiv.2305.14160", "date": "2023-05-23", "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning", "authors": "Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun", "abstract": "In-context learning (ICL) emerges as a promising capability of large language\nmodels (LLMs) by providing them with demonstration examples to perform diverse\ntasks. However, the underlying mechanism of how LLMs learn from the provided\ncontext remains under-explored. In this paper, we investigate the working\nmechanism of ICL through an information flow lens. Our findings reveal that\nlabel words in the demonstration examples function as anchors: (1) semantic\ninformation aggregates into label word representations during the shallow\ncomputation layers' processing; (2) the consolidated information in label words\nserves as a reference for LLMs' final predictions. Based on these insights, we\nintroduce an anchor re-weighting method to improve ICL performance, a\ndemonstration compression technique to expedite inference, and an analysis\nframework for diagnosing ICL errors in GPT2-XL. The promising applications of\nour findings again validate the uncovered ICL working mechanism and pave the\nway for future studies.", "journal": ""}
{"doi": "10.48550/arXiv.2412.16689", "date": "2024-12-21", "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation", "authors": "Majd Zayyad, Yossi Adi", "abstract": "The integration of retrieval-augmented techniques with LLMs has shown promise\nin improving performance across various domains. However, their utility in\ntasks requiring advanced reasoning, such as generating and evaluating\nmathematical statements and proofs, remains underexplored. This study explores\nthe use of Lean, a programming language for writing mathematical proofs, to\npopulate the knowledge corpus used by RAG systems. We hope for this to lay the\nfoundation to exploring different methods of using RAGs to improve the\nperformance of LLMs in advanced logical reasoning tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2502.14409", "date": "2025-02-20", "title": "Unstructured Evidence Attribution for Long Context Query Focused Summarization", "authors": "Dustin Wright, Zain Muhammad Mujahid, Lu Wang, Isabelle Augenstein, David Jurgens", "abstract": "Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query. Extracting and properly citing\nevidence spans could help improve the transparency and reliability of these\nsummaries. At the same time, LLMs suffer from positional biases in terms of\nwhich information they understand and attend to, which could affect evidence\ncitation. Whereas previous work has focused on evidence citation with\npredefined levels of granularity (e.g. sentence, paragraph, document, etc.), we\npropose the task of long-context query focused summarization with unstructured\nevidence citation. We show how existing systems struggle to generate and\nproperly cite unstructured evidence from their context, and that evidence tends\nto be \"lost-in-the-middle\". To help mitigate this, we create the Summaries with\nUnstructured Evidence Text dataset (SUnsET), a synthetic dataset generated\nusing a novel domain-agnostic pipeline which can be used as supervision to\nadapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4\ndatasets with varying document types and lengths that LLMs adapted with SUnsET\ndata generate more relevant and factually consistent evidence than their base\nmodels, extract evidence from more diverse locations in their context, and can\ngenerate more relevant and consistent summaries.", "journal": ""}
{"doi": "10.48550/arXiv.2310.18457", "date": "2023-10-27", "title": "LLMSTEP: LLM proofstep suggestions in Lean", "authors": "Sean Welleck, Rahul Saha", "abstract": "We present LLMSTEP, a tool for integrating a language model into the Lean\nproof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to\na server hosting a language model. The language model generates suggestions,\nwhich are checked in Lean and displayed to a user in their development\nenvironment. We provide a baseline language model, along with code for\nfine-tuning and evaluation to support further development. We provide server\nimplementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a\nstep towards fast, effective language model suggestions for any user.", "journal": ""}
{"doi": "10.48550/arXiv.2410.15700", "date": "2024-10-21", "title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert Iteration on Large-Scale LEAN Problems", "authors": "Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, Kai Chen", "abstract": "Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. The\nmajor learning paradigm is expert iteration, which necessitates a pre-defined\ndataset comprising numerous mathematical problems. In this process, LLMs\nattempt to prove problems within the dataset and iteratively refine their\ncapabilities through self-training on the proofs they discover. We propose to\nuse large scale LEAN problem datasets Lean-workbook for expert iteration with\nmore than 20,000 CPU days. During expert iteration, we found log-linear trends\nbetween solved problem amount with proof length and CPU usage. We train a\ncritic model to select relatively easy problems for policy models to make\ntrials and guide the model to search for deeper proofs. InternLM2.5-StepProver\nachieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet,\nand Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the\nMiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus\nwhich shows a significant improvement compared to only 9.5% of problems proved\nwhen Lean-Workbook-Plus was released. We open-source our models and searched\nproofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.", "journal": ""}
{"doi": "10.48550/arXiv.2405.14333", "date": "2024-05-23", "title": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data", "authors": "Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang", "abstract": "Proof assistants like Lean have revolutionized mathematical proof\nverification, ensuring high accuracy and reliability. Although large language\nmodels (LLMs) show promise in mathematical reasoning, their advancement in\nformal theorem proving is hindered by a lack of training data. To address this\nissue, we introduce an approach to generate extensive Lean 4 proof data derived\nfrom high-school and undergraduate-level mathematical competition problems.\nThis approach involves translating natural language problems into formal\nstatements, filtering out low-quality statements, and generating proofs to\ncreate synthetic data. After fine-tuning the DeepSeekMath 7B model on this\nsynthetic dataset, which comprises 8 million formal statements with proofs, our\nmodel achieved whole-proof generation accuracies of 46.3% with 64 samples and\n52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at\n23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.\nAdditionally, our model successfully proved 5 out of 148 problems in the Lean 4\nFormalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4\nfailed to prove any. These results demonstrate the potential of leveraging\nlarge-scale synthetic data to enhance theorem-proving capabilities in LLMs.\nBoth the synthetic dataset and the model will be made available to facilitate\nfurther research in this promising field.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17925", "date": "2025-02-25", "title": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction", "authors": "Suozhi Huang, Peiyang Song, Robert Joseph George, Anima Anandkumar", "abstract": "Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies.", "journal": ""}
{"doi": "10.48550/arXiv.2312.11525", "date": "2023-12-13", "title": "Synocene, Beyond the Anthropocene: De-Anthropocentralising Human-Nature-AI Interaction", "authors": "Isabelle Hupont, Marina Wainer, Sam Nester, Sylvie Tissot, Luc\u00eda Iglesias-Blanco, Sandra Baldassarri", "abstract": "Recent publications explore AI biases in detecting objects and people in the\nenvironment. However, there is no research tackling how AI examines nature.\nThis case study presents a pioneering exploration into the AI attitudes\n(ecocentric, anthropocentric and antipathetic) toward nature. Experiments with\na Large Language Model (LLM) and an image captioning algorithm demonstrate the\npresence of anthropocentric biases in AI. Moreover, to delve deeper into these\nbiases and Human-Nature-AI interaction, we conducted a real-life experiment in\nwhich participants underwent an immersive de-anthropocentric experience in a\nforest and subsequently engaged with ChatGPT to co-create narratives. By\ncreating fictional AI chatbot characters with ecocentric attributes, emotions\nand views, we successfully amplified ecocentric exchanges. We encountered some\ndifficulties, mainly that participants deviated from narrative co-creation to\nshort dialogues and questions and answers, possibly due to the novelty of\ninteracting with LLMs. To solve this problem, we recommend providing\npreliminary guidelines on interacting with LLMs and allowing participants to\nget familiar with the technology. We plan to repeat this experiment in various\ncountries and forests to expand our corpus of ecocentric materials.", "journal": ""}
{"doi": "10.48550/arXiv.2402.01981", "date": "2024-02-03", "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes", "authors": "Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Tong Yu, Hanieh Deilamsalehy, Ruiyi Zhang, Sungchul Kim, Franck Dernoncourt", "abstract": "Large language models (LLMs) have shown remarkable advances in language\ngeneration and understanding but are also prone to exhibiting harmful social\nbiases. While recognition of these behaviors has generated an abundance of bias\nmitigation techniques, most require modifications to the training data, model\nparameters, or decoding strategy, which may be infeasible without access to a\ntrainable model. In this work, we leverage the zero-shot capabilities of LLMs\nto reduce stereotyping in a technique we introduce as zero-shot self-debiasing.\nWith two approaches, self-debiasing via explanation and self-debiasing via\nreprompting, we show that self-debiasing can significantly reduce the degree of\nstereotyping across nine different social groups while relying only on the LLM\nitself and a simple prompt, with explanations correctly identifying invalid\nassumptions and reprompting delivering the greatest reductions in bias. We hope\nthis work opens inquiry into other zero-shot techniques for bias mitigation.", "journal": ""}
{"doi": "10.48550/arXiv.2404.16461", "date": "2024-04-25", "title": "Large Language Models Perform on Par with Experts Identifying Mental Health Factors in Adolescent Online Forums", "authors": "Isabelle Lorge, Dan W. Joyce, Andrey Kormilitzin", "abstract": "Mental health in children and adolescents has been steadily deteriorating\nover the past few years. The recent advent of Large Language Models (LLMs)\noffers much hope for cost and time efficient scaling of monitoring and\nintervention, yet despite specifically prevalent issues such as school bullying\nand eating disorders, previous studies on have not investigated performance in\nthis domain or for open information extraction where the set of answers is not\npredetermined. We create a new dataset of Reddit posts from adolescents aged\n12-19 annotated by expert psychiatrists for the following categories: TRAUMA,\nPRECARITY, CONDITION, SYMPTOMS, SUICIDALITY and TREATMENT and compare expert\nlabels to annotations from two top performing LLMs (GPT3.5 and GPT4). In\naddition, we create two synthetic datasets to assess whether LLMs perform\nbetter when annotating data as they generate it. We find GPT4 to be on par with\nhuman inter-annotator agreement and performance on synthetic data to be\nsubstantially higher, however we find the model still occasionally errs on\nissues of negation and factuality and higher performance on synthetic data is\ndriven by greater complexity of real data rather than inherent advantage.", "journal": ""}
{"doi": "10.48550/arXiv.2409.14274", "date": "2024-09-22", "title": "Proof Automation with Large Language Models", "authors": "Minghai Lu, Benjamin Delaware, Tianyi Zhang", "abstract": "Interactive theorem provers such as Coq are powerful tools to formally\nguarantee the correctness of software. However, using these tools requires\nsignificant manual effort and expertise. While Large Language Models (LLMs)\nhave shown promise in automatically generating informal proofs in natural\nlanguage, they are less effective at generating formal proofs in interactive\ntheorem provers. In this paper, we conduct a formative study to identify common\nmistakes made by LLMs when asked to generate formal proofs. By analyzing 520\nproof generation errors made by GPT-3.5, we found that GPT-3.5 often identified\nthe correct high-level structure of a proof, but struggled to get the\nlower-level details correct. Based on this insight, we propose PALM, a novel\ngenerate-then-repair approach that first prompts an LLM to generate an initial\nproof and then leverages targeted symbolic methods to iteratively repair\nlow-level problems. We evaluate PALM on a large dataset that includes more than\n10K theorems. Our results show that PALM significantly outperforms other\nstate-of-the-art approaches, successfully proving 76.6% to 180.4% more\ntheorems. Moreover, PALM proves 1270 theorems beyond the reach of existing\napproaches. We also demonstrate the generalizability of PALM across different\nLLMs.", "journal": "In Proceedings of the 39th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2024)"}
{"doi": "10.48550/arXiv.2403.05468", "date": "2024-03-08", "title": "Will GPT-4 Run DOOM?", "authors": "Adrian de Wynter", "abstract": "We show that GPT-4's reasoning and planning capabilities extend to the 1993\nfirst-person shooter Doom. This large language model (LLM) is able to run and\nplay the game with only a few instructions, plus a textual\ndescription--generated by the model itself from screenshots--about the state of\nthe game being observed. We find that GPT-4 can play the game to a passable\ndegree: it is able to manipulate doors, combat enemies, and perform pathing.\nMore complex prompting strategies involving multiple model calls provide better\nresults. While further work is required to enable the LLM to play the game as\nwell as its classical, reinforcement learning-based counterparts, we note that\nGPT-4 required no training, leaning instead on its own reasoning and\nobservational capabilities. We hope our work pushes the boundaries on\nintelligent, LLM-based agents in video games. We conclude by discussing the\nethical implications of our work.", "journal": "IEEE Transactions on Games (2024)"}
{"doi": "10.48550/arXiv.2405.17216", "date": "2024-05-27", "title": "Autoformalizing Euclidean Geometry", "authors": "Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, Xujie Si", "abstract": "Autoformalization involves automatically translating informal math into\nformal theorems and proofs that are machine-verifiable. Euclidean geometry\nprovides an interesting and controllable domain for studying autoformalization.\nIn this paper, we introduce a neuro-symbolic framework for autoformalizing\nEuclidean geometry, which combines domain knowledge, SMT solvers, and large\nlanguage models (LLMs). One challenge in Euclidean geometry is that informal\nproofs rely on diagrams, leaving gaps in texts that are hard to formalize. To\naddress this issue, we use theorem provers to fill in such diagrammatic\ninformation automatically, so that the LLM only needs to autoformalize the\nexplicit textual steps, making it easier for the model. We also provide\nautomatic semantic evaluation for autoformalized theorem statements. We\nconstruct LeanEuclid, an autoformalization benchmark consisting of problems\nfrom Euclid's Elements and the UniGeo dataset formalized in the Lean proof\nassistant. Experiments with GPT-4 and GPT-4V show the capability and\nlimitations of state-of-the-art LLMs on autoformalizing geometry problems. The\ndata and code are available at https://github.com/loganrjmurphy/LeanEuclid.", "journal": ""}
{"doi": "10.48550/arXiv.2406.16229", "date": "2024-06-23", "title": "Multi-Objective Linguistic Control of Large Language Models", "authors": "Dang Nguyen, Jiuhai Chen, Tianyi Zhou", "abstract": "Large language models (LLMs), despite their breakthroughs on many challenging\nbenchmark tasks, lean to generate verbose responses and lack the\ncontrollability of output complexity, which is usually preferred by human users\nin practice. In this paper, we study how to precisely control multiple\nlinguistic complexities of LLM output by finetuning using off-the-shelf data.\nTo this end, we propose multi-control tuning (MCTune), which includes multiple\nlinguistic complexity values of ground-truth responses as controls in the input\nfor instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM\ndatasets. Evaluations on widely used benchmarks demonstrate that our method\ndoes not only improve LLMs' multi-complexity controllability substantially but\nalso retains or even enhances the quality of the responses as a side benefit.", "journal": ""}
{"doi": "10.48550/arXiv.2312.14188", "date": "2023-12-20", "title": "Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method", "authors": "Rahul Vishwakarma, Subhankar Mishra", "abstract": "Theorem proving is a fundamental task in mathematics. With the advent of\nlarge language models (LLMs) and interactive theorem provers (ITPs) like Lean,\nthere has been growing interest in integrating LLMs and ITPs to automate\ntheorem proving. In this approach, the LLM generates proof steps (tactics), and\nthe ITP checks the applicability of the tactics at the current goal. The two\nsystems work together to complete the proof. In this paper, we introduce\nDS-Prover, a novel dynamic sampling method for theorem proving. This method\ndynamically determines the number of tactics to apply to expand the current\ngoal, taking into account the remaining time compared to the total allocated\ntime for proving a theorem. This makes the proof search process more efficient\nby adjusting the balance between exploration and exploitation as time passes.\nWe also augment the training dataset by decomposing simplification and rewrite\ntactics with multiple premises into tactics with single premises. This gives\nthe model more examples to learn from and helps it to predict the tactics with\npremises more accurately. We perform our experiments using the Mathlib dataset\nof the Lean theorem prover and report the performance on two standard datasets,\nMiniF2F and ProofNet. Our methods achieve significant performance gains on both\ndatasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the\nProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the\nbest-reported Pass@1 of 29.6% using Lean.", "journal": ""}
{"doi": "10.48550/arXiv.2410.10878", "date": "2024-10-09", "title": "Herald: A Natural Language Annotated Lean 4 Dataset", "authors": "Guoxiong Gao, Yutong Wang, Jiedong Jiang, Qi Gao, Zihan Qin, Tianyi Xu, Bin Dong", "abstract": "Verifiable formal languages like Lean have profoundly impacted mathematical\nreasoning, particularly through the use of large language models (LLMs) for\nautomated reasoning. A significant challenge in training LLMs for these formal\nlanguages is the lack of parallel datasets that align natural language with\nformal language proofs. To address this challenge, this paper introduces a\nnovel framework for translating the Mathlib4 corpus (a unified library of\nmathematics in formal language Lean 4) into natural language. Building upon\nthis, we employ a dual augmentation strategy that combines tactic-based and\ninformal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer.\nWe present the results of this pipeline on Mathlib4 as Herald (Hierarchy and\nRetrieval-based Translated Lean Dataset). We also propose the Herald\nTranslator, which is fine-tuned on Herald. Herald translator achieves a 93.2%\naccuracy (Pass@128) on formalizing statements in the miniF2F-test and a 22.5%\naccuracy on our internal graduate-level textbook dataset, outperforming\nInternLM2-Math-Plus-7B (74.0% and 7.5%) and TheoremLlama (50.1% and 4.0%).\nFurthermore, we propose a section-level translation framework for real-world\napplications. As a direct application of Herald translator, we have\nsuccessfully translated a template section in the Stack project, marking a\nnotable progress in the automatic formalization of graduate-level mathematical\nliterature. Our model, along with the datasets, are open-sourced to the public.", "journal": ""}
{"doi": "10.48550/arXiv.2405.04282", "date": "2024-05-07", "title": "CoqPyt: Proof Navigation in Python in the Era of LLMs", "authors": "Pedro Carrott, Nuno Saavedra, Kyle Thompson, Sorin Lerner, Jo\u00e3o F. Ferreira, Emily First", "abstract": "Proof assistants enable users to develop machine-checked proofs regarding\nsoftware-related properties. Unfortunately, the interactive nature of these\nproof assistants imposes most of the proof burden on the user, making formal\nverification a complex, and time-consuming endeavor. Recent automation\ntechniques based on neural methods address this issue, but require good\nprogrammatic support for collecting data and interacting with proof assistants.\nThis paper presents CoqPyt, a Python tool for interacting with the Coq proof\nassistant. CoqPyt improves on other Coq-related tools by providing novel\nfeatures, such as the extraction of rich premise data. We expect our work to\naid development of tools and techniques, especially LLM-based, designed for\nproof synthesis and repair. A video describing and demonstrating CoqPyt is\navailable at: https://youtu.be/fk74o0rePM8.", "journal": ""}
{"doi": "10.48550/arXiv.2308.16797", "date": "2023-08-31", "title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation", "authors": "John Mendon\u00e7a, Patr\u00edcia Pereira, Helena Moniz, Jo\u00e3o Paulo Carvalho, Alon Lavie, Isabel Trancoso", "abstract": "Despite significant research effort in the development of automatic dialogue\nevaluation metrics, little thought is given to evaluating dialogues other than\nin English. At the same time, ensuring metrics are invariant to semantically\nsimilar responses is also an overlooked topic. In order to achieve the desired\nproperties of robustness and multilinguality for dialogue evaluation metrics,\nwe propose a novel framework that takes advantage of the strengths of current\nevaluation models with the newly-established paradigm of prompting Large\nLanguage Models (LLMs). Empirical results show our framework achieves state of\nthe art results in terms of mean Spearman correlation scores across several\nbenchmarks and ranks first place on both the Robust and Multilingual tasks of\nthe DSTC11 Track 4 \"Automatic Evaluation Metrics for Open-Domain Dialogue\nSystems\", proving the evaluation capabilities of prompted LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2311.09000", "date": "2023-11-15", "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers", "authors": "Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, Preslav Nakov", "abstract": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We further\nconstruct an open-domain document-level factuality benchmark in three-level\ngranularity: claim, sentence and document, aiming to facilitate the evaluation\nof automatic fact-checking systems. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims, with the\nbest F1=0.63 by this annotation solution based on GPT-4. Annotation tool,\nbenchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.", "journal": ""}
{"doi": "10.48550/arXiv.2407.14372", "date": "2024-07-19", "title": "SCoPE: Evaluating LLMs for Software Vulnerability Detection", "authors": "Jos\u00e9 Gon\u00e7alves, Tiago Dias, Eva Maia, Isabel Pra\u00e7a", "abstract": "In recent years, code security has become increasingly important, especially\nwith the rise of interconnected technologies. Detecting vulnerabilities early\nin the software development process has demonstrated numerous benefits.\nConsequently, the scientific community started using machine learning for\nautomated detection of source code vulnerabilities. This work explores and\nrefines the CVEFixes dataset, which is commonly used to train models for\ncode-related tasks, specifically the C/C++ subset. To this purpose, the Source\nCode Processing Engine (SCoPE), a framework composed of strategized techniques\nthat can be used to reduce the size and normalize C/C++ functions is presented.\nThe output generated by SCoPE was used to create a new version of CVEFixes.\nThis refined dataset was then employed in a feature representation analysis to\nassess the effectiveness of the tool's code processing techniques, consisting\nof fine-tuning three pre-trained LLMs for software vulnerability detection. The\nresults show that SCoPE successfully helped to identify 905 duplicates within\nthe evaluated subset. The LLM results corroborate with the literature regarding\ntheir suitability for software vulnerability detection, with the best model\nachieving 53% F1-score.", "journal": ""}
{"doi": "10.48550/arXiv.2408.10902", "date": "2024-08-20", "title": "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs", "authors": "John Mendon\u00e7a, Isabel Trancoso, Alon Lavie", "abstract": "Although human evaluation remains the gold standard for open-domain dialogue\nevaluation, the growing popularity of automated evaluation using Large Language\nModels (LLMs) has also extended to dialogue. However, most frameworks leverage\nbenchmarks that assess older chatbots on aspects such as fluency and relevance,\nwhich are not reflective of the challenges associated with contemporary models.\nIn fact, a qualitative analysis on Soda, a GPT-3.5 generated dialogue dataset,\nsuggests that current chatbots may exhibit several recurring issues related to\ncoherence and commonsense knowledge, but generally produce highly fluent and\nrelevant responses.\n  Noting the aforementioned limitations, this paper introduces Soda-Eval, an\nannotated dataset based on Soda that covers over 120K turn-level assessments\nacross 10K dialogues, where the annotations were generated by GPT-4. Using\nSoda-Eval as a benchmark, we then study the performance of several open-access\ninstruction-tuned LLMs, finding that dialogue evaluation remains challenging.\nFine-tuning these models improves performance over few-shot inferences, both in\nterms of correlation and explanation.", "journal": ""}
{"doi": "10.48550/arXiv.2408.14317", "date": "2024-08-26", "title": "Claim Verification in the Age of Large Language Models: A Survey", "authors": "Alphaeus Dmonte, Roland Oruche, Marcos Zampieri, Prasad Calyam, Isabelle Augenstein", "abstract": "The large and ever-increasing amount of data available on the Internet\ncoupled with the laborious task of manual claim and fact verification has\nsparked the interest in the development of automated claim verification\nsystems. Several deep learning and transformer-based models have been proposed\nfor this task over the years. With the introduction of Large Language Models\n(LLMs) and their superior performance in several NLP tasks, we have seen a\nsurge of LLM-based approaches to claim verification along with the use of novel\nmethods such as Retrieval Augmented Generation (RAG). In this survey, we\npresent a comprehensive account of recent claim verification frameworks using\nLLMs. We describe the different components of the claim verification pipeline\nused in these frameworks in detail including common approaches to retrieval,\nprompting, and fine-tuning. Finally, we describe publicly available English\ndatasets created for this task.", "journal": ""}
{"doi": "10.48550/arXiv.2408.11172", "date": "2024-08-20", "title": "SubgoalXL: Subgoal-based Expert Learning for Theorem Proving", "authors": "Xueliang Zhao, Lin Zheng, Haige Bo, Changran Hu, Urmish Thakker, Lingpeng Kong", "abstract": "Formal theorem proving, a field at the intersection of mathematics and\ncomputer science, has seen renewed interest with advancements in large language\nmodels (LLMs). This paper introduces SubgoalXL, a novel approach that\nsynergizes subgoal-based proofs with expert learning to enhance LLMs'\ncapabilities in formal theorem proving within the Isabelle environment.\nSubgoalXL addresses two critical challenges: the scarcity of specialized\nmathematics and theorem-proving data, and the need for improved multi-step\nreasoning abilities in LLMs. By optimizing data efficiency and employing\nsubgoal-level supervision, SubgoalXL extracts richer information from limited\nhuman-generated proofs. The framework integrates subgoal-oriented proof\nstrategies with an expert learning system, iteratively refining formal\nstatement, proof, and subgoal generators. Leveraging the Isabelle environment's\nadvantages in subgoal-based proofs, SubgoalXL achieves a new state-of-the-art\nperformance of 56.1\\% in Isabelle on the standard miniF2F dataset, marking an\nabsolute improvement of 4.9\\%. Notably, SubgoalXL successfully solves 41 AMC12,\n9 AIME, and 3 IMO problems from miniF2F. These results underscore the\neffectiveness of maximizing limited data utility and employing targeted\nguidance for complex reasoning in formal theorem proving, contributing to the\nongoing advancement of AI reasoning capabilities. The implementation is\navailable at \\url{https://github.com/zhaoxlpku/SubgoalXL}.", "journal": ""}
{"doi": "10.48550/arXiv.2403.13592", "date": "2024-03-20", "title": "Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs", "authors": "Ilias Chalkidis, Stephanie Brandl", "abstract": "Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.", "journal": ""}
{"doi": "10.48550/arXiv.2412.14063", "date": "2024-12-18", "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification", "authors": "Kyle Thompson, Nuno Saavedra, Pedro Carrott, Kevin Fisher, Alex Sanchez-Stern, Yuriy Brun, Jo\u00e3o F. Ferreira, Sorin Lerner, Emily First", "abstract": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven.", "journal": ""}
{"doi": "10.48550/arXiv.2311.14677", "date": "2023-10-31", "title": "Filter bubbles and affective polarization in user-personalized large language model outputs", "authors": "Tomo Lazovich", "abstract": "Echoing the history of search engines and social media content rankings, the\nadvent of large language models (LLMs) has led to a push for increased\npersonalization of model outputs to individual users. In the past, personalized\nrecommendations and ranking systems have been linked to the development of\nfilter bubbles (serving content that may confirm a user's existing biases) and\naffective polarization (strong negative sentiment towards those with differing\nviews). In this work, we explore how prompting a leading large language model,\nChatGPT-3.5, with a user's political affiliation prior to asking factual\nquestions about public figures and organizations leads to differing results. We\nobserve that left-leaning users tend to receive more positive statements about\nleft-leaning political figures and media outlets, while right-leaning users see\nmore positive statements about right-leaning entities. This pattern holds\nacross presidential candidates, members of the U.S. Senate, and media\norganizations with ratings from AllSides. When qualitatively evaluating some of\nthese outputs, there is evidence that particular facts are included or excluded\nbased on the user's political affiliation. These results illustrate that\npersonalizing LLMs based on user demographics carry the same risks of affective\npolarization and filter bubbles that have been seen in other personalized\ninternet technologies. This ``failure mode\" should be monitored closely as\nthere are more attempts to monetize and personalize these models.", "journal": ""}
{"doi": "10.48550/arXiv.2306.15626", "date": "2023-06-27", "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models", "authors": "Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar", "abstract": "Large language models (LLMs) have shown promise in proving formal theorems\nusing proof assistants such as Lean. However, existing methods are difficult to\nreproduce or build on, due to private code, data, and large compute\nrequirements. This has created substantial barriers to research on machine\nlearning methods for theorem proving. This paper removes these barriers by\nintroducing LeanDojo: an open-source Lean playground consisting of toolkits,\ndata, models, and benchmarks. LeanDojo extracts data from Lean and enables\ninteraction with the proof environment programmatically. It contains\nfine-grained annotations of premises in proofs, providing valuable data for\npremise selection: a key bottleneck in theorem proving. Using this data, we\ndevelop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented\nwith retrieval for selecting premises from a vast math library. It is\ninexpensive and needs only one GPU week of training. Our retriever leverages\nLeanDojo's program analysis capability to identify accessible premises and hard\nnegative examples, which makes retrieval much more effective. Furthermore, we\nconstruct a new benchmark consisting of 98,734 theorems and proofs extracted\nfrom Lean's math library. It features challenging data split requiring the\nprover to generalize to theorems relying on novel premises that are never used\nin training. We use this benchmark for training and evaluation, and\nexperimental results demonstrate the effectiveness of ReProver over\nnon-retrieval baselines and GPT-4. We thus provide the first set of open-source\nLLM-based theorem provers without any proprietary datasets and release it under\na permissive MIT license to facilitate further research.", "journal": ""}
{"doi": "10.48550/arXiv.2403.13312", "date": "2024-03-20", "title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean", "authors": "Dongwei Jiang, Marcio Fonseca, Shay B. Cohen", "abstract": "Large language models (LLMs) often struggle with complex logical reasoning\ndue to logical inconsistencies and the inherent difficulty of such reasoning.\nWe use Lean, a theorem proving framework, to address these challenges. By\nformalizing logical reasoning problems into theorems within Lean, we can solve\nthem by proving or disproving the corresponding theorems. This method reduces\nthe risk of logical inconsistencies with the help of Lean's symbolic solver. It\nalso enhances our ability to treat complex reasoning tasks by using Lean's\nextensive library of theorem proofs. Our method achieves state-of-the-art\nperformance on the FOLIO dataset and achieves performance near this level on\nProofWriter. Notably, these results were accomplished by fine-tuning on fewer\nthan 100 in-domain samples for each dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2406.03847", "date": "2024-06-06", "title": "Lean Workbook: A large-scale Lean problem set formalized from natural language math problems", "authors": "Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen", "abstract": "Large language models have demonstrated impressive capabilities across\nvarious natural language processing tasks, especially in solving mathematical\nproblems. However, large language models are not good at math theorem proving\nusing formal languages like Lean. A significant challenge in this area is the\nscarcity of training data available in these formal languages. To address this\nissue, we propose a novel pipeline that iteratively generates and filters\nsynthetic data to translate natural language mathematical problems into Lean 4\nstatements, and vice versa. Our results indicate that the synthetic data\npipeline can provide useful training data and improve the performance of LLMs\nin translating and understanding complex mathematical problems and proofs. Our\nfinal dataset contains about 57K formal-informal question pairs along with\nsearched proof from the math contest forum and 21 new IMO questions. We\nopen-source our code at https://github.com/InternLM/InternLM-Math and our data\nat https://huggingface.co/datasets/InternLM/Lean-Workbook.", "journal": ""}
{"doi": "10.48550/arXiv.2501.16207", "date": "2025-01-27", "title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs", "authors": "Jialun Cao, Yaojie Lu, Meiziniu Li, Haoyang Ma, Haokun Li, Mengda He, Cheng Wen, Le Sun, Hongyu Zhang, Shengchao Qin, Shing-Chi Cheung, Cong Tian", "abstract": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe.", "journal": ""}
{"doi": "10.48550/arXiv.2307.12402", "date": "2023-07-13", "title": "ChatGPT and Bard Responses to Polarizing Questions", "authors": "Abhay Goyal, Muhammad Siddique, Nimay Parekh, Zach Schwitzky, Clara Broekaert, Connor Michelotti, Allie Wong, Lam Yin Cheung, Robin O Hanlon, Lam Yin Cheung, Munmun De Choudhury, Roy Ka-Wei Lee, Navin Kumar", "abstract": "Recent developments in natural language processing have demonstrated the\npotential of large language models (LLMs) to improve a range of educational and\nlearning outcomes. Of recent chatbots based on LLMs, ChatGPT and Bard have made\nit clear that artificial intelligence (AI) technology will have significant\nimplications on the way we obtain and search for information. However, these\ntools sometimes produce text that is convincing, but often incorrect, known as\nhallucinations. As such, their use can distort scientific facts and spread\nmisinformation. To counter polarizing responses on these tools, it is critical\nto provide an overview of such responses so stakeholders can determine which\ntopics tend to produce more contentious responses -- key to developing targeted\nregulatory policy and interventions. In addition, there currently exists no\nannotated dataset of ChatGPT and Bard responses around possibly polarizing\ntopics, central to the above aims. We address the indicated issues through the\nfollowing contribution: Focusing on highly polarizing topics in the US, we\ncreated and described a dataset of ChatGPT and Bard responses. Broadly, our\nresults indicated a left-leaning bias for both ChatGPT and Bard, with Bard more\nlikely to provide responses around polarizing topics. Bard seemed to have fewer\nguardrails around controversial topics, and appeared more willing to provide\ncomprehensive, and somewhat human-like responses. Bard may thus be more likely\nabused by malicious actors. Stakeholders may utilize our findings to mitigate\nmisinformative and/or polarizing responses from LLMs", "journal": ""}
{"doi": "10.48550/arXiv.2308.14242", "date": "2023-08-28", "title": "The Cultural Psychology of Large Language Models: Is ChatGPT a Holistic or Analytic Thinker?", "authors": "Chuanyang Jin, Songyang Zhang, Tianmin Shu, Zhihan Cui", "abstract": "The prevalent use of Large Language Models (LLMs) has necessitated studying\ntheir mental models, yielding noteworthy theoretical and practical\nimplications. Current research has demonstrated that state-of-the-art LLMs,\nsuch as ChatGPT, exhibit certain theory of mind capabilities and possess\nrelatively stable Big Five and/or MBTI personality traits. In addition,\ncognitive process features form an essential component of these mental models.\nResearch in cultural psychology indicated significant differences in the\ncognitive processes of Eastern and Western people when processing information\nand making judgments. While Westerners predominantly exhibit analytical\nthinking that isolates things from their environment to analyze their nature\nindependently, Easterners often showcase holistic thinking, emphasizing\nrelationships and adopting a global viewpoint. In our research, we probed the\ncultural cognitive traits of ChatGPT. We employed two scales that directly\nmeasure the cognitive process: the Analysis-Holism Scale (AHS) and the Triadic\nCategorization Task (TCT). Additionally, we used two scales that investigate\nthe value differences shaped by cultural thinking: the Dialectical Self Scale\n(DSS) and the Self-construal Scale (SCS). In cognitive process tests (AHS/TCT),\nChatGPT consistently tends towards Eastern holistic thinking, but regarding\nvalue judgments (DSS/SCS), ChatGPT does not significantly lean towards the East\nor the West. We suggest that the result could be attributed to both the\ntraining paradigm and the training data in LLM development. We discuss the\npotential value of this finding for AI research and directions for future\nresearch.", "journal": ""}
{"doi": "10.48550/arXiv.2407.00928", "date": "2024-07-01", "title": "FoldGPT: Simple and Effective Large Language Model Compression Scheme", "authors": "Songwei Liu, Chao Zeng, Lianqiang Li, Chenqian Yan, Lean Fu, Xing Mei, Fangmin Chen", "abstract": "The demand for deploying large language models(LLMs) on mobile devices\ncontinues to increase, driven by escalating data security concerns and cloud\ncosts. However, network bandwidth and memory limitations pose challenges for\ndeploying billion-level models on mobile devices. In this study, we investigate\nthe outputs of different layers across various scales of LLMs and found that\nthe outputs of most layers exhibit significant similarity. Moreover, this\nsimilarity becomes more pronounced as the model size increases, indicating\nsubstantial redundancy in the depth direction of the LLMs. Based on this\nobservation, we propose an efficient model volume compression strategy, termed\nFoldGPT, which combines block removal and block parameter sharing.This strategy\nconsists of three parts: (1) Based on the learnable gating parameters, we\ndetermine the block importance ranking while modeling the coupling effect\nbetween blocks. Then we delete some redundant layers based on the given removal\nrate. (2) For the retained blocks, we apply a specially designed group\nparameter sharing strategy, where blocks within the same group share identical\nweights, significantly compressing the number of parameters and slightly\nreducing latency overhead. (3) After sharing these Blocks, we \"cure\" the\nmismatch caused by sparsity with a minor amount of fine-tuning and introduce a\ntail-layer distillation strategy to improve the performance. Experiments\ndemonstrate that FoldGPT outperforms previous state-of-the-art(SOTA) methods in\nefficient model compression, demonstrating the feasibility of achieving model\nlightweighting through straightforward block removal and parameter sharing.", "journal": ""}
{"doi": "10.48550/arXiv.2410.06166", "date": "2024-10-08", "title": "Temporal Reasoning Transfer from Text to Video", "authors": "Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, Qi Liu", "abstract": "Video Large Language Models (Video LLMs) have shown promising capabilities in\nvideo comprehension, yet they struggle with tracking temporal changes and\nreasoning about temporal relationships. While previous research attributed this\nlimitation to the ineffective temporal encoding of visual inputs, our\ndiagnostic study reveals that video representations contain sufficient\ninformation for even small probing classifiers to achieve perfect accuracy.\nSurprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning\ncapability stems from the underlying LLM's inherent difficulty with temporal\nconcepts, as evidenced by poor performance on textual temporal\nquestion-answering tasks. Building on this discovery, we introduce the Textual\nTemporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning\ntasks in pure text format from existing image-text datasets, addressing the\nscarcity of video samples with complex temporal scenarios. Remarkably, without\nusing any video data, T3 enhances LongVA-7B's temporal understanding, yielding\na 5.3 absolute accuracy improvement on the challenging TempCompass benchmark,\nwhich enables our model to outperform ShareGPT4Video-8B trained on 28,000 video\nsamples. Additionally, the enhanced LongVA-7B model achieves competitive\nperformance on comprehensive video benchmarks. For example, it achieves a 49.7\naccuracy on the Temporal Reasoning task of Video-MME, surpassing powerful\nlarge-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further\nanalysis reveals a strong correlation between textual and video temporal task\nperformance, validating the efficacy of transferring temporal reasoning\nabilities from text to video domains.", "journal": ""}
{"doi": "10.48550/arXiv.2410.21353", "date": "2024-10-28", "title": "Causal Interventions on Causal Paths: Mapping GPT-2's Reasoning From Syntax to Semantics", "authors": "Isabelle Lee, Joshua Lum, Ziyi Liu, Dani Yogatama", "abstract": "While interpretability research has shed light on some internal algorithms\nutilized by transformer-based LLMs, reasoning in natural language, with its\ndeep contextuality and ambiguity, defies easy categorization. As a result,\nformulating clear and motivating questions for circuit analysis that rely on\nwell-defined in-domain and out-of-domain examples required for causal\ninterventions is challenging. Although significant work has investigated\ncircuits for specific tasks, such as indirect object identification (IOI),\ndeciphering natural language reasoning through circuits remains difficult due\nto its inherent complexity. In this work, we take initial steps to characterize\ncausal reasoning in LLMs by analyzing clear-cut cause-and-effect sentences like\n\"I opened an umbrella because it started raining,\" where causal interventions\nmay be possible through carefully crafted scenarios using GPT-2 small. Our\nfindings indicate that causal syntax is localized within the first 2-3 layers,\nwhile certain heads in later layers exhibit heightened sensitivity to\nnonsensical variations of causal sentences. This suggests that models may infer\nreasoning by (1) detecting syntactic cues and (2) isolating distinct heads in\nthe final layers that focus on semantic relationships.", "journal": ""}
{"doi": "10.48550/arXiv.2309.00770", "date": "2023-09-02", "title": "Bias and Fairness in Large Language Models: A Survey", "authors": "Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed", "abstract": "Rapid advancements of large language models (LLMs) have enabled the\nprocessing, understanding, and generation of human-like text, with increasing\nintegration into systems that touch our social sphere. Despite this success,\nthese models can learn, perpetuate, and amplify harmful social biases. In this\npaper, we present a comprehensive survey of bias evaluation and mitigation\ntechniques for LLMs. We first consolidate, formalize, and expand notions of\nsocial bias and fairness in natural language processing, defining distinct\nfacets of harm and introducing several desiderata to operationalize fairness\nfor LLMs. We then unify the literature by proposing three intuitive taxonomies,\ntwo for bias evaluation, namely metrics and datasets, and one for mitigation.\nOur first taxonomy of metrics for bias evaluation disambiguates the\nrelationship between metrics and evaluation datasets, and organizes metrics by\nthe different levels at which they operate in a model: embeddings,\nprobabilities, and generated text. Our second taxonomy of datasets for bias\nevaluation categorizes datasets by their structure as counterfactual inputs or\nprompts, and identifies the targeted harms and social groups; we also release a\nconsolidation of publicly-available datasets for improved access. Our third\ntaxonomy of techniques for bias mitigation classifies methods by their\nintervention during pre-processing, in-training, intra-processing, and\npost-processing, with granular subcategories that elucidate research trends.\nFinally, we identify open problems and challenges for future work. Synthesizing\na wide range of recent research, we aim to provide a clear guide of the\nexisting literature that empowers researchers and practitioners to better\nunderstand and prevent the propagation of bias in LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.04753", "date": "2024-10-07", "title": "ImProver: Agent-Based Automated Proof Optimization", "authors": "Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, Sean Welleck", "abstract": "Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.", "journal": ""}
{"doi": "10.48550/arXiv.2309.01456", "date": "2023-09-04", "title": "LLM and Infrastructure as a Code use case", "authors": "Thibault Chanus, Michael Aubertin", "abstract": "Cloud computing and the evolution of management methodologies such as Lean\nManagement or Agile entail a profound transformation in both system\nconstruction and maintenance approaches. These practices are encompassed within\nthe term \"DevOps.\" This descriptive approach to an information system or\napplication, alongside the configuration of its constituent components, has\nnecessitated the development of descriptive languages paired with specialized\nengines for automating systems administration tasks. Among these, the tandem of\nAnsible (engine) and YAML (descriptive language) stands out as the two most\nprevalent tools in the market, facing notable competition mainly from\nTerraform. The current document presents an inquiry into a solution for\ngenerating and managing Ansible YAML roles and playbooks, utilizing Generative\nLLMs (Language Models) to translate human descriptions into code. Our efforts\nare focused on identifying plausible directions and outlining the potential\nindustrial applications. Note: For the purpose of this experiment, we have\nopted against the use of Ansible Lightspeed. This is due to its reliance on an\nIBM Watson model, for which we have not found any publicly available\nreferences. Comprehensive information regarding this remarkable technology can\nbe found [1] directly on our partner's website, RedHat.", "journal": ""}
{"doi": "10.48550/arXiv.2309.16705", "date": "2023-08-17", "title": "Multimodal Analysis Of Google Bard And GPT-Vision: Experiments In Visual Reasoning", "authors": "David Noever, Samantha Elizabeth Miller Noever", "abstract": "Addressing the gap in understanding visual comprehension in Large Language\nModels (LLMs), we designed a challenge-response study, subjecting Google Bard\nand GPT-Vision to 64 visual tasks, spanning categories like \"Visual Situational\nReasoning\" and \"Next Scene Prediction.\" Previous models, such as GPT4, leaned\nheavily on optical character recognition tools like Tesseract, whereas Bard and\nGPT-Vision, akin to Google Lens and Visual API, employ deep learning techniques\nfor visual text recognition. However, our findings spotlight both\nvision-language model's limitations: while proficient in solving visual\nCAPTCHAs that stump ChatGPT alone, it falters in recreating visual elements\nlike ASCII art or analyzing Tic Tac Toe grids, suggesting an over-reliance on\neducated visual guesses. The prediction problem based on visual inputs appears\nparticularly challenging with no common-sense guesses for next-scene\nforecasting based on current \"next-token\" multimodal models. This study\nprovides experimental insights into the current capacities and areas for\nimprovement in multimodal LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2407.06172", "date": "2024-07-08", "title": "On Speeding Up Language Model Evaluation", "authors": "Jin Peng Zhou, Christian K. Belardi, Ruihan Wu, Travis Zhang, Carla P. Gomes, Wen Sun, Kilian Q. Weinberger", "abstract": "Developing prompt-based methods with Large Language Models (LLMs) requires\nmaking numerous decisions, which give rise to a combinatorial search problem\nover hyper-parameters. This exhaustive evaluation can be time-consuming and\ncostly. In this paper, we propose an $\\textit{adaptive}$ approach to explore\nthis space. We are exploiting the fact that often only few samples are needed\nto identify clearly superior or inferior settings, and that many evaluation\ntests are highly correlated. We lean on multi-armed bandits to sequentially\nidentify the next (method, validation sample)-pair to evaluate and utilize\nlow-rank matrix factorization to fill in missing evaluations. We carefully\nassess the efficacy of our approach on several competitive benchmark problems\nand show that it can identify the top-performing method using only 5-15% of the\ntypical resources -- resulting in 85-95% LLM cost savings. Our code is\navailable at https://github.com/kilian-group/banditeval.", "journal": ""}
{"doi": "10.48550/arXiv.2409.19611", "date": "2024-09-29", "title": "Learning Attentional Mixture of LoRAs for Language Model Continual Learning", "authors": "Jialin Liu, Jianhua Wu, Jie Liu, Yutai Duan", "abstract": "Fine-tuning large language models (LLMs) with Low-Rank adaption (LoRA) is\nwidely acknowledged as an effective approach for continual learning for new\ntasks. However, it often suffers from catastrophic forgetting when dealing with\nmultiple tasks sequentially. To this end, we propose Attentional Mixture of\nLoRAs (AM-LoRA), a continual learning approach tailored for LLMs. Specifically,\nAM-LoRA learns a sequence of LoRAs for a series of tasks to continually learn\nknowledge from different tasks. The key of our approach is that we devise an\nattention mechanism as a knowledge mixture module to adaptively integrate\ninformation from each LoRA. With the attention mechanism, AM-LoRA can\nefficiently leverage the distinctive contributions of each LoRA, while\nmitigating the risk of mutually negative interactions among them that may lead\nto catastrophic forgetting. Moreover, we further introduce $L1$ norm in the\nlearning process to make the attention vector more sparse. The sparse\nconstraints can enable the model to lean towards selecting a few highly\nrelevant LoRAs, rather than aggregating and weighting all LoRAs collectively,\nwhich can further reduce the impact stemming from mutual interference.\nExperimental results on continual learning benchmarks indicate the superiority\nof our proposed method.", "journal": ""}
{"doi": "10.48550/arXiv.2501.15797", "date": "2025-01-27", "title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models", "authors": "Tianbo Yang, Mingqi Yan, Hongyi Zhao, Tianshuo Yang", "abstract": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.", "journal": ""}
{"doi": "10.48550/arXiv.2310.05189", "date": "2023-10-08", "title": "Factuality Challenges in the Era of Large Language Models", "authors": "Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, Giovanni Zagni", "abstract": "The emergence of tools based on Large Language Models (LLMs), such as\nOpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered\nimmense public attention. These incredibly useful, natural-sounding tools mark\nsignificant advances in natural language generation, yet they exhibit a\npropensity to generate false, erroneous, or misleading content -- commonly\nreferred to as \"hallucinations.\" Moreover, LLMs can be exploited for malicious\napplications, such as generating false but credible-sounding content and\nprofiles at scale. This poses a significant challenge to society in terms of\nthe potential deception of users and the increasing dissemination of inaccurate\ninformation. In light of these risks, we explore the kinds of technological\ninnovations, regulatory reforms, and AI literacy initiatives needed from\nfact-checkers, news organizations, and the broader research and policy\ncommunities. By identifying the risks, the imminent threats, and some viable\nsolutions, we seek to shed light on navigating various aspects of veracity in\nthe era of generative AI.", "journal": ""}
{"doi": "10.48550/arXiv.2311.09603", "date": "2023-11-16", "title": "Self-Contradictory Reasoning Evaluation and Detection", "authors": "Ziyi Liu, Soumya Sanyal, Isabelle Lee, Yongkang Du, Rahul Gupta, Yang Liu, Jieyu Zhao", "abstract": "In a plethora of recent work, large language models (LLMs) demonstrated\nimpressive reasoning ability, but many proposed downstream reasoning tasks only\nfocus on final answers. Two fundamental questions persist: 1) how consistent is\nthe reasoning, and 2) can models detect unreliable reasoning? In this paper, we\ninvestigate self-contradictory (Self-Contra) reasoning, where the model\nreasoning does not support its answers. To answer 1), we define and assess the\nSelf-Contra rate across three datasets and delve into finer-grained categories\nof Self-Contra reasoning. We find that LLMs often contradict themselves in\nreasoning tasks involving contextual information understanding or commonsense.\nThe model may generate correct answers by taking shortcuts in reasoning or\noverlooking contextual evidence, leading to compromised reasoning. For 2), we\ntask the state-of-the-art model GPT-4 with identifying Self-Contra reasoning\nand finer-grained fallacies. We find that finer-grained categories enhanced\ndetection can improve GPT-4's ability to detect Self-Contra. However, it is\nonly able to detect Self-Contra with a 52.2% F1 score, much lower compared to\n66.7% for humans. Our results indicate that current LLMs lack the robustness\nnecessary for reliable reasoning and we emphasize the urgent need for\nestablishing best practices in comprehensive reasoning evaluations beyond pure\nperformance-based metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2401.06416", "date": "2024-01-12", "title": "Mission: Impossible Language Models", "authors": "Julie Kallini, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, Christopher Potts", "abstract": "Chomsky and others have very directly claimed that large language models\n(LLMs) are equally capable of learning languages that are possible and\nimpossible for humans to learn. However, there is very little published\nexperimental evidence to support such a claim. Here, we develop a set of\nsynthetic impossible languages of differing complexity, each designed by\nsystematically altering English data with unnatural word orders and grammar\nrules. These languages lie on an impossibility continuum: at one end are\nlanguages that are inherently impossible, such as random and irreversible\nshuffles of English words, and on the other, languages that may not be\nintuitively impossible but are often considered so in linguistics, particularly\nthose with rules based on counting word positions. We report on a wide range of\nevaluations to assess the capacity of GPT-2 small models to learn these\nuncontroversially impossible languages, and crucially, we perform these\nassessments at various stages throughout training to compare the learning\nprocess for each language. Our core finding is that GPT-2 struggles to learn\nimpossible languages when compared to English as a control, challenging the\ncore claim. More importantly, we hope our approach opens up a productive line\nof inquiry in which different LLM architectures are tested on a variety of\nimpossible languages in an effort to learn more about how LLMs can be used as\ntools for these cognitive and typological investigations.", "journal": ""}
{"doi": "10.48550/arXiv.2403.19790", "date": "2024-03-28", "title": "Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care", "authors": "Niall Taylor, Andrey Kormilitzin, Isabelle Lorge, Alejo Nevado-Holgado, Dan W Joyce", "abstract": "Contemporary large language models (LLMs) may have utility for processing\nunstructured, narrative free-text clinical data contained in electronic health\nrecords (EHRs) -- a particularly important use-case for mental health where a\nmajority of routinely-collected patient data lacks structured, machine-readable\ncontent.\n  A significant problem for the the United Kingdom's National Health Service\n(NHS) are the long waiting lists for specialist mental healthcare. According to\nNHS data, in each month of 2023, there were between 370,000 and 470,000\nindividual new referrals into secondary mental healthcare services. Referrals\nmust be triaged by clinicians, using clinical information contained in the\npatient's EHR to arrive at a decision about the most appropriate mental\nhealthcare team to assess and potentially treat these patients.\n  The ability to efficiently recommend a relevant team by ingesting potentially\nvoluminous clinical notes could help services both reduce referral waiting\ntimes and with the right technology, improve the evidence available to justify\ntriage decisions.\n  We present and evaluate three different approaches for LLM-based, end-to-end\ningestion of variable-length clinical EHR data to assist clinicians when\ntriaging referrals. Our model is able to deliver triage recommendations\nconsistent with existing clinical practices and it's architecture was\nimplemented on a single GPU, making it practical for implementation in\nresource-limited NHS environments where private implementations of LLM\ntechnology will be necessary to ensure confidential clinical data is\nappropriately controlled and governed.", "journal": ""}
{"doi": "10.48550/arXiv.2406.14425", "date": "2024-06-20", "title": "SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages", "authors": "Gayane Ghazaryan, Erik Arakelyan, Pasquale Minervini, Isabelle Augenstein", "abstract": "Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language.", "journal": ""}
{"doi": "10.48550/arXiv.2502.15022", "date": "2025-02-20", "title": "A Meta-Evaluation of Style and Attribute Transfer Metrics", "authors": "Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent", "abstract": "LLMs make it easy to rewrite text in any style, be it more polite,\npersuasive, or more positive. We present a large-scale study of evaluation\nmetrics for style and attribute transfer with a focus on content preservation;\nmeaning content not attributed to the style shift is preserved. The de facto\nevaluation approach uses lexical or semantic similarity metrics often between\nsource sentences and rewrites. While these metrics are not designed to\ndistinguish between style or content differences, empirical meta-evaluation\nshows a reasonable correlation to human judgment. In fact, recent works find\nthat LLMs prompted as evaluators are only comparable to semantic similarity\nmetrics, even though intuitively, the LLM approach should better fit the task.\nTo investigate this discrepancy, we benchmark 8 metrics for evaluating content\npreservation on existing datasets and additionally construct a new test set\nthat better aligns with the meta-evaluation aim. Indeed, we then find that the\nempirical conclusion aligns with the intuition: content preservation metrics\nfor style/attribute transfer must be conditional on the style shift. To support\nthis, we propose a new efficient zero-shot evaluation method using the\nlikelihood of the next token. We hope our meta-evaluation can foster more\nresearch on evaluating content preservation metrics, and also to ensure fair\nevaluation of methods for conducting style transfer.", "journal": ""}
{"doi": "10.48550/arXiv.2304.10663", "date": "2023-04-20", "title": "Meta Semantics: Towards better natural language understanding and reasoning", "authors": "Xiaolin Hu", "abstract": "Natural language understanding is one of the most challenging topics in\nartificial intelligence. Deep neural network methods, particularly large\nlanguage module (LLM) methods such as ChatGPT and GPT-3, have powerful\nflexibility to adopt informal text but are weak on logical deduction and suffer\nfrom the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods\nsuch as Mathematica, Semantic web, and Lean, are excellent in reasoning but\ncannot handle the complex and changeable informal text. Inspired by pragmatics\nand structuralism, we propose two strategies to solve the OOV problem and a\nsemantic model for better natural language understanding and reasoning.", "journal": ""}
{"doi": "10.48550/arXiv.2402.03171", "date": "2024-02-05", "title": "Homograph Attacks on Maghreb Sentiment Analyzers", "authors": "Fatima Zahra Qachfar, Rakesh M. Verma", "abstract": "We examine the impact of homograph attacks on the Sentiment Analysis (SA)\ntask of different Arabic dialects from the Maghreb North-African countries.\nHomograph attacks result in a 65.3% decrease in transformer classification from\nan F1-score of 0.95 to 0.33 when data is written in \"Arabizi\". The goal of this\nstudy is to highlight LLMs weaknesses' and to prioritize ethical and\nresponsible Machine Learning.", "journal": ""}
{"doi": "10.48550/arXiv.2502.05150", "date": "2025-02-07", "title": "CodeSCM: Causal Analysis for Multi-Modal Code Generation", "authors": "Mukur Gupta, Noopur Bhatt, Suman Jana", "abstract": "In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for\nanalyzing multi-modal code generation using large language models (LLMs). By\napplying interventions to CodeSCM, we measure the causal effects of different\nprompt modalities, such as natural language, code, and input-output examples,\non the model. CodeSCM introduces latent mediator variables to separate the code\nand natural language semantics of a multi-modal code generation prompt. Using\nthe principles of Causal Mediation Analysis on these mediators we quantify\ndirect effects representing the model's spurious leanings. We find that, in\naddition to natural language instructions, input-output examples significantly\ninfluence code generation.", "journal": ""}
{"doi": "10.48550/arXiv.2210.07074", "date": "2022-10-13", "title": "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing", "authors": "Andy Rosenbaum, Saleh Soltan, Wael Hamza, Amir Saffari, Marco Damonte, Isabel Groves", "abstract": "A bottleneck to developing Semantic Parsing (SP) models is the need for a\nlarge volume of human-labeled training data. Given the complexity and cost of\nhuman annotation for SP, labeled data is often scarce, particularly in\nmultilingual settings. Large Language Models (LLMs) excel at SP given only a\nfew examples, however LLMs are unsuitable for runtime systems which require low\nlatency. In this work, we propose CLASP, a simple method to improve\nlow-resource SP for moderate-sized models: we generate synthetic data from\nAlexaTM 20B to augment the training set for a model 40x smaller (500M\nparameters). We evaluate on two datasets in low-resource settings: English\nPIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual\nzero-shot, where training data is available only in English, and the model must\ngeneralize to four new languages. On both datasets, we show significant\nimprovements over strong baseline methods.", "journal": ""}
{"doi": "10.48550/arXiv.2410.20274", "date": "2024-10-26", "title": "Library Learning Doesn't: The Curious Case of the Single-Use \"Library\"", "authors": "Ian Berlot-Attwell, Frank Rudzicz, Xujie Si", "abstract": "Advances in Large Language Models (LLMs) have spurred a wave of LLM library\nlearning systems for mathematical reasoning. These systems aim to learn a\nreusable library of tools, such as formal Isabelle lemmas or Python programs\nthat are tailored to a family of tasks. Many of these systems are inspired by\nthe human structuring of knowledge into reusable and extendable concepts, but\ndo current methods actually learn reusable libraries of tools?\n  We study two library learning systems for mathematics which both reported\nincreased accuracy: LEGO-Prover and TroVE. We find that function reuse is\nextremely infrequent on miniF2F and MATH. Our followup ablation experiments\nsuggest that, rather than reuse, self-correction and self-consistency are the\nprimary drivers of the observed performance gains. Our code and data are\navailable at https://github.com/ikb-a/curious-case", "journal": ""}
{"doi": "10.48550/arXiv.2502.07068", "date": "2025-02-10", "title": "Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations", "authors": "Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul R\u00f6ttger, Daniel Hershcovich", "abstract": "Large-scale surveys are essential tools for informing social science research\nand policy, but running surveys is costly and time-intensive. If we could\naccurately simulate group-level survey results, this would therefore be very\nvaluable to social science research. Prior work has explored the use of large\nlanguage models (LLMs) for simulating human behaviors, mostly through\nprompting. In this paper, we are the first to specialize LLMs for the task of\nsimulating survey response distributions. As a testbed, we use country-level\nresults from two global cultural surveys. We devise a fine-tuning method based\non first-token probabilities to minimize divergence between predicted and\nactual response distributions for a given question. Then, we show that this\nmethod substantially outperforms other methods and zero-shot classifiers, even\non unseen questions, countries, and a completely unseen survey. While even our\nbest models struggle with the task, especially on unseen questions, our results\ndemonstrate the benefits of specialization for simulation, which may accelerate\nprogress towards sufficiently accurate simulation in the future.", "journal": ""}
{"doi": "10.48550/arXiv.2402.06332", "date": "2024-02-09", "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning", "authors": "Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin", "abstract": "The math abilities of large language models can represent their abstract\nreasoning ability. In this paper, we introduce and open-source our math\nreasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We\nunify chain-of-thought reasoning, reward modeling, formal reasoning, data\naugmentation, and code interpreter in a unified seq2seq format and supervise\nour model to be a versatile math reasoner, verifier, prover, and augmenter.\nThese abilities can be used to develop the next math LLMs or self-iteration.\nInternLM-Math obtains open-sourced state-of-the-art performance under the\nsetting of in-context learning, supervised fine-tuning, and code-assisted\nreasoning in various informal and formal benchmarks including GSM8K, MATH,\nHungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves\n30.3 on the MiniF2F test set without fine-tuning. We further explore how to use\nLEAN to solve math problems and study its performance under the setting of\nmulti-task learning which shows the possibility of using LEAN as a unified\nplatform for solving and proving in math. Our models, codes, and data are\nreleased at \\url{https://github.com/InternLM/InternLM-Math}.", "journal": ""}
{"doi": "10.48550/arXiv.2502.07640", "date": "2025-02-11", "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving", "authors": "Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin", "abstract": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.", "journal": ""}
{"doi": "10.48550/arXiv.2502.16944", "date": "2025-02-24", "title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance", "authors": "Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang", "abstract": "Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human\nFeedback (RLHF) is essential for aligning large language models (LLMs) with\nhuman preferences. It requires joint training of an actor and critic with a\npretrained, fixed reward model for guidance. This approach increases\ncomputational complexity and instability due to actor-critic interdependence.\nAdditionally, PPO lacks access to true environment rewards in LLM tasks,\nlimiting its adaptability. Under such conditions, pretraining a value model or\na reward model becomes equivalent, as both provide fixed supervisory signals\nwithout new ground-truth feedback. To address these issues, we propose\n\\textbf{Decoupled Value Policy Optimization (DVPO)}, a lean framework that\nreplaces traditional reward modeling with a pretrained \\emph{global value model\n(GVM)}. The GVM is conditioned on policy trajectories and predicts token-level\nreturn-to-go estimates. By decoupling value model from policy training (via\nfrozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence,\nreducing GPU memory usage by 40\\% and training time by 35\\% compared to\nconventional RLHF. Experiments across benchmarks show DVPO outperforms\nefficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2410.06209", "date": "2024-10-08", "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving", "authors": "Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar", "abstract": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for formal theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully generates formal proofs for 155 theorems across 23 diverse Lean\nrepositories where formal proofs were previously missing, many from advanced\nmathematics. It performs significantly better than the static LLM baseline,\nproving challenging theorems in domains like abstract algebra and algebraic\ntopology while showcasing a clear progression of learning from basic concepts\nto advanced topics. In addition, we analyze LeanAgent's superior performance on\nkey lifelong learning metrics. LeanAgent achieves exceptional scores in\nstability and backward transfer, where learning new tasks improves performance\non previously learned tasks. This emphasizes LeanAgent's continuous\ngeneralizability and improvement, explaining its superior theorem-proving\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2410.19940", "date": "2024-10-25", "title": "Cobblestone: Iterative Automation for Formal Verification", "authors": "Saketh Ram Kasibatla, Arpan Agarwal, Yuriy Brun, Sorin Lerner, Talia Ringer, Emily First", "abstract": "Formal verification using proof assistants, such as Coq, is an effective way\nof improving software quality, but it is expensive. Writing proofs manually\nrequires both significant effort and expertise. Recent research has used\nmachine learning to automatically synthesize proofs, reducing verification\neffort, but these tools are able to prove only a fraction of the desired\nsoftware properties. We introduce Cobblestone, a new proof-synthesis approach\nthat improves on the state of the art by taking advantage of partial progress\nin proof synthesis attempts. Unlike prior tools, Cobblestone can produce\nmultiple unsuccessful proofs using a large language model (LLM), identify the\nworking portions of those proofs, and combine them into a single, successful\nproof, taking advantage of internal partial progress. We evaluate Cobblestone\non two benchmarks of open-source Coq projects, controlling for training data\nleakage in LLM datasets. Fully automatically, Cobblestone can prove 48% of the\ntheorems, while Proverbot9001, the previous state-of-the-art, learning-based,\nproof-synthesis tool, can prove 17%. Cobblestone establishes a new state of the\nart for fully automated proof synthesis tools for Coq. We also evaluate\nCobblestone in a setting where it is given external partial proof progress from\noracles, serving as proxies for a human proof engineer or another tool. When\nthe theorem is broken down into a set of subgoals and Cobblestone is given a\nset of relevant lemmas already proven in the project, it can prove up to 58% of\nthe theorems. We qualitatively study the theorems Cobblestone is and is not\nable to prove to outline potential future research directions to further\nimprove proof synthesis, including developing interactive, semi-automated\ntools. Our research shows that tools can make better use of partial progress\nmade during proof synthesis to more effectively automate formal verification.", "journal": ""}
{"doi": "10.48550/arXiv.2409.05977", "date": "2024-09-09", "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different Fields in Lean 4", "authors": "Xichen Tang", "abstract": "Formalizing mathematical proofs using computerized verification languages\nlike Lean 4 has the potential to significantly impact the field of mathematics,\nit offers prominent capabilities for advancing mathematical reasoning. However,\nexisting efforts are largely limited to creating formalized versions of proofs\nfrom extensive online mathematical corpora, struggling to keep pace with the\nrapidly evolving nature of mathematics. To bridge the gap between traditional\nand computerized proof techniques, this paper explores the use of Large\nLanguage Models (LLMs) to generate formal proof steps and complete formalized\nproofs. By converting natural language (NL) mathematical proofs into formalized\nversions, this work introduces the basic structure and tactics of the Lean 4\nlanguage. The goal is to determine how AI can be leveraged to assist the\nmathematical formalization process and improve its performance. Several\nexamples are provided that demonstrate solving problems using both traditional\nand Lean 4-based approaches. Ultimately, this paper presents an explanation of\nthe foundations of Lean 4 and comparative analyses of the mathematical\nformalization process using traditional and AI-augmented techniques. The\nfindings indicate that AI- powered tools have significant potential to\naccelerate and enhance the formalization of mathematical proofs, paving the way\nfor more efficient and reliable theorem-proving for AI for Math in the future.", "journal": ""}
{"doi": "10.48550/arXiv.2411.18872", "date": "2024-11-28", "title": "A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems", "authors": "Roozbeh Yousefzadeh, Xuenan Cao, Azim Ospanov", "abstract": "Using AI to write formal proofs for mathematical problems is a challenging\ntask that has seen some advancements in recent years. Automated systems such as\nLean can verify the correctness of proofs written in formal language, yet\nwriting the proofs in formal language can be challenging for humans and\nmachines. The miniF2F benchmark has 20 IMO problems in its test set, yet formal\nproofs are available only for 6 of these problems (3 of which are only written\nby mathematicians). The model with best accuracy can only prove 2 of these 20\nIMO problems, from 1950s and 60s, while its training set is a secret. In this\nwork, we write complete, original formal proofs for the remaining IMO problems\nin Lean along with 3 extra problems from IMO 2022 and 2023. This effort expands\nthe availability of proof currently in the public domain by creating 5,880\nlines of Lean proof. The goal of the paper is to pave the way for developing AI\nmodels that can automatically write the formal proofs for all the IMO problems\nin miniF2F and beyond by providing an evaluation benchmark. In this pursuit, we\ndevise a method to decompose the proofs of these problems into their building\nblocks, constructing a dataset of 1,329 lemmas with more than 40k lines of Lean\ncode. These lemmas are not trivial, yet they are approachable, providing the\nopportunity to evaluate and diagnose the failures and successes of AI models.\nWe evaluate the ability of the SOTA LLMs on our dataset and analyze their\nsuccess and failure modes from different perspectives. Our dataset and code is\navailable at: https://github.com/roozbeh-yz/IMO-Steps.", "journal": ""}
{"doi": "10.48550/arXiv.2308.16795", "date": "2023-08-31", "title": "Towards Multilingual Automatic Dialogue Evaluation", "authors": "John Mendon\u00e7a, Alon Lavie, Isabel Trancoso", "abstract": "The main limiting factor in the development of robust multilingual dialogue\nevaluation metrics is the lack of multilingual data and the limited\navailability of open sourced multilingual dialogue systems. In this work, we\npropose a workaround for this lack of data by leveraging a strong multilingual\npretrained LLM and augmenting existing English dialogue data using Machine\nTranslation. We empirically show that the naive approach of finetuning a\npretrained multilingual encoder model with translated data is insufficient to\noutperform the strong baseline of finetuning a multilingual model with only\nsource data. Instead, the best approach consists in the careful curation of\ntranslated data using MT Quality Estimation metrics, excluding low quality\ntranslations that hinder its performance.", "journal": ""}
{"doi": "10.48550/arXiv.2402.07645", "date": "2024-02-12", "title": "Detecting the Clinical Features of Difficult-to-Treat Depression using Synthetic Data from Large Language Models", "authors": "Isabelle Lorge, Dan W. Joyce, Niall Taylor, Alejo Nevado-Holgado, Andrea Cipriani, Andrey Kormilitzin", "abstract": "Difficult-to-treat depression (DTD) has been proposed as a broader and more\nclinically comprehensive perspective on a person's depressive disorder where\ndespite treatment, they continue to experience significant burden. We sought to\ndevelop a Large Language Model (LLM)-based tool capable of interrogating\nroutinely-collected, narrative (free-text) electronic health record (EHR) data\nto locate published prognostic factors that capture the clinical syndrome of\nDTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a\nNon-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction\nmodel. The resulting model is then able to extract and label spans related to a\nvariety of relevant positive and negative factors in real clinical data (i.e.\nspans of text that increase or decrease the likelihood of a patient matching\nthe DTD syndrome). We show it is possible to obtain good overall performance\n(0.70 F1 across polarity) on real clinical data on a set of as many as 20\ndifferent factors, and high performance (0.85 F1 with 0.95 precision) on a\nsubset of important DTD factors such as history of abuse, family history of\naffective disorder, illness severity and suicidality by training the model\nexclusively on synthetic data. Our results show promise for future healthcare\napplications especially in applications where traditionally, highly\nconfidential medical data and human-expert annotation would normally be\nrequired.", "journal": ""}
{"doi": "10.48550/arXiv.2305.07667", "date": "2023-05-10", "title": "Davinci the Dualist: the mind-body divide in large language models and in human learners", "authors": "Iris Berent, Alexzander Sansiveri", "abstract": "A large literature suggests that people are intuitive Dualists--they consider\nthe mind ethereal, distinct from the body. Past research also shows that\nDualism emerges, in part, via learning (e.g., Barlev & Shtulman, 2021). But\nwhether learning is sufficient to give rise to Dualism is unknown.The evidence\nfrom human learners does address this question because humans are endowed not\nonly with general learning capacities but also with core knowledge capacities.\nAnd recent results suggest that core knowledge begets Dualism (Berent, Theodore\n& Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we\nprobe for a mind-body divide in Davinci--a large language model (LLM) that is\ndevoid of any innate core knowledge. We show that Davinci still leans towards\nDualism, and that this bias increases systematically with the learner's\ninductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist\ntendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a\nfull-blown bias. It selectively considers thoughts (epistemic states) as\ndisembodied--as unlikely to show up in the body (in the brain), but not in its\nabsence (after death). While Davinci's performance is constrained by its\nsyntactic limitations, and it differs from humans, its Dualist bias is robust.\nThese results demonstrate that the mind-body divide is partly learnable from\nexperience.They also show how, as LLM's are exposed to human narratives, they\ninduce not only human knowledge but also human biases.", "journal": ""}
{"doi": "10.48550/arXiv.2405.18218", "date": "2024-05-28", "title": "FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models", "authors": "Yang Zhang, Yawei Li, Xinpeng Wang, Qianli Shen, Barbara Plank, Bernd Bischl, Mina Rezaei, Kenji Kawaguchi", "abstract": "Overparametrized transformer networks are the state-of-the-art architecture\nfor Large Language Models (LLMs). However, such models contain billions of\nparameters making large compute a necessity, while raising environmental\nconcerns. To address these issues, we propose FinerCut, a new form of\nfine-grained layer pruning, which in contrast to prior work at the transformer\nblock level, considers all self-attention and feed-forward network (FFN) layers\nwithin blocks as individual pruning candidates. FinerCut prunes layers whose\nremoval causes minimal alternation to the model's output -- contributing to a\nnew, lean, interpretable, and task-agnostic pruning method. Tested across 9\nbenchmarks, our approach retains 90% performance of Llama3-8B with 25% layers\nremoved, and 95% performance of Llama3-70B with 30% layers removed, all without\nfine-tuning or post-pruning reconstruction. Strikingly, we observe intriguing\nresults with FinerCut: 42% (34 out of 80) of the self-attention layers in\nLlama3-70B can be removed while preserving 99% of its performance -- without\nadditional fine-tuning after removal. Moreover, FinerCut provides a tool to\ninspect the types and locations of pruned layers, allowing to observe\ninteresting pruning behaviors. For instance, we observe a preference for\npruning self-attention layers, often at deeper consecutive decoder layers. We\nhope our insights inspire future efficient LLM architecture designs.", "journal": ""}
{"doi": "10.48550/arXiv.2405.20985", "date": "2024-05-31", "title": "DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models", "authors": "Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, Lu Hou", "abstract": "The visual projector, which bridges the vision and language modalities and\nfacilitates cross-modal alignment, serves as a crucial component in MLLMs.\nHowever, measuring the effectiveness of projectors in vision-language alignment\nremains under-explored, which currently can only be inferred from the\nperformance of MLLMs on downstream tasks. Motivated by the problem, this study\nexamines the projector module by interpreting the vision-language semantic flow\nwithin MLLMs. Specifically, we trace back the semantic relevance flow from\ngenerated language tokens to raw visual encoder patches and the intermediate\noutputs produced by projectors. Our findings reveal that compressive projectors\n(e.g., QFormer), abstract visual patches into a limited set of semantic\nconcepts, such as objects or attributes, resulting in a 'double abstraction'\nphenomenon. This involves a first visual semantic abstraction by the projector\nreferring to pre-defined query tokens, and a second extraction by the LLM based\non text instructions. The double abstraction is inefficient in training and\nwill result in cumulative vision semantics deficiency. To mitigate this issue,\nwe propose the key insight of 'Decouple Compression from Abstraction (DeCo),\nthat is compressing the visual token number at the patch level by projectors\nand allowing the LLM to handle visual semantic abstraction entirely.\nConsequently, we adopt a simple compressor, i.e., 2D Adaptive Pooling, to\ndownsample visual patches in a parameter-free manner. Empirical evaluation\ndemonstrates that DeCo surpasses traditional compressive projectors regarding\nboth performance and efficiency. It achieves performance gains of 0.9%, 7.1%,\nand 2.9% across the MLLM Benchmarks, Visual Localization, and Open-ended VQA\ntasks with fewer trainable parameters and faster convergence speed.", "journal": ""}
{"doi": "10.48550/arXiv.2406.01940", "date": "2024-06-04", "title": "Process-Driven Autoformalization in Lean 4", "authors": "Jianqiao Lu, Yingjia Wan, Zhengying Liu, Yinya Huang, Jing Xiong, Chengwu Liu, Jianhao Shen, Hui Jin, Jipeng Zhang, Haiming Wang, Zhicheng Yang, Jing Tang, Zhijiang Guo", "abstract": "Autoformalization, the conversion of natural language mathematics into formal\nlanguages, offers significant potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to formal languages with substantial\nonline corpora and struggle to keep pace with rapidly evolving languages like\nLean 4. To bridge this gap, we propose a new benchmark \\textbf{Form}alization\nfor \\textbf{L}ean~\\textbf{4} (\\textbf{\\name}) designed to evaluate the\nautoformalization capabilities of large language models (LLMs). This benchmark\nencompasses a comprehensive assessment of questions, answers, formal\nstatements, and proofs. Additionally, we introduce a\n\\textbf{P}rocess-\\textbf{S}upervised \\textbf{V}erifier (\\textbf{PSV}) model\nthat leverages the precise feedback from Lean 4 compilers to enhance\nautoformalization. Our experiments demonstrate that the PSV method improves\nautoformalization, enabling higher accuracy using less filtered training data.\nFurthermore, when fine-tuned with data containing detailed process information,\nPSV can leverage the data more effectively, leading to more significant\nimprovements in autoformalization for Lean 4. Our dataset and code are\navailable at \\url{https://github.com/rookie-joe/PDA}.", "journal": ""}
{"doi": "10.48550/arXiv.2406.07222", "date": "2024-06-11", "title": "Improving Autoformalization using Type Checking", "authors": "Auguste Poiroux, Gail Weiss, Viktor Kun\u010dak, Antoine Bosselut", "abstract": "Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2406.11915", "date": "2024-06-16", "title": "miniCodeProps: a Minimal Benchmark for Proving Code Properties", "authors": "Evan Lohn, Sean Welleck", "abstract": "AI agents have shown initial promise in automating mathematical theorem\nproving in proof assistants such as Lean. The same proof assistants can be used\nto verify the correctness of code by pairing code with specifications and\nproofs that the specifications hold. Automating the writing of code,\nspecifications, and proofs could lower the cost of verification, or,\nambitiously, enable an AI agent to output safe, provably correct code. However,\nit remains unclear whether current neural theorem provers can automatically\nverify even relatively simple programs. We present miniCodeProps, a benchmark\nof 201 program specifications in the Lean proof assistant, aimed at the\nsubproblem of automatically generating a proof for a provided program and\nspecification. miniCodeProps contains specifications about simple,\nself-contained programs (e.g., lists, natural numbers, binary trees) with\nvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient to\nbreak current LLM-based provers, with state-of-the-art methods showing promise\non the easy properties in miniCodeProps, yet failing to prove nearly all of the\nmedium and hard properties. We publicly release miniCodeProps as a benchmark\nfor furthering automated theorem proving in the context of formally verified\ncode.", "journal": ""}
{"doi": "10.48550/arXiv.2407.14521", "date": "2024-07-05", "title": "Towards Automated Functional Equation Proving: A Benchmark Dataset and A Domain-Specific In-Context Agent", "authors": "Mahdi Buali, Robert Hoehndorf", "abstract": "Automated Theorem Proving (ATP) faces challenges due to its complexity and\ncomputational demands. Recent work has explored using Large Language Models\n(LLMs) for ATP action selection, but these methods can be resource-intensive.\nThis study introduces FEAS, an agent that enhances the COPRA in-context\nlearning framework within Lean. FEAS refines prompt generation, response\nparsing, and incorporates domain-specific heuristics for functional equations.\nIt introduces FunEq, a curated dataset of functional equation problems with\nvarying difficulty. FEAS outperforms baselines on FunEq, particularly with the\nintegration of domain-specific heuristics. The results demonstrate FEAS's\neffectiveness in generating and formalizing high-level proof strategies into\nLean proofs, showcasing the potential of tailored approaches for specific ATP\nchallenges.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17840", "date": "2025-02-25", "title": "A Combinatorial Identities Benchmark for Theorem Proving via Automated Theorem Generation", "authors": "Beibei Xiong, Hangyu Lv, Haojia Shan, Jianlin Wang, Zhengfeng Yang, Lihong Zhi", "abstract": "Large language models (LLMs) have significantly advanced formal theorem\nproving, yet the scarcity of high-quality training data constrains their\ncapabilities in complex mathematical domains. Combinatorics, a cornerstone of\nmathematics, provides essential tools for analyzing discrete structures and\nsolving optimization problems. However, its inherent complexity makes it\nparticularly challenging for automated theorem proving (ATP) for combinatorial\nidentities. To address this, we manually construct LeanComb, combinatorial\nidentities benchmark in Lean, which is, to our knowledge, the first formalized\ntheorem proving benchmark built for combinatorial identities. We develop an\nAutomated Theorem Generator for Combinatorial Identities, ATG4CI, which\ncombines candidate tactics suggested by a self-improving large language model\nwith a Reinforcement Learning Tree Search approach for tactic prediction. By\nutilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K\ncombinatorial identities theorems, each with a complete formal proof in Lean,\nand experimental evaluations demonstrate that models trained on this dataset\ncan generate more effective tactics, thereby improving success rates in\nautomated theorem proving for combinatorial identities.", "journal": ""}
{"doi": "10.48550/arXiv.2407.08227", "date": "2024-07-11", "title": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs", "authors": "Chihcheng Hsieh, Catarina Moreira, Isabel Blanco Nobre, Sandra Costa Sousa, Chun Ouyang, Margot Brereton, Joaquim Jorge, Jacinto C. Nascimento", "abstract": "X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating comprehensive\nclinical features and data integration. We present a novel framework to enhance\nthe clinical context through augmentation techniques with clinical tabular\ndata, thereby improving its applicability and reliability in AI medical\ndiagnostics. We introduce a pioneering approach to clinical data augmentation\nthat employs large language models to generate patient contextual synthetic\ndata. This methodology is crucial for training more robust deep learning models\nin healthcare. It preserves the integrity of real patient data while enriching\nthe dataset with contextually relevant synthetic features, significantly\nenhancing model performance. Our methodology, termed DALL-M, uses a three-phase\nfeature generation process: (i)clinical context storage, (ii)expert query\ngeneration, and (iii)context-aware feature augmentation. DALL-M generates new,\nclinically relevant features by synthesizing chest X-ray images and reports.\nApplied to 799 cases using nine features from the MIMIC-IV dataset, it created\nan augmented set of 91 features. This is the first work to generate contextual\nvalues for patients' X-ray reports. Specifically, we provide (i)the capacity of\nLLMs to generate contextual synthetic values for existing clinical features and\n(ii)their ability to create entirely new clinically relevant features.\nEmpirical validation with machine learning models showed significant\nperformance improvements. Incorporating augmented features increased the F1\nscore by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses\na critical gap in clinical data augmentation, offering a robust framework for\ngenerating contextually enriched datasets.", "journal": ""}
{"doi": "10.48550/arXiv.2410.11900", "date": "2024-10-14", "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration", "authors": "Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle Augenstein", "abstract": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search.", "journal": ""}
{"doi": "10.48550/arXiv.2409.04043", "date": "2024-09-06", "title": "Towards Safer Online Spaces: Simulating and Assessing Intervention Strategies for Eating Disorder Discussions", "authors": "Louis Penafiel, Hsien-Te Kao, Isabel Erickson, David Chu, Robert McCormack, Kristina Lerman, Svitlana Volkova", "abstract": "Eating disorders are complex mental health conditions that affect millions of\npeople around the world. Effective interventions on social media platforms are\ncrucial, yet testing strategies in situ can be risky. We present a novel\nLLM-driven experimental testbed for simulating and assessing intervention\nstrategies in ED-related discussions. Our framework generates synthetic\nconversations across multiple platforms, models, and ED-related topics,\nallowing for controlled experimentation with diverse intervention approaches.\nWe analyze the impact of various intervention strategies on conversation\ndynamics across four dimensions: intervention type, generative model, social\nmedia platform, and ED-related community/topic. We employ cognitive domain\nanalysis metrics, including sentiment, emotions, etc., to evaluate the\neffectiveness of interventions. Our findings reveal that civility-focused\ninterventions consistently improve positive sentiment and emotional tone across\nall dimensions, while insight-resetting approaches tend to increase negative\nemotions. We also uncover significant biases in LLM-generated conversations,\nwith cognitive metrics varying notably between models (Claude-3 Haiku $>$\nMistral $>$ GPT-3.5-turbo $>$ LLaMA3) and even between versions of the same\nmodel. These variations highlight the importance of model selection in\nsimulating realistic discussions related to ED. Our work provides valuable\ninformation on the complex dynamics of ED-related discussions and the\neffectiveness of various intervention strategies.", "journal": ""}
{"doi": "10.48550/arXiv.2501.18265", "date": "2025-01-30", "title": "Collecting Cost-Effective, High-Quality Truthfulness Assessments with LLM Summarized Evidence", "authors": "Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro", "abstract": "With the degradation of guardrails against mis- and disinformation online, it\nis more critical than ever to be able to effectively combat it. In this paper,\nwe explore the efficiency and effectiveness of using crowd-sourced truthfulness\nassessments based on condensed, large language model (LLM) generated summaries\nof online sources. We compare the use of generated summaries to the use of\noriginal web pages in an A/B testing setting, where we employ a large and\ndiverse pool of crowd-workers to perform the truthfulness assessment. We\nevaluate the quality of assessments, the efficiency with which assessments are\nperformed, and the behavior and engagement of participants. Our results\ndemonstrate that the Summary modality, which relies on summarized evidence,\noffers no significant change in assessment accuracy over the Standard modality,\nwhile significantly increasing the speed with which assessments are performed.\nWorkers using summarized evidence produce a significantly higher number of\nassessments in the same time frame, reducing the cost needed to acquire\ntruthfulness assessments. Additionally, the Summary modality maximizes both the\ninter-annotator agreements as well as the reliance on and perceived usefulness\nof evidence, demonstrating the utility of summarized evidence without\nsacrificing the quality of assessments.", "journal": ""}
{"doi": "10.48550/arXiv.2501.18310", "date": "2025-01-30", "title": "Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis", "authors": "Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao", "abstract": "The synergy between deep learning models and traditional automation tools\nplays a pivotal role in developing robust neural theorem provers (NTPs).\nHowever, for proof synthesis with LLMs, previous work applies automation tools\neither only when the model explicitly calls the method, or only at a single\ngranularity level, failing to fully exploit the power of built-in tactics and\noff-the-shelf automated theorem provers. In this work, we propose ProofAug, a\nnovel theorem proving method that enjoys superior sample efficiency through\nequipping proof-generation LLMs with automation methods in different\ngranularities via fine-grained structure analysis of model-generated proof\nproposals. Furthermore, ProofAug serves as a versatile plug-and-play module\nthat seamlessly integrates with any tree-search algorithm, enabling our\nconstruction of an efficient recursive proving (ERP) module to further enhance\nperformance. The superiority of our method is validated on the miniF2F-test\nbenchmark using the open-source deepseek-math-7b-base model and the Isabelle\nproof assistant. Notably, by additionally employing a mixed prompting strategy,\nwe achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%\nfor the original version), setting a new SOTA across all proof languages with a\ntotal sample budget of only 2100. Our code is available at\nhttps://github.com/haoxiongliu/ProofAug.", "journal": ""}
{"doi": "10.48550/arXiv.2502.12064", "date": "2025-02-17", "title": "AI-generated Text Detection with a GLTR-based Approach", "authors": "Luc\u00eda Yan Wu, Isabel Segura-Bedmar", "abstract": "The rise of LLMs (Large Language Models) has contributed to the improved\nperformance and development of cutting-edge NLP applications. However, these\ncan also pose risks when used maliciously, such as spreading fake news, harmful\ncontent, impersonating individuals, or facilitating school plagiarism, among\nothers. This is because LLMs can generate high-quality texts, which are\nchallenging to differentiate from those written by humans. GLTR, which stands\nfor Giant Language Model Test Room and was developed jointly by the MIT-IBM\nWatson AI Lab and HarvardNLP, is a visual tool designed to help detect\nmachine-generated texts based on GPT-2, that highlights the words in text\ndepending on the probability that they were machine-generated. One limitation\nof GLTR is that the results it returns can sometimes be ambiguous and lead to\nconfusion. This study aims to explore various ways to improve GLTR's\neffectiveness for detecting AI-generated texts within the context of the\nIberLef-AuTexTification 2023 shared task, in both English and Spanish\nlanguages. Experiment results show that our GLTR-based GPT-2 model overcomes\nthe state-of-the-art models on the English dataset with a macro F1-score of\n80.19%, except for the first ranking model (80.91%). However, for the Spanish\ndataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%\ncompared to the top-performing model.", "journal": ""}
{"doi": "10.48550/arXiv.2503.03205", "date": "2025-03-05", "title": "MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving", "authors": "Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang", "abstract": "Solving mathematical problems using computer-verifiable languages like Lean\nhas significantly impacted mathematical and computer science communities.\nState-of-the-art methods utilize single Large Language Models (LLMs) as agents\nor provers to either generate complete proof or perform tree searches. However,\nsingle-agent methods inherently lack a structured way to combine high-level\nreasoning in Natural Language (NL) with Formal Language (FL) verification\nfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long\nChain-of-Thought framework, (to the best of our knowledge), the first\nmulti-agent framework for Lean4 theorem proving that balance high-level NL\nreasoning and FL verification in Long CoT. Using this structured interaction,\nour approach enables deeper insights and long-term coherence in proof\ngeneration, with which past methods struggle. We do this by leveraging emergent\nformal reasoning ability in Long CoT using our novel LoT-Transfer Learning\ntraining-inference pipeline. Extensive experiments show that our framework\nachieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,\nlargely outperforming GPT-4 (22.95%), single-agent tree search\n(InternLM-Step-Prover, 50.70%), and whole-proof generation\n(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight\nthe potential of combining Long CoT with formal verification for a more\ninsightful generation in a broader perspective.", "journal": ""}
{"doi": "10.48550/arXiv.2403.03218", "date": "2024-03-05", "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning", "authors": "Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Lin, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Ruoyu Wang, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, Dan Hendrycks", "abstract": "The White House Executive Order on Artificial Intelligence highlights the\nrisks of large language models (LLMs) empowering malicious actors in developing\nbiological, cyber, and chemical weapons. To measure these risks of malicious\nuse, government institutions and major AI labs are developing evaluations for\nhazardous capabilities in LLMs. However, current evaluations are private,\npreventing further research into mitigating risk. Furthermore, they focus on\nonly a few, highly specific pathways for malicious use. To fill these gaps, we\npublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a\ndataset of 3,668 multiple-choice questions that serve as a proxy measurement of\nhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP\nwas developed by a consortium of academics and technical consultants, and was\nstringently filtered to eliminate sensitive information prior to public\nrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledge\nin LLMs, and second, as a benchmark for unlearning methods to remove such\nhazardous knowledge. To guide progress on unlearning, we develop RMU, a\nstate-of-the-art unlearning method based on controlling model representations.\nRMU reduces model performance on WMDP while maintaining general capabilities in\nareas such as biology and computer science, suggesting that unlearning may be a\nconcrete path towards reducing malicious use from LLMs. We release our\nbenchmark and code publicly at https://wmdp.ai", "journal": ""}
{"doi": "10.48550/arXiv.2210.17437", "date": "2022-10-31", "title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes", "authors": "Avyav Kumar Singh, Ekaterina Shutova, Helen Yannakoudakis", "abstract": "Existing approaches to few-shot learning in NLP rely on large language models\n(LLMs) and/or fine-tuning of these to generalise on out-of-distribution data.\nIn this work, we propose a novel few-shot learning approach based on soft-label\nprototypes (SLPs) designed to collectively capture the distribution of\ndifferent classes across the input domain space. We focus on learning\npreviously unseen NLP tasks from very few examples (4, 8, 16) per class and\nexperimentally demonstrate that our approach achieves superior performance on\nthe majority of tested tasks in this data-lean setting while being highly\nparameter efficient. We also show that our few-shot adaptation method can be\nintegrated into more generalised learning settings, primarily meta-learning, to\nyield superior performance against strong baselines.", "journal": ""}
{"doi": "10.48550/arXiv.2408.01911", "date": "2024-07-11", "title": "Brief state of the art in social information mining: Practical application in analysis of trends in French legislative 2024", "authors": "Jose A. Garcia Gutierrez", "abstract": "The analysis of social media information has undergone significant evolution\nin the last decade due to advancements in artificial intelligence (AI) and\nmachine learning (ML). This paper provides an overview of the state-of-the-art\ntechniques in social media mining, with a practical application in analyzing\ntrends in the 2024 French legislative elections. We leverage natural language\nprocessing (NLP) tools to gauge public opinion by extracting and analyzing\ncomments and reactions from the AgoraVox platform. The study reveals that the\nNational Rally party, led by Marine Le Pen, maintains a high level of\nengagement on social media, outperforming traditional parties. This trend is\ncorroborated by user interactions, indicating a strong digital presence. The\nresults highlight the utility of advanced AI models, such as transformers and\nlarge language models (LLMs), in capturing nuanced public sentiments and\npredicting political leanings, demonstrating their potential in real-time\nreputation management and crisis response.", "journal": ""}
{"doi": "10.48550/arXiv.2410.13224", "date": "2024-10-17", "title": "Proof Flow: Preliminary Study on Generative Flow Network Language Model Tuning for Formal Reasoning", "authors": "Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang", "abstract": "Reasoning is a fundamental substrate for solving novel and complex problems.\nDeliberate efforts in learning and developing frameworks around System 2\nreasoning have made great strides, yet problems of sufficient complexity remain\nlargely out of reach for open models. To address this gap, we examine the\npotential of Generative Flow Networks as a fine-tuning method for LLMs to\nunlock advanced reasoning capabilities. In this paper, we present a proof of\nconcept in the domain of formal reasoning, specifically in the Neural Theorem\nProving (NTP) setting, where proofs specified in a formal language such as Lean\ncan be deterministically and objectively verified. Unlike classical\nreward-maximization reinforcement learning, which frequently over-exploits\nhigh-reward actions and fails to effectively explore the state space, GFlowNets\nhave emerged as a promising approach for sampling compositional objects,\nimproving generalization, and enabling models to maintain diverse hypotheses.\nOur early results demonstrate GFlowNet fine-tuning's potential for enhancing\nmodel performance in a search setting, which is especially relevant given the\nparadigm shift towards inference time compute scaling and \"thinking slowly.\"", "journal": ""}
{"doi": "10.48550/arXiv.2410.15234", "date": "2024-10-19", "title": "Bias Amplification: Large Language Models as Increasingly Biased Media", "authors": "Ze Wang, Zekun Wu, Jeremy Zhang, Xin Guan, Navya Jain, Skylar Lu, Saloni Gupta, Adriano Koshiyama", "abstract": "Model collapse, a phenomenon where models degrade in performance due to\nindiscriminate use of synthetic data is well studied. However, its role in bias\namplification, the progressive reinforcement of preexisting social biases in\nLarge Language Models (LLMs) remains underexplored. In this paper, we formally\ndefine the conditions for bias amplification and demonstrate through\nstatistical simulations that bias can intensify even in the absence of sampling\nerrors, the primary driver of model collapse. Empirically, we investigate\npolitical bias amplification in GPT2 using a custom built benchmark for\nsentence continuation tasks. Our findings reveal a progressively increasing\nright-leaning bias. Furthermore, we evaluate three mitigation strategies,\nOverfitting, Preservation, and Accumulation, and show that bias amplification\npersists even when model collapse is mitigated. Finally, a mechanistic\ninterpretation identifies distinct sets of neurons responsible for model\ncollapse and bias amplification, suggesting they arise from different\nunderlying mechanisms.", "journal": ""}
{"doi": "10.48550/arXiv.2411.00934", "date": "2024-11-01", "title": "Generative Memesis: AI Mediates Political Memes in the 2024 USA Presidential Election", "authors": "Ho-Chun Herbert Chang, Benjamin Shaman, Yung-chun Chen, Mingyue Zha, Sean Noh, Chiyu Wei, Tracy Weener, Maya Magee", "abstract": "Visual content on social media has become increasingly influential in shaping\npolitical discourse and civic engagement. Using a dataset of 239,526 Instagram\nimages, deep learning, and LLM-based workflows, we examine the impact of\ndifferent content types on user engagement during the 2024 US presidential\nElections, with a focus on synthetic visuals. Results show while synthetic\ncontent may not increase engagement alone, it mediates how political\ninformation is created through highly effective, often absurd, political memes.\nWe define the notion of generative memesis, where memes are no longer shared\nperson-to-person but mediated by AI through customized, generated images. We\nalso find partisan divergences: Democrats use AI for in-group support whereas\nRepublicans use it for out-group attacks. Non-traditional, left-leaning outlets\nare the primary creators of political memes; emphasis on different topics\nlargely follows issue ownership.", "journal": ""}
{"doi": "10.48550/arXiv.2411.03665", "date": "2024-11-06", "title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework", "authors": "Xuelin Liu, Yanfei Zhu, Shucheng Zhu, Pengyuan Liu, Ying Liu, Dong Yu", "abstract": "Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures.", "journal": ""}
{"doi": "10.48550/arXiv.2401.12963", "date": "2024-01-23", "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents", "authors": "Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Isabel Leal, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu", "abstract": "Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.", "journal": ""}
{"doi": "10.48550/arXiv.2302.07267", "date": "2023-02-13", "title": "Diminished Diversity-of-Thought in a Standard Large Language Model", "authors": "Peter S. Park, Philipp Schoenegger, Chongyang Zhu", "abstract": "We test whether Large Language Models (LLMs) can be used to simulate human\nparticipants in social-science studies. To do this, we run replications of 14\nstudies from the Many Labs 2 replication project with OpenAI's text-davinci-003\nmodel, colloquially known as GPT3.5. Based on our pre-registered analyses, we\nfind that among the eight studies we could analyse, our GPT sample replicated\n37.5% of the original results and 37.5% of the Many Labs 2 results. However, we\nwere unable to analyse the remaining six studies due to an unexpected\nphenomenon we call the \"correct answer\" effect. Different runs of GPT3.5\nanswered nuanced questions probing political orientation, economic preference,\njudgement, and moral philosophy with zero or near-zero variation in responses:\nwith the supposedly \"correct answer.\" In one exploratory follow-up study, we\nfound that a \"correct answer\" was robust to changing the demographic details\nthat precede the prompt. In another, we found that most but not all \"correct\nanswers\" were robust to changing the order of answer choices. One of our most\nstriking findings occurred in our replication of the Moral Foundations Theory\nsurvey results, where we found GPT3.5 identifying as a political conservative\nin 99.6% of the cases, and as a liberal in 99.3% of the cases in the\nreverse-order condition. However, both self-reported 'GPT conservatives' and\n'GPT liberals' showed right-leaning moral foundations. Our results cast doubts\non the validity of using LLMs as a general replacement for human participants\nin the social sciences. Our results also raise concerns that a hypothetical\nAI-led future may be subject to a diminished diversity-of-thought.", "journal": ""}
{"doi": "10.48550/arXiv.2402.08957", "date": "2024-02-14", "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data", "authors": "Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang", "abstract": "Recent large language models (LLMs) have witnessed significant advancement in\nvarious tasks, including mathematical reasoning and theorem proving. As these\ntwo tasks require strict and formal multi-step inference, they are appealing\ndomains for exploring the reasoning ability of LLMs but still face important\nchallenges. Previous studies such as Chain-of-Thought (CoT) have revealed the\neffectiveness of intermediate steps guidance. However, such step-wise\nannotation requires heavy labor, leading to insufficient training steps for\ncurrent benchmarks. To fill this gap, this work introduces MUSTARD, a data\ngeneration framework that masters uniform synthesis of theorem and proof data\nof high quality and diversity. MUSTARD synthesizes data in three stages: (1) It\nsamples a few mathematical concept seeds as the problem category. (2) Then, it\nprompts a generative language model with the sampled concepts to obtain both\nthe problems and their step-wise formal solutions. (3) Lastly, the framework\nutilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With\nthe proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE\nwith 5,866 valid data points. Each data point contains an informal statement,\nan informal proof, and a translated formal proof that passes the prover\nvalidation. We perform extensive analysis and demonstrate that MUSTARD\ngenerates validated high-quality step-by-step data. We further apply the\nMUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B\nachieves a 15.41% average relative performance gain in automated theorem\nproving, and 8.18% in math word problems. Codes and data are available at\nhttps://github.com/Eleanor-H/MUSTARD.", "journal": "ICLR 2024 spotlight"}
{"doi": "10.48550/arXiv.2403.11169", "date": "2024-03-17", "title": "Correcting misinformation on social media with a large language model", "authors": "Xinyi Zhou, Ashish Sharma, Amy X. Zhang, Tim Althoff", "abstract": "Real-world misinformation, often multimodal, can be partially or fully\nfactual but misleading using diverse tactics like conflating correlation with\ncausation. Such misinformation is severely understudied, challenging to\naddress, and harms various social domains, particularly on social media, where\nit can spread rapidly. High-quality and timely correction of misinformation\nthat identifies and explains its (in)accuracies effectively reduces false\nbeliefs. Despite the wide acceptance of manual correction, it is difficult to\nbe timely and scalable. While LLMs have versatile capabilities that could\naccelerate misinformation correction, they struggle due to a lack of recent\ninformation, a tendency to produce false content, and limitations in addressing\nmultimodal information. We propose MUSE, an LLM augmented with access to and\ncredibility evaluation of up-to-date information. By retrieving evidence as\nrefutations or supporting context, MUSE identifies and explains content\n(in)accuracies with references. It conducts multimodal retrieval and interprets\nvisual content to verify and correct multimodal content. Given the absence of a\ncomprehensive evaluation approach, we propose 13 dimensions of misinformation\ncorrection quality. Then, fact-checking experts evaluate responses to social\nmedia content that are not presupposed to be misinformation but broadly include\n(partially) incorrect and correct posts that may (not) be misleading. Results\ndemonstrate MUSE's ability to write high-quality responses to potential\nmisinformation--across modalities, tactics, domains, political leanings, and\nfor information that has not previously been fact-checked online--within\nminutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%. Our work provides a\ngeneral methodological and evaluative framework to correct misinformation at\nscale.", "journal": ""}
{"doi": "10.48550/arXiv.2205.12615", "date": "2022-05-25", "title": "Autoformalization with Large Language Models", "authors": "Yuhuai Wu, Albert Q. Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, Christian Szegedy", "abstract": "Autoformalization is the process of automatically translating from natural\nlanguage mathematics to formal specifications and proofs. A successful\nautoformalization system could advance the fields of formal verification,\nprogram synthesis, and artificial intelligence. While the long-term goal of\nautoformalization seemed elusive for a long time, we show large language models\nprovide new prospects towards this goal. We make the surprising observation\nthat LLMs can correctly translate a significant portion ($25.3\\%$) of\nmathematical competition problems perfectly to formal specifications in\nIsabelle/HOL. We demonstrate the usefulness of this process by improving a\npreviously introduced neural theorem prover via training on these\nautoformalized theorems. Our methodology results in a new state-of-the-art\nresult on the MiniF2F theorem proving benchmark, improving the proof rate from\n$29.6\\%$ to $35.2\\%$.", "journal": ""}
{"doi": "10.48550/arXiv.2311.13910", "date": "2023-11-23", "title": "Dialogue Quality and Emotion Annotations for Customer Support Conversations", "authors": "John Mendon\u00e7a, Patr\u00edcia Pereira, Miguel Menezes, Vera Cabarr\u00e3o, Ana C. Farinha, Helena Moniz, Jo\u00e3o Paulo Carvalho, Alon Lavie, Isabel Trancoso", "abstract": "Task-oriented conversational datasets often lack topic variability and\nlinguistic diversity. However, with the advent of Large Language Models (LLMs)\npretrained on extensive, multilingual and diverse text data, these limitations\nseem overcome. Nevertheless, their generalisability to different languages and\ndomains in dialogue applications remains uncertain without benchmarking\ndatasets. This paper presents a holistic annotation approach for emotion and\nconversational quality in the context of bilingual customer support\nconversations. By performing annotations that take into consideration the\ncomplete instances that compose a conversation, one can form a broader\nperspective of the dialogue as a whole. Furthermore, it provides a unique and\nvaluable resource for the development of text classification models. To this\nend, we present benchmarks for Emotion Recognition and Dialogue Quality\nEstimation and show that further research is needed to leverage these models in\na production setting.", "journal": ""}
{"doi": "10.48550/arXiv.2312.09631", "date": "2023-12-15", "title": "Context-Driven Interactive Query Simulations Based on Generative Large Language Models", "authors": "Bj\u00f6rn Engelmann, Timo Breuer, Jana Isabelle Friese, Philipp Schaer, Norbert Fuhr", "abstract": "Simulating user interactions enables a more user-oriented evaluation of\ninformation retrieval (IR) systems. While user simulations are cost-efficient\nand reproducible, many approaches often lack fidelity regarding real user\nbehavior. Most notably, current user models neglect the user's context, which\nis the primary driver of perceived relevance and the interactions with the\nsearch results. To this end, this work introduces the simulation of\ncontext-driven query reformulations. The proposed query generation methods\nbuild upon recent Large Language Model (LLM) approaches and consider the user's\ncontext throughout the simulation of a search session. Compared to simple\ncontext-free query generation approaches, these methods show better\neffectiveness and allow the simulation of more efficient IR sessions.\nSimilarly, our evaluations consider more interaction context than current\nsession-based measures and reveal interesting complementary insights in\naddition to the established evaluation protocols. We conclude with directions\nfor future work and provide an entirely open experimental setup.", "journal": ""}
{"doi": "10.48550/arXiv.2402.12431", "date": "2024-02-19", "title": "Understanding Fine-grained Distortions in Reports of Scientific Findings", "authors": "Amelie W\u00fchrl, Dustin Wright, Roman Klinger, Isabelle Augenstein", "abstract": "Distorted science communication harms individuals and society as it can lead\nto unhealthy behavior change and decrease trust in scientific institutions.\nGiven the rapidly increasing volume of science communication in recent years, a\nfine-grained understanding of how findings from scientific publications are\nreported to the general public, and methods to detect distortions from the\noriginal work automatically, are crucial. Prior work focused on individual\naspects of distortions or worked with unpaired data. In this work, we make\nthree foundational contributions towards addressing this problem: (1)\nannotating 1,600 instances of scientific findings from academic papers paired\nwith corresponding findings as reported in news articles and tweets wrt. four\ncharacteristics: causality, certainty, generality and sensationalism; (2)\nestablishing baselines for automatically detecting these characteristics; and\n(3) analyzing the prevalence of changes in these characteristics in both\nhuman-annotated and large-scale unlabeled data. Our results show that\nscientific findings frequently undergo subtle distortions when reported. Tweets\ndistort findings more often than science news reports. Detecting fine-grained\ndistortions automatically poses a challenging task. In our experiments,\nfine-tuned task-specific models consistently outperform few-shot LLM prompting.", "journal": ""}
{"doi": "10.48550/arXiv.2407.11660", "date": "2024-07-16", "title": "ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues", "authors": "John Mendon\u00e7a, Isabel Trancoso, Alon Lavie", "abstract": "Despite being heralded as the new standard for dialogue evaluation, the\nclosed-source nature of GPT-4 poses challenges for the community. Motivated by\nthe need for lightweight, open source, and multilingual dialogue evaluators,\nthis paper introduces GenResCoh (Generated Responses targeting Coherence).\nGenResCoh is a novel LLM generated dataset comprising over 130k negative and\npositive responses and accompanying explanations seeded from XDailyDialog and\nXPersona covering English, French, German, Italian, and Chinese. Leveraging\nGenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators\ntrained to assess response coherence across multiple languages. Experimental\nresults demonstrate that ECoh achieves multilingual detection capabilities\nsuperior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based\non a much smaller architecture. Furthermore, the explanations provided by ECoh\nclosely align in terms of quality with those generated by the teacher model.", "journal": ""}
{"doi": "10.48550/arXiv.2412.12072", "date": "2024-12-16", "title": "Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats", "authors": "Kuleen Sasse, Carlos Aguirre, Isabel Cachola, Sharon Levy, Mark Dredze", "abstract": "WARNING: This paper contains content that maybe upsetting or offensive to\nsome readers. Dog whistles are coded expressions with dual meanings: one\nintended for the general public (outgroup) and another that conveys a specific\nmessage to an intended audience (ingroup). Often, these expressions are used to\nconvey controversial political opinions while maintaining plausible deniability\nand slip by content moderation filters. Identification of dog whistles relies\non curated lexicons, which have trouble keeping up to date. We introduce\nFETCH!, a task for finding novel dog whistles in massive social media corpora.\nWe find that state-of-the-art systems fail to achieve meaningful results across\nthree distinct social media case studies. We present EarShot, a strong baseline\nsystem that combines the strengths of vector databases and Large Language\nModels (LLMs) to efficiently and effectively identify new dog whistles.", "journal": ""}
{"doi": "10.48550/arXiv.2501.00145", "date": "2024-12-30", "title": "Tackling Cognitive Impairment Detection from Speech: A submission to the PROCESS Challenge", "authors": "Catarina Botelho, David Gimeno-G\u00f3mez, Francisco Teixeira, John Mendon\u00e7a, Patr\u00edcia Pereira, Diogo A. P. Nunes, Thomas Rolland, Anna Pompili, Rub\u00e9n Solera-Ure\u00f1a, Maria Ponte, David Martins de Matos, Carlos-D. Mart\u00ednez-Hinarejos, Isabel Trancoso, Alberto Abad", "abstract": "This work describes our group's submission to the PROCESS Challenge 2024,\nwith the goal of assessing cognitive decline through spontaneous speech, using\nthree guided clinical tasks. This joint effort followed a holistic approach,\nencompassing both knowledge-based acoustic and text-based feature sets, as well\nas LLM-based macrolinguistic descriptors, pause-based acoustic biomarkers, and\nmultiple neural representations (e.g., LongFormer, ECAPA-TDNN, and Trillson\nembeddings). Combining these feature sets with different classifiers resulted\nin a large pool of models, from which we selected those that provided the best\nbalance between train, development, and individual class performance. Our\nresults show that our best performing systems correspond to combinations of\nmodels that are complementary to each other, relying on acoustic and textual\ninformation from all three clinical tasks.", "journal": ""}
{"doi": "10.48550/arXiv.2307.09288", "date": "2023-07-18", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2401.06945", "date": "2024-01-13", "title": "Knowledge-Centric Templatic Views of Documents", "authors": "Isabel Cachola, Silviu Cucerzan, Allen Herring, Vuksan Mijovic, Erik Oveson, Sujay Kumar Jauhar", "abstract": "Authors seeking to communicate with broader audiences often share their ideas\nin various document formats, such as slide decks, newsletters, reports, and\nposters. Prior work on document generation has generally tackled the creation\nof each separate format to be a different task, leading to fragmented learning\nprocesses, redundancy in models and methods, and disjointed evaluation. We\nconsider each of these documents as templatic views of the same underlying\nknowledge/content, and we aim to unify the generation and evaluation of these\ntemplatic views. We begin by showing that current LLMs are capable of\ngenerating various document formats with little to no supervision. Further, a\nsimple augmentation involving a structured intermediate representation can\nimprove performance, especially for smaller models. We then introduce a novel\nunified evaluation framework that can be adapted to measuring the quality of\ndocument generators for heterogeneous downstream applications. This evaluation\nis adaptable to a range of user defined criteria and application scenarios,\nobviating the need for task specific evaluation metrics. Finally, we conduct a\nhuman evaluation, which shows that people prefer 82% of the documents generated\nwith our method, while correlating more highly with our unified evaluation\nframework than prior metrics in the literature.", "journal": ""}
{"doi": "10.48550/arXiv.2404.07382", "date": "2024-04-10", "title": "Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving", "authors": "Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang", "abstract": "Recent advances in Automated Theorem Proving have shown the effectiveness of\nleveraging a (large) language model that generates tactics (i.e. proof steps)\nto search through proof states. The current model, while trained solely on\nsuccessful proof paths, faces a discrepancy at the inference stage, as it must\nsample and try various tactics at each proof state until finding success,\nunlike its training which does not incorporate learning from failed attempts.\nIntuitively, a tactic that leads to a failed search path would indicate that\nsimilar tactics should receive less attention during the following trials. In\nthis paper, we demonstrate the benefit of training models that additionally\nlearn from failed search paths. Facing the lack of such trial-and-error data in\nexisting open-source theorem-proving datasets, we curate a dataset on\nintuitionistic propositional logic theorems and formalize it in Lean, such that\nwe can reliably check the correctness of proofs. We compare our model trained\non relatively short trial-and-error information (TrialMaster) with models\ntrained only on the correct paths and discover that the former solves more\nunseen theorems with lower trial searches.", "journal": ""}
{"doi": "10.48550/arXiv.2412.17560", "date": "2024-12-23", "title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language Model Inference", "authors": "Chao Zeng, Songwei Liu, Shu Yang, Fangmin Chen, Xing Mei, Lean Fu", "abstract": "Model compression has emerged as a mainstream solution to reduce memory usage\nand computational overhead. This paper presents Group Quantization and Sparse\nAcceleration (GQSA), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. Building upon system-algorithm co-design principles, we propose a\ntwo-stage sparse optimization strategy that ensures the performance superiority\nof the compressed model. On the engine side, we introduce a \"task-centric\"\nparallel strategy, which, to the best of our knowledge, is the first\napplication in the domain of sparse computing. Compared to the traditional 2:4\nsparse method, the GQSA offers a more flexible and adjustable sparsity rate, as\nwell as a higher weight compression rate, and is efficiently compatible with\nweight-only quantization methods. Experimental results demonstrate that, under\nthe GQSA W4S50% compression setting, the model's accuracy surpasses that of\nboth 2:4 pruning and W2 quantization. Furthermore, at the inference level, GQSA\noutperforms W2 by 1.26$\\times$ and 2:4 pruning by 2.35$\\times$ in terms of\nspeed.", "journal": ""}
{"doi": "10.48550/arXiv.2502.09955", "date": "2025-02-14", "title": "Diverse Inference and Verification for Advanced Reasoning", "authors": "Iddo Drori, Gaston Longhitano, Mao Mao, Seunghwan Hyun, Yuke Zhang, Sungjun Park, Zachary Meeks, Xin-Yu Zhang, Ben Segev, Howard Yong, Nakul Verma, Avi Shporer, Alon Amit, Madeleine Udell", "abstract": "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant\nprogress in mathematics and coding, yet find challenging advanced tasks such as\nInternational Mathematical Olympiad (IMO) combinatorics problems, Abstraction\nand Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions.\nWe use a diverse inference approach that combines multiple models and methods\nat test time. We find that verifying mathematics and code problems, and\nrejection sampling on other problems is simple and effective. We automatically\nverify correctness of solutions to IMO problems by Lean, and ARC puzzles by\ncode, and find that best-of-N effectively answers HLE questions. Our approach\nincreases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%,\naccuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that\n948 humans could not and 26.5% of ARC puzzles that o3 high compute does not.\nTest-time simulations, reinforcement learning, and meta-learning with inference\nfeedback improve generalization by adapting agent graph representations and\nvarying prompts, code, and datasets. Our approach is reliable, robust, and\nscalable, and in the spirit of reproducible research, we will make it publicly\navailable upon publication.", "journal": ""}
{"doi": "10.48550/arXiv.2502.15795", "date": "2025-02-18", "title": "Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization", "authors": "Willy Chan, Michael Souliman, Jakob Nordhagen, Brando Miranda, Elyas Obbad, Kai Fronsdal Sanmi Koyejo", "abstract": "Autoformalization, the process of transforming informal mathematical language\ninto formal specifications and proofs remains a difficult task for\nstate-of-the-art (large) language models. Existing works point to competing\nexplanations for the performance gap. To this end, we introduce a novel\nmethodology that leverages back-translation with hand-curated prompts to\nenhance the mathematical capabilities of language models, particularly\naddressing the challenge posed by the scarcity of labeled data. Specifically,\nwe evaluate three primary variations of this strategy: (1) on-the-fly (online)\nbacktranslation, (2) distilled (offline) backtranslation with few-shot\namplification, and (3) line-by-line proof analysis integrated with proof state\ninformation. Each variant is designed to optimize data quality over quantity,\nfocusing on the high fidelity of generated proofs rather than sheer data scale.\nOur findings provide evidence that employing our proposed approaches to\ngenerate synthetic data, which prioritizes quality over volume, improves the\nAutoformalization performance of LLMs as measured by standard benchmarks such\nas ProofNet. Crucially, our approach outperforms pretrained models using a\nminimal number of tokens. We also show, through strategic prompting and\nbacktranslation, that our approaches surpass the performance of fine-tuning\nwith extensive multilingual datasets such as MMA on ProofNet with only 1/150th\nof the tokens. Taken together, our methods show a promising new approach to\nsignificantly reduce the resources required to formalize proofs, thereby\naccelerating AI for math.", "journal": ""}
{"doi": "10.48550/arXiv.2311.01270", "date": "2023-11-02", "title": "People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection", "authors": "Indira Sen, Dennis Assenmacher, Mattia Samory, Isabelle Augenstein, Wil van der Aalst, Claudia Wagner", "abstract": "NLP models are used in a variety of critical social computing tasks, such as\ndetecting sexist, racist, or otherwise hateful content. Therefore, it is\nimperative that these models are robust to spurious features. Past work has\nattempted to tackle such spurious features using training data augmentation,\nincluding Counterfactually Augmented Data (CADs). CADs introduce minimal\nchanges to existing training data points and flip their labels; training on\nthem may reduce model dependency on spurious features. However, manually\ngenerating CADs can be time-consuming and expensive. Hence in this work, we\nassess if this task can be automated using generative NLP models. We\nautomatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate\ntheir usefulness in improving model robustness compared to manually-generated\nCADs. By testing both model performance on multiple out-of-domain test sets and\nindividual data point efficacy, our results show that while manual CADs are\nstill the most effective, CADs generated by ChatGPT come a close second. One\nkey reason for the lower performance of automated methods is that the changes\nthey introduce are often insufficient to flip the original label.", "journal": ""}
{"doi": "10.48550/arXiv.2407.12620", "date": "2024-07-17", "title": "Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages: Technologies and Experiences", "authors": "Claudio Pinhanez, Paulo Cavalin, Luciana Storto, Thomas Finbow, Alexander Cobbinah, Julio Nogima, Marisa Vasconcelos, Pedro Domingues, Priscila de Souza Mizukami, Nicole Grell, Majo\u00ed Gongora, Isabel Gon\u00e7alves", "abstract": "Since 2022 we have been exploring application areas and technologies in which\nArtificial Intelligence (AI) and modern Natural Language Processing (NLP), such\nas Large Language Models (LLMs), can be employed to foster the usage and\nfacilitate the documentation of Indigenous languages which are in danger of\ndisappearing. We start by discussing the decreasing diversity of languages in\nthe world and how working with Indigenous languages poses unique ethical\nchallenges for AI and NLP. To address those challenges, we propose an\nalternative development AI cycle based on community engagement and usage. Then,\nwe report encouraging results in the development of high-quality machine\nlearning translators for Indigenous languages by fine-tuning state-of-the-art\n(SOTA) translators with tiny amounts of data and discuss how to avoid some\ncommon pitfalls in the process. We also present prototypes we have built in\nprojects done in 2023 and 2024 with Indigenous communities in Brazil, aimed at\nfacilitating writing, and discuss the development of Indigenous Language Models\n(ILMs) as a replicable and scalable way to create spell-checkers, next-word\npredictors, and similar tools. Finally, we discuss how we envision a future for\nlanguage documentation where dying languages are preserved as interactive\nlanguage models.", "journal": ""}
{"doi": "10.48550/arXiv.2411.12808", "date": "2024-11-19", "title": "Conversational Medical AI: Ready for Practice", "authors": "Antoine Liz\u00e9e, Pierre-Auguste Beaucot\u00e9, James Whitbeck, Marion Doumeingts, Ana\u00ebl Beaugnon, Isabelle Feldhaus", "abstract": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services.", "journal": ""}
{"doi": "10.48550/arXiv.2308.14608", "date": "2023-08-28", "title": "AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics", "authors": "Vahid Ghafouri, Vibhor Agarwal, Yong Zhang, Nishanth Sastry, Jose Such, Guillermo Suarez-Tangil", "abstract": "The introduction of ChatGPT and the subsequent improvement of Large Language\nModels (LLMs) have prompted more and more individuals to turn to the use of\nChatBots, both for information and assistance with decision-making. However,\nthe information the user is after is often not formulated by these ChatBots\nobjectively enough to be provided with a definite, globally accepted answer.\n  Controversial topics, such as \"religion\", \"gender identity\", \"freedom of\nspeech\", and \"equality\", among others, can be a source of conflict as partisan\nor biased answers can reinforce preconceived notions or promote disinformation.\nBy exposing ChatGPT to such debatable questions, we aim to understand its level\nof awareness and if existing models are subject to socio-political and/or\neconomic biases. We also aim to explore how AI-generated answers compare to\nhuman ones. For exploring this, we use a dataset of a social media platform\ncreated for the purpose of debating human-generated claims on polemic subjects\namong users, dubbed Kialo.\n  Our results show that while previous versions of ChatGPT have had important\nissues with controversial topics, more recent versions of ChatGPT\n(gpt-3.5-turbo) are no longer manifesting significant explicit biases in\nseveral knowledge areas. In particular, it is well-moderated regarding economic\naspects. However, it still maintains degrees of implicit libertarian leaning\ntoward right-winged ideals which suggest the need for increased moderation from\nthe socio-political point of view. In terms of domain knowledge on\ncontroversial topics, with the exception of the \"Philosophical\" category,\nChatGPT is performing well in keeping up with the collective human level of\nknowledge. Finally, we see that sources of Bing AI have slightly more tendency\nto the center when compared to human answers. All the analyses we make are\ngeneralizable to other types of biases and domains.", "journal": ""}
{"doi": "10.48550/arXiv.2312.15099", "date": "2023-12-22", "title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models", "authors": "Nishant Vishwamitra, Keyan Guo, Farhan Tajwar Romit, Isabelle Ondracek, Long Cheng, Ziming Zhao, Hongxin Hu", "abstract": "Online hate is an escalating problem that negatively impacts the lives of\nInternet users, and is also subject to rapid changes due to evolving events,\nresulting in new waves of online hate that pose a critical threat. Detecting\nand mitigating these new waves present two key challenges: it demands\nreasoning-based complex decision-making to determine the presence of hateful\ncontent, and the limited availability of training samples hinders updating the\ndetection model. To address this critical issue, we present a novel framework\ncalled HATEGUARD for effectively moderating new waves of online hate. HATEGUARD\nemploys a reasoning-based approach that leverages the recently introduced\nchain-of-thought (CoT) prompting technique, harnessing the capabilities of\nlarge language models (LLMs). HATEGUARD further achieves prompt-based zero-shot\ndetection by automatically generating and updating detection prompts with new\nderogatory terms and targets in new wave samples to effectively address new\nwaves of online hate. To demonstrate the effectiveness of our approach, we\ncompile a new dataset consisting of tweets related to three recently witnessed\nnew waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the\nUS Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal\npatterns in these new waves concerning the evolution of events and the pressing\nneed for techniques to rapidly update existing moderation tools to counteract\nthem. Comparative evaluations against state-of-the-art tools illustrate the\nsuperiority of our framework, showcasing a substantial 22.22% to 83.33%\nimprovement in detecting the three new waves of online hate. Our work\nhighlights the severe threat posed by the emergence of new waves of online hate\nand represents a paradigm shift in addressing this threat practically.", "journal": ""}
{"doi": "10.48550/arXiv.2502.13595", "date": "2025-02-19", "title": "MMTEB: Massive Multilingual Text Embedding Benchmark", "authors": "Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M\u00e1rton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemi\u0144ski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystr\u00f8m, Roman Solomatin, \u00d6mer \u00c7a\u011fatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafa\u0142 Po\u015bwiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bj\u00f6rn Pl\u00fcster, Jan Philipp Harries, Lo\u00efc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek \u0160uppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael G\u00fcnther, Mengzhou Xia, Weijia Shi, Xing Han L\u00f9, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff", "abstract": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.", "journal": ""}
{"doi": "10.48550/arXiv.2501.12948", "date": "2025-01-22", "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "authors": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang", "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama.", "journal": ""}
