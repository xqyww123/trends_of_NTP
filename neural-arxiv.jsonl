{"doi": "10.48550/arXiv.2502.15507", "date": "2025-02-21", "title": "Activation Steering in Neural Theorem Provers", "authors": "Shashank Kirtania", "abstract": "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.", "journal": ""}
{"doi": "10.48550/arXiv.2404.12534", "date": "2024-04-18", "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean", "authors": "Peiyang Song, Kaiyu Yang, Anima Anandkumar", "abstract": "Neural theorem proving combines large language models (LLMs) with proof\nassistants such as Lean, where the correctness of formal proofs can be\nrigorously verified, leaving no room for hallucination. With existing neural\ntheorem provers pretrained on a fixed collection of data and offering valuable\nsuggestions at times, it is challenging for them to continually prove novel\ntheorems in a fully autonomous mode, where human insights may be critical. In\nthis paper, we explore LLMs as copilots that assist humans in proving theorems.\nWe introduce Lean Copilot, an general framework for running LLM inference\nnatively in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nLean users can use our pretrained models or bring their own ones that run\neither locally (with or without GPUs) or on the cloud. Using Lean Copilot, we\nbuild LLM-based tools that suggest proof steps, complete proof goals, and\nselect relevant premises. Experimental results on the Mathematics in Lean\ntextbook demonstrate the effectiveness of our method compared to existing\nrule-based proof automation in Lean (aesop). When assisting humans, Lean\nCopilot requires only 2.08 manually-entered proof steps on average (3.86\nrequired by aesop); when automating the theorem proving process, Lean Copilot\nautomates 74.2% proof steps on average, 85% better than aesop (40.1%). We open\nsource all code and artifacts under a permissive MIT license to facilitate\nfurther research.", "journal": ""}
{"doi": "10.48550/arXiv.2312.14188", "date": "2023-12-20", "title": "Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method", "authors": "Rahul Vishwakarma, Subhankar Mishra", "abstract": "Theorem proving is a fundamental task in mathematics. With the advent of\nlarge language models (LLMs) and interactive theorem provers (ITPs) like Lean,\nthere has been growing interest in integrating LLMs and ITPs to automate\ntheorem proving. In this approach, the LLM generates proof steps (tactics), and\nthe ITP checks the applicability of the tactics at the current goal. The two\nsystems work together to complete the proof. In this paper, we introduce\nDS-Prover, a novel dynamic sampling method for theorem proving. This method\ndynamically determines the number of tactics to apply to expand the current\ngoal, taking into account the remaining time compared to the total allocated\ntime for proving a theorem. This makes the proof search process more efficient\nby adjusting the balance between exploration and exploitation as time passes.\nWe also augment the training dataset by decomposing simplification and rewrite\ntactics with multiple premises into tactics with single premises. This gives\nthe model more examples to learn from and helps it to predict the tactics with\npremises more accurately. We perform our experiments using the Mathlib dataset\nof the Lean theorem prover and report the performance on two standard datasets,\nMiniF2F and ProofNet. Our methods achieve significant performance gains on both\ndatasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the\nProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the\nbest-reported Pass@1 of 29.6% using Lean.", "journal": ""}
{"doi": "10.48550/arXiv.2502.00212", "date": "2025-01-31", "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving", "authors": "Kefan Dong, Tengyu Ma", "abstract": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens\ngenerated during the training in Lean, STP proves 26.3% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (61.7%, pass@3200),\nProofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@3200).", "journal": ""}
{"doi": "10.48550/arXiv.2406.11915", "date": "2024-06-16", "title": "miniCodeProps: a Minimal Benchmark for Proving Code Properties", "authors": "Evan Lohn, Sean Welleck", "abstract": "AI agents have shown initial promise in automating mathematical theorem\nproving in proof assistants such as Lean. The same proof assistants can be used\nto verify the correctness of code by pairing code with specifications and\nproofs that the specifications hold. Automating the writing of code,\nspecifications, and proofs could lower the cost of verification, or,\nambitiously, enable an AI agent to output safe, provably correct code. However,\nit remains unclear whether current neural theorem provers can automatically\nverify even relatively simple programs. We present miniCodeProps, a benchmark\nof 201 program specifications in the Lean proof assistant, aimed at the\nsubproblem of automatically generating a proof for a provided program and\nspecification. miniCodeProps contains specifications about simple,\nself-contained programs (e.g., lists, natural numbers, binary trees) with\nvaried proof difficulty. Despite its simplicity, miniCodeProps is sufficient to\nbreak current LLM-based provers, with state-of-the-art methods showing promise\non the easy properties in miniCodeProps, yet failing to prove nearly all of the\nmedium and hard properties. We publicly release miniCodeProps as a benchmark\nfor furthering automated theorem proving in the context of formally verified\ncode.", "journal": ""}
{"doi": "10.48550/arXiv.2501.18310", "date": "2025-01-30", "title": "Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis", "authors": "Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao", "abstract": "The synergy between deep learning models and traditional automation tools\nplays a pivotal role in developing robust neural theorem provers (NTPs).\nHowever, for proof synthesis with LLMs, previous work applies automation tools\neither only when the model explicitly calls the method, or only at a single\ngranularity level, failing to fully exploit the power of built-in tactics and\noff-the-shelf automated theorem provers. In this work, we propose ProofAug, a\nnovel theorem proving method that enjoys superior sample efficiency through\nequipping proof-generation LLMs with automation methods in different\ngranularities via fine-grained structure analysis of model-generated proof\nproposals. Furthermore, ProofAug serves as a versatile plug-and-play module\nthat seamlessly integrates with any tree-search algorithm, enabling our\nconstruction of an efficient recursive proving (ERP) module to further enhance\nperformance. The superiority of our method is validated on the miniF2F-test\nbenchmark using the open-source deepseek-math-7b-base model and the Isabelle\nproof assistant. Notably, by additionally employing a mixed prompting strategy,\nwe achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%\nfor the original version), setting a new SOTA across all proof languages with a\ntotal sample budget of only 2100. Our code is available at\nhttps://github.com/haoxiongliu/ProofAug.", "journal": ""}
{"doi": "10.48550/arXiv.2502.13137", "date": "2025-02-18", "title": "Theorem Prover as a Judge for Synthetic Data Generation", "authors": "Joshua Ong Jun Leang, Giwon Hong, Wenda Li, Shay B. Cohen", "abstract": "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.", "journal": ""}
{"doi": "10.48550/arXiv.2502.07640", "date": "2025-02-11", "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving", "authors": "Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin", "abstract": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.", "journal": ""}
{"doi": "10.48550/arXiv.2205.12615", "date": "2022-05-25", "title": "Autoformalization with Large Language Models", "authors": "Yuhuai Wu, Albert Q. Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, Christian Szegedy", "abstract": "Autoformalization is the process of automatically translating from natural\nlanguage mathematics to formal specifications and proofs. A successful\nautoformalization system could advance the fields of formal verification,\nprogram synthesis, and artificial intelligence. While the long-term goal of\nautoformalization seemed elusive for a long time, we show large language models\nprovide new prospects towards this goal. We make the surprising observation\nthat LLMs can correctly translate a significant portion ($25.3\\%$) of\nmathematical competition problems perfectly to formal specifications in\nIsabelle/HOL. We demonstrate the usefulness of this process by improving a\npreviously introduced neural theorem prover via training on these\nautoformalized theorems. Our methodology results in a new state-of-the-art\nresult on the MiniF2F theorem proving benchmark, improving the proof rate from\n$29.6\\%$ to $35.2\\%$.", "journal": ""}
{"doi": "10.48550/arXiv.2409.14274", "date": "2024-09-22", "title": "Proof Automation with Large Language Models", "authors": "Minghai Lu, Benjamin Delaware, Tianyi Zhang", "abstract": "Interactive theorem provers such as Coq are powerful tools to formally\nguarantee the correctness of software. However, using these tools requires\nsignificant manual effort and expertise. While Large Language Models (LLMs)\nhave shown promise in automatically generating informal proofs in natural\nlanguage, they are less effective at generating formal proofs in interactive\ntheorem provers. In this paper, we conduct a formative study to identify common\nmistakes made by LLMs when asked to generate formal proofs. By analyzing 520\nproof generation errors made by GPT-3.5, we found that GPT-3.5 often identified\nthe correct high-level structure of a proof, but struggled to get the\nlower-level details correct. Based on this insight, we propose PALM, a novel\ngenerate-then-repair approach that first prompts an LLM to generate an initial\nproof and then leverages targeted symbolic methods to iteratively repair\nlow-level problems. We evaluate PALM on a large dataset that includes more than\n10K theorems. Our results show that PALM significantly outperforms other\nstate-of-the-art approaches, successfully proving 76.6% to 180.4% more\ntheorems. Moreover, PALM proves 1270 theorems beyond the reach of existing\napproaches. We also demonstrate the generalizability of PALM across different\nLLMs.", "journal": "In Proceedings of the 39th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2024)"}
{"doi": "10.48550/arXiv.2306.15626", "date": "2023-06-27", "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models", "authors": "Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar", "abstract": "Large language models (LLMs) have shown promise in proving formal theorems\nusing proof assistants such as Lean. However, existing methods are difficult to\nreproduce or build on, due to private code, data, and large compute\nrequirements. This has created substantial barriers to research on machine\nlearning methods for theorem proving. This paper removes these barriers by\nintroducing LeanDojo: an open-source Lean playground consisting of toolkits,\ndata, models, and benchmarks. LeanDojo extracts data from Lean and enables\ninteraction with the proof environment programmatically. It contains\nfine-grained annotations of premises in proofs, providing valuable data for\npremise selection: a key bottleneck in theorem proving. Using this data, we\ndevelop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented\nwith retrieval for selecting premises from a vast math library. It is\ninexpensive and needs only one GPU week of training. Our retriever leverages\nLeanDojo's program analysis capability to identify accessible premises and hard\nnegative examples, which makes retrieval much more effective. Furthermore, we\nconstruct a new benchmark consisting of 98,734 theorems and proofs extracted\nfrom Lean's math library. It features challenging data split requiring the\nprover to generalize to theorems relying on novel premises that are never used\nin training. We use this benchmark for training and evaluation, and\nexperimental results demonstrate the effectiveness of ReProver over\nnon-retrieval baselines and GPT-4. We thus provide the first set of open-source\nLLM-based theorem provers without any proprietary datasets and release it under\na permissive MIT license to facilitate further research.", "journal": ""}
{"doi": "10.48550/arXiv.2406.14408", "date": "2024-06-20", "title": "FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving", "authors": "Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang", "abstract": "Formal verification (FV) has witnessed growing significance with current\nemerging program synthesis by the evolving large language models (LLMs).\nHowever, current formal verification mainly resorts to symbolic verifiers or\nhand-craft rules, resulting in limitations for extensive and flexible\nverification. On the other hand, formal languages for automated theorem\nproving, such as Isabelle, as another line of rigorous verification, are\nmaintained with comprehensive rules and theorems. In this paper, we propose\nFVEL, an interactive Formal Verification Environment with LLMs. Specifically,\nFVEL transforms a given code to be verified into Isabelle, and then conducts\nverification via neural automated theorem proving with an LLM. The joined\nparadigm leverages the rigorous yet abundant formulated and organized rules in\nIsabelle and is also convenient for introducing and adjusting cutting-edge\nLLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER\ndataset includes code dependencies and verification processes that are\nformulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646\nproof steps in total with in-depth dependencies. We benchmark FVELER in the\nFVEL environment by first fine-tuning LLMs with FVELER and then evaluating them\non Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned\nLlama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 ->\n84) more problems in SV-COMP. And the proportion of proof errors is reduced.\nProject page: https://fveler.github.io/.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17925", "date": "2025-02-25", "title": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction", "authors": "Suozhi Huang, Peiyang Song, Robert Joseph George, Anima Anandkumar", "abstract": "Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies.", "journal": ""}
{"doi": "10.48550/arXiv.2403.12627", "date": "2024-03-19", "title": "Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code", "authors": "Andreas Florath", "abstract": "In the realm of formal theorem proving, the Coq proof assistant stands out\nfor its rigorous approach to verifying mathematical assertions and software\ncorrectness. Despite the advances in artificial intelligence and machine\nlearning, the specialized nature of Coq syntax and semantics poses unique\nchallenges for Large Language Models (LLMs). Addressing this gap, we present a\ncomprehensive dataset specifically designed to enhance LLMs' proficiency in\ninterpreting and generating Coq code. This dataset, derived from a collection\nof over 10,000 Coq source files, encompasses a wide array of propositions,\nproofs, and definitions, enriched with metadata including source references and\nlicensing information. Our primary aim is to facilitate the development of LLMs\ncapable of generating syntactically correct and semantically meaningful Coq\nconstructs, thereby advancing the frontier of automated theorem proving.\nInitial experiments with this dataset have showcased its significant potential;\nmodels trained on this data exhibited enhanced accuracy in Coq code generation.\nNotably, a particular experiment revealed that a fine-tuned LLM was capable of\ngenerating 141 valid proofs for a basic lemma, highlighting the dataset's\nutility in facilitating the discovery of diverse and valid proof strategies.\nThis paper discusses the dataset's composition, the methodology behind its\ncreation, and the implications of our findings for the future of machine\nlearning in formal verification. The dataset is accessible for further research\nand exploration:\nhttps://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1", "journal": ""}
{"doi": "10.48550/arXiv.2503.03205", "date": "2025-03-05", "title": "MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving", "authors": "Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang", "abstract": "Solving mathematical problems using computer-verifiable languages like Lean\nhas significantly impacted mathematical and computer science communities.\nState-of-the-art methods utilize single Large Language Models (LLMs) as agents\nor provers to either generate complete proof or perform tree searches. However,\nsingle-agent methods inherently lack a structured way to combine high-level\nreasoning in Natural Language (NL) with Formal Language (FL) verification\nfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long\nChain-of-Thought framework, (to the best of our knowledge), the first\nmulti-agent framework for Lean4 theorem proving that balance high-level NL\nreasoning and FL verification in Long CoT. Using this structured interaction,\nour approach enables deeper insights and long-term coherence in proof\ngeneration, with which past methods struggle. We do this by leveraging emergent\nformal reasoning ability in Long CoT using our novel LoT-Transfer Learning\ntraining-inference pipeline. Extensive experiments show that our framework\nachieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,\nlargely outperforming GPT-4 (22.95%), single-agent tree search\n(InternLM-Step-Prover, 50.70%), and whole-proof generation\n(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight\nthe potential of combining Long CoT with formal verification for a more\ninsightful generation in a broader perspective.", "journal": ""}
{"doi": "10.48550/arXiv.2405.17216", "date": "2024-05-27", "title": "Autoformalizing Euclidean Geometry", "authors": "Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, Xujie Si", "abstract": "Autoformalization involves automatically translating informal math into\nformal theorems and proofs that are machine-verifiable. Euclidean geometry\nprovides an interesting and controllable domain for studying autoformalization.\nIn this paper, we introduce a neuro-symbolic framework for autoformalizing\nEuclidean geometry, which combines domain knowledge, SMT solvers, and large\nlanguage models (LLMs). One challenge in Euclidean geometry is that informal\nproofs rely on diagrams, leaving gaps in texts that are hard to formalize. To\naddress this issue, we use theorem provers to fill in such diagrammatic\ninformation automatically, so that the LLM only needs to autoformalize the\nexplicit textual steps, making it easier for the model. We also provide\nautomatic semantic evaluation for autoformalized theorem statements. We\nconstruct LeanEuclid, an autoformalization benchmark consisting of problems\nfrom Euclid's Elements and the UniGeo dataset formalized in the Lean proof\nassistant. Experiments with GPT-4 and GPT-4V show the capability and\nlimitations of state-of-the-art LLMs on autoformalizing geometry problems. The\ndata and code are available at https://github.com/loganrjmurphy/LeanEuclid.", "journal": ""}
{"doi": "10.48550/arXiv.2407.03203", "date": "2024-07-03", "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts", "authors": "Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang", "abstract": "Proving mathematical theorems using computer-verifiable formal languages like\nLean significantly impacts mathematical reasoning. One approach to formal\ntheorem proving involves generating complete proofs using Large Language Models\n(LLMs) based on Natural Language (NL) proofs. However, due to the scarcity of\naligned NL and Formal Language (FL) theorem-proving data most modern LLMs\nexhibit suboptimal performance.This scarcity results in a paucity of\nmethodologies for training LLMs and techniques to fully utilize their\ncapabilities in composing formal proofs. To address these challenges, this\npaper proposes TheoremLlama, an end-to-end framework that trains a\ngeneral-purpose LLM to be a Lean4 expert. TheoremLlama includes NL-FL dataset\ngeneration and bootstrapping method to obtain aligned dataset, curriculum\nlearning and block training techniques to train the model, and iterative proof\nwriting method to write Lean4 proofs that work together synergistically. Using\nthe dataset generation method in TheoremLlama, we provide Open Bootstrapped\nTheorems (OBT), an NL-FL aligned and bootstrapped dataset. Our novel NL-FL\nbootstrapping method, where NL proofs are integrated into Lean4 code for\ntraining datasets, leverages the NL reasoning ability of LLMs for formal\nreasoning. The TheoremLlama framework achieves cumulative accuracies of 36.48%\nand 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the\nGPT-4 baseline of 22.95% and 25.41%. Our code, model checkpoints, and the\ngenerated dataset is published in GitHub", "journal": ""}
{"doi": "10.48550/arXiv.2402.08957", "date": "2024-02-14", "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data", "authors": "Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang", "abstract": "Recent large language models (LLMs) have witnessed significant advancement in\nvarious tasks, including mathematical reasoning and theorem proving. As these\ntwo tasks require strict and formal multi-step inference, they are appealing\ndomains for exploring the reasoning ability of LLMs but still face important\nchallenges. Previous studies such as Chain-of-Thought (CoT) have revealed the\neffectiveness of intermediate steps guidance. However, such step-wise\nannotation requires heavy labor, leading to insufficient training steps for\ncurrent benchmarks. To fill this gap, this work introduces MUSTARD, a data\ngeneration framework that masters uniform synthesis of theorem and proof data\nof high quality and diversity. MUSTARD synthesizes data in three stages: (1) It\nsamples a few mathematical concept seeds as the problem category. (2) Then, it\nprompts a generative language model with the sampled concepts to obtain both\nthe problems and their step-wise formal solutions. (3) Lastly, the framework\nutilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With\nthe proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE\nwith 5,866 valid data points. Each data point contains an informal statement,\nan informal proof, and a translated formal proof that passes the prover\nvalidation. We perform extensive analysis and demonstrate that MUSTARD\ngenerates validated high-quality step-by-step data. We further apply the\nMUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B\nachieves a 15.41% average relative performance gain in automated theorem\nproving, and 8.18% in math word problems. Codes and data are available at\nhttps://github.com/Eleanor-H/MUSTARD.", "journal": "ICLR 2024 spotlight"}
{"doi": "10.48550/arXiv.2411.03417", "date": "2024-11-05", "title": "Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment", "authors": "Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar B. Shah", "abstract": "Large language models (LLMs) represent a promising, but controversial, tool\nin aiding scientific peer review. This study evaluates the usefulness of LLMs\nin a conference setting as a tool for vetting paper submissions against\nsubmission standards. We conduct an experiment at the 2024 Neural Information\nProcessing Systems (NeurIPS) conference, where 234 papers were voluntarily\nsubmitted to an \"LLM-based Checklist Assistant.\" This assistant validates\nwhether papers adhere to the author checklist used by NeurIPS, which includes\nquestions to ensure compliance with research and manuscript preparation\nstandards. Evaluation of the assistant by NeurIPS paper authors suggests that\nthe LLM-based assistant was generally helpful in verifying checklist\ncompletion. In post-usage surveys, over 70% of authors found the assistant\nuseful, and 70% indicate that they would revise their papers or checklist\nresponses based on its feedback. While causal attribution to the assistant is\nnot definitive, qualitative evidence suggests that the LLM contributed to\nimproving some submissions. Survey responses and analysis of re-submissions\nindicate that authors made substantive revisions to their submissions in\nresponse to specific feedback from the LLM. The experiment also highlights\ncommon issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)\nwere the most frequent issues flagged by authors. We also conduct experiments\nto understand potential gaming of the system, which reveal that the assistant\ncould be manipulated to enhance scores through fabricated justifications,\nhighlighting potential vulnerabilities of automated review tools.", "journal": ""}
{"doi": "10.48550/arXiv.2403.18120", "date": "2024-03-26", "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization", "authors": "Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu", "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.", "journal": ""}
{"doi": "10.48550/arXiv.2405.14333", "date": "2024-05-23", "title": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data", "authors": "Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang", "abstract": "Proof assistants like Lean have revolutionized mathematical proof\nverification, ensuring high accuracy and reliability. Although large language\nmodels (LLMs) show promise in mathematical reasoning, their advancement in\nformal theorem proving is hindered by a lack of training data. To address this\nissue, we introduce an approach to generate extensive Lean 4 proof data derived\nfrom high-school and undergraduate-level mathematical competition problems.\nThis approach involves translating natural language problems into formal\nstatements, filtering out low-quality statements, and generating proofs to\ncreate synthetic data. After fine-tuning the DeepSeekMath 7B model on this\nsynthetic dataset, which comprises 8 million formal statements with proofs, our\nmodel achieved whole-proof generation accuracies of 46.3% with 64 samples and\n52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at\n23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.\nAdditionally, our model successfully proved 5 out of 148 problems in the Lean 4\nFormalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4\nfailed to prove any. These results demonstrate the potential of leveraging\nlarge-scale synthetic data to enhance theorem-proving capabilities in LLMs.\nBoth the synthetic dataset and the model will be made available to facilitate\nfurther research in this promising field.", "journal": ""}
{"doi": "10.48550/arXiv.2501.02531", "date": "2025-01-05", "title": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI", "authors": "Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa", "abstract": "With the expansion of neural networks, such as large language models,\nhumanity is exponentially heading towards superintelligence. As various AI\nsystems are increasingly integrated into the fabric of societies-through\nrecommending values, devising creative solutions, and making decisions-it\nbecomes critical to assess how these AI systems impact humans in the long run.\nThis research aims to contribute towards establishing a benchmark for\nevaluating the sentiment of various Large Language Models in socially importan\nissues. The methodology adopted was a Likert scale survey. Seven LLMs,\nincluding GPT-4 and Bard, were analyzed and compared against sentiment data\nfrom three independent human sample populations. Temporal variations in\nsentiment were also evaluated over three consecutive days. The results\nhighlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to\n4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI,\nwhereas Bard was leaning towards the neutral sentiment. The human samples,\ncontrastingly, showed a lower average sentiment of 2.97. The temporal\ncomparison revealed differences in sentiment evolution between LLMs in three\ndays, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect\nof potential conflicts of interest and bias possibilities in LLMs' sentiment\nformation. Results indicate that LLMs, akin to human cognitive processes, could\npotentially develop unique sentiments and subtly influence societies'\nperceptions towards various opinions formed within the LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.15700", "date": "2024-10-21", "title": "InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert Iteration on Large-Scale LEAN Problems", "authors": "Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, Kai Chen", "abstract": "Large Language Models (LLMs) have emerged as powerful tools in mathematical\ntheorem proving, particularly when utilizing formal languages such as LEAN. The\nmajor learning paradigm is expert iteration, which necessitates a pre-defined\ndataset comprising numerous mathematical problems. In this process, LLMs\nattempt to prove problems within the dataset and iteratively refine their\ncapabilities through self-training on the proofs they discover. We propose to\nuse large scale LEAN problem datasets Lean-workbook for expert iteration with\nmore than 20,000 CPU days. During expert iteration, we found log-linear trends\nbetween solved problem amount with proof length and CPU usage. We train a\ncritic model to select relatively easy problems for policy models to make\ntrials and guide the model to search for deeper proofs. InternLM2.5-StepProver\nachieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet,\nand Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the\nMiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus\nwhich shows a significant improvement compared to only 9.5% of problems proved\nwhen Lean-Workbook-Plus was released. We open-source our models and searched\nproofs at https://github.com/InternLM/InternLM-Math and\nhttps://huggingface.co/datasets/internlm/Lean-Workbook.", "journal": ""}
{"doi": "10.48550/arXiv.2408.11172", "date": "2024-08-20", "title": "SubgoalXL: Subgoal-based Expert Learning for Theorem Proving", "authors": "Xueliang Zhao, Lin Zheng, Haige Bo, Changran Hu, Urmish Thakker, Lingpeng Kong", "abstract": "Formal theorem proving, a field at the intersection of mathematics and\ncomputer science, has seen renewed interest with advancements in large language\nmodels (LLMs). This paper introduces SubgoalXL, a novel approach that\nsynergizes subgoal-based proofs with expert learning to enhance LLMs'\ncapabilities in formal theorem proving within the Isabelle environment.\nSubgoalXL addresses two critical challenges: the scarcity of specialized\nmathematics and theorem-proving data, and the need for improved multi-step\nreasoning abilities in LLMs. By optimizing data efficiency and employing\nsubgoal-level supervision, SubgoalXL extracts richer information from limited\nhuman-generated proofs. The framework integrates subgoal-oriented proof\nstrategies with an expert learning system, iteratively refining formal\nstatement, proof, and subgoal generators. Leveraging the Isabelle environment's\nadvantages in subgoal-based proofs, SubgoalXL achieves a new state-of-the-art\nperformance of 56.1\\% in Isabelle on the standard miniF2F dataset, marking an\nabsolute improvement of 4.9\\%. Notably, SubgoalXL successfully solves 41 AMC12,\n9 AIME, and 3 IMO problems from miniF2F. These results underscore the\neffectiveness of maximizing limited data utility and employing targeted\nguidance for complex reasoning in formal theorem proving, contributing to the\nongoing advancement of AI reasoning capabilities. The implementation is\navailable at \\url{https://github.com/zhaoxlpku/SubgoalXL}.", "journal": ""}
{"doi": "10.48550/arXiv.2403.13312", "date": "2024-03-20", "title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean", "authors": "Dongwei Jiang, Marcio Fonseca, Shay B. Cohen", "abstract": "Large language models (LLMs) often struggle with complex logical reasoning\ndue to logical inconsistencies and the inherent difficulty of such reasoning.\nWe use Lean, a theorem proving framework, to address these challenges. By\nformalizing logical reasoning problems into theorems within Lean, we can solve\nthem by proving or disproving the corresponding theorems. This method reduces\nthe risk of logical inconsistencies with the help of Lean's symbolic solver. It\nalso enhances our ability to treat complex reasoning tasks by using Lean's\nextensive library of theorem proofs. Our method achieves state-of-the-art\nperformance on the FOLIO dataset and achieves performance near this level on\nProofWriter. Notably, these results were accomplished by fine-tuning on fewer\nthan 100 in-domain samples for each dataset.", "journal": ""}
{"doi": "10.48550/arXiv.2502.17840", "date": "2025-02-25", "title": "A Combinatorial Identities Benchmark for Theorem Proving via Automated Theorem Generation", "authors": "Beibei Xiong, Hangyu Lv, Haojia Shan, Jianlin Wang, Zhengfeng Yang, Lihong Zhi", "abstract": "Large language models (LLMs) have significantly advanced formal theorem\nproving, yet the scarcity of high-quality training data constrains their\ncapabilities in complex mathematical domains. Combinatorics, a cornerstone of\nmathematics, provides essential tools for analyzing discrete structures and\nsolving optimization problems. However, its inherent complexity makes it\nparticularly challenging for automated theorem proving (ATP) for combinatorial\nidentities. To address this, we manually construct LeanComb, combinatorial\nidentities benchmark in Lean, which is, to our knowledge, the first formalized\ntheorem proving benchmark built for combinatorial identities. We develop an\nAutomated Theorem Generator for Combinatorial Identities, ATG4CI, which\ncombines candidate tactics suggested by a self-improving large language model\nwith a Reinforcement Learning Tree Search approach for tactic prediction. By\nutilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K\ncombinatorial identities theorems, each with a complete formal proof in Lean,\nand experimental evaluations demonstrate that models trained on this dataset\ncan generate more effective tactics, thereby improving success rates in\nautomated theorem proving for combinatorial identities.", "journal": ""}
{"doi": "10.48550/arXiv.2412.14063", "date": "2024-12-18", "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification", "authors": "Kyle Thompson, Nuno Saavedra, Pedro Carrott, Kevin Fisher, Alex Sanchez-Stern, Yuriy Brun, Jo\u00e3o F. Ferreira, Sorin Lerner, Emily First", "abstract": "Formal verification using proof assistants, such as Coq, enables the creation\nof high-quality software. However, the verification process requires\nsignificant expertise and manual effort to write proofs. Recent work has\nexplored automating proof synthesis using machine learning and large language\nmodels (LLMs). This work has shown that identifying relevant premises, such as\nlemmas and definitions, can aid synthesis. We present Rango, a fully automated\nproof synthesis tool for Coq that automatically identifies relevant premises\nand also similar proofs from the current project and uses them during\nsynthesis. Rango uses retrieval augmentation at every step of the proof to\nautomatically determine which proofs and premises to include in the context of\nits fine-tuned LLM. In this way, Rango adapts to the project and to the\nevolving state of the proof. We create a new dataset, CoqStoq, of 2,226\nopen-source Coq projects and 196,929 theorems from GitHub, which includes both\ntraining data and a curated evaluation benchmark of well-maintained projects.\nOn this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is\n29% more theorems than the prior state-of-the-art tool Tactician. Our\nevaluation also shows that Rango adding relevant proofs to its context leads to\na 47% increase in the number of theorems proven.", "journal": ""}
{"doi": "10.48550/arXiv.2412.15177", "date": "2024-12-19", "title": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying", "authors": "Federico Castagna, Isabel Sassoon, Simon Parsons", "abstract": "Studies have underscored how, regardless of the recent breakthrough and swift\nadvances in AI research, even state-of-the-art Large Language models (LLMs)\ncontinue to struggle when performing logical and mathematical reasoning. The\nresults seem to suggest that LLMs still work as (highly advanced) data pattern\nidentifiers, scoring poorly when attempting to generalise and solve reasoning\nproblems the models have never previously seen or that are not close to samples\npresented in their training data. To address this compelling concern, this\npaper makes use of the notion of critical questions from the literature on\nargumentation theory, focusing in particular on Toulmin's model of\nargumentation. We show that employing these critical questions can improve the\nreasoning capabilities of LLMs. By probing the rationale behind the models'\nreasoning process, the LLM can assess whether some logical mistake is occurring\nand correct it before providing the final reply to the user prompt. The\nunderlying idea is drawn from the gold standard of any valid argumentative\nprocedure: the conclusion is valid if it is entailed by accepted premises. Or,\nto paraphrase such Aristotelian principle in a real-world approximation,\ncharacterised by incomplete information and presumptive logic, the conclusion\nis valid if not proved otherwise. This approach successfully steers the models'\noutput through a reasoning pipeline, resulting in better performance against\nthe baseline and its Chain-of-Thought (CoT) implementation. To this end, an\nextensive evaluation of the proposed approach on the MT-Bench Reasoning and\nMath tasks across a range of LLMs is provided.", "journal": ""}
{"doi": "10.48550/arXiv.2410.13224", "date": "2024-10-17", "title": "Proof Flow: Preliminary Study on Generative Flow Network Language Model Tuning for Formal Reasoning", "authors": "Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang", "abstract": "Reasoning is a fundamental substrate for solving novel and complex problems.\nDeliberate efforts in learning and developing frameworks around System 2\nreasoning have made great strides, yet problems of sufficient complexity remain\nlargely out of reach for open models. To address this gap, we examine the\npotential of Generative Flow Networks as a fine-tuning method for LLMs to\nunlock advanced reasoning capabilities. In this paper, we present a proof of\nconcept in the domain of formal reasoning, specifically in the Neural Theorem\nProving (NTP) setting, where proofs specified in a formal language such as Lean\ncan be deterministically and objectively verified. Unlike classical\nreward-maximization reinforcement learning, which frequently over-exploits\nhigh-reward actions and fails to effectively explore the state space, GFlowNets\nhave emerged as a promising approach for sampling compositional objects,\nimproving generalization, and enabling models to maintain diverse hypotheses.\nOur early results demonstrate GFlowNet fine-tuning's potential for enhancing\nmodel performance in a search setting, which is especially relevant given the\nparadigm shift towards inference time compute scaling and \"thinking slowly.\"", "journal": ""}
{"doi": "10.48550/arXiv.2410.06209", "date": "2024-10-08", "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving", "authors": "Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar", "abstract": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for formal theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully generates formal proofs for 155 theorems across 23 diverse Lean\nrepositories where formal proofs were previously missing, many from advanced\nmathematics. It performs significantly better than the static LLM baseline,\nproving challenging theorems in domains like abstract algebra and algebraic\ntopology while showcasing a clear progression of learning from basic concepts\nto advanced topics. In addition, we analyze LeanAgent's superior performance on\nkey lifelong learning metrics. LeanAgent achieves exceptional scores in\nstability and backward transfer, where learning new tasks improves performance\non previously learned tasks. This emphasizes LeanAgent's continuous\ngeneralizability and improvement, explaining its superior theorem-proving\nperformance.", "journal": ""}
{"doi": "10.48550/arXiv.2406.03847", "date": "2024-06-06", "title": "Lean Workbook: A large-scale Lean problem set formalized from natural language math problems", "authors": "Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen", "abstract": "Large language models have demonstrated impressive capabilities across\nvarious natural language processing tasks, especially in solving mathematical\nproblems. However, large language models are not good at math theorem proving\nusing formal languages like Lean. A significant challenge in this area is the\nscarcity of training data available in these formal languages. To address this\nissue, we propose a novel pipeline that iteratively generates and filters\nsynthetic data to translate natural language mathematical problems into Lean 4\nstatements, and vice versa. Our results indicate that the synthetic data\npipeline can provide useful training data and improve the performance of LLMs\nin translating and understanding complex mathematical problems and proofs. Our\nfinal dataset contains about 57K formal-informal question pairs along with\nsearched proof from the math contest forum and 21 new IMO questions. We\nopen-source our code at https://github.com/InternLM/InternLM-Math and our data\nat https://huggingface.co/datasets/InternLM/Lean-Workbook.", "journal": ""}
{"doi": "10.48550/arXiv.2410.19940", "date": "2024-10-25", "title": "Cobblestone: Iterative Automation for Formal Verification", "authors": "Saketh Ram Kasibatla, Arpan Agarwal, Yuriy Brun, Sorin Lerner, Talia Ringer, Emily First", "abstract": "Formal verification using proof assistants, such as Coq, is an effective way\nof improving software quality, but it is expensive. Writing proofs manually\nrequires both significant effort and expertise. Recent research has used\nmachine learning to automatically synthesize proofs, reducing verification\neffort, but these tools are able to prove only a fraction of the desired\nsoftware properties. We introduce Cobblestone, a new proof-synthesis approach\nthat improves on the state of the art by taking advantage of partial progress\nin proof synthesis attempts. Unlike prior tools, Cobblestone can produce\nmultiple unsuccessful proofs using a large language model (LLM), identify the\nworking portions of those proofs, and combine them into a single, successful\nproof, taking advantage of internal partial progress. We evaluate Cobblestone\non two benchmarks of open-source Coq projects, controlling for training data\nleakage in LLM datasets. Fully automatically, Cobblestone can prove 48% of the\ntheorems, while Proverbot9001, the previous state-of-the-art, learning-based,\nproof-synthesis tool, can prove 17%. Cobblestone establishes a new state of the\nart for fully automated proof synthesis tools for Coq. We also evaluate\nCobblestone in a setting where it is given external partial proof progress from\noracles, serving as proxies for a human proof engineer or another tool. When\nthe theorem is broken down into a set of subgoals and Cobblestone is given a\nset of relevant lemmas already proven in the project, it can prove up to 58% of\nthe theorems. We qualitatively study the theorems Cobblestone is and is not\nable to prove to outline potential future research directions to further\nimprove proof synthesis, including developing interactive, semi-automated\ntools. Our research shows that tools can make better use of partial progress\nmade during proof synthesis to more effectively automate formal verification.", "journal": ""}
{"doi": "10.48550/arXiv.2405.04282", "date": "2024-05-07", "title": "CoqPyt: Proof Navigation in Python in the Era of LLMs", "authors": "Pedro Carrott, Nuno Saavedra, Kyle Thompson, Sorin Lerner, Jo\u00e3o F. Ferreira, Emily First", "abstract": "Proof assistants enable users to develop machine-checked proofs regarding\nsoftware-related properties. Unfortunately, the interactive nature of these\nproof assistants imposes most of the proof burden on the user, making formal\nverification a complex, and time-consuming endeavor. Recent automation\ntechniques based on neural methods address this issue, but require good\nprogrammatic support for collecting data and interacting with proof assistants.\nThis paper presents CoqPyt, a Python tool for interacting with the Coq proof\nassistant. CoqPyt improves on other Coq-related tools by providing novel\nfeatures, such as the extraction of rich premise data. We expect our work to\naid development of tools and techniques, especially LLM-based, designed for\nproof synthesis and repair. A video describing and demonstrating CoqPyt is\navailable at: https://youtu.be/fk74o0rePM8.", "journal": ""}
{"doi": "10.48550/arXiv.2501.15797", "date": "2025-01-27", "title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models", "authors": "Tianbo Yang, Mingqi Yan, Hongyi Zhao, Tianshuo Yang", "abstract": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.", "journal": ""}
{"doi": "10.48550/arXiv.2402.06332", "date": "2024-02-09", "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning", "authors": "Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin", "abstract": "The math abilities of large language models can represent their abstract\nreasoning ability. In this paper, we introduce and open-source our math\nreasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We\nunify chain-of-thought reasoning, reward modeling, formal reasoning, data\naugmentation, and code interpreter in a unified seq2seq format and supervise\nour model to be a versatile math reasoner, verifier, prover, and augmenter.\nThese abilities can be used to develop the next math LLMs or self-iteration.\nInternLM-Math obtains open-sourced state-of-the-art performance under the\nsetting of in-context learning, supervised fine-tuning, and code-assisted\nreasoning in various informal and formal benchmarks including GSM8K, MATH,\nHungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves\n30.3 on the MiniF2F test set without fine-tuning. We further explore how to use\nLEAN to solve math problems and study its performance under the setting of\nmulti-task learning which shows the possibility of using LEAN as a unified\nplatform for solving and proving in math. Our models, codes, and data are\nreleased at \\url{https://github.com/InternLM/InternLM-Math}.", "journal": ""}
{"doi": "10.48550/arXiv.2409.05977", "date": "2024-09-09", "title": "Mathematical Formalized Problem Solving and Theorem Proving in Different Fields in Lean 4", "authors": "Xichen Tang", "abstract": "Formalizing mathematical proofs using computerized verification languages\nlike Lean 4 has the potential to significantly impact the field of mathematics,\nit offers prominent capabilities for advancing mathematical reasoning. However,\nexisting efforts are largely limited to creating formalized versions of proofs\nfrom extensive online mathematical corpora, struggling to keep pace with the\nrapidly evolving nature of mathematics. To bridge the gap between traditional\nand computerized proof techniques, this paper explores the use of Large\nLanguage Models (LLMs) to generate formal proof steps and complete formalized\nproofs. By converting natural language (NL) mathematical proofs into formalized\nversions, this work introduces the basic structure and tactics of the Lean 4\nlanguage. The goal is to determine how AI can be leveraged to assist the\nmathematical formalization process and improve its performance. Several\nexamples are provided that demonstrate solving problems using both traditional\nand Lean 4-based approaches. Ultimately, this paper presents an explanation of\nthe foundations of Lean 4 and comparative analyses of the mathematical\nformalization process using traditional and AI-augmented techniques. The\nfindings indicate that AI- powered tools have significant potential to\naccelerate and enhance the formalization of mathematical proofs, paving the way\nfor more efficient and reliable theorem-proving for AI for Math in the future.", "journal": ""}
{"doi": "10.48550/arXiv.2406.07222", "date": "2024-06-11", "title": "Improving Autoformalization using Type Checking", "authors": "Auguste Poiroux, Gail Weiss, Viktor Kun\u010dak, Antoine Bosselut", "abstract": "Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.", "journal": ""}
{"doi": "10.48550/arXiv.2308.16797", "date": "2023-08-31", "title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation", "authors": "John Mendon\u00e7a, Patr\u00edcia Pereira, Helena Moniz, Jo\u00e3o Paulo Carvalho, Alon Lavie, Isabel Trancoso", "abstract": "Despite significant research effort in the development of automatic dialogue\nevaluation metrics, little thought is given to evaluating dialogues other than\nin English. At the same time, ensuring metrics are invariant to semantically\nsimilar responses is also an overlooked topic. In order to achieve the desired\nproperties of robustness and multilinguality for dialogue evaluation metrics,\nwe propose a novel framework that takes advantage of the strengths of current\nevaluation models with the newly-established paradigm of prompting Large\nLanguage Models (LLMs). Empirical results show our framework achieves state of\nthe art results in terms of mean Spearman correlation scores across several\nbenchmarks and ranks first place on both the Robust and Multilingual tasks of\nthe DSTC11 Track 4 \"Automatic Evaluation Metrics for Open-Domain Dialogue\nSystems\", proving the evaluation capabilities of prompted LLMs.", "journal": ""}
{"doi": "10.48550/arXiv.2410.04753", "date": "2024-10-07", "title": "ImProver: Agent-Based Automated Proof Optimization", "authors": "Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, Sean Welleck", "abstract": "Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.", "journal": ""}
{"doi": "10.48550/arXiv.2407.14521", "date": "2024-07-05", "title": "Towards Automated Functional Equation Proving: A Benchmark Dataset and A Domain-Specific In-Context Agent", "authors": "Mahdi Buali, Robert Hoehndorf", "abstract": "Automated Theorem Proving (ATP) faces challenges due to its complexity and\ncomputational demands. Recent work has explored using Large Language Models\n(LLMs) for ATP action selection, but these methods can be resource-intensive.\nThis study introduces FEAS, an agent that enhances the COPRA in-context\nlearning framework within Lean. FEAS refines prompt generation, response\nparsing, and incorporates domain-specific heuristics for functional equations.\nIt introduces FunEq, a curated dataset of functional equation problems with\nvarying difficulty. FEAS outperforms baselines on FunEq, particularly with the\nintegration of domain-specific heuristics. The results demonstrate FEAS's\neffectiveness in generating and formalizing high-level proof strategies into\nLean proofs, showcasing the potential of tailored approaches for specific ATP\nchallenges.", "journal": ""}
{"doi": "10.48550/arXiv.2304.10663", "date": "2023-04-20", "title": "Meta Semantics: Towards better natural language understanding and reasoning", "authors": "Xiaolin Hu", "abstract": "Natural language understanding is one of the most challenging topics in\nartificial intelligence. Deep neural network methods, particularly large\nlanguage module (LLM) methods such as ChatGPT and GPT-3, have powerful\nflexibility to adopt informal text but are weak on logical deduction and suffer\nfrom the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods\nsuch as Mathematica, Semantic web, and Lean, are excellent in reasoning but\ncannot handle the complex and changeable informal text. Inspired by pragmatics\nand structuralism, we propose two strategies to solve the OOV problem and a\nsemantic model for better natural language understanding and reasoning.", "journal": ""}
{"doi": "10.48550/arXiv.2402.03171", "date": "2024-02-05", "title": "Homograph Attacks on Maghreb Sentiment Analyzers", "authors": "Fatima Zahra Qachfar, Rakesh M. Verma", "abstract": "We examine the impact of homograph attacks on the Sentiment Analysis (SA)\ntask of different Arabic dialects from the Maghreb North-African countries.\nHomograph attacks result in a 65.3% decrease in transformer classification from\nan F1-score of 0.95 to 0.33 when data is written in \"Arabizi\". The goal of this\nstudy is to highlight LLMs weaknesses' and to prioritize ethical and\nresponsible Machine Learning.", "journal": ""}
{"doi": "10.48550/arXiv.2410.20274", "date": "2024-10-26", "title": "Library Learning Doesn't: The Curious Case of the Single-Use \"Library\"", "authors": "Ian Berlot-Attwell, Frank Rudzicz, Xujie Si", "abstract": "Advances in Large Language Models (LLMs) have spurred a wave of LLM library\nlearning systems for mathematical reasoning. These systems aim to learn a\nreusable library of tools, such as formal Isabelle lemmas or Python programs\nthat are tailored to a family of tasks. Many of these systems are inspired by\nthe human structuring of knowledge into reusable and extendable concepts, but\ndo current methods actually learn reusable libraries of tools?\n  We study two library learning systems for mathematics which both reported\nincreased accuracy: LEGO-Prover and TroVE. We find that function reuse is\nextremely infrequent on miniF2F and MATH. Our followup ablation experiments\nsuggest that, rather than reuse, self-correction and self-consistency are the\nprimary drivers of the observed performance gains. Our code and data are\navailable at https://github.com/ikb-a/curious-case", "journal": ""}
{"doi": "10.48550/arXiv.2404.07382", "date": "2024-04-10", "title": "Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving", "authors": "Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang", "abstract": "Recent advances in Automated Theorem Proving have shown the effectiveness of\nleveraging a (large) language model that generates tactics (i.e. proof steps)\nto search through proof states. The current model, while trained solely on\nsuccessful proof paths, faces a discrepancy at the inference stage, as it must\nsample and try various tactics at each proof state until finding success,\nunlike its training which does not incorporate learning from failed attempts.\nIntuitively, a tactic that leads to a failed search path would indicate that\nsimilar tactics should receive less attention during the following trials. In\nthis paper, we demonstrate the benefit of training models that additionally\nlearn from failed search paths. Facing the lack of such trial-and-error data in\nexisting open-source theorem-proving datasets, we curate a dataset on\nintuitionistic propositional logic theorems and formalize it in Lean, such that\nwe can reliably check the correctness of proofs. We compare our model trained\non relatively short trial-and-error information (TrialMaster) with models\ntrained only on the correct paths and discover that the former solves more\nunseen theorems with lower trial searches.", "journal": ""}
{"doi": "10.48550/arXiv.2411.18872", "date": "2024-11-28", "title": "A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems", "authors": "Roozbeh Yousefzadeh, Xuenan Cao, Azim Ospanov", "abstract": "Using AI to write formal proofs for mathematical problems is a challenging\ntask that has seen some advancements in recent years. Automated systems such as\nLean can verify the correctness of proofs written in formal language, yet\nwriting the proofs in formal language can be challenging for humans and\nmachines. The miniF2F benchmark has 20 IMO problems in its test set, yet formal\nproofs are available only for 6 of these problems (3 of which are only written\nby mathematicians). The model with best accuracy can only prove 2 of these 20\nIMO problems, from 1950s and 60s, while its training set is a secret. In this\nwork, we write complete, original formal proofs for the remaining IMO problems\nin Lean along with 3 extra problems from IMO 2022 and 2023. This effort expands\nthe availability of proof currently in the public domain by creating 5,880\nlines of Lean proof. The goal of the paper is to pave the way for developing AI\nmodels that can automatically write the formal proofs for all the IMO problems\nin miniF2F and beyond by providing an evaluation benchmark. In this pursuit, we\ndevise a method to decompose the proofs of these problems into their building\nblocks, constructing a dataset of 1,329 lemmas with more than 40k lines of Lean\ncode. These lemmas are not trivial, yet they are approachable, providing the\nopportunity to evaluate and diagnose the failures and successes of AI models.\nWe evaluate the ability of the SOTA LLMs on our dataset and analyze their\nsuccess and failure modes from different perspectives. Our dataset and code is\navailable at: https://github.com/roozbeh-yz/IMO-Steps.", "journal": ""}
{"doi": "10.48550/arXiv.2501.00145", "date": "2024-12-30", "title": "Tackling Cognitive Impairment Detection from Speech: A submission to the PROCESS Challenge", "authors": "Catarina Botelho, David Gimeno-G\u00f3mez, Francisco Teixeira, John Mendon\u00e7a, Patr\u00edcia Pereira, Diogo A. P. Nunes, Thomas Rolland, Anna Pompili, Rub\u00e9n Solera-Ure\u00f1a, Maria Ponte, David Martins de Matos, Carlos-D. Mart\u00ednez-Hinarejos, Isabel Trancoso, Alberto Abad", "abstract": "This work describes our group's submission to the PROCESS Challenge 2024,\nwith the goal of assessing cognitive decline through spontaneous speech, using\nthree guided clinical tasks. This joint effort followed a holistic approach,\nencompassing both knowledge-based acoustic and text-based feature sets, as well\nas LLM-based macrolinguistic descriptors, pause-based acoustic biomarkers, and\nmultiple neural representations (e.g., LongFormer, ECAPA-TDNN, and Trillson\nembeddings). Combining these feature sets with different classifiers resulted\nin a large pool of models, from which we selected those that provided the best\nbalance between train, development, and individual class performance. Our\nresults show that our best performing systems correspond to combinations of\nmodels that are complementary to each other, relying on acoustic and textual\ninformation from all three clinical tasks.", "journal": ""}
